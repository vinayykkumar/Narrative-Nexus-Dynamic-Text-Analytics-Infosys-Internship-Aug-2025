{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcb01a8",
   "metadata": {},
   "source": [
    "## NarrativeNexus Project: Text Cleaning Implementation\n",
    "\n",
    "**Objectives:**\n",
    "- Remove special characters, punctuation, and stop words\n",
    "- Apply preprocessing to BBC, CNN/DailyMail, and IMDB datasets\n",
    "- Save cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84ed395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Loaded 198 stop words.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"‚úÖ Setup complete. Loaded {len(stop_words)} stop words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c59571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Define text cleaning functions\n",
    "def clean_special_characters(text):\n",
    "    \"\"\"Remove special characters, keep only letters, numbers, and spaces\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Remove special characters\n",
    "    cleaned = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def remove_stop_words(text, keep_negations=True):\n",
    "    \"\"\"Remove stop words while preserving negations\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Keep important negation words\n",
    "    stop_words_filtered = stop_words.copy()\n",
    "    if keep_negations:\n",
    "        important_words = {'not', 'no', 'never', 'none', 'neither', 'nobody', 'nothing'}\n",
    "        stop_words_filtered = stop_words_filtered - important_words\n",
    "    \n",
    "    # Tokenize and filter\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words_filtered]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def clean_text_pipeline(text):\n",
    "    \"\"\"Complete text cleaning pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Convert to lowercase\n",
    "    cleaned = str(text).lower()\n",
    "    \n",
    "    # Step 2: Remove special characters\n",
    "    cleaned = clean_special_characters(cleaned)\n",
    "    \n",
    "    # Step 3: Remove stop words\n",
    "    cleaned = remove_stop_words(cleaned, keep_negations=True)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"‚úÖ Text cleaning functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3c64bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC Dataset: 2225 articles loaded\n",
      "‚úÖ CNN Dataset: 5000 articles loaded\n",
      "‚úÖ IMDB Dataset: 1000 reviews loaded\n",
      "\n",
      "üìä Total datasets loaded: 3\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_dir = \"../data\"\n",
    "datasets = {}\n",
    "\n",
    "# Load BBC News Dataset\n",
    "try:\n",
    "    bbc_df = pd.read_csv(f\"{data_dir}/bbc-text.csv\")\n",
    "    datasets['BBC'] = bbc_df\n",
    "    print(f\"‚úÖ BBC Dataset: {len(bbc_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading BBC dataset: {e}\")\n",
    "\n",
    "# Load CNN/DailyMail Dataset\n",
    "try:\n",
    "    cnn_df = pd.read_csv(f\"{data_dir}/cnn_dailymail.csv\")\n",
    "    datasets['CNN'] = cnn_df\n",
    "    print(f\"‚úÖ CNN Dataset: {len(cnn_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CNN dataset: {e}\")\n",
    "\n",
    "# Load IMDB Dataset (subset for demo)\n",
    "try:\n",
    "    imdb_df = pd.read_csv(f\"{data_dir}/imdb-dataset.csv\", nrows=1000)\n",
    "    datasets['IMDB'] = imdb_df\n",
    "    print(f\"‚úÖ IMDB Dataset: {len(imdb_df)} reviews loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading IMDB dataset: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921f4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning BBC News Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing BBC: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:02<00:00, 1091.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2263 characters\n",
      "   ‚Ä¢ Cleaned avg length: 1584 characters\n",
      "   ‚Ä¢ Reduction: 30.0%\n",
      "‚úÖ BBC cleaning completed\n"
     ]
    }
   ],
   "source": [
    "# Clean BBC News Dataset\n",
    "if 'BBC' in datasets:\n",
    "    print(\"üßπ Cleaning BBC News Dataset...\")\n",
    "    bbc_df = datasets['BBC'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing BBC\")\n",
    "    bbc_df['text_cleaned'] = bbc_df['text'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = bbc_df['text'].str.len().mean()\n",
    "    cleaned_avg = bbc_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['BBC_cleaned'] = bbc_df\n",
    "    print(\"‚úÖ BBC cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå BBC dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6f73a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning CNN/DailyMail Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CNN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:04<00:00, 1041.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2518 characters\n",
      "   ‚Ä¢ Cleaned avg length: 2518 characters\n",
      "   ‚Ä¢ Reduction: 0.0%\n",
      "‚úÖ CNN cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean CNN/DailyMail Dataset\n",
    "if 'CNN' in datasets:\n",
    "    print(\"üßπ Cleaning CNN/DailyMail Dataset...\")\n",
    "    cnn_df = datasets['CNN'].copy()\n",
    "    \n",
    "    # Identify text column\n",
    "    text_column = 'article' if 'article' in cnn_df.columns else 'text'\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing CNN\")\n",
    "    cnn_df['text_cleaned'] = cnn_df[text_column].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = cnn_df[text_column].str.len().mean()\n",
    "    cleaned_avg = cnn_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['CNN_cleaned'] = cnn_df\n",
    "    print(\"‚úÖ CNN cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå CNN dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf1f2884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning IMDB Reviews Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IMDB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1759.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 1311 characters\n",
      "   ‚Ä¢ Cleaned avg length: 839 characters\n",
      "   ‚Ä¢ Reduction: 36.0%\n",
      "‚úÖ IMDB cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean IMDB Dataset\n",
    "if 'IMDB' in datasets:\n",
    "    print(\"üßπ Cleaning IMDB Reviews Dataset...\")\n",
    "    imdb_df = datasets['IMDB'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing IMDB\")\n",
    "    imdb_df['review_cleaned'] = imdb_df['review'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = imdb_df['review'].str.len().mean()\n",
    "    cleaned_avg = imdb_df['review_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['IMDB_cleaned'] = imdb_df\n",
    "    print(\"‚úÖ IMDB cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå IMDB dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8014c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC dataset saved: 2225 articles\n",
      "‚úÖ CNN dataset saved: 5000 articles\n",
      "‚úÖ IMDB dataset saved: 1000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create cleaned data directory\n",
    "cleaned_dir = \"../data/cleaned\"\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "# Save BBC cleaned dataset\n",
    "if 'BBC_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"bbc_news_cleaned.csv\")\n",
    "    datasets['BBC_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"BBC: {filepath}\")\n",
    "    print(f\"‚úÖ BBC dataset saved: {len(datasets['BBC_cleaned'])} articles\")\n",
    "\n",
    "# Save CNN cleaned dataset\n",
    "if 'CNN_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"cnn_dailymail_cleaned.csv\")\n",
    "    datasets['CNN_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"CNN: {filepath}\")\n",
    "    print(f\"‚úÖ CNN dataset saved: {len(datasets['CNN_cleaned'])} articles\")\n",
    "\n",
    "# Save IMDB cleaned dataset\n",
    "if 'IMDB_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"imdb_reviews_cleaned.csv\")\n",
    "    datasets['IMDB_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"IMDB: {filepath}\")\n",
    "    print(f\"‚úÖ IMDB dataset saved: {len(datasets['IMDB_cleaned'])} reviews\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'cleaning_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'day': 'Day 8-9',\n",
    "    'objective': 'Text cleaning: remove special characters, punctuation, stop words',\n",
    "    'files_created': saved_files,\n",
    "    'next_step': 'Week 3: Topic modeling with LDA/NMF'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(cleaned_dir, \"preprocessing_metadata.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3be2cb",
   "metadata": {},
   "source": [
    "## Day 10-11: Text Normalization with Stemming and Lemmatization\n",
    "\n",
    "**Objectives:**\n",
    "- Implement stemming using Porter Stemmer\n",
    "- Implement lemmatization using WordNet Lemmatizer\n",
    "- Compare performance between stemming and lemmatization\n",
    "- Apply normalization to all cleaned datasets\n",
    "- Analyze the impact of normalization on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6126a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stemming and Lemmatization tools initialized\n",
      "üìù Porter Stemmer ready\n",
      "üìù WordNet Lemmatizer ready\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for stemming and lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "import time\n",
    "\n",
    "# Download additional NLTK data\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"‚úÖ Stemming and Lemmatization tools initialized\")\n",
    "print(f\"üìù Porter Stemmer ready\")\n",
    "print(f\"üìù WordNet Lemmatizer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbe2f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text normalization functions defined\n",
      "üìã Available methods: 'stem' (Porter Stemmer), 'lemma' (WordNet Lemmatizer)\n"
     ]
    }
   ],
   "source": [
    "# Define normalization functions\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert TreeBank POS tags to WordNet POS tags for better lemmatization\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default to noun\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Apply Porter Stemming to text\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Apply Lemmatization with POS tagging to text\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter only alphabetic tokens\n",
    "    alpha_tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Get POS tags\n",
    "    pos_tags = pos_tag(alpha_tokens)\n",
    "    \n",
    "    # Lemmatize with appropriate POS tags\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(pos)) \n",
    "        for token, pos in pos_tags\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def normalize_text_pipeline(text, method='lemma'):\n",
    "    \"\"\"Complete text normalization pipeline with stemming or lemmatization\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if method == 'stem':\n",
    "        return stem_text(text)\n",
    "    elif method == 'lemma':\n",
    "        return lemmatize_text(text)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'stem' or 'lemma'\")\n",
    "\n",
    "print(\"‚úÖ Text normalization functions defined\")\n",
    "print(\"üìã Available methods: 'stem' (Porter Stemmer), 'lemma' (WordNet Lemmatizer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "910833b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Stemming vs Lemmatization Comparison:\n",
      "Original text: The children were running and jumping happily in the beautiful gardens while their parents were watching\n",
      "\n",
      "Stemmed text: the children were run and jump happili in the beauti garden while their parent were watch\n",
      "Lemmatized text: the child be run and jump happily in the beautiful garden while their parent be watch\n",
      "\n",
      "üìä Key Differences:\n",
      "‚Ä¢ Stemming: Faster, rule-based, may create non-words\n",
      "‚Ä¢ Lemmatization: Slower, dictionary-based, preserves valid words\n",
      "‚Ä¢ Lemmatization with POS: Most accurate, context-aware\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate stemming vs lemmatization\n",
    "sample_text = \"The children were running and jumping happily in the beautiful gardens while their parents were watching\"\n",
    "\n",
    "print(\"üîç Stemming vs Lemmatization Comparison:\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed = stem_text(sample_text)\n",
    "print(f\"Stemmed text: {stemmed}\")\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized = lemmatize_text(sample_text)\n",
    "print(f\"Lemmatized text: {lemmatized}\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Key Differences:\")\n",
    "print(\"‚Ä¢ Stemming: Faster, rule-based, may create non-words\")\n",
    "print(\"‚Ä¢ Lemmatization: Slower, dictionary-based, preserves valid words\")\n",
    "print(\"‚Ä¢ Lemmatization with POS: Most accurate, context-aware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41407199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing BBC News Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BBC Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:15<00:00, 143.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BBC Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:31<00:00, 69.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 1584 characters\n",
      "   ‚Ä¢ Stemmed avg length: 1368 characters (‚Üì13.6%)\n",
      "   ‚Ä¢ Lemmatized avg length: 1471 characters (‚Üì7.1%)\n",
      "‚úÖ BBC normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to BBC News Dataset\n",
    "if 'BBC_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing BBC News Dataset...\")\n",
    "    bbc_df = datasets['BBC_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"BBC Stemming\")\n",
    "    bbc_df['text_stemmed'] = bbc_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"BBC Lemmatization\")\n",
    "    bbc_df['text_lemmatized'] = bbc_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = bbc_df['text_cleaned'].str.len().mean()\n",
    "    stemmed_avg = bbc_df['text_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = bbc_df['text_lemmatized'].str.len().mean()\n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['BBC_normalized'] = bbc_df\n",
    "    print(\"‚úÖ BBC normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå BBC cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f83c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing CNN/DailyMail Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:00<00:00, 82.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:57<00:00, 42.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 2518 characters\n",
      "   ‚Ä¢ Stemmed avg length: 2210 characters (‚Üì12.2%)\n",
      "   ‚Ä¢ Lemmatized avg length: 2387 characters (‚Üì5.2%)\n",
      "‚úÖ CNN normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to CNN/DailyMail Dataset\n",
    "if 'CNN_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing CNN/DailyMail Dataset...\")\n",
    "    cnn_df = datasets['CNN_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"CNN Stemming\")\n",
    "    cnn_df['text_stemmed'] = cnn_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"CNN Lemmatization\")\n",
    "    cnn_df['text_lemmatized'] = cnn_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = cnn_df['text_cleaned'].str.len().mean()\n",
    "    stemmed_avg = cnn_df['text_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = cnn_df['text_lemmatized'].str.len().mean()\n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['CNN_normalized'] = cnn_df\n",
    "    print(\"‚úÖ CNN normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå CNN cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e9e59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing IMDB Reviews Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMDB Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03<00:00, 250.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMDB Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:07<00:00, 126.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 839 characters\n",
      "   ‚Ä¢ Stemmed avg length: 740 characters (‚Üì11.7%)\n",
      "   ‚Ä¢ Lemmatized avg length: 796 characters (‚Üì5.1%)\n",
      "‚úÖ IMDB normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to IMDB Reviews Dataset\n",
    "if 'IMDB_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing IMDB Reviews Dataset...\")\n",
    "    imdb_df = datasets['IMDB_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"IMDB Stemming\")\n",
    "    imdb_df['review_stemmed'] = imdb_df['review_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"IMDB Lemmatization\")\n",
    "    imdb_df['review_lemmatized'] = imdb_df['review_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = imdb_df['review_cleaned'].str.len().mean()\n",
    "    stemmed_avg = imdb_df['review_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = imdb_df['review_lemmatized'].str.len().mean()\n",
    "    \n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['IMDB_normalized'] = imdb_df\n",
    "    print(\"‚úÖ IMDB normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå IMDB cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2a7c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Normalization Performance Summary\n",
      "============================================================\n",
      "\n",
      "üìã BBC Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 5,947 unique words\n",
      "     ‚Ä¢ Stemmed: 4,235 unique words (‚Üì28.8%)\n",
      "     ‚Ä¢ Lemmatized: 4,669 unique words (‚Üì21.5%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 1584 characters\n",
      "     ‚Ä¢ Stemmed: 1368 characters\n",
      "     ‚Ä¢ Lemmatized: 1471 characters\n",
      "\n",
      "üìã CNN Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 8,999 unique words\n",
      "     ‚Ä¢ Stemmed: 6,506 unique words (‚Üì27.7%)\n",
      "     ‚Ä¢ Lemmatized: 7,408 unique words (‚Üì17.7%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 2518 characters\n",
      "     ‚Ä¢ Stemmed: 2210 characters\n",
      "     ‚Ä¢ Lemmatized: 2387 characters\n",
      "\n",
      "üìã IMDB Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 4,648 unique words\n",
      "     ‚Ä¢ Stemmed: 3,641 unique words (‚Üì21.7%)\n",
      "     ‚Ä¢ Lemmatized: 3,914 unique words (‚Üì15.8%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 839 characters\n",
      "     ‚Ä¢ Stemmed: 740 characters\n",
      "     ‚Ä¢ Lemmatized: 796 characters\n",
      "\n",
      "‚úÖ Normalization analysis completed for 3 datasets\n"
     ]
    }
   ],
   "source": [
    "# Comparative Analysis of Normalization Results\n",
    "print(\"üìä Normalization Performance Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "# Analyze each dataset\n",
    "for dataset_name in ['BBC', 'CNN', 'IMDB']:\n",
    "    normalized_key = f\"{dataset_name}_normalized\"\n",
    "    \n",
    "    if normalized_key in datasets:\n",
    "        df = datasets[normalized_key]\n",
    "        \n",
    "        if dataset_name == 'IMDB':\n",
    "            text_col = 'review_cleaned'\n",
    "            stem_col = 'review_stemmed'\n",
    "            lemma_col = 'review_lemmatized'\n",
    "        else:\n",
    "            text_col = 'text_cleaned'\n",
    "            stem_col = 'text_stemmed'\n",
    "            lemma_col = 'text_lemmatized'\n",
    "        \n",
    "        # Calculate vocabulary reduction\n",
    "        def get_vocab_size(series):\n",
    "            all_words = set()\n",
    "            for text in series:\n",
    "                if pd.notna(text) and text != \"\":\n",
    "                    all_words.update(text.split())\n",
    "            return len(all_words)\n",
    "        \n",
    "        # Get sample for vocabulary analysis (first 100 entries for performance)\n",
    "        sample_df = df.head(100)\n",
    "        \n",
    "        original_vocab = get_vocab_size(sample_df[text_col])\n",
    "        stemmed_vocab = get_vocab_size(sample_df[stem_col])\n",
    "        lemmatized_vocab = get_vocab_size(sample_df[lemma_col])\n",
    "        \n",
    "        stem_vocab_reduction = ((original_vocab - stemmed_vocab) / original_vocab * 100)\n",
    "        lemma_vocab_reduction = ((original_vocab - lemmatized_vocab) / original_vocab * 100)\n",
    "        \n",
    "        # Average lengths\n",
    "        clean_len = df[text_col].str.len().mean()\n",
    "        stem_len = df[stem_col].str.len().mean()\n",
    "        lemma_len = df[lemma_col].str.len().mean()\n",
    "        \n",
    "        result = {\n",
    "            'dataset': dataset_name,\n",
    "            'original_vocab': original_vocab,\n",
    "            'stemmed_vocab': stemmed_vocab,\n",
    "            'lemmatized_vocab': lemmatized_vocab,\n",
    "            'stem_vocab_reduction': stem_vocab_reduction,\n",
    "            'lemma_vocab_reduction': lemma_vocab_reduction,\n",
    "            'clean_avg_length': clean_len,\n",
    "            'stem_avg_length': stem_len,\n",
    "            'lemma_avg_length': lemma_len\n",
    "        }\n",
    "        \n",
    "        analysis_results.append(result)\n",
    "        \n",
    "        print(f\"\\nüìã {dataset_name} Dataset Analysis:\")\n",
    "        print(f\"   Vocabulary Size (sample):\")\n",
    "        print(f\"     ‚Ä¢ Original: {original_vocab:,} unique words\")\n",
    "        print(f\"     ‚Ä¢ Stemmed: {stemmed_vocab:,} unique words (‚Üì{stem_vocab_reduction:.1f}%)\")\n",
    "        print(f\"     ‚Ä¢ Lemmatized: {lemmatized_vocab:,} unique words (‚Üì{lemma_vocab_reduction:.1f}%)\")\n",
    "        print(f\"   Average Text Length:\")\n",
    "        print(f\"     ‚Ä¢ Cleaned: {clean_len:.0f} characters\")\n",
    "        print(f\"     ‚Ä¢ Stemmed: {stem_len:.0f} characters\")\n",
    "        print(f\"     ‚Ä¢ Lemmatized: {lemma_len:.0f} characters\")\n",
    "\n",
    "print(f\"\\n‚úÖ Normalization analysis completed for {len(analysis_results)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "830d2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving normalized datasets...\n",
      "‚úÖ BBC normalized dataset saved: 2225 articles\n",
      "‚úÖ CNN normalized dataset saved: 5000 articles\n",
      "‚úÖ IMDB normalized dataset saved: 1000 reviews\n",
      "‚úÖ Normalization metadata saved: ../data/normalized\\normalization_metadata.json\n",
      "üéØ Day 10-11 objectives completed successfully!\n",
      "üìÅ Files ready for next phase: Topic modeling and sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "# Save normalized datasets\n",
    "print(\"üíæ Saving normalized datasets...\")\n",
    "\n",
    "# Create normalized data directory\n",
    "normalized_dir = \"../data/normalized\"\n",
    "os.makedirs(normalized_dir, exist_ok=True)\n",
    "\n",
    "saved_normalized_files = []\n",
    "\n",
    "# Save BBC normalized dataset\n",
    "if 'BBC_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"bbc_news_normalized.csv\")\n",
    "    datasets['BBC_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"BBC: {filepath}\")\n",
    "    print(f\"‚úÖ BBC normalized dataset saved: {len(datasets['BBC_normalized'])} articles\")\n",
    "\n",
    "# Save CNN normalized dataset\n",
    "if 'CNN_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"cnn_dailymail_normalized.csv\")\n",
    "    datasets['CNN_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"CNN: {filepath}\")\n",
    "    print(f\"‚úÖ CNN normalized dataset saved: {len(datasets['CNN_normalized'])} articles\")\n",
    "\n",
    "# Save IMDB normalized dataset\n",
    "if 'IMDB_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"imdb_reviews_normalized.csv\")\n",
    "    datasets['IMDB_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"IMDB: {filepath}\")\n",
    "    print(f\"‚úÖ IMDB normalized dataset saved: {len(datasets['IMDB_normalized'])} reviews\")\n",
    "\n",
    "# Update metadata with normalization information\n",
    "normalization_metadata = {\n",
    "    'normalization_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'day': 'Day 10-11',\n",
    "    'objective': 'Text normalization: stemming and lemmatization',\n",
    "    'methods_used': {\n",
    "        'stemming': 'Porter Stemmer',\n",
    "        'lemmatization': 'WordNet Lemmatizer with POS tagging'\n",
    "    },\n",
    "    'files_created': saved_normalized_files,\n",
    "    'analysis_results': analysis_results,\n",
    "    'next_step': 'Topic modeling and sentiment analysis with normalized text',\n",
    "    'recommendations': {\n",
    "        'stemming': 'Faster processing, good for large-scale analysis',\n",
    "        'lemmatization': 'Better semantic preservation, recommended for accuracy'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save normalization metadata\n",
    "metadata_path = os.path.join(normalized_dir, \"normalization_metadata.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(normalization_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Normalization metadata saved: {metadata_path}\")\n",
    "print(f\"üéØ Day 10-11 objectives completed successfully!\")\n",
    "print(f\"üìÅ Files ready for next phase: Topic modeling and sentiment analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5600dd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è transformers not installed or no internet, skipping subword tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\subod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# download punkt for nltk if not already\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# huggingface optional (for subword tokenization)\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "except Exception as e:\n",
    "    hf_tokenizer = None\n",
    "    print(\"‚ö†Ô∏è transformers not installed or no internet, skipping subword tokenization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56fdb0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_level_tokens(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    return [t for t in word_tokenize(text) if t.strip()]\n",
    "\n",
    "def hf_encode(texts, tokenizer, max_len=256):\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    out = []\n",
    "    for i in range(len(texts)):\n",
    "        out.append({\n",
    "            \"input_ids\": enc[\"input_ids\"][i],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][i],\n",
    "        })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0b5deff",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'tokenized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     bbc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_lemmatized_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m enc_lemma]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# save\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m bbc\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tokenized/bbc_news_tokenized.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3034\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3114\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3115\u001b[0m     path,\n\u001b[0;32m   3116\u001b[0m     engine,\n\u001b[0;32m   3117\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3118\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3119\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3120\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3122\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 480\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    481\u001b[0m     df,\n\u001b[0;32m    482\u001b[0m     path_or_buf,\n\u001b[0;32m    483\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    484\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    485\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m    486\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    487\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    489\u001b[0m )\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:198\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     merged_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexisting_metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdf_metadata}\n\u001b[0;32m    196\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mreplace_schema_metadata(merged_metadata)\n\u001b[1;32m--> 198\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    199\u001b[0m     path,\n\u001b[0;32m    200\u001b[0m     filesystem,\n\u001b[0;32m    201\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    202\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    203\u001b[0m     is_dir\u001b[38;5;241m=\u001b[39mpartition_cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io\u001b[38;5;241m.\u001b[39mBufferedWriter)\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[0;32m    209\u001b[0m ):\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mbytes\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'tokenized'"
     ]
    }
   ],
   "source": [
    "bbc = datasets[\"BBC_normalized\"].copy()\n",
    "\n",
    "# word-level\n",
    "bbc[\"tokens_clean\"]      = bbc[\"text_cleaned\"].apply(word_level_tokens)\n",
    "bbc[\"tokens_stemmed\"]    = bbc[\"text_stemmed\"].apply(word_level_tokens)\n",
    "bbc[\"tokens_lemmatized\"] = bbc[\"text_lemmatized\"].apply(word_level_tokens)\n",
    "\n",
    "# optional: subword (only clean + lemma, not stemmed)\n",
    "if hf_tokenizer:\n",
    "    enc_clean = hf_encode(bbc[\"text_cleaned\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "    enc_lemma = hf_encode(bbc[\"text_lemmatized\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "\n",
    "    bbc[\"hf_clean_ids\"]       = [e[\"input_ids\"] for e in enc_clean]\n",
    "    bbc[\"hf_clean_mask\"]      = [e[\"attention_mask\"] for e in enc_clean]\n",
    "    bbc[\"hf_lemmatized_ids\"]  = [e[\"input_ids\"] for e in enc_lemma]\n",
    "    bbc[\"hf_lemmatized_mask\"] = [e[\"attention_mask\"] for e in enc_lemma]\n",
    "\n",
    "# save\n",
    "bbc.to_parquet(\"./tokenized/bbc_news_tokenized.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7deb039f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenization Function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_text\u001b[39m(text, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenize text into words or sentences\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(text) \u001b[38;5;129;01mor\u001b[39;00m text \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_wait_suspend(thread, frame, event, arg)\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tokenization Function\n",
    "def tokenize_text(text, method=\"word\"):\n",
    "    \"\"\"Tokenize text into words or sentences\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    if method == \"word\":\n",
    "        return word_tokenize(text.lower())   # Word-level tokens\n",
    "    elif method == \"sentence\":\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        return sent_tokenize(text)           # Sentence-level tokens\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'word' or 'sentence'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
