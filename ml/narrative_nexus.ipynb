{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcb01a8",
   "metadata": {},
   "source": [
    "# Text Preprocessing - Day 8-9\n",
    "## NarrativeNexus Project: Text Cleaning Implementation\n",
    "\n",
    "**Objectives:**\n",
    "- Remove special characters, punctuation, and stop words\n",
    "- Apply preprocessing to BBC, CNN/DailyMail, and IMDB datasets\n",
    "- Save cleaned datasets for Week 3 topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ed395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Loaded 198 stop words.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"‚úÖ Setup complete. Loaded {len(stop_words)} stop words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c59571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Define text cleaning functions\n",
    "def clean_special_characters(text):\n",
    "    \"\"\"Remove special characters, keep only letters, numbers, and spaces\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Remove special characters\n",
    "    cleaned = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def remove_stop_words(text, keep_negations=True):\n",
    "    \"\"\"Remove stop words while preserving negations\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Keep important negation words\n",
    "    stop_words_filtered = stop_words.copy()\n",
    "    if keep_negations:\n",
    "        important_words = {'not', 'no', 'never', 'none', 'neither', 'nobody', 'nothing'}\n",
    "        stop_words_filtered = stop_words_filtered - important_words\n",
    "    \n",
    "    # Tokenize and filter\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words_filtered]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def clean_text_pipeline(text):\n",
    "    \"\"\"Complete text cleaning pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Convert to lowercase\n",
    "    cleaned = str(text).lower()\n",
    "    \n",
    "    # Step 2: Remove special characters\n",
    "    cleaned = clean_special_characters(cleaned)\n",
    "    \n",
    "    # Step 3: Remove stop words\n",
    "    cleaned = remove_stop_words(cleaned, keep_negations=True)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"‚úÖ Text cleaning functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c64bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC Dataset: 2225 articles loaded\n",
      "‚úÖ CNN Dataset: 5000 articles loaded\n",
      "‚úÖ IMDB Dataset: 1000 reviews loaded\n",
      "\n",
      "üìä Total datasets loaded: 3\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_dir = \"../data\"\n",
    "datasets = {}\n",
    "\n",
    "# Load BBC News Dataset\n",
    "try:\n",
    "    bbc_df = pd.read_csv(f\"{data_dir}/bbc-text.csv\")\n",
    "    datasets['BBC'] = bbc_df\n",
    "    print(f\"‚úÖ BBC Dataset: {len(bbc_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading BBC dataset: {e}\")\n",
    "\n",
    "# Load CNN/DailyMail Dataset\n",
    "try:\n",
    "    cnn_df = pd.read_csv(f\"{data_dir}/cnn_dailymail.csv\")\n",
    "    datasets['CNN'] = cnn_df\n",
    "    print(f\"‚úÖ CNN Dataset: {len(cnn_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CNN dataset: {e}\")\n",
    "\n",
    "# Load IMDB Dataset (subset for demo)\n",
    "try:\n",
    "    imdb_df = pd.read_csv(f\"{data_dir}/imdb-dataset.csv\", nrows=1000)\n",
    "    datasets['IMDB'] = imdb_df\n",
    "    print(f\"‚úÖ IMDB Dataset: {len(imdb_df)} reviews loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading IMDB dataset: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921f4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning BBC News Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing BBC: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:02<00:00, 1108.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2263 characters\n",
      "   ‚Ä¢ Cleaned avg length: 1584 characters\n",
      "   ‚Ä¢ Reduction: 30.0%\n",
      "‚úÖ BBC cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean BBC News Dataset\n",
    "if 'BBC' in datasets:\n",
    "    print(\"üßπ Cleaning BBC News Dataset...\")\n",
    "    bbc_df = datasets['BBC'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing BBC\")\n",
    "    bbc_df['text_cleaned'] = bbc_df['text'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = bbc_df['text'].str.len().mean()\n",
    "    cleaned_avg = bbc_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['BBC_cleaned'] = bbc_df\n",
    "    print(\"‚úÖ BBC cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå BBC dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f73a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning CNN/DailyMail Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CNN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:04<00:00, 1072.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2518 characters\n",
      "   ‚Ä¢ Cleaned avg length: 2518 characters\n",
      "   ‚Ä¢ Reduction: 0.0%\n",
      "‚úÖ CNN cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean CNN/DailyMail Dataset\n",
    "if 'CNN' in datasets:\n",
    "    print(\"üßπ Cleaning CNN/DailyMail Dataset...\")\n",
    "    cnn_df = datasets['CNN'].copy()\n",
    "    \n",
    "    # Identify text column\n",
    "    text_column = 'article' if 'article' in cnn_df.columns else 'text'\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing CNN\")\n",
    "    cnn_df['text_cleaned'] = cnn_df[text_column].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = cnn_df[text_column].str.len().mean()\n",
    "    cleaned_avg = cnn_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['CNN_cleaned'] = cnn_df\n",
    "    print(\"‚úÖ CNN cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå CNN dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f2884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning IMDB Reviews Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IMDB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1818.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 1311 characters\n",
      "   ‚Ä¢ Cleaned avg length: 839 characters\n",
      "   ‚Ä¢ Reduction: 36.0%\n",
      "‚úÖ IMDB cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean IMDB Dataset\n",
    "if 'IMDB' in datasets:\n",
    "    print(\"üßπ Cleaning IMDB Reviews Dataset...\")\n",
    "    imdb_df = datasets['IMDB'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing IMDB\")\n",
    "    imdb_df['review_cleaned'] = imdb_df['review'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = imdb_df['review'].str.len().mean()\n",
    "    cleaned_avg = imdb_df['review_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['IMDB_cleaned'] = imdb_df\n",
    "    print(\"‚úÖ IMDB cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå IMDB dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8014c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC dataset saved: 2225 articles\n",
      "‚úÖ CNN dataset saved: 5000 articles\n",
      "‚úÖ IMDB dataset saved: 1000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create cleaned data directory\n",
    "cleaned_dir = \"../data/cleaned\"\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "# Save BBC cleaned dataset\n",
    "if 'BBC_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"bbc_news_cleaned.csv\")\n",
    "    datasets['BBC_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"BBC: {filepath}\")\n",
    "    print(f\"‚úÖ BBC dataset saved: {len(datasets['BBC_cleaned'])} articles\")\n",
    "\n",
    "# Save CNN cleaned dataset\n",
    "if 'CNN_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"cnn_dailymail_cleaned.csv\")\n",
    "    datasets['CNN_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"CNN: {filepath}\")\n",
    "    print(f\"‚úÖ CNN dataset saved: {len(datasets['CNN_cleaned'])} articles\")\n",
    "\n",
    "# Save IMDB cleaned dataset\n",
    "if 'IMDB_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"imdb_reviews_cleaned.csv\")\n",
    "    datasets['IMDB_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"IMDB: {filepath}\")\n",
    "    print(f\"‚úÖ IMDB dataset saved: {len(datasets['IMDB_cleaned'])} reviews\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'cleaning_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'day': 'Day 8-9',\n",
    "    'objective': 'Text cleaning: remove special characters, punctuation, stop words',\n",
    "    'files_created': saved_files,\n",
    "    'next_step': 'Week 3: Topic modeling with LDA/NMF'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(cleaned_dir, \"preprocessing_metadata.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
