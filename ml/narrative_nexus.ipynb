{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcb01a8",
   "metadata": {},
   "source": [
    "## NarrativeNexus Project: Text Cleaning Implementation\n",
    "\n",
    "**Objectives:**\n",
    "- Remove special characters, punctuation, and stop words\n",
    "- Apply preprocessing to BBC, CNN/DailyMail, and IMDB datasets\n",
    "- Save cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ed395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Loaded 198 stop words.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"‚úÖ Setup complete. Loaded {len(stop_words)} stop words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c59571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Define text cleaning functions\n",
    "def clean_special_characters(text):\n",
    "    \"\"\"Remove special characters, keep only letters, numbers, and spaces\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Remove special characters\n",
    "    cleaned = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def remove_stop_words(text, keep_negations=True):\n",
    "    \"\"\"Remove stop words while preserving negations\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Keep important negation words\n",
    "    stop_words_filtered = stop_words.copy()\n",
    "    if keep_negations:\n",
    "        important_words = {'not', 'no', 'never', 'none', 'neither', 'nobody', 'nothing'}\n",
    "        stop_words_filtered = stop_words_filtered - important_words\n",
    "    \n",
    "    # Tokenize and filter\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words_filtered]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def clean_text_pipeline(text):\n",
    "    \"\"\"Complete text cleaning pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Convert to lowercase\n",
    "    cleaned = str(text).lower()\n",
    "    \n",
    "    # Step 2: Remove special characters\n",
    "    cleaned = clean_special_characters(cleaned)\n",
    "    \n",
    "    # Step 3: Remove stop words\n",
    "    cleaned = remove_stop_words(cleaned, keep_negations=True)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"‚úÖ Text cleaning functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c64bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC Dataset: 2225 articles loaded\n",
      "‚úÖ CNN Dataset: 5000 articles loaded\n",
      "‚úÖ IMDB Dataset: 1000 reviews loaded\n",
      "\n",
      "üìä Total datasets loaded: 3\n",
      "‚úÖ CNN Dataset: 5000 articles loaded\n",
      "‚úÖ IMDB Dataset: 1000 reviews loaded\n",
      "\n",
      "üìä Total datasets loaded: 3\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_dir = \"../data\"\n",
    "datasets = {}\n",
    "\n",
    "# Load BBC News Dataset\n",
    "try:\n",
    "    bbc_df = pd.read_csv(f\"{data_dir}/bbc-text.csv\")\n",
    "    datasets['BBC'] = bbc_df\n",
    "    print(f\"‚úÖ BBC Dataset: {len(bbc_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading BBC dataset: {e}\")\n",
    "\n",
    "# Load CNN/DailyMail Dataset\n",
    "try:\n",
    "    cnn_df = pd.read_csv(f\"{data_dir}/cnn_dailymail.csv\")\n",
    "    datasets['CNN'] = cnn_df\n",
    "    print(f\"‚úÖ CNN Dataset: {len(cnn_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CNN dataset: {e}\")\n",
    "\n",
    "# Load IMDB Dataset (subset for demo)\n",
    "try:\n",
    "    imdb_df = pd.read_csv(f\"{data_dir}/imdb-dataset.csv\", nrows=1000)\n",
    "    datasets['IMDB'] = imdb_df\n",
    "    print(f\"‚úÖ IMDB Dataset: {len(imdb_df)} reviews loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading IMDB dataset: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921f4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning BBC News Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing BBC: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:02<00:00, 1087.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2263 characters\n",
      "   ‚Ä¢ Cleaned avg length: 1584 characters\n",
      "   ‚Ä¢ Reduction: 30.0%\n",
      "‚úÖ BBC cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean BBC News Dataset\n",
    "if 'BBC' in datasets:\n",
    "    print(\"üßπ Cleaning BBC News Dataset...\")\n",
    "    bbc_df = datasets['BBC'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing BBC\")\n",
    "    bbc_df['text_cleaned'] = bbc_df['text'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = bbc_df['text'].str.len().mean()\n",
    "    cleaned_avg = bbc_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['BBC_cleaned'] = bbc_df\n",
    "    print(\"‚úÖ BBC cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå BBC dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f73a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning CNN/DailyMail Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CNN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:04<00:00, 1060.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2518 characters\n",
      "   ‚Ä¢ Cleaned avg length: 2518 characters\n",
      "   ‚Ä¢ Reduction: 0.0%\n",
      "‚úÖ CNN cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean CNN/DailyMail Dataset\n",
    "if 'CNN' in datasets:\n",
    "    print(\"üßπ Cleaning CNN/DailyMail Dataset...\")\n",
    "    cnn_df = datasets['CNN'].copy()\n",
    "    \n",
    "    # Identify text column\n",
    "    text_column = 'article' if 'article' in cnn_df.columns else 'text'\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing CNN\")\n",
    "    cnn_df['text_cleaned'] = cnn_df[text_column].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = cnn_df[text_column].str.len().mean()\n",
    "    cleaned_avg = cnn_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['CNN_cleaned'] = cnn_df\n",
    "    print(\"‚úÖ CNN cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå CNN dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1f2884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning IMDB Reviews Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IMDB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1783.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 1311 characters\n",
      "   ‚Ä¢ Cleaned avg length: 839 characters\n",
      "   ‚Ä¢ Reduction: 36.0%\n",
      "‚úÖ IMDB cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean IMDB Dataset\n",
    "if 'IMDB' in datasets:\n",
    "    print(\"üßπ Cleaning IMDB Reviews Dataset...\")\n",
    "    imdb_df = datasets['IMDB'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing IMDB\")\n",
    "    imdb_df['review_cleaned'] = imdb_df['review'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = imdb_df['review'].str.len().mean()\n",
    "    cleaned_avg = imdb_df['review_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['IMDB_cleaned'] = imdb_df\n",
    "    print(\"‚úÖ IMDB cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå IMDB dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8014c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC dataset saved: 2225 articles\n",
      "‚úÖ CNN dataset saved: 5000 articles\n",
      "‚úÖ IMDB dataset saved: 1000 reviews\n",
      "‚úÖ CNN dataset saved: 5000 articles\n",
      "‚úÖ IMDB dataset saved: 1000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create cleaned data directory\n",
    "cleaned_dir = \"../data/cleaned\"\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "# Save BBC cleaned dataset\n",
    "if 'BBC_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"bbc_news_cleaned.csv\")\n",
    "    datasets['BBC_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"BBC: {filepath}\")\n",
    "    print(f\"‚úÖ BBC dataset saved: {len(datasets['BBC_cleaned'])} articles\")\n",
    "\n",
    "# Save CNN cleaned dataset\n",
    "if 'CNN_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"cnn_dailymail_cleaned.csv\")\n",
    "    datasets['CNN_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"CNN: {filepath}\")\n",
    "    print(f\"‚úÖ CNN dataset saved: {len(datasets['CNN_cleaned'])} articles\")\n",
    "\n",
    "# Save IMDB cleaned dataset\n",
    "if 'IMDB_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"imdb_reviews_cleaned.csv\")\n",
    "    datasets['IMDB_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"IMDB: {filepath}\")\n",
    "    print(f\"‚úÖ IMDB dataset saved: {len(datasets['IMDB_cleaned'])} reviews\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'cleaning_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'day': 'Day 8-9',\n",
    "    'objective': 'Text cleaning: remove special characters, punctuation, stop words',\n",
    "    'files_created': saved_files,\n",
    "    'next_step': 'Week 3: Topic modeling with LDA/NMF'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(cleaned_dir, \"preprocessing_metadata.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3be2cb",
   "metadata": {},
   "source": [
    "## Day 10-11: Text Normalization with Stemming and Lemmatization\n",
    "\n",
    "**Objectives:**\n",
    "- Implement stemming using Porter Stemmer\n",
    "- Implement lemmatization using WordNet Lemmatizer\n",
    "- Compare performance between stemming and lemmatization\n",
    "- Apply normalization to all cleaned datasets\n",
    "- Analyze the impact of normalization on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6126a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stemming and Lemmatization tools initialized\n",
      "üìù Porter Stemmer ready\n",
      "üìù WordNet Lemmatizer ready\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for stemming and lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "import time\n",
    "\n",
    "# Download additional NLTK data\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"‚úÖ Stemming and Lemmatization tools initialized\")\n",
    "print(f\"üìù Porter Stemmer ready\")\n",
    "print(f\"üìù WordNet Lemmatizer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe2f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text normalization functions defined\n",
      "üìã Available methods: 'stem' (Porter Stemmer), 'lemma' (WordNet Lemmatizer)\n"
     ]
    }
   ],
   "source": [
    "# Define normalization functions\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert TreeBank POS tags to WordNet POS tags for better lemmatization\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default to noun\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Apply Porter Stemming to text\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Apply Lemmatization with POS tagging to text\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter only alphabetic tokens\n",
    "    alpha_tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Get POS tags\n",
    "    pos_tags = pos_tag(alpha_tokens)\n",
    "    \n",
    "    # Lemmatize with appropriate POS tags\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(pos)) \n",
    "        for token, pos in pos_tags\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def normalize_text_pipeline(text, method='lemma'):\n",
    "    \"\"\"Complete text normalization pipeline with stemming or lemmatization\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if method == 'stem':\n",
    "        return stem_text(text)\n",
    "    elif method == 'lemma':\n",
    "        return lemmatize_text(text)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'stem' or 'lemma'\")\n",
    "\n",
    "print(\"‚úÖ Text normalization functions defined\")\n",
    "print(\"üìã Available methods: 'stem' (Porter Stemmer), 'lemma' (WordNet Lemmatizer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910833b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Stemming vs Lemmatization Comparison:\n",
      "Original text: The children were running and jumping happily in the beautiful gardens while their parents were watching\n",
      "\n",
      "Stemmed text: the children were run and jump happili in the beauti garden while their parent were watch\n",
      "Lemmatized text: the child be run and jump happily in the beautiful garden while their parent be watch\n",
      "\n",
      "üìä Key Differences:\n",
      "‚Ä¢ Stemming: Faster, rule-based, may create non-words\n",
      "‚Ä¢ Lemmatization: Slower, dictionary-based, preserves valid words\n",
      "‚Ä¢ Lemmatization with POS: Most accurate, context-aware\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate stemming vs lemmatization\n",
    "sample_text = \"The children were running and jumping happily in the beautiful gardens while their parents were watching\"\n",
    "\n",
    "print(\"üîç Stemming vs Lemmatization Comparison:\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed = stem_text(sample_text)\n",
    "print(f\"Stemmed text: {stemmed}\")\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized = lemmatize_text(sample_text)\n",
    "print(f\"Lemmatized text: {lemmatized}\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Key Differences:\")\n",
    "print(\"‚Ä¢ Stemming: Faster, rule-based, may create non-words\")\n",
    "print(\"‚Ä¢ Lemmatization: Slower, dictionary-based, preserves valid words\")\n",
    "print(\"‚Ä¢ Lemmatization with POS: Most accurate, context-aware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41407199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing BBC News Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BBC Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:06<00:00, 359.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BBC Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:16<00:00, 137.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 1584 characters\n",
      "   ‚Ä¢ Stemmed avg length: 1368 characters (‚Üì13.6%)\n",
      "   ‚Ä¢ Lemmatized avg length: 1471 characters (‚Üì7.1%)\n",
      "‚úÖ BBC normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to BBC News Dataset\n",
    "if 'BBC_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing BBC News Dataset...\")\n",
    "    bbc_df = datasets['BBC_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"BBC Stemming\")\n",
    "    bbc_df['text_stemmed'] = bbc_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"BBC Lemmatization\")\n",
    "    bbc_df['text_lemmatized'] = bbc_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = bbc_df['text_cleaned'].str.len().mean()\n",
    "    stemmed_avg = bbc_df['text_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = bbc_df['text_lemmatized'].str.len().mean()\n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['BBC_normalized'] = bbc_df\n",
    "    print(\"‚úÖ BBC normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå BBC cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f83c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing CNN/DailyMail Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:21<00:00, 227.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:04<00:00, 77.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 2518 characters\n",
      "   ‚Ä¢ Stemmed avg length: 2210 characters (‚Üì12.2%)\n",
      "   ‚Ä¢ Lemmatized avg length: 2387 characters (‚Üì5.2%)\n",
      "‚úÖ CNN normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to CNN/DailyMail Dataset\n",
    "if 'CNN_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing CNN/DailyMail Dataset...\")\n",
    "    cnn_df = datasets['CNN_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"CNN Stemming\")\n",
    "    cnn_df['text_stemmed'] = cnn_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"CNN Lemmatization\")\n",
    "    cnn_df['text_lemmatized'] = cnn_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = cnn_df['text_cleaned'].str.len().mean()\n",
    "    stemmed_avg = cnn_df['text_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = cnn_df['text_lemmatized'].str.len().mean()\n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['CNN_normalized'] = cnn_df\n",
    "    print(\"‚úÖ CNN normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå CNN cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e9e59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing IMDB Reviews Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMDB Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 553.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMDB Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 207.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 839 characters\n",
      "   ‚Ä¢ Stemmed avg length: 740 characters (‚Üì11.7%)\n",
      "   ‚Ä¢ Lemmatized avg length: 796 characters (‚Üì5.1%)\n",
      "‚úÖ IMDB normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to IMDB Reviews Dataset\n",
    "if 'IMDB_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing IMDB Reviews Dataset...\")\n",
    "    imdb_df = datasets['IMDB_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"IMDB Stemming\")\n",
    "    imdb_df['review_stemmed'] = imdb_df['review_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"IMDB Lemmatization\")\n",
    "    imdb_df['review_lemmatized'] = imdb_df['review_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = imdb_df['review_cleaned'].str.len().mean()\n",
    "    stemmed_avg = imdb_df['review_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = imdb_df['review_lemmatized'].str.len().mean()\n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['IMDB_normalized'] = imdb_df\n",
    "    print(\"‚úÖ IMDB normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå IMDB cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2a7c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Normalization Performance Summary\n",
      "============================================================\n",
      "\n",
      "üìã BBC Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 5,947 unique words\n",
      "     ‚Ä¢ Stemmed: 4,235 unique words (‚Üì28.8%)\n",
      "     ‚Ä¢ Lemmatized: 4,669 unique words (‚Üì21.5%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 1584 characters\n",
      "     ‚Ä¢ Stemmed: 1368 characters\n",
      "     ‚Ä¢ Lemmatized: 1471 characters\n",
      "\n",
      "üìã CNN Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 8,999 unique words\n",
      "     ‚Ä¢ Stemmed: 6,506 unique words (‚Üì27.7%)\n",
      "     ‚Ä¢ Lemmatized: 7,408 unique words (‚Üì17.7%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 2518 characters\n",
      "     ‚Ä¢ Stemmed: 2210 characters\n",
      "     ‚Ä¢ Lemmatized: 2387 characters\n",
      "\n",
      "üìã IMDB Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 4,648 unique words\n",
      "     ‚Ä¢ Stemmed: 3,641 unique words (‚Üì21.7%)\n",
      "     ‚Ä¢ Lemmatized: 3,914 unique words (‚Üì15.8%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 839 characters\n",
      "     ‚Ä¢ Stemmed: 740 characters\n",
      "     ‚Ä¢ Lemmatized: 796 characters\n",
      "\n",
      "‚úÖ Normalization analysis completed for 3 datasets\n"
     ]
    }
   ],
   "source": [
    "# Comparative Analysis of Normalization Results\n",
    "print(\"üìä Normalization Performance Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "# Analyze each dataset\n",
    "for dataset_name in ['BBC', 'CNN', 'IMDB']:\n",
    "    normalized_key = f\"{dataset_name}_normalized\"\n",
    "    \n",
    "    if normalized_key in datasets:\n",
    "        df = datasets[normalized_key]\n",
    "        \n",
    "        if dataset_name == 'IMDB':\n",
    "            text_col = 'review_cleaned'\n",
    "            stem_col = 'review_stemmed'\n",
    "            lemma_col = 'review_lemmatized'\n",
    "        else:\n",
    "            text_col = 'text_cleaned'\n",
    "            stem_col = 'text_stemmed'\n",
    "            lemma_col = 'text_lemmatized'\n",
    "        \n",
    "        # Calculate vocabulary reduction\n",
    "        def get_vocab_size(series):\n",
    "            all_words = set()\n",
    "            for text in series:\n",
    "                if pd.notna(text) and text != \"\":\n",
    "                    all_words.update(text.split())\n",
    "            return len(all_words)\n",
    "        \n",
    "        # Get sample for vocabulary analysis (first 100 entries for performance)\n",
    "        sample_df = df.head(100)\n",
    "        \n",
    "        original_vocab = get_vocab_size(sample_df[text_col])\n",
    "        stemmed_vocab = get_vocab_size(sample_df[stem_col])\n",
    "        lemmatized_vocab = get_vocab_size(sample_df[lemma_col])\n",
    "        \n",
    "        stem_vocab_reduction = ((original_vocab - stemmed_vocab) / original_vocab * 100)\n",
    "        lemma_vocab_reduction = ((original_vocab - lemmatized_vocab) / original_vocab * 100)\n",
    "        \n",
    "        # Average lengths\n",
    "        clean_len = df[text_col].str.len().mean()\n",
    "        stem_len = df[stem_col].str.len().mean()\n",
    "        lemma_len = df[lemma_col].str.len().mean()\n",
    "        \n",
    "        result = {\n",
    "            'dataset': dataset_name,\n",
    "            'original_vocab': original_vocab,\n",
    "            'stemmed_vocab': stemmed_vocab,\n",
    "            'lemmatized_vocab': lemmatized_vocab,\n",
    "            'stem_vocab_reduction': stem_vocab_reduction,\n",
    "            'lemma_vocab_reduction': lemma_vocab_reduction,\n",
    "            'clean_avg_length': clean_len,\n",
    "            'stem_avg_length': stem_len,\n",
    "            'lemma_avg_length': lemma_len\n",
    "        }\n",
    "        \n",
    "        analysis_results.append(result)\n",
    "        \n",
    "        print(f\"\\nüìã {dataset_name} Dataset Analysis:\")\n",
    "        print(f\"   Vocabulary Size (sample):\")\n",
    "        print(f\"     ‚Ä¢ Original: {original_vocab:,} unique words\")\n",
    "        print(f\"     ‚Ä¢ Stemmed: {stemmed_vocab:,} unique words (‚Üì{stem_vocab_reduction:.1f}%)\")\n",
    "        print(f\"     ‚Ä¢ Lemmatized: {lemmatized_vocab:,} unique words (‚Üì{lemma_vocab_reduction:.1f}%)\")\n",
    "        print(f\"   Average Text Length:\")\n",
    "        print(f\"     ‚Ä¢ Cleaned: {clean_len:.0f} characters\")\n",
    "        print(f\"     ‚Ä¢ Stemmed: {stem_len:.0f} characters\")\n",
    "        print(f\"     ‚Ä¢ Lemmatized: {lemma_len:.0f} characters\")\n",
    "\n",
    "print(f\"\\n‚úÖ Normalization analysis completed for {len(analysis_results)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "830d2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving normalized datasets...\n",
      "‚úÖ BBC normalized dataset saved: 2225 articles\n",
      "‚úÖ CNN normalized dataset saved: 5000 articles\n",
      "‚úÖ IMDB normalized dataset saved: 1000 reviews\n",
      "‚úÖ Normalization metadata saved: ../data/normalized\\normalization_metadata.json\n",
      "üéØ Day 10-11 objectives completed successfully!\n",
      "üìÅ Files ready for next phase: Topic modeling and sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "# Save normalized datasets\n",
    "print(\"üíæ Saving normalized datasets...\")\n",
    "\n",
    "# Create normalized data directory\n",
    "normalized_dir = \"../data/normalized\"\n",
    "os.makedirs(normalized_dir, exist_ok=True)\n",
    "\n",
    "saved_normalized_files = []\n",
    "\n",
    "# Save BBC normalized dataset\n",
    "if 'BBC_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"bbc_news_normalized.csv\")\n",
    "    datasets['BBC_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"BBC: {filepath}\")\n",
    "    print(f\"‚úÖ BBC normalized dataset saved: {len(datasets['BBC_normalized'])} articles\")\n",
    "\n",
    "# Save CNN normalized dataset\n",
    "if 'CNN_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"cnn_dailymail_normalized.csv\")\n",
    "    datasets['CNN_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"CNN: {filepath}\")\n",
    "    print(f\"‚úÖ CNN normalized dataset saved: {len(datasets['CNN_normalized'])} articles\")\n",
    "\n",
    "# Save IMDB normalized dataset\n",
    "if 'IMDB_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"imdb_reviews_normalized.csv\")\n",
    "    datasets['IMDB_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"IMDB: {filepath}\")\n",
    "    print(f\"‚úÖ IMDB normalized dataset saved: {len(datasets['IMDB_normalized'])} reviews\")\n",
    "\n",
    "# Update metadata with normalization information\n",
    "normalization_metadata = {\n",
    "    'normalization_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'day': 'Day 10-11',\n",
    "    'objective': 'Text normalization: stemming and lemmatization',\n",
    "    'methods_used': {\n",
    "        'stemming': 'Porter Stemmer',\n",
    "        'lemmatization': 'WordNet Lemmatizer with POS tagging'\n",
    "    },\n",
    "    'files_created': saved_normalized_files,\n",
    "    'analysis_results': analysis_results,\n",
    "    'next_step': 'Topic modeling and sentiment analysis with normalized text',\n",
    "    'recommendations': {\n",
    "        'stemming': 'Faster processing, good for large-scale analysis',\n",
    "        'lemmatization': 'Better semantic preservation, recommended for accuracy'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save normalization metadata\n",
    "metadata_path = os.path.join(normalized_dir, \"normalization_metadata.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(normalization_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Normalization metadata saved: {metadata_path}\")\n",
    "print(f\"üéØ Day 10-11 objectives completed successfully!\")\n",
    "print(f\"üìÅ Files ready for next phase: Topic modeling and sentiment analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
