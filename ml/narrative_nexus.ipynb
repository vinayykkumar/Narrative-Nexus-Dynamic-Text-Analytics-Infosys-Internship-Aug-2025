{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab22dbb",
   "metadata": {},
   "source": [
    "# NarrativeNexus\n",
    "\n",
    "**Pipeline:** Text Cleaning ‚Üí Normalization ‚Üí Tokenization ‚Üí Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18aec7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Download NLTK data\n",
    "for item in ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger']:\n",
    "    nltk.download(item, quiet=True)\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3e2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Functions defined\n"
     ]
    }
   ],
   "source": [
    "# Core preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text: lowercase, remove special chars, remove stop words\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean and tokenize\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words (keep negations)\n",
    "    keep_words = {'not', 'no', 'never'}\n",
    "    filtered_stops = stop_words - keep_words\n",
    "    tokens = [word for word in tokens if word not in filtered_stops and word.isalpha()]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize text with POS tagging\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    tokens = word_tokenize(str(text))\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Convert POS tags for lemmatizer\n",
    "    def get_pos(tag):\n",
    "        if tag.startswith('V'): return 'v'\n",
    "        elif tag.startswith('N'): return 'n'\n",
    "        elif tag.startswith('R'): return 'r'\n",
    "        elif tag.startswith('J'): return 'a'\n",
    "        return 'n'\n",
    "    \n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_pos(pos)) for word, pos in pos_tags]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Simple word tokenization\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    return word_tokenize(str(text))\n",
    "\n",
    "print(\"‚úÖ Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc7cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading datasets...\n",
      "‚úÖ BBC: 2,225 documents\n",
      "‚úÖ CNN: 5,000 documents\n",
      "‚úÖ IMDB: 1,000 documents\n",
      "\n",
      "üìä Total: 8,225 documents\n",
      "‚úÖ BBC: 2,225 documents\n",
      "‚úÖ CNN: 5,000 documents\n",
      "‚úÖ IMDB: 1,000 documents\n",
      "\n",
      "üìä Total: 8,225 documents\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "\n",
    "datasets = {}\n",
    "try:\n",
    "    datasets['BBC'] = pd.read_csv(\"../data/bbc-text.csv\")\n",
    "    datasets['CNN'] = pd.read_csv(\"../data/cnn_dailymail.csv\")\n",
    "    datasets['IMDB'] = pd.read_csv(\"../data/imdb-dataset.csv\", nrows=1000)\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        print(f\"‚úÖ {name}: {len(df):,} documents\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total: {sum(len(df) for df in datasets.values()):,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b51155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing datasets...\n",
      "\n",
      "Processing BBC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:03<00:00, 613.70it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:25<00:00, 87.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Avg tokens: 220, Vocabulary: 23,122\n",
      "\n",
      "Processing CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:06<00:00, 831.17it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:16<00:00, 65.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Avg tokens: 345, Vocabulary: 59,444\n",
      "\n",
      "Processing IMDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1324.64it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05<00:00, 174.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Avg tokens: 124, Vocabulary: 14,619\n",
      "\n",
      "‚úÖ All datasets processed\n"
     ]
    }
   ],
   "source": [
    "# Process all datasets\n",
    "print(\"üîÑ Processing datasets...\\n\")\n",
    "\n",
    "processed = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    \n",
    "    # Get text column\n",
    "    text_col = 'review' if name == 'IMDB' else ('article' if 'article' in df.columns else 'text')\n",
    "    \n",
    "    # Apply pipeline\n",
    "    df_processed = df.copy()\n",
    "    df_processed['cleaned'] = df[text_col].progress_apply(clean_text)\n",
    "    df_processed['lemmatized'] = df_processed['cleaned'].progress_apply(lemmatize_text)\n",
    "    df_processed['tokens'] = df_processed['lemmatized'].apply(tokenize_text)\n",
    "    df_processed['token_count'] = df_processed['tokens'].apply(len)\n",
    "    \n",
    "    # Stats\n",
    "    avg_tokens = df_processed['token_count'].mean()\n",
    "    vocab_size = len(set([token for tokens in df_processed['tokens'] for token in tokens]))\n",
    "    \n",
    "    print(f\"  ‚úÖ Avg tokens: {avg_tokens:.0f}, Vocabulary: {vocab_size:,}\\n\")\n",
    "    \n",
    "    processed[name] = df_processed\n",
    "\n",
    "print(\"‚úÖ All datasets processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edae82b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã PREPROCESSING SUMMARY\n",
      "========================================\n",
      "\n",
      "BBC:\n",
      "  ‚Ä¢ Documents: 2,225\n",
      "  ‚Ä¢ Avg tokens: 220\n",
      "  ‚Ä¢ Vocabulary: 23,122\n",
      "  ‚Ä¢ Status: ‚úÖ Preprocessed\n",
      "\n",
      "CNN:\n",
      "  ‚Ä¢ Documents: 5,000\n",
      "  ‚Ä¢ Avg tokens: 345\n",
      "  ‚Ä¢ Vocabulary: 59,444\n",
      "  ‚Ä¢ Status: ‚úÖ Preprocessed\n",
      "\n",
      "CNN:\n",
      "  ‚Ä¢ Documents: 5,000\n",
      "  ‚Ä¢ Avg tokens: 345\n",
      "  ‚Ä¢ Vocabulary: 59,444\n",
      "  ‚Ä¢ Status: ‚úÖ Preprocessed\n",
      "\n",
      "IMDB:\n",
      "  ‚Ä¢ Documents: 1,000\n",
      "  ‚Ä¢ Avg tokens: 124\n",
      "  ‚Ä¢ Vocabulary: 14,619\n",
      "  ‚Ä¢ Status: ‚úÖ Preprocessed\n",
      "\n",
      "üéØ TOTALS:\n",
      "  ‚Ä¢ Total documents: 8,225\n",
      "  ‚Ä¢ Total vocabulary: 69,131\n",
      "\n",
      "IMDB:\n",
      "  ‚Ä¢ Documents: 1,000\n",
      "  ‚Ä¢ Avg tokens: 124\n",
      "  ‚Ä¢ Vocabulary: 14,619\n",
      "  ‚Ä¢ Status: ‚úÖ Preprocessed\n",
      "\n",
      "üéØ TOTALS:\n",
      "  ‚Ä¢ Total documents: 8,225\n",
      "  ‚Ä¢ Total vocabulary: 69,131\n"
     ]
    }
   ],
   "source": [
    "# Summary and export\n",
    "print(\"üìã PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "total_docs = 0\n",
    "total_vocab = set()\n",
    "\n",
    "for name, df in processed.items():\n",
    "    docs = len(df)\n",
    "    avg_tokens = df['token_count'].mean()\n",
    "    vocab = set([token for tokens in df['tokens'] for token in tokens])\n",
    "    \n",
    "    total_docs += docs\n",
    "    total_vocab.update(vocab)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  ‚Ä¢ Documents: {docs:,}\")\n",
    "    print(f\"  ‚Ä¢ Avg tokens: {avg_tokens:.0f}\")\n",
    "    print(f\"  ‚Ä¢ Vocabulary: {len(vocab):,}\")\n",
    "    print(f\"  ‚Ä¢ Status: ‚úÖ Preprocessed\")\n",
    "\n",
    "print(f\"\\nüéØ TOTALS:\")\n",
    "print(f\"  ‚Ä¢ Total documents: {total_docs:,}\")\n",
    "print(f\"  ‚Ä¢ Total vocabulary: {len(total_vocab):,}\")\n",
    "\n",
    "# Quick export\n",
    "import os\n",
    "os.makedirs(\"../data/final\", exist_ok=True)\n",
    "\n",
    "for name, df in processed.items():\n",
    "    # Keep only essential columns for Week 2 completion\n",
    "    export_df = df[['cleaned', 'lemmatized', 'tokens', 'token_count']].copy()\n",
    "    export_df.to_csv(f\"../data/final/{name.lower()}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f7dc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Topic Modeling Implementation\n",
      "========================================\n",
      "\n",
      "üìä BBC Dataset:\n",
      "  LDA Topics:\n",
      "    Topic 1: mr, government, year, labour, party\n",
      "    Topic 2: game, use, people, make, player\n",
      "    Topic 3: year, film, company, best, market\n",
      "  NMF Topics:\n",
      "    Topic 1: mr, government, labour, election, people\n",
      "    Topic 2: game, play, win, player, england\n",
      "    Topic 3: film, award, best, star, actor\n",
      "\n",
      "üìä CNN Dataset:\n",
      "  LDA Topics:\n",
      "    Topic 1: people, year, like, make, time\n",
      "    Topic 2: obama, new, president, year, make\n",
      "    Topic 3: state, government, report, official, people\n",
      "  NMF Topics:\n",
      "    Topic 1: like, year, think, make, time\n",
      "    Topic 2: police, report, kill, official, government\n",
      "    Topic 3: obama, president, clinton, bush, mccain\n",
      "\n",
      "üìä IMDB Dataset:\n",
      "  LDA Topics:\n",
      "    Topic 1: movie, film, like, great, make\n",
      "    Topic 2: movie, br, bad, like, make\n",
      "    Topic 3: br, film, make, character, like\n",
      "  NMF Topics:\n",
      "    Topic 1: br, character, know, really, episode\n",
      "    Topic 2: movie, bad, watch, like, good\n",
      "    Topic 3: film, make, character, like, story\n",
      "\n",
      "‚úÖ Topic modeling complete!\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling Implementation (LDA & NMF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "print(\"üîç Topic Modeling Implementation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Prepare data for topic modeling\n",
    "for name, df in processed.items():\n",
    "    print(f\"\\nüìä {name} Dataset:\")\n",
    "    \n",
    "    # Prepare documents\n",
    "    documents = df['lemmatized'].fillna('')\n",
    "    \n",
    "    # LDA Implementation\n",
    "    count_vectorizer = CountVectorizer(max_features=500, min_df=2, max_df=0.8, stop_words='english')\n",
    "    count_matrix = count_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "    lda.fit(count_matrix)\n",
    "    \n",
    "    print(\"  LDA Topics:\")\n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[-5:][::-1]]\n",
    "        print(f\"    Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # NMF Implementation  \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=500, min_df=2, max_df=0.8, stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    nmf = NMF(n_components=3, random_state=42)\n",
    "    nmf.fit(tfidf_matrix)\n",
    "    \n",
    "    print(\"  NMF Topics:\")\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[-5:][::-1]]\n",
    "        print(f\"    Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Topic modeling complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
