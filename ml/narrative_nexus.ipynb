{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcb01a8",
   "metadata": {},
   "source": [
    "## NarrativeNexus Project: Text Cleaning Implementation\n",
    "\n",
    "**Objectives:**\n",
    "- Remove special characters, punctuation, and stop words\n",
    "- Apply preprocessing to BBC, CNN/DailyMail, and IMDB datasets\n",
    "- Save cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ed395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Loaded 198 stop words.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"‚úÖ Setup complete. Loaded {len(stop_words)} stop words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c59571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Define text cleaning functions\n",
    "def clean_special_characters(text):\n",
    "    \"\"\"Remove special characters, keep only letters, numbers, and spaces\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Remove special characters\n",
    "    cleaned = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def remove_stop_words(text, keep_negations=True):\n",
    "    \"\"\"Remove stop words while preserving negations\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Keep important negation words\n",
    "    stop_words_filtered = stop_words.copy()\n",
    "    if keep_negations:\n",
    "        important_words = {'not', 'no', 'never', 'none', 'neither', 'nobody', 'nothing'}\n",
    "        stop_words_filtered = stop_words_filtered - important_words\n",
    "    \n",
    "    # Tokenize and filter\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words_filtered]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def clean_text_pipeline(text):\n",
    "    \"\"\"Complete text cleaning pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Convert to lowercase\n",
    "    cleaned = str(text).lower()\n",
    "    \n",
    "    # Step 2: Remove special characters\n",
    "    cleaned = clean_special_characters(cleaned)\n",
    "    \n",
    "    # Step 3: Remove stop words\n",
    "    cleaned = remove_stop_words(cleaned, keep_negations=True)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"‚úÖ Text cleaning functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c64bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC Dataset: 2225 articles loaded\n",
      "‚úÖ CNN Dataset: 5000 articles loaded\n",
      "‚úÖ IMDB Dataset: 1000 reviews loaded\n",
      "\n",
      "üìä Total datasets loaded: 3\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_dir = \"../data\"\n",
    "datasets = {}\n",
    "\n",
    "# Load BBC News Dataset\n",
    "try:\n",
    "    bbc_df = pd.read_csv(f\"{data_dir}/bbc-text.csv\")\n",
    "    datasets['BBC'] = bbc_df\n",
    "    print(f\"‚úÖ BBC Dataset: {len(bbc_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading BBC dataset: {e}\")\n",
    "\n",
    "# Load CNN/DailyMail Dataset\n",
    "try:\n",
    "    cnn_df = pd.read_csv(f\"{data_dir}/cnn_dailymail.csv\")\n",
    "    datasets['CNN'] = cnn_df\n",
    "    print(f\"‚úÖ CNN Dataset: {len(cnn_df)} articles loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CNN dataset: {e}\")\n",
    "\n",
    "# Load IMDB Dataset (subset for demo)\n",
    "try:\n",
    "    imdb_df = pd.read_csv(f\"{data_dir}/imdb-dataset.csv\", nrows=1000)\n",
    "    datasets['IMDB'] = imdb_df\n",
    "    print(f\"‚úÖ IMDB Dataset: {len(imdb_df)} reviews loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading IMDB dataset: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "921f4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning BBC News Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing BBC: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:02<00:00, 1064.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2263 characters\n",
      "   ‚Ä¢ Cleaned avg length: 1584 characters\n",
      "   ‚Ä¢ Reduction: 30.0%\n",
      "‚úÖ BBC cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean BBC News Dataset\n",
    "if 'BBC' in datasets:\n",
    "    print(\"üßπ Cleaning BBC News Dataset...\")\n",
    "    bbc_df = datasets['BBC'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing BBC\")\n",
    "    bbc_df['text_cleaned'] = bbc_df['text'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = bbc_df['text'].str.len().mean()\n",
    "    cleaned_avg = bbc_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['BBC_cleaned'] = bbc_df\n",
    "    print(\"‚úÖ BBC cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå BBC dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f73a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning CNN/DailyMail Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CNN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:04<00:00, 1006.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 2518 characters\n",
      "   ‚Ä¢ Cleaned avg length: 2518 characters\n",
      "   ‚Ä¢ Reduction: 0.0%\n",
      "‚úÖ CNN cleaning completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean CNN/DailyMail Dataset\n",
    "if 'CNN' in datasets:\n",
    "    print(\"üßπ Cleaning CNN/DailyMail Dataset...\")\n",
    "    cnn_df = datasets['CNN'].copy()\n",
    "    \n",
    "    # Identify text column\n",
    "    text_column = 'article' if 'article' in cnn_df.columns else 'text'\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing CNN\")\n",
    "    cnn_df['text_cleaned'] = cnn_df[text_column].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = cnn_df[text_column].str.len().mean()\n",
    "    cleaned_avg = cnn_df['text_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['CNN_cleaned'] = cnn_df\n",
    "    print(\"‚úÖ CNN cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå CNN dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf1f2884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning IMDB Reviews Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IMDB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1660.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Original avg length: 1311 characters\n",
      "   ‚Ä¢ Cleaned avg length: 839 characters\n",
      "   ‚Ä¢ Reduction: 36.0%\n",
      "‚úÖ IMDB cleaning completed\n"
     ]
    }
   ],
   "source": [
    "# Clean IMDB Dataset\n",
    "if 'IMDB' in datasets:\n",
    "    print(\"üßπ Cleaning IMDB Reviews Dataset...\")\n",
    "    imdb_df = datasets['IMDB'].copy()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    tqdm.pandas(desc=\"Processing IMDB\")\n",
    "    imdb_df['review_cleaned'] = imdb_df['review'].progress_apply(clean_text_pipeline)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_avg = imdb_df['review'].str.len().mean()\n",
    "    cleaned_avg = imdb_df['review_cleaned'].str.len().mean()\n",
    "    reduction = ((original_avg - cleaned_avg) / original_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Original avg length: {original_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    datasets['IMDB_cleaned'] = imdb_df\n",
    "    print(\"‚úÖ IMDB cleaning completed\")\n",
    "else:\n",
    "    print(\"‚ùå IMDB dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8014c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BBC dataset saved: 2225 articles\n",
      "‚úÖ CNN dataset saved: 5000 articles\n",
      "‚úÖ IMDB dataset saved: 1000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned datasets\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create cleaned data directory\n",
    "cleaned_dir = \"../data/cleaned\"\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "# Save BBC cleaned dataset\n",
    "if 'BBC_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"bbc_news_cleaned.csv\")\n",
    "    datasets['BBC_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"BBC: {filepath}\")\n",
    "    print(f\"‚úÖ BBC dataset saved: {len(datasets['BBC_cleaned'])} articles\")\n",
    "\n",
    "# Save CNN cleaned dataset\n",
    "if 'CNN_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"cnn_dailymail_cleaned.csv\")\n",
    "    datasets['CNN_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"CNN: {filepath}\")\n",
    "    print(f\"‚úÖ CNN dataset saved: {len(datasets['CNN_cleaned'])} articles\")\n",
    "\n",
    "# Save IMDB cleaned dataset\n",
    "if 'IMDB_cleaned' in datasets:\n",
    "    filepath = os.path.join(cleaned_dir, \"imdb_reviews_cleaned.csv\")\n",
    "    datasets['IMDB_cleaned'].to_csv(filepath, index=False)\n",
    "    saved_files.append(f\"IMDB: {filepath}\")\n",
    "    print(f\"‚úÖ IMDB dataset saved: {len(datasets['IMDB_cleaned'])} reviews\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'cleaning_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'day': 'Day 8-9',\n",
    "    'objective': 'Text cleaning: remove special characters, punctuation, stop words',\n",
    "    'files_created': saved_files,\n",
    "    'next_step': 'Week 3: Topic modeling with LDA/NMF'\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(cleaned_dir, \"preprocessing_metadata.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3be2cb",
   "metadata": {},
   "source": [
    "## Day 10-11: Text Normalization with Stemming and Lemmatization\n",
    "\n",
    "**Objectives:**\n",
    "- Implement stemming using Porter Stemmer\n",
    "- Implement lemmatization using WordNet Lemmatizer\n",
    "- Compare performance between stemming and lemmatization\n",
    "- Apply normalization to all cleaned datasets\n",
    "- Analyze the impact of normalization on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6126a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stemming and Lemmatization tools initialized\n",
      "üìù Porter Stemmer ready\n",
      "üìù WordNet Lemmatizer ready\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries for stemming and lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "import time\n",
    "\n",
    "# Download additional NLTK data\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"‚úÖ Stemming and Lemmatization tools initialized\")\n",
    "print(f\"üìù Porter Stemmer ready\")\n",
    "print(f\"üìù WordNet Lemmatizer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe2f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text normalization functions defined\n",
      "üìã Available methods: 'stem' (Porter Stemmer), 'lemma' (WordNet Lemmatizer)\n"
     ]
    }
   ],
   "source": [
    "# Define normalization functions\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert TreeBank POS tags to WordNet POS tags for better lemmatization\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default to noun\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Apply Porter Stemming to text\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Apply Lemmatization with POS tagging to text\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter only alphabetic tokens\n",
    "    alpha_tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Get POS tags\n",
    "    pos_tags = pos_tag(alpha_tokens)\n",
    "    \n",
    "    # Lemmatize with appropriate POS tags\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(pos)) \n",
    "        for token, pos in pos_tags\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def normalize_text_pipeline(text, method='lemma'):\n",
    "    \"\"\"Complete text normalization pipeline with stemming or lemmatization\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if method == 'stem':\n",
    "        return stem_text(text)\n",
    "    elif method == 'lemma':\n",
    "        return lemmatize_text(text)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'stem' or 'lemma'\")\n",
    "\n",
    "print(\"‚úÖ Text normalization functions defined\")\n",
    "print(\"üìã Available methods: 'stem' (Porter Stemmer), 'lemma' (WordNet Lemmatizer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "910833b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Stemming vs Lemmatization Comparison:\n",
      "Original text: The children were running and jumping happily in the beautiful gardens while their parents were watching\n",
      "\n",
      "Stemmed text: the children were run and jump happili in the beauti garden while their parent were watch\n",
      "Lemmatized text: the child be run and jump happily in the beautiful garden while their parent be watch\n",
      "\n",
      "üìä Key Differences:\n",
      "‚Ä¢ Stemming: Faster, rule-based, may create non-words\n",
      "‚Ä¢ Lemmatization: Slower, dictionary-based, preserves valid words\n",
      "‚Ä¢ Lemmatization with POS: Most accurate, context-aware\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate stemming vs lemmatization\n",
    "sample_text = \"The children were running and jumping happily in the beautiful gardens while their parents were watching\"\n",
    "\n",
    "print(\"üîç Stemming vs Lemmatization Comparison:\")\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed = stem_text(sample_text)\n",
    "print(f\"Stemmed text: {stemmed}\")\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized = lemmatize_text(sample_text)\n",
    "print(f\"Lemmatized text: {lemmatized}\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Key Differences:\")\n",
    "print(\"‚Ä¢ Stemming: Faster, rule-based, may create non-words\")\n",
    "print(\"‚Ä¢ Lemmatization: Slower, dictionary-based, preserves valid words\")\n",
    "print(\"‚Ä¢ Lemmatization with POS: Most accurate, context-aware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41407199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing BBC News Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BBC Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:07<00:00, 278.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BBC Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2225/2225 [00:21<00:00, 101.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 1584 characters\n",
      "   ‚Ä¢ Stemmed avg length: 1368 characters (‚Üì13.6%)\n",
      "   ‚Ä¢ Lemmatized avg length: 1471 characters (‚Üì7.1%)\n",
      "‚úÖ BBC normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to BBC News Dataset\n",
    "if 'BBC_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing BBC News Dataset...\")\n",
    "    bbc_df = datasets['BBC_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"BBC Stemming\")\n",
    "    bbc_df['text_stemmed'] = bbc_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"BBC Lemmatization\")\n",
    "    bbc_df['text_lemmatized'] = bbc_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = bbc_df['text_cleaned'].str.len().mean()\n",
    "    stemmed_avg = bbc_df['text_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = bbc_df['text_lemmatized'].str.len().mean()\n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['BBC_normalized'] = bbc_df\n",
    "    print(\"‚úÖ BBC normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå BBC cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f83c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing CNN/DailyMail Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:28<00:00, 172.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CNN Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:12<00:00, 69.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 2518 characters\n",
      "   ‚Ä¢ Stemmed avg length: 2210 characters (‚Üì12.2%)\n",
      "   ‚Ä¢ Lemmatized avg length: 2387 characters (‚Üì5.2%)\n",
      "‚úÖ CNN normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to CNN/DailyMail Dataset\n",
    "if 'CNN_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing CNN/DailyMail Dataset...\")\n",
    "    cnn_df = datasets['CNN_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"CNN Stemming\")\n",
    "    cnn_df['text_stemmed'] = cnn_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"CNN Lemmatization\")\n",
    "    cnn_df['text_lemmatized'] = cnn_df['text_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = cnn_df['text_cleaned'].str.len().mean()\n",
    "    stemmed_avg = cnn_df['text_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = cnn_df['text_lemmatized'].str.len().mean()\n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['CNN_normalized'] = cnn_df\n",
    "    print(\"‚úÖ CNN normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå CNN cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e9e59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizing IMDB Reviews Dataset...\n",
      "   üìù Applying stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMDB Stemming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 520.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Applying lemmatization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMDB Lemmatization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05<00:00, 187.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚Ä¢ Cleaned avg length: 839 characters\n",
      "   ‚Ä¢ Stemmed avg length: 740 characters (‚Üì11.7%)\n",
      "   ‚Ä¢ Lemmatized avg length: 796 characters (‚Üì5.1%)\n",
      "‚úÖ IMDB normalization completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply normalization to IMDB Reviews Dataset\n",
    "if 'IMDB_cleaned' in datasets:\n",
    "    print(\"üîÑ Normalizing IMDB Reviews Dataset...\")\n",
    "    imdb_df = datasets['IMDB_cleaned'].copy()\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    print(\"   üìù Applying stemming...\")\n",
    "    tqdm.pandas(desc=\"IMDB Stemming\")\n",
    "    imdb_df['review_stemmed'] = imdb_df['review_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='stem')\n",
    "    )\n",
    "    \n",
    "    print(\"   üìù Applying lemmatization...\")\n",
    "    tqdm.pandas(desc=\"IMDB Lemmatization\")\n",
    "    imdb_df['review_lemmatized'] = imdb_df['review_cleaned'].progress_apply(\n",
    "        lambda x: normalize_text_pipeline(x, method='lemma')\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cleaned_avg = imdb_df['review_cleaned'].str.len().mean()\n",
    "    stemmed_avg = imdb_df['review_stemmed'].str.len().mean()\n",
    "    lemmatized_avg = imdb_df['review_lemmatized'].str.len().mean()\n",
    "    \n",
    "    \n",
    "    stem_reduction = ((cleaned_avg - stemmed_avg) / cleaned_avg * 100)\n",
    "    lemma_reduction = ((cleaned_avg - lemmatized_avg) / cleaned_avg * 100)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_avg:.0f} characters\")\n",
    "    print(f\"   ‚Ä¢ Stemmed avg length: {stemmed_avg:.0f} characters (‚Üì{stem_reduction:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Lemmatized avg length: {lemmatized_avg:.0f} characters (‚Üì{lemma_reduction:.1f}%)\")\n",
    "    \n",
    "    datasets['IMDB_normalized'] = imdb_df\n",
    "    print(\"‚úÖ IMDB normalization completed\")\n",
    "else:\n",
    "    print(\"‚ùå IMDB cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a7c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Normalization Performance Summary\n",
      "============================================================\n",
      "\n",
      "üìã BBC Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 5,947 unique words\n",
      "     ‚Ä¢ Stemmed: 4,235 unique words (‚Üì28.8%)\n",
      "     ‚Ä¢ Lemmatized: 4,669 unique words (‚Üì21.5%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 1584 characters\n",
      "     ‚Ä¢ Stemmed: 1368 characters\n",
      "     ‚Ä¢ Lemmatized: 1471 characters\n",
      "\n",
      "üìã CNN Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 8,999 unique words\n",
      "     ‚Ä¢ Stemmed: 6,506 unique words (‚Üì27.7%)\n",
      "     ‚Ä¢ Lemmatized: 7,408 unique words (‚Üì17.7%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 2518 characters\n",
      "     ‚Ä¢ Stemmed: 2210 characters\n",
      "     ‚Ä¢ Lemmatized: 2387 characters\n",
      "\n",
      "üìã IMDB Dataset Analysis:\n",
      "   Vocabulary Size (sample):\n",
      "     ‚Ä¢ Original: 4,648 unique words\n",
      "     ‚Ä¢ Stemmed: 3,641 unique words (‚Üì21.7%)\n",
      "     ‚Ä¢ Lemmatized: 3,914 unique words (‚Üì15.8%)\n",
      "   Average Text Length:\n",
      "     ‚Ä¢ Cleaned: 839 characters\n",
      "     ‚Ä¢ Stemmed: 740 characters\n",
      "     ‚Ä¢ Lemmatized: 796 characters\n",
      "\n",
      "‚úÖ Normalization analysis completed for 3 datasets\n"
     ]
    }
   ],
   "source": [
    "# Comparative Analysis of Normalization Results\n",
    "print(\"üìä Normalization Performance Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "# Analyze each dataset\n",
    "for dataset_name in ['BBC', 'CNN', 'IMDB']:\n",
    "    normalized_key = f\"{dataset_name}_normalized\"\n",
    "    \n",
    "    if normalized_key in datasets:\n",
    "        df = datasets[normalized_key]\n",
    "        \n",
    "        if dataset_name == 'IMDB':\n",
    "            text_col = 'review_cleaned'\n",
    "            stem_col = 'review_stemmed'\n",
    "            lemma_col = 'review_lemmatized'\n",
    "        else:\n",
    "            text_col = 'text_cleaned'\n",
    "            stem_col = 'text_stemmed'\n",
    "            lemma_col = 'text_lemmatized'\n",
    "        \n",
    "        # Calculate vocabulary reduction\n",
    "        def get_vocab_size(series):\n",
    "            all_words = set()\n",
    "            for text in series:\n",
    "                if pd.notna(text) and text != \"\":\n",
    "                    all_words.update(text.split())\n",
    "            return len(all_words)\n",
    "        \n",
    "        # Get sample for vocabulary analysis (first 100 entries for performance)\n",
    "        sample_df = df.head(100)\n",
    "        \n",
    "        original_vocab = get_vocab_size(sample_df[text_col])\n",
    "        stemmed_vocab = get_vocab_size(sample_df[stem_col])\n",
    "        lemmatized_vocab = get_vocab_size(sample_df[lemma_col])\n",
    "        \n",
    "        stem_vocab_reduction = ((original_vocab - stemmed_vocab) / original_vocab * 100)\n",
    "        lemma_vocab_reduction = ((original_vocab - lemmatized_vocab) / original_vocab * 100)\n",
    "        \n",
    "        # Average lengths\n",
    "        clean_len = df[text_col].str.len().mean()\n",
    "        stem_len = df[stem_col].str.len().mean()\n",
    "        lemma_len = df[lemma_col].str.len().mean()\n",
    "        \n",
    "        result = {\n",
    "            'dataset': dataset_name,\n",
    "            'original_vocab': original_vocab,\n",
    "            'stemmed_vocab': stemmed_vocab,\n",
    "            'lemmatized_vocab': lemmatized_vocab,\n",
    "            'stem_vocab_reduction': stem_vocab_reduction,\n",
    "            'lemma_vocab_reduction': lemma_vocab_reduction,\n",
    "            'clean_avg_length': clean_len,\n",
    "            'stem_avg_length': stem_len,\n",
    "            'lemma_avg_length': lemma_len\n",
    "        }\n",
    "        \n",
    "        analysis_results.append(result)\n",
    "        \n",
    "        print(f\"\\nüìã {dataset_name} Dataset Analysis:\")\n",
    "        print(f\"   Vocabulary Size (sample):\")\n",
    "        print(f\"     ‚Ä¢ Original: {original_vocab:,} unique words\")\n",
    "        print(f\"     ‚Ä¢ Stemmed: {stemmed_vocab:,} unique words (‚Üì{stem_vocab_reduction:.1f}%)\")\n",
    "        print(f\"     ‚Ä¢ Lemmatized: {lemmatized_vocab:,} unique words (‚Üì{lemma_vocab_reduction:.1f}%)\")\n",
    "        print(f\"   Average Text Length:\")\n",
    "        print(f\"     ‚Ä¢ Cleaned: {clean_len:.0f} characters\")\n",
    "        print(f\"     ‚Ä¢ Stemmed: {stem_len:.0f} characters\")\n",
    "        print(f\"     ‚Ä¢ Lemmatized: {lemma_len:.0f} characters\")\n",
    "\n",
    "print(f\"\\n‚úÖ Normalization analysis completed for {len(analysis_results)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "830d2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving normalized datasets...\n",
      "‚úÖ BBC normalized dataset saved: 2225 articles\n",
      "‚úÖ CNN normalized dataset saved: 5000 articles\n",
      "‚úÖ IMDB normalized dataset saved: 1000 reviews\n",
      "‚úÖ Normalization metadata saved: ../data/normalized\\normalization_metadata.json\n",
      "üéØ Day 10-11 objectives completed successfully!\n",
      "üìÅ Files ready for next phase: Topic modeling and sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "# Save normalized datasets\n",
    "print(\"üíæ Saving normalized datasets...\")\n",
    "\n",
    "# Create normalized data directory\n",
    "normalized_dir = \"../data/normalized\"\n",
    "os.makedirs(normalized_dir, exist_ok=True)\n",
    "\n",
    "saved_normalized_files = []\n",
    "\n",
    "# Save BBC normalized dataset\n",
    "if 'BBC_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"bbc_news_normalized.csv\")\n",
    "    datasets['BBC_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"BBC: {filepath}\")\n",
    "    print(f\"‚úÖ BBC normalized dataset saved: {len(datasets['BBC_normalized'])} articles\")\n",
    "\n",
    "# Save CNN normalized dataset\n",
    "if 'CNN_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"cnn_dailymail_normalized.csv\")\n",
    "    datasets['CNN_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"CNN: {filepath}\")\n",
    "    print(f\"‚úÖ CNN normalized dataset saved: {len(datasets['CNN_normalized'])} articles\")\n",
    "\n",
    "# Save IMDB normalized dataset\n",
    "if 'IMDB_normalized' in datasets:\n",
    "    filepath = os.path.join(normalized_dir, \"imdb_reviews_normalized.csv\")\n",
    "    datasets['IMDB_normalized'].to_csv(filepath, index=False)\n",
    "    saved_normalized_files.append(f\"IMDB: {filepath}\")\n",
    "    print(f\"‚úÖ IMDB normalized dataset saved: {len(datasets['IMDB_normalized'])} reviews\")\n",
    "\n",
    "# Update metadata with normalization information\n",
    "normalization_metadata = {\n",
    "    'normalization_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'day': 'Day 10-11',\n",
    "    'objective': 'Text normalization: stemming and lemmatization',\n",
    "    'methods_used': {\n",
    "        'stemming': 'Porter Stemmer',\n",
    "        'lemmatization': 'WordNet Lemmatizer with POS tagging'\n",
    "    },\n",
    "    'files_created': saved_normalized_files,\n",
    "    'analysis_results': analysis_results,\n",
    "    'next_step': 'Topic modeling and sentiment analysis with normalized text',\n",
    "    'recommendations': {\n",
    "        'stemming': 'Faster processing, good for large-scale analysis',\n",
    "        'lemmatization': 'Better semantic preservation, recommended for accuracy'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save normalization metadata\n",
    "metadata_path = os.path.join(normalized_dir, \"normalization_metadata.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(normalization_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Normalization metadata saved: {metadata_path}\")\n",
    "print(f\"üéØ Day 10-11 objectives completed successfully!\")\n",
    "print(f\"üìÅ Files ready for next phase: Topic modeling and sentiment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0cf7c",
   "metadata": {},
   "source": [
    "## NEXT STEP IS TO TOKENIZE THE DATASET\n",
    "**TOKENIZING EACH DATASET ONE BY ONE AND THEN ADDING THE SAME INTO A FOLDER NAMED TOKENIZED WITH AT LAST THERE SUMMARY INTO A JSON FILE FORMAT.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5600dd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è transformers not installed or no internet, skipping subword tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\subod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# download punkt for nltk if not already\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# huggingface optional (for subword tokenization)\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "except Exception as e:\n",
    "    hf_tokenizer = None\n",
    "    print(\"‚ö†Ô∏è transformers not installed or no internet, skipping subword tokenization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56fdb0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_level_tokens(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    return [t for t in word_tokenize(text) if t.strip()]\n",
    "\n",
    "def hf_encode(texts, tokenizer, max_len=256):\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    out = []\n",
    "    for i in range(len(texts)):\n",
    "        out.append({\n",
    "            \"input_ids\": enc[\"input_ids\"][i],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][i],\n",
    "        })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0b5deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df = datasets[\"BBC_normalized\"].copy()\n",
    "\n",
    "# word-level\n",
    "bbc_df[\"tokens_clean\"]      = bbc_df[\"text_cleaned\"].apply(word_level_tokens)\n",
    "bbc_df[\"tokens_stemmed\"]    = bbc_df[\"text_stemmed\"].apply(word_level_tokens)\n",
    "bbc_df[\"tokens_lemmatized\"] = bbc_df[\"text_lemmatized\"].apply(word_level_tokens)\n",
    "\n",
    "# optional: subword (only clean + lemma, not stemmed)\n",
    "if hf_tokenizer:\n",
    "    enc_clean = hf_encode(bbc_df[\"text_cleaned\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "    enc_lemma = hf_encode(bbc_df[\"text_lemmatized\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "\n",
    "    bbc_df[\"hf_clean_ids\"]       = [e[\"input_ids\"] for e in enc_clean]\n",
    "    bbc_df[\"hf_clean_mask\"]      = [e[\"attention_mask\"] for e in enc_clean]\n",
    "    bbc_df[\"hf_lemmatized_ids\"]  = [e[\"input_ids\"] for e in enc_lemma]\n",
    "    bbc_df[\"hf_lemmatized_mask\"] = [e[\"attention_mask\"] for e in enc_lemma]\n",
    "\n",
    "# save\n",
    "bbc_df.to_parquet(\"./tokenized/bbc_news_tokenized.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4501666",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = datasets[\"CNN_normalized\"].copy()\n",
    "\n",
    "cnn[\"tokens_clean\"]      = cnn[\"text_cleaned\"].apply(word_level_tokens)\n",
    "cnn[\"tokens_stemmed\"]    = cnn[\"text_stemmed\"].apply(word_level_tokens)\n",
    "cnn[\"tokens_lemmatized\"] = cnn[\"text_lemmatized\"].apply(word_level_tokens)\n",
    "\n",
    "if hf_tokenizer:\n",
    "    enc_clean = hf_encode(cnn[\"text_cleaned\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "    enc_lemma = hf_encode(cnn[\"text_lemmatized\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "\n",
    "    cnn[\"hf_clean_ids\"]       = [e[\"input_ids\"] for e in enc_clean]\n",
    "    cnn[\"hf_clean_mask\"]      = [e[\"attention_mask\"] for e in enc_clean]\n",
    "    cnn[\"hf_lemmatized_ids\"]  = [e[\"input_ids\"] for e in enc_lemma]\n",
    "    cnn[\"hf_lemmatized_mask\"] = [e[\"attention_mask\"] for e in enc_lemma]\n",
    "\n",
    "cnn.to_parquet(\"./tokenized/cnn_dailymail_tokenized.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "455563db",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets[\"IMDB_normalized\"].copy()\n",
    "\n",
    "imdb[\"tokens_clean\"]      = imdb[\"review_cleaned\"].apply(word_level_tokens)\n",
    "imdb[\"tokens_stemmed\"]    = imdb[\"review_stemmed\"].apply(word_level_tokens)\n",
    "imdb[\"tokens_lemmatized\"] = imdb[\"review_lemmatized\"].apply(word_level_tokens)\n",
    "\n",
    "if hf_tokenizer:\n",
    "    enc_clean = hf_encode(imdb[\"review_cleaned\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "    enc_lemma = hf_encode(imdb[\"review_lemmatized\"].fillna(\"\").astype(str).tolist(), hf_tokenizer)\n",
    "\n",
    "    imdb[\"hf_clean_ids\"]       = [e[\"input_ids\"] for e in enc_clean]\n",
    "    imdb[\"hf_clean_mask\"]      = [e[\"attention_mask\"] for e in enc_clean]\n",
    "    imdb[\"hf_lemmatized_ids\"]  = [e[\"input_ids\"] for e in enc_lemma]\n",
    "    imdb[\"hf_lemmatized_mask\"] = [e[\"attention_mask\"] for e in enc_lemma]\n",
    "\n",
    "imdb.to_parquet(\"./tokenized/imdb_reviews_tokenized.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa483d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['category', 'text', 'text_cleaned', 'text_stemmed', 'text_lemmatized',\n",
      "       'tokens_clean', 'tokens_stemmed', 'tokens_lemmatized'],\n",
      "      dtype='object')\n",
      "                                        text_cleaned  \\\n",
      "0  tv future hands viewers home theatre systems p...   \n",
      "1  worldcom boss left books alone former worldcom...   \n",
      "2  tigers wary farrell gamble leicester say not r...   \n",
      "\n",
      "                                   tokens_lemmatized  \n",
      "0  [tv, future, hand, viewer, home, theatre, syst...  \n",
      "1  [worldcom, bos, leave, book, alone, former, wo...  \n",
      "2  [tiger, wary, farrell, gamble, leicester, say,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"./tokenized/bbc_news_tokenized.parquet\")\n",
    "print(df.columns)\n",
    "print(df[[\"text_cleaned\",\"tokens_lemmatized\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50e943f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä BBC: {'tokens_clean': {'count': 2225, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}, 'tokens_stemmed': {'count': 2225, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}, 'tokens_lemmatized': {'count': 2225, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}}\n",
      "üìä CNN: {'tokens_clean': {'count': 5000, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}, 'tokens_stemmed': {'count': 5000, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}, 'tokens_lemmatized': {'count': 5000, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}}\n",
      "üìä IMDB: {'tokens_clean': {'count': 1000, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}, 'tokens_stemmed': {'count': 1000, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}, 'tokens_lemmatized': {'count': 1000, 'avg_len': 0.0, 'p50_len': 0.0, 'p90_len': 0.0, 'p95_len': 0.0, 'max_len': 0}}\n",
      "\n",
      "‚úÖ tokenization report saved to ./tokenized/tokenization_report.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def summarize_token_lengths(df, cols):\n",
    "    summary = {}\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            lengths = df[c].apply(\n",
    "                lambda x: len(x) if isinstance(x, (list, tuple)) else 0\n",
    "            )\n",
    "            summary[c] = {\n",
    "                \"count\": int(lengths.shape[0]),\n",
    "                \"avg_len\": float(lengths.mean()) if len(lengths) else 0.0,\n",
    "                \"p50_len\": float(lengths.quantile(0.50)) if len(lengths) else 0.0,\n",
    "                \"p90_len\": float(lengths.quantile(0.90)) if len(lengths) else 0.0,\n",
    "                \"p95_len\": float(lengths.quantile(0.95)) if len(lengths) else 0.0,\n",
    "                \"max_len\": int(lengths.max()) if len(lengths) else 0,\n",
    "            }\n",
    "    return summary\n",
    "\n",
    "report = {}\n",
    "\n",
    "# --- BBC ---\n",
    "bbc_loaded = pd.read_parquet(\"./tokenized/bbc_news_tokenized.parquet\")\n",
    "report[\"BBC\"] = summarize_token_lengths(\n",
    "    bbc_loaded,\n",
    "    [\"tokens_clean\",\"tokens_stemmed\",\"tokens_lemmatized\",\n",
    "     \"hf_clean_ids\",\"hf_lemmatized_ids\"]\n",
    ")\n",
    "print(\"üìä BBC:\", report[\"BBC\"])\n",
    "\n",
    "# --- CNN ---\n",
    "cnn_loaded = pd.read_parquet(\"./tokenized/cnn_dailymail_tokenized.parquet\")\n",
    "report[\"CNN\"] = summarize_token_lengths(\n",
    "    cnn_loaded,\n",
    "    [\"tokens_clean\",\"tokens_stemmed\",\"tokens_lemmatized\",\n",
    "     \"hf_clean_ids\",\"hf_lemmatized_ids\"]\n",
    ")\n",
    "print(\"üìä CNN:\", report[\"CNN\"])\n",
    "\n",
    "# --- IMDB ---\n",
    "imdb_loaded = pd.read_parquet(\"./tokenized/imdb_reviews_tokenized.parquet\")\n",
    "report[\"IMDB\"] = summarize_token_lengths(\n",
    "    imdb_loaded,\n",
    "    [\"tokens_clean\",\"tokens_stemmed\",\"tokens_lemmatized\",\n",
    "     \"hf_clean_ids\",\"hf_lemmatized_ids\"]\n",
    ")\n",
    "print(\"üìä IMDB:\", report[\"IMDB\"])\n",
    "\n",
    "# --- save all summaries into JSON ---\n",
    "with open(\"./tokenized/tokenization_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ tokenization report saved to ./tokenized/tokenization_report.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
