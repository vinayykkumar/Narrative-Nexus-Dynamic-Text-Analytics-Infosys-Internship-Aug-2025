[
  {
    "label":4,
    "text":"star trek imax would awesom hope show imax sometim seen yesday",
    "cleaned_text":"star trek imax would awesom hope show imax sometim seen yesday",
    "normalized_text":"star trek imax would awesom hope show imax sometim seen yesday",
    "tokens":[
      "star",
      "trek",
      "imax",
      "awesom",
      "hope",
      "show",
      "imax",
      "sometim",
      "seen",
      "yesday"
    ],
    "token_count":10,
    "processed_text":"star trek imax awesom hope show imax sometim seen yesday"
  },
  {
    "label":4,
    "text":"jb live chat hilari cant wait next week jona brother lt",
    "cleaned_text":"jb live chat hilari cant wait next week jona brother lt",
    "normalized_text":"jb live chat hilari cant wait next week jona brother lt",
    "tokens":[
      "jb",
      "live",
      "chat",
      "hilari",
      "cant",
      "wait",
      "next",
      "week",
      "jona",
      "brother",
      "lt"
    ],
    "token_count":11,
    "processed_text":"jb live chat hilari cant wait next week jona brother lt"
  },
  {
    "label":4,
    "text":"fake tan dri ah pretti brown though need anoth coat bring king",
    "cleaned_text":"fake tan dri ah pretti brown though need anoth coat bring king",
    "normalized_text":"fake tan dri ah pretti brown though need anoth coat bring king",
    "tokens":[
      "fake",
      "tan",
      "dri",
      "ah",
      "pretti",
      "brown",
      "though",
      "need",
      "anoth",
      "coat",
      "bring",
      "king"
    ],
    "token_count":12,
    "processed_text":"fake tan dri ah pretti brown though need anoth coat bring king"
  },
  {
    "label":0,
    "text":"sun shine shame nice florida",
    "cleaned_text":"sun shine shame nice florida",
    "normalized_text":"sun shine shame nice florida",
    "tokens":[
      "sun",
      "shine",
      "shame",
      "nice",
      "florida"
    ],
    "token_count":5,
    "processed_text":"sun shine shame nice florida"
  },
  {
    "label":4,
    "text":"thx rt how halifax cool sunni blue sky look like love day",
    "cleaned_text":"thx rt how halifax cool sunni blue sky look like love day",
    "normalized_text":"thx rt how halifax cool sunni blue sky look like love day",
    "tokens":[
      "thx",
      "rt",
      "halifax",
      "cool",
      "sunni",
      "blue",
      "sky",
      "look",
      "like",
      "love",
      "day"
    ],
    "token_count":11,
    "processed_text":"thx rt halifax cool sunni blue sky look like love day"
  },
  {
    "label":4,
    "text":"ya know thought text drive bad text tri dock mph that reckless",
    "cleaned_text":"ya know thought text drive bad text tri dock mph that reckless",
    "normalized_text":"ya know thought text drive bad text tri dock mph that reckless",
    "tokens":[
      "ya",
      "know",
      "thought",
      "text",
      "drive",
      "bad",
      "text",
      "tri",
      "dock",
      "mph",
      "reckless"
    ],
    "token_count":11,
    "processed_text":"ya know thought text drive bad text tri dock mph reckless"
  },
  {
    "label":4,
    "text":"went sch find lower pri teacher dont sch today one told us mac breakfast",
    "cleaned_text":"went sch find lower pri teacher dont sch today one told us mac breakfast",
    "normalized_text":"went sch find lower pri teacher dont sch today one told us mac breakfast",
    "tokens":[
      "went",
      "sch",
      "find",
      "lower",
      "pri",
      "teacher",
      "dont",
      "sch",
      "today",
      "one",
      "told",
      "us",
      "mac",
      "breakfast"
    ],
    "token_count":14,
    "processed_text":"went sch find lower pri teacher dont sch today one told us mac breakfast"
  },
  {
    "label":0,
    "text":"dont want spam",
    "cleaned_text":"dont want spam",
    "normalized_text":"dont want spam",
    "tokens":[
      "dont",
      "want",
      "spam"
    ],
    "token_count":3,
    "processed_text":"dont want spam"
  },
  {
    "label":4,
    "text":"woohoo today friday gotta get today day fun",
    "cleaned_text":"woohoo today friday gotta get today day fun",
    "normalized_text":"woohoo today friday gotta get today day fun",
    "tokens":[
      "woohoo",
      "today",
      "friday",
      "got",
      "ta",
      "get",
      "today",
      "day",
      "fun"
    ],
    "token_count":9,
    "processed_text":"woohoo today friday got ta get today day fun"
  },
  {
    "label":4,
    "text":"one last tweet head bed realli hope your feel better your awesom dream lava",
    "cleaned_text":"one last tweet head bed realli hope your feel better your awesom dream lava",
    "normalized_text":"one last tweet head bed realli hope your feel better your awesom dream lava",
    "tokens":[
      "one",
      "last",
      "tweet",
      "head",
      "bed",
      "realli",
      "hope",
      "feel",
      "better",
      "awesom",
      "dream",
      "lava"
    ],
    "token_count":12,
    "processed_text":"one last tweet head bed realli hope feel better awesom dream lava"
  },
  {
    "label":0,
    "text":"tooth unbear pain",
    "cleaned_text":"tooth unbear pain",
    "normalized_text":"tooth unbear pain",
    "tokens":[
      "tooth",
      "unbear",
      "pain"
    ],
    "token_count":3,
    "processed_text":"tooth unbear pain"
  },
  {
    "label":0,
    "text":"sad jason via ifriendfeedcom",
    "cleaned_text":"sad jason via ifriendfeedcom",
    "normalized_text":"sad jason via ifriendfeedcom",
    "tokens":[
      "sad",
      "jason",
      "via",
      "ifriendfeedcom"
    ],
    "token_count":4,
    "processed_text":"sad jason via ifriendfeedcom"
  },
  {
    "label":0,
    "text":"haha matter time feel guilti though",
    "cleaned_text":"haha matter time feel guilti though",
    "normalized_text":"haha matter time feel guilti though",
    "tokens":[
      "haha",
      "matter",
      "time",
      "feel",
      "guilti",
      "though"
    ],
    "token_count":6,
    "processed_text":"haha matter time feel guilti though"
  },
  {
    "label":4,
    "text":"hey pooh",
    "cleaned_text":"hey pooh",
    "normalized_text":"hey pooh",
    "tokens":[
      "hey",
      "pooh"
    ],
    "token_count":2,
    "processed_text":"hey pooh"
  },
  {
    "label":0,
    "text":"think lunch cant decid",
    "cleaned_text":"think lunch cant decid",
    "normalized_text":"think lunch cant decid",
    "tokens":[
      "think",
      "lunch",
      "cant",
      "decid"
    ],
    "token_count":4,
    "processed_text":"think lunch cant decid"
  },
  {
    "label":4,
    "text":"footi train tonight went well prep semi final tomorrow good one",
    "cleaned_text":"footi train tonight went well prep semi final tomorrow good one",
    "normalized_text":"footi train tonight went well prep semi final tomorrow good one",
    "tokens":[
      "footi",
      "train",
      "tonight",
      "went",
      "well",
      "prep",
      "semi",
      "final",
      "tomorrow",
      "good",
      "one"
    ],
    "token_count":11,
    "processed_text":"footi train tonight went well prep semi final tomorrow good one"
  },
  {
    "label":4,
    "text":"two favorit male artist dont know like love somebodi way love",
    "cleaned_text":"two favorit male artist dont know like love somebodi way love",
    "normalized_text":"two favorit male artist dont know like love somebodi way love",
    "tokens":[
      "two",
      "favorit",
      "male",
      "artist",
      "dont",
      "know",
      "like",
      "love",
      "somebodi",
      "way",
      "love"
    ],
    "token_count":11,
    "processed_text":"two favorit male artist dont know like love somebodi way love"
  },
  {
    "label":0,
    "text":"miss uu u dont even get u get shut haha",
    "cleaned_text":"miss uu u dont even get u get shut haha",
    "normalized_text":"miss uu u dont even get u get shut haha",
    "tokens":[
      "miss",
      "uu",
      "dont",
      "even",
      "get",
      "get",
      "shut",
      "haha"
    ],
    "token_count":8,
    "processed_text":"miss uu dont even get get shut haha"
  },
  {
    "label":0,
    "text":"oh wowwwww that realli aw hope sort relief come soon",
    "cleaned_text":"oh wowwwww that realli aw hope sort relief come soon",
    "normalized_text":"oh wowwwww that realli aw hope sort relief come soon",
    "tokens":[
      "oh",
      "wowwwww",
      "realli",
      "aw",
      "hope",
      "sort",
      "relief",
      "come",
      "soon"
    ],
    "token_count":9,
    "processed_text":"oh wowwwww realli aw hope sort relief come soon"
  },
  {
    "label":4,
    "text":"congrat heather excit",
    "cleaned_text":"congrat heather excit",
    "normalized_text":"congrat heather excit",
    "tokens":[
      "congrat",
      "heather",
      "excit"
    ],
    "token_count":3,
    "processed_text":"congrat heather excit"
  },
  {
    "label":0,
    "text":"theyv make fun blog fair idea",
    "cleaned_text":"theyv make fun blog fair idea",
    "normalized_text":"theyv make fun blog fair idea",
    "tokens":[
      "theyv",
      "make",
      "fun",
      "blog",
      "fair",
      "idea"
    ],
    "token_count":6,
    "processed_text":"theyv make fun blog fair idea"
  },
  {
    "label":0,
    "text":"guy guy alway make cri",
    "cleaned_text":"guy guy alway make cri",
    "normalized_text":"guy guy alway make cri",
    "tokens":[
      "guy",
      "guy",
      "alway",
      "make",
      "cri"
    ],
    "token_count":5,
    "processed_text":"guy guy alway make cri"
  },
  {
    "label":4,
    "text":"ha mr fairytal get twitter",
    "cleaned_text":"ha mr fairytal get twitter",
    "normalized_text":"ha mr fairytal get twitter",
    "tokens":[
      "ha",
      "mr",
      "fairyt",
      "get",
      "twitter"
    ],
    "token_count":5,
    "processed_text":"ha mr fairyt get twitter"
  },
  {
    "label":4,
    "text":"listen front tragedi morn",
    "cleaned_text":"listen front tragedi morn",
    "normalized_text":"listen front tragedi morn",
    "tokens":[
      "listen",
      "front",
      "tragedi",
      "morn"
    ],
    "token_count":4,
    "processed_text":"listen front tragedi morn"
  },
  {
    "label":0,
    "text":"hey guy im sad",
    "cleaned_text":"hey guy im sad",
    "normalized_text":"hey guy im sad",
    "tokens":[
      "hey",
      "guy",
      "im",
      "sad"
    ],
    "token_count":4,
    "processed_text":"hey guy im sad"
  },
  {
    "label":0,
    "text":"grandmoth hospit die realli upset bout start school tommorow wooo im actual excit",
    "cleaned_text":"grandmoth hospit die realli upset bout start school tommorow wooo im actual excit",
    "normalized_text":"grandmoth hospit die realli upset bout start school tommorow wooo im actual excit",
    "tokens":[
      "grandmoth",
      "hospit",
      "die",
      "realli",
      "upset",
      "bout",
      "start",
      "school",
      "tommorow",
      "wooo",
      "im",
      "actual",
      "excit"
    ],
    "token_count":13,
    "processed_text":"grandmoth hospit die realli upset bout start school tommorow wooo im actual excit"
  },
  {
    "label":0,
    "text":"less joy sound stutter badli hackintosh quotbugreportquot forum get ignor time switch ubuntu",
    "cleaned_text":"less joy sound stutter badli hackintosh quotbugreportquot forum get ignor time switch ubuntu",
    "normalized_text":"less joy sound stutter badli hackintosh quotbugreportquot forum get ignor time switch ubuntu",
    "tokens":[
      "less",
      "joy",
      "sound",
      "stutter",
      "badli",
      "hackintosh",
      "forum",
      "get",
      "ignor",
      "time",
      "switch",
      "ubuntu"
    ],
    "token_count":12,
    "processed_text":"less joy sound stutter badli hackintosh forum get ignor time switch ubuntu"
  },
  {
    "label":4,
    "text":"haha psssh way project list que big tri chip way dont fret",
    "cleaned_text":"haha psssh way project list que big tri chip way dont fret",
    "normalized_text":"haha psssh way project list que big tri chip way dont fret",
    "tokens":[
      "haha",
      "psssh",
      "way",
      "project",
      "list",
      "que",
      "big",
      "tri",
      "chip",
      "way",
      "dont",
      "fret"
    ],
    "token_count":12,
    "processed_text":"haha psssh way project list que big tri chip way dont fret"
  },
  {
    "label":4,
    "text":"errand ae magnolia bakeri sinc send client cake cours get cupcak red velvet",
    "cleaned_text":"errand ae magnolia bakeri sinc send client cake cours get cupcak red velvet",
    "normalized_text":"errand ae magnolia bakeri sinc send client cake cours get cupcak red velvet",
    "tokens":[
      "errand",
      "ae",
      "magnolia",
      "bakeri",
      "sinc",
      "send",
      "client",
      "cake",
      "cour",
      "get",
      "cupcak",
      "red",
      "velvet"
    ],
    "token_count":13,
    "processed_text":"errand ae magnolia bakeri sinc send client cake cour get cupcak red velvet"
  },
  {
    "label":4,
    "text":"ohhthat get read part stori today ive amp havent good chat u",
    "cleaned_text":"ohhthat get read part stori today ive amp havent good chat u",
    "normalized_text":"ohhthat get read part stori today ive amp havent good chat u",
    "tokens":[
      "ohhthat",
      "get",
      "read",
      "part",
      "stori",
      "today",
      "ive",
      "amp",
      "havent",
      "good",
      "chat"
    ],
    "token_count":11,
    "processed_text":"ohhthat get read part stori today ive amp havent good chat"
  },
  {
    "label":4,
    "text":"drag aaron away someon cool",
    "cleaned_text":"drag aaron away someon cool",
    "normalized_text":"drag aaron away someon cool",
    "tokens":[
      "drag",
      "aaron",
      "away",
      "someon",
      "cool"
    ],
    "token_count":5,
    "processed_text":"drag aaron away someon cool"
  },
  {
    "label":0,
    "text":"duuuud tim said theyr make red vs blue anim seri itll either adult swim g hope coz dont g",
    "cleaned_text":"duuuud tim said theyr make red vs blue anim seri itll either adult swim g hope coz dont g",
    "normalized_text":"duuuud tim said theyr make red vs blue anim seri itll either adult swim g hope coz dont g",
    "tokens":[
      "duuuud",
      "tim",
      "said",
      "theyr",
      "make",
      "red",
      "vs",
      "blue",
      "anim",
      "seri",
      "itll",
      "either",
      "adult",
      "swim",
      "hope",
      "coz",
      "dont"
    ],
    "token_count":17,
    "processed_text":"duuuud tim said theyr make red vs blue anim seri itll either adult swim hope coz dont"
  },
  {
    "label":0,
    "text":"god plan alway differ ourstoday gonna long day miss littl kain",
    "cleaned_text":"god plan alway differ ourstoday gonna long day miss littl kain",
    "normalized_text":"god plan alway differ ourstoday gonna long day miss littl kain",
    "tokens":[
      "god",
      "plan",
      "alway",
      "differ",
      "ourstoday",
      "gon",
      "na",
      "long",
      "day",
      "miss",
      "littl",
      "kain"
    ],
    "token_count":12,
    "processed_text":"god plan alway differ ourstoday gon na long day miss littl kain"
  },
  {
    "label":0,
    "text":"oven spoilt share food got burnt redo wasnt fault",
    "cleaned_text":"oven spoilt share food got burnt redo wasnt fault",
    "normalized_text":"oven spoilt share food got burnt redo wasnt fault",
    "tokens":[
      "oven",
      "spoilt",
      "share",
      "food",
      "got",
      "burnt",
      "redo",
      "wasnt",
      "fault"
    ],
    "token_count":9,
    "processed_text":"oven spoilt share food got burnt redo wasnt fault"
  },
  {
    "label":4,
    "text":"hmmmmmmm",
    "cleaned_text":"hmmmmmmm",
    "normalized_text":"hmmmmmmm",
    "tokens":[
      "hmmmmmmm"
    ],
    "token_count":1,
    "processed_text":"hmmmmmmm"
  },
  {
    "label":0,
    "text":"english p chemistri tomorrow",
    "cleaned_text":"english p chemistri tomorrow",
    "normalized_text":"english p chemistri tomorrow",
    "tokens":[
      "english",
      "chemistri",
      "tomorrow"
    ],
    "token_count":3,
    "processed_text":"english chemistri tomorrow"
  },
  {
    "label":0,
    "text":"wish hadnt read londonist report",
    "cleaned_text":"wish hadnt read londonist report",
    "normalized_text":"wish hadnt read londonist report",
    "tokens":[
      "wish",
      "hadnt",
      "read",
      "londonist",
      "report"
    ],
    "token_count":5,
    "processed_text":"wish hadnt read londonist report"
  },
  {
    "label":0,
    "text":"come back amsterdam im beg lt",
    "cleaned_text":"come back amsterdam im beg lt",
    "normalized_text":"come back amsterdam im beg lt",
    "tokens":[
      "come",
      "back",
      "amsterdam",
      "im",
      "beg",
      "lt"
    ],
    "token_count":6,
    "processed_text":"come back amsterdam im beg lt"
  },
  {
    "label":4,
    "text":"board plane outta denver vega hour",
    "cleaned_text":"board plane outta denver vega hour",
    "normalized_text":"board plane outta denver vega hour",
    "tokens":[
      "board",
      "plane",
      "outta",
      "denver",
      "vega",
      "hour"
    ],
    "token_count":6,
    "processed_text":"board plane outta denver vega hour"
  },
  {
    "label":0,
    "text":"cant make new york weekend see",
    "cleaned_text":"cant make new york weekend see",
    "normalized_text":"cant make new york weekend see",
    "tokens":[
      "cant",
      "make",
      "new",
      "york",
      "weekend",
      "see"
    ],
    "token_count":6,
    "processed_text":"cant make new york weekend see"
  },
  {
    "label":0,
    "text":"finger numb sinc noon would",
    "cleaned_text":"finger numb sinc noon would",
    "normalized_text":"finger numb sinc noon would",
    "tokens":[
      "finger",
      "numb",
      "sinc",
      "noon"
    ],
    "token_count":4,
    "processed_text":"finger numb sinc noon"
  },
  {
    "label":0,
    "text":"time fli ur break techlong weekend got tech tomorrow that ok one class",
    "cleaned_text":"time fli ur break techlong weekend got tech tomorrow that ok one class",
    "normalized_text":"time fli ur break techlong weekend got tech tomorrow that ok one class",
    "tokens":[
      "time",
      "fli",
      "ur",
      "break",
      "techlong",
      "weekend",
      "got",
      "tech",
      "tomorrow",
      "ok",
      "one",
      "class"
    ],
    "token_count":12,
    "processed_text":"time fli ur break techlong weekend got tech tomorrow ok one class"
  },
  {
    "label":0,
    "text":"oh fml kind day",
    "cleaned_text":"oh fml kind day",
    "normalized_text":"oh fml kind day",
    "tokens":[
      "oh",
      "fml",
      "kind",
      "day"
    ],
    "token_count":4,
    "processed_text":"oh fml kind day"
  },
  {
    "label":0,
    "text":"damn im stuck name either annabel avaric slave name tiein backstori",
    "cleaned_text":"damn im stuck name either annabel avaric slave name tiein backstori",
    "normalized_text":"damn im stuck name either annabel avaric slave name tiein backstori",
    "tokens":[
      "damn",
      "im",
      "stuck",
      "name",
      "either",
      "annabel",
      "avar",
      "slave",
      "name",
      "tiein",
      "backstori"
    ],
    "token_count":11,
    "processed_text":"damn im stuck name either annabel avar slave name tiein backstori"
  },
  {
    "label":0,
    "text":"grrrrr shower chemistri revis",
    "cleaned_text":"grrrrr shower chemistri revis",
    "normalized_text":"grrrrr shower chemistri revis",
    "tokens":[
      "grrrrr",
      "shower",
      "chemistri",
      "revi"
    ],
    "token_count":4,
    "processed_text":"grrrrr shower chemistri revi"
  },
  {
    "label":0,
    "text":"yet real struggl satisfi everyon household time shop crazi busi wel",
    "cleaned_text":"yet real struggl satisfi everyon household time shop crazi busi wel",
    "normalized_text":"yet real struggl satisfi everyon household time shop crazi busi wel",
    "tokens":[
      "yet",
      "real",
      "struggl",
      "satisfi",
      "everyon",
      "household",
      "time",
      "shop",
      "crazi",
      "busi",
      "wel"
    ],
    "token_count":11,
    "processed_text":"yet real struggl satisfi everyon household time shop crazi busi wel"
  },
  {
    "label":0,
    "text":"get easili lol",
    "cleaned_text":"get easili lol",
    "normalized_text":"get easili lol",
    "tokens":[
      "get",
      "easili",
      "lol"
    ],
    "token_count":3,
    "processed_text":"get easili lol"
  },
  {
    "label":4,
    "text":"iphon land facefirst hard bitumen shini new iphon shini new",
    "cleaned_text":"iphon land facefirst hard bitumen shini new iphon shini new",
    "normalized_text":"iphon land facefirst hard bitumen shini new iphon shini new",
    "tokens":[
      "iphon",
      "land",
      "facefirst",
      "hard",
      "bitumen",
      "shini",
      "new",
      "iphon",
      "shini",
      "new"
    ],
    "token_count":10,
    "processed_text":"iphon land facefirst hard bitumen shini new iphon shini new"
  },
  {
    "label":0,
    "text":"aww man yet anoth parti last nighti miss new jersey anyon anyth today",
    "cleaned_text":"aww man yet anoth parti last nighti miss new jersey anyon anyth today",
    "normalized_text":"aww man yet anoth parti last nighti miss new jersey anyon anyth today",
    "tokens":[
      "aww",
      "man",
      "yet",
      "anoth",
      "parti",
      "last",
      "nighti",
      "miss",
      "new",
      "jersey",
      "anyon",
      "anyth",
      "today"
    ],
    "token_count":13,
    "processed_text":"aww man yet anoth parti last nighti miss new jersey anyon anyth today"
  },
  {
    "label":4,
    "text":"today th wed anniversari",
    "cleaned_text":"today th wed anniversari",
    "normalized_text":"today th wed anniversari",
    "tokens":[
      "today",
      "th",
      "wed",
      "anniversari"
    ],
    "token_count":4,
    "processed_text":"today th wed anniversari"
  },
  {
    "label":4,
    "text":"strawberri banana vanilla wafer yogurt land im quit happi right",
    "cleaned_text":"strawberri banana vanilla wafer yogurt land im quit happi right",
    "normalized_text":"strawberri banana vanilla wafer yogurt land im quit happi right",
    "tokens":[
      "strawberri",
      "banana",
      "vanilla",
      "wafer",
      "yogurt",
      "land",
      "im",
      "quit",
      "happi",
      "right"
    ],
    "token_count":10,
    "processed_text":"strawberri banana vanilla wafer yogurt land im quit happi right"
  },
  {
    "label":4,
    "text":"oh ok good glad misunderstood",
    "cleaned_text":"oh ok good glad misunderstood",
    "normalized_text":"oh ok good glad misunderstood",
    "tokens":[
      "oh",
      "ok",
      "good",
      "glad",
      "misunderstood"
    ],
    "token_count":5,
    "processed_text":"oh ok good glad misunderstood"
  },
  {
    "label":0,
    "text":"hahaha oh oh fail real song stuck head",
    "cleaned_text":"hahaha oh oh fail real song stuck head",
    "normalized_text":"hahaha oh oh fail real song stuck head",
    "tokens":[
      "hahaha",
      "oh",
      "oh",
      "fail",
      "real",
      "song",
      "stuck",
      "head"
    ],
    "token_count":8,
    "processed_text":"hahaha oh oh fail real song stuck head"
  },
  {
    "label":4,
    "text":"sent messag guy might come back year",
    "cleaned_text":"sent messag guy might come back year",
    "normalized_text":"sent messag guy might come back year",
    "tokens":[
      "sent",
      "messag",
      "guy",
      "come",
      "back",
      "year"
    ],
    "token_count":6,
    "processed_text":"sent messag guy come back year"
  },
  {
    "label":0,
    "text":"brighton yesterday yeah market poor bumb jimmi catherin",
    "cleaned_text":"brighton yesterday yeah market poor bumb jimmi catherin",
    "normalized_text":"brighton yesterday yeah market poor bumb jimmi catherin",
    "tokens":[
      "brighton",
      "yesterday",
      "yeah",
      "market",
      "poor",
      "bumb",
      "jimmi",
      "catherin"
    ],
    "token_count":8,
    "processed_text":"brighton yesterday yeah market poor bumb jimmi catherin"
  },
  {
    "label":0,
    "text":"like cant get outta mind good way",
    "cleaned_text":"like cant get outta mind good way",
    "normalized_text":"like cant get outta mind good way",
    "tokens":[
      "like",
      "cant",
      "get",
      "outta",
      "mind",
      "good",
      "way"
    ],
    "token_count":7,
    "processed_text":"like cant get outta mind good way"
  },
  {
    "label":0,
    "text":"awak mideven nap im edit pictur find speaker laptop appear broken",
    "cleaned_text":"awak mideven nap im edit pictur find speaker laptop appear broken",
    "normalized_text":"awak mideven nap im edit pictur find speaker laptop appear broken",
    "tokens":[
      "awak",
      "mideven",
      "nap",
      "im",
      "edit",
      "pictur",
      "find",
      "speaker",
      "laptop",
      "appear",
      "broken"
    ],
    "token_count":11,
    "processed_text":"awak mideven nap im edit pictur find speaker laptop appear broken"
  },
  {
    "label":4,
    "text":"yard duti th weeken missin hlp frm boyz lv u son c u tomar still lot left hv nice bbq n mom lemonaid aftr",
    "cleaned_text":"yard duti th weeken missin hlp frm boyz lv u son c u tomar still lot left hv nice bbq n mom lemonaid aftr",
    "normalized_text":"yard duti th weeken missin hlp frm boyz lv u son c u tomar still lot left hv nice bbq n mom lemonaid aftr",
    "tokens":[
      "yard",
      "duti",
      "th",
      "weeken",
      "missin",
      "hlp",
      "frm",
      "boyz",
      "lv",
      "son",
      "tomar",
      "still",
      "lot",
      "left",
      "hv",
      "nice",
      "bbq",
      "mom",
      "lemonaid",
      "aftr"
    ],
    "token_count":20,
    "processed_text":"yard duti th weeken missin hlp frm boyz lv son tomar still lot left hv nice bbq mom lemonaid aftr"
  },
  {
    "label":0,
    "text":"pain show uk itun yet",
    "cleaned_text":"pain show uk itun yet",
    "normalized_text":"pain show uk itun yet",
    "tokens":[
      "pain",
      "show",
      "uk",
      "itun",
      "yet"
    ],
    "token_count":5,
    "processed_text":"pain show uk itun yet"
  },
  {
    "label":4,
    "text":"woo im appl store london awesom",
    "cleaned_text":"woo im appl store london awesom",
    "normalized_text":"woo im appl store london awesom",
    "tokens":[
      "woo",
      "im",
      "appl",
      "store",
      "london",
      "awesom"
    ],
    "token_count":6,
    "processed_text":"woo im appl store london awesom"
  },
  {
    "label":4,
    "text":"made bag far hand made yo keep improv",
    "cleaned_text":"made bag far hand made yo keep improv",
    "normalized_text":"made bag far hand made yo keep improv",
    "tokens":[
      "made",
      "bag",
      "far",
      "hand",
      "made",
      "yo",
      "keep",
      "improv"
    ],
    "token_count":8,
    "processed_text":"made bag far hand made yo keep improv"
  },
  {
    "label":0,
    "text":"rebgt nnooo cring tv amp embarrass us brummi bless",
    "cleaned_text":"rebgt nnooo cring tv amp embarrass us brummi bless",
    "normalized_text":"rebgt nnooo cring tv amp embarrass us brummi bless",
    "tokens":[
      "rebgt",
      "nnooo",
      "cring",
      "tv",
      "amp",
      "embarrass",
      "us",
      "brummi",
      "bless"
    ],
    "token_count":9,
    "processed_text":"rebgt nnooo cring tv amp embarrass us brummi bless"
  },
  {
    "label":0,
    "text":"heart break wtf",
    "cleaned_text":"heart break wtf",
    "normalized_text":"heart break wtf",
    "tokens":[
      "heart",
      "break",
      "wtf"
    ],
    "token_count":3,
    "processed_text":"heart break wtf"
  },
  {
    "label":4,
    "text":"dude next time come go one nicer nyc beach realli want",
    "cleaned_text":"dude next time come go one nicer nyc beach realli want",
    "normalized_text":"dude next time come go one nicer nyc beach realli want",
    "tokens":[
      "dude",
      "next",
      "time",
      "come",
      "go",
      "one",
      "nicer",
      "nyc",
      "beach",
      "realli",
      "want"
    ],
    "token_count":11,
    "processed_text":"dude next time come go one nicer nyc beach realli want"
  },
  {
    "label":0,
    "text":"finish anoth head feel like im comin dwn cold",
    "cleaned_text":"finish anoth head feel like im comin dwn cold",
    "normalized_text":"finish anoth head feel like im comin dwn cold",
    "tokens":[
      "finish",
      "anoth",
      "head",
      "feel",
      "like",
      "im",
      "comin",
      "dwn",
      "cold"
    ],
    "token_count":9,
    "processed_text":"finish anoth head feel like im comin dwn cold"
  },
  {
    "label":0,
    "text":"wish could watch bash",
    "cleaned_text":"wish could watch bash",
    "normalized_text":"wish could watch bash",
    "tokens":[
      "wish",
      "watch",
      "bash"
    ],
    "token_count":3,
    "processed_text":"wish watch bash"
  },
  {
    "label":4,
    "text":"neither",
    "cleaned_text":"neither",
    "normalized_text":"neither",
    "tokens":[
      "neither"
    ],
    "token_count":1,
    "processed_text":"neither"
  },
  {
    "label":4,
    "text":"hey gr wbu",
    "cleaned_text":"hey gr wbu",
    "normalized_text":"hey gr wbu",
    "tokens":[
      "hey",
      "gr",
      "wbu"
    ],
    "token_count":3,
    "processed_text":"hey gr wbu"
  },
  {
    "label":4,
    "text":"order far fan edit",
    "cleaned_text":"order far fan edit",
    "normalized_text":"order far fan edit",
    "tokens":[
      "order",
      "far",
      "fan",
      "edit"
    ],
    "token_count":4,
    "processed_text":"order far fan edit"
  },
  {
    "label":4,
    "text":"clothdiap awesom",
    "cleaned_text":"clothdiap awesom",
    "normalized_text":"clothdiap awesom",
    "tokens":[
      "clothdiap",
      "awesom"
    ],
    "token_count":2,
    "processed_text":"clothdiap awesom"
  },
  {
    "label":4,
    "text":"podcast episod upload type subscrib pleas wwwrandolphstproductionsmypodcastcom thank",
    "cleaned_text":"podcast episod upload type subscrib pleas wwwrandolphstproductionsmypodcastcom thank",
    "normalized_text":"podcast episod upload type subscrib pleas wwwrandolphstproductionsmypodcastcom thank",
    "tokens":[
      "podcast",
      "episod",
      "upload",
      "type",
      "subscrib",
      "plea",
      "thank"
    ],
    "token_count":7,
    "processed_text":"podcast episod upload type subscrib plea thank"
  },
  {
    "label":4,
    "text":"hellooo im fine",
    "cleaned_text":"hellooo im fine",
    "normalized_text":"hellooo im fine",
    "tokens":[
      "hellooo",
      "im",
      "fine"
    ],
    "token_count":3,
    "processed_text":"hellooo im fine"
  },
  {
    "label":4,
    "text":"great day today",
    "cleaned_text":"great day today",
    "normalized_text":"great day today",
    "tokens":[
      "great",
      "day",
      "today"
    ],
    "token_count":3,
    "processed_text":"great day today"
  },
  {
    "label":0,
    "text":"bass easi play guitar hard",
    "cleaned_text":"bass easi play guitar hard",
    "normalized_text":"bass easi play guitar hard",
    "tokens":[
      "bass",
      "easi",
      "play",
      "guitar",
      "hard"
    ],
    "token_count":5,
    "processed_text":"bass easi play guitar hard"
  },
  {
    "label":4,
    "text":"great",
    "cleaned_text":"great",
    "normalized_text":"great",
    "tokens":[
      "great"
    ],
    "token_count":1,
    "processed_text":"great"
  },
  {
    "label":0,
    "text":"awww watch tv today kept cut movi commerci",
    "cleaned_text":"awww watch tv today kept cut movi commerci",
    "normalized_text":"awww watch tv today kept cut movi commerci",
    "tokens":[
      "awww",
      "watch",
      "tv",
      "today",
      "kept",
      "cut",
      "movi",
      "commerci"
    ],
    "token_count":8,
    "processed_text":"awww watch tv today kept cut movi commerci"
  },
  {
    "label":4,
    "text":"got ps amp killzon",
    "cleaned_text":"got ps amp killzon",
    "normalized_text":"got ps amp killzon",
    "tokens":[
      "got",
      "ps",
      "amp",
      "killzon"
    ],
    "token_count":4,
    "processed_text":"got ps amp killzon"
  },
  {
    "label":0,
    "text":"go sleep caus sinus act",
    "cleaned_text":"go sleep caus sinus act",
    "normalized_text":"go sleep caus sinus act",
    "tokens":[
      "go",
      "sleep",
      "cau",
      "sinu",
      "act"
    ],
    "token_count":5,
    "processed_text":"go sleep cau sinu act"
  },
  {
    "label":4,
    "text":"read quotbrwrich quot nice see half live amp realiz one lost money year",
    "cleaned_text":"read quotbrwrich quot nice see half live amp realiz one lost money year",
    "normalized_text":"read quotbrwrich quot nice see half live amp realiz one lost money year",
    "tokens":[
      "read",
      "quotbrwrich",
      "quot",
      "nice",
      "see",
      "half",
      "live",
      "amp",
      "realiz",
      "one",
      "lost",
      "money",
      "year"
    ],
    "token_count":13,
    "processed_text":"read quotbrwrich quot nice see half live amp realiz one lost money year"
  },
  {
    "label":0,
    "text":"squar one realli need bsg game",
    "cleaned_text":"squar one realli need bsg game",
    "normalized_text":"squar one realli need bsg game",
    "tokens":[
      "squar",
      "one",
      "realli",
      "need",
      "bsg",
      "game"
    ],
    "token_count":6,
    "processed_text":"squar one realli need bsg game"
  },
  {
    "label":0,
    "text":"oh miss u crazi peopl alreadi",
    "cleaned_text":"oh miss u crazi peopl alreadi",
    "normalized_text":"oh miss u crazi peopl alreadi",
    "tokens":[
      "oh",
      "miss",
      "crazi",
      "peopl",
      "alreadi"
    ],
    "token_count":5,
    "processed_text":"oh miss crazi peopl alreadi"
  },
  {
    "label":0,
    "text":"littl upset dont know mayb emot im feel realli day like regular cheer self ill b ok",
    "cleaned_text":"littl upset dont know mayb emot im feel realli day like regular cheer self ill b ok",
    "normalized_text":"littl upset dont know mayb emot im feel realli day like regular cheer self ill b ok",
    "tokens":[
      "littl",
      "upset",
      "dont",
      "know",
      "mayb",
      "emot",
      "im",
      "feel",
      "realli",
      "day",
      "like",
      "regular",
      "cheer",
      "self",
      "ill",
      "ok"
    ],
    "token_count":16,
    "processed_text":"littl upset dont know mayb emot im feel realli day like regular cheer self ill ok"
  },
  {
    "label":0,
    "text":"got rain pool",
    "cleaned_text":"got rain pool",
    "normalized_text":"got rain pool",
    "tokens":[
      "got",
      "rain",
      "pool"
    ],
    "token_count":3,
    "processed_text":"got rain pool"
  },
  {
    "label":0,
    "text":"ah well least eat solid food want bowl cheerio right",
    "cleaned_text":"ah well least eat solid food want bowl cheerio right",
    "normalized_text":"ah well least eat solid food want bowl cheerio right",
    "tokens":[
      "ah",
      "well",
      "least",
      "eat",
      "solid",
      "food",
      "want",
      "bowl",
      "cheerio",
      "right"
    ],
    "token_count":10,
    "processed_text":"ah well least eat solid food want bowl cheerio right"
  },
  {
    "label":0,
    "text":"happen midwest",
    "cleaned_text":"happen midwest",
    "normalized_text":"happen midwest",
    "tokens":[
      "happen",
      "midwest"
    ],
    "token_count":2,
    "processed_text":"happen midwest"
  },
  {
    "label":4,
    "text":"day hope wonder",
    "cleaned_text":"day hope wonder",
    "normalized_text":"day hope wonder",
    "tokens":[
      "day",
      "hope",
      "wonder"
    ],
    "token_count":3,
    "processed_text":"day hope wonder"
  },
  {
    "label":0,
    "text":"goodsex heard guy name got butt",
    "cleaned_text":"goodsex heard guy name got butt",
    "normalized_text":"goodsex heard guy name got butt",
    "tokens":[
      "goodsex",
      "heard",
      "guy",
      "name",
      "got",
      "butt"
    ],
    "token_count":6,
    "processed_text":"goodsex heard guy name got butt"
  },
  {
    "label":4,
    "text":"cant wait new laptop",
    "cleaned_text":"cant wait new laptop",
    "normalized_text":"cant wait new laptop",
    "tokens":[
      "cant",
      "wait",
      "new",
      "laptop"
    ],
    "token_count":4,
    "processed_text":"cant wait new laptop"
  },
  {
    "label":4,
    "text":"nah look better label top latin name access way need turn anyth",
    "cleaned_text":"nah look better label top latin name access way need turn anyth",
    "normalized_text":"nah look better label top latin name access way need turn anyth",
    "tokens":[
      "nah",
      "look",
      "better",
      "label",
      "top",
      "latin",
      "name",
      "access",
      "way",
      "need",
      "turn",
      "anyth"
    ],
    "token_count":12,
    "processed_text":"nah look better label top latin name access way need turn anyth"
  },
  {
    "label":0,
    "text":"pleas dont forget french fan",
    "cleaned_text":"pleas dont forget french fan",
    "normalized_text":"pleas dont forget french fan",
    "tokens":[
      "plea",
      "dont",
      "forget",
      "french",
      "fan"
    ],
    "token_count":5,
    "processed_text":"plea dont forget french fan"
  },
  {
    "label":4,
    "text":"yeah add yoga teacher mixwait evict notic state except need tax gnite",
    "cleaned_text":"yeah add yoga teacher mixwait evict notic state except need tax gnite",
    "normalized_text":"yeah add yoga teacher mixwait evict notic state except need tax gnite",
    "tokens":[
      "yeah",
      "add",
      "yoga",
      "teacher",
      "mixwait",
      "evict",
      "notic",
      "state",
      "except",
      "need",
      "tax",
      "gnite"
    ],
    "token_count":12,
    "processed_text":"yeah add yoga teacher mixwait evict notic state except need tax gnite"
  },
  {
    "label":0,
    "text":"lock sinc min late amp meet public school dont blame em still im today",
    "cleaned_text":"lock sinc min late amp meet public school dont blame em still im today",
    "normalized_text":"lock sinc min late amp meet public school dont blame em still im today",
    "tokens":[
      "lock",
      "sinc",
      "min",
      "late",
      "amp",
      "meet",
      "public",
      "school",
      "dont",
      "blame",
      "em",
      "still",
      "im",
      "today"
    ],
    "token_count":14,
    "processed_text":"lock sinc min late amp meet public school dont blame em still im today"
  },
  {
    "label":0,
    "text":"noth like turn hour train journey hour journey eh",
    "cleaned_text":"noth like turn hour train journey hour journey eh",
    "normalized_text":"noth like turn hour train journey hour journey eh",
    "tokens":[
      "noth",
      "like",
      "turn",
      "hour",
      "train",
      "journey",
      "hour",
      "journey",
      "eh"
    ],
    "token_count":9,
    "processed_text":"noth like turn hour train journey hour journey eh"
  },
  {
    "label":4,
    "text":"luv thatthank help wit movement",
    "cleaned_text":"luv thatthank help wit movement",
    "normalized_text":"luv thatthank help wit movement",
    "tokens":[
      "luv",
      "thatthank",
      "help",
      "wit",
      "movement"
    ],
    "token_count":5,
    "processed_text":"luv thatthank help wit movement"
  },
  {
    "label":0,
    "text":"isnt pictur show hmmm mayb co ive got face radio twitter",
    "cleaned_text":"isnt pictur show hmmm mayb co ive got face radio twitter",
    "normalized_text":"isnt pictur show hmmm mayb co ive got face radio twitter",
    "tokens":[
      "isnt",
      "pictur",
      "show",
      "hmmm",
      "mayb",
      "co",
      "ive",
      "got",
      "face",
      "radio",
      "twitter"
    ],
    "token_count":11,
    "processed_text":"isnt pictur show hmmm mayb co ive got face radio twitter"
  },
  {
    "label":0,
    "text":"favorit famili better get divorc",
    "cleaned_text":"favorit famili better get divorc",
    "normalized_text":"favorit famili better get divorc",
    "tokens":[
      "favorit",
      "famili",
      "better",
      "get",
      "divorc"
    ],
    "token_count":5,
    "processed_text":"favorit famili better get divorc"
  },
  {
    "label":4,
    "text":"feel distract cant cocentr studiesit may guy kno jetson awsom lt",
    "cleaned_text":"feel distract cant cocentr studiesit may guy kno jetson awsom lt",
    "normalized_text":"feel distract cant cocentr studiesit may guy kno jetson awsom lt",
    "tokens":[
      "feel",
      "distract",
      "cant",
      "cocentr",
      "studiesit",
      "may",
      "guy",
      "kno",
      "jetson",
      "awsom",
      "lt"
    ],
    "token_count":11,
    "processed_text":"feel distract cant cocentr studiesit may guy kno jetson awsom lt"
  },
  {
    "label":0,
    "text":"sittin car rain",
    "cleaned_text":"sittin car rain",
    "normalized_text":"sittin car rain",
    "tokens":[
      "sittin",
      "car",
      "rain"
    ],
    "token_count":3,
    "processed_text":"sittin car rain"
  },
  {
    "label":0,
    "text":"updat new iphon softwar wish new iphon",
    "cleaned_text":"updat new iphon softwar wish new iphon",
    "normalized_text":"updat new iphon softwar wish new iphon",
    "tokens":[
      "updat",
      "new",
      "iphon",
      "softwar",
      "wish",
      "new",
      "iphon"
    ],
    "token_count":7,
    "processed_text":"updat new iphon softwar wish new iphon"
  },
  {
    "label":0,
    "text":"everyon come town weekendexcit realli rd gp feel age ill let babi fun",
    "cleaned_text":"everyon come town weekendexcit realli rd gp feel age ill let babi fun",
    "normalized_text":"everyon come town weekendexcit realli rd gp feel age ill let babi fun",
    "tokens":[
      "everyon",
      "come",
      "town",
      "weekendexcit",
      "realli",
      "rd",
      "gp",
      "feel",
      "age",
      "ill",
      "let",
      "babi",
      "fun"
    ],
    "token_count":13,
    "processed_text":"everyon come town weekendexcit realli rd gp feel age ill let babi fun"
  },
  {
    "label":0,
    "text":"wish someon get twitta beg come bednop dont anyon",
    "cleaned_text":"wish someon get twitta beg come bednop dont anyon",
    "normalized_text":"wish someon get twitta beg come bednop dont anyon",
    "tokens":[
      "wish",
      "someon",
      "get",
      "twitta",
      "beg",
      "come",
      "bednop",
      "dont",
      "anyon"
    ],
    "token_count":9,
    "processed_text":"wish someon get twitta beg come bednop dont anyon"
  },
  {
    "label":4,
    "text":"certain local figur act like opportunist infect parti that becom immunocompromis stay tune",
    "cleaned_text":"certain local figur act like opportunist infect parti that becom immunocompromis stay tune",
    "normalized_text":"certain local figur act like opportunist infect parti that becom immunocompromis stay tune",
    "tokens":[
      "certain",
      "local",
      "figur",
      "act",
      "like",
      "opportunist",
      "infect",
      "parti",
      "becom",
      "immunocompromi",
      "stay",
      "tune"
    ],
    "token_count":12,
    "processed_text":"certain local figur act like opportunist infect parti becom immunocompromi stay tune"
  },
  {
    "label":4,
    "text":"fli paranoid black key much better cant wait album",
    "cleaned_text":"fli paranoid black key much better cant wait album",
    "normalized_text":"fli paranoid black key much better cant wait album",
    "tokens":[
      "fli",
      "paranoid",
      "black",
      "key",
      "much",
      "better",
      "cant",
      "wait",
      "album"
    ],
    "token_count":9,
    "processed_text":"fli paranoid black key much better cant wait album"
  },
  {
    "label":0,
    "text":"im go bed gotta get tomorrow work",
    "cleaned_text":"im go bed gotta get tomorrow work",
    "normalized_text":"im go bed gotta get tomorrow work",
    "tokens":[
      "im",
      "go",
      "bed",
      "got",
      "ta",
      "get",
      "tomorrow",
      "work"
    ],
    "token_count":8,
    "processed_text":"im go bed got ta get tomorrow work"
  },
  {
    "label":0,
    "text":"pillow smell like coffe",
    "cleaned_text":"pillow smell like coffe",
    "normalized_text":"pillow smell like coffe",
    "tokens":[
      "pillow",
      "smell",
      "like",
      "coff"
    ],
    "token_count":4,
    "processed_text":"pillow smell like coff"
  },
  {
    "label":4,
    "text":"anoth sar come",
    "cleaned_text":"anoth sar come",
    "normalized_text":"anoth sar come",
    "tokens":[
      "anoth",
      "sar",
      "come"
    ],
    "token_count":3,
    "processed_text":"anoth sar come"
  },
  {
    "label":4,
    "text":"dont know account im social worker cant afford go",
    "cleaned_text":"dont know account im social worker cant afford go",
    "normalized_text":"dont know account im social worker cant afford go",
    "tokens":[
      "dont",
      "know",
      "account",
      "im",
      "social",
      "worker",
      "cant",
      "afford",
      "go"
    ],
    "token_count":9,
    "processed_text":"dont know account im social worker cant afford go"
  },
  {
    "label":4,
    "text":"visit sever time yesterday awesom",
    "cleaned_text":"visit sever time yesterday awesom",
    "normalized_text":"visit sever time yesterday awesom",
    "tokens":[
      "visit",
      "sever",
      "time",
      "yesterday",
      "awesom"
    ],
    "token_count":5,
    "processed_text":"visit sever time yesterday awesom"
  },
  {
    "label":0,
    "text":"oh goshi look forward short vacat daysth stress neck heavi",
    "cleaned_text":"oh goshi look forward short vacat daysth stress neck heavi",
    "normalized_text":"oh goshi look forward short vacat daysth stress neck heavi",
    "tokens":[
      "oh",
      "goshi",
      "look",
      "forward",
      "short",
      "vacat",
      "daysth",
      "stress",
      "neck",
      "heavi"
    ],
    "token_count":10,
    "processed_text":"oh goshi look forward short vacat daysth stress neck heavi"
  },
  {
    "label":4,
    "text":"follow would cri tear joy year",
    "cleaned_text":"follow would cri tear joy year",
    "normalized_text":"follow would cri tear joy year",
    "tokens":[
      "follow",
      "cri",
      "tear",
      "joy",
      "year"
    ],
    "token_count":5,
    "processed_text":"follow cri tear joy year"
  },
  {
    "label":0,
    "text":"chines food sammi bust least pick two hot pair pump fab pair sued boot fall",
    "cleaned_text":"chines food sammi bust least pick two hot pair pump fab pair sued boot fall",
    "normalized_text":"chines food sammi bust least pick two hot pair pump fab pair sued boot fall",
    "tokens":[
      "chine",
      "food",
      "sammi",
      "bust",
      "least",
      "pick",
      "two",
      "hot",
      "pair",
      "pump",
      "fab",
      "pair",
      "su",
      "boot",
      "fall"
    ],
    "token_count":15,
    "processed_text":"chine food sammi bust least pick two hot pair pump fab pair su boot fall"
  },
  {
    "label":0,
    "text":"back hotel late la earli morn madrid work day start",
    "cleaned_text":"back hotel late la earli morn madrid work day start",
    "normalized_text":"back hotel late la earli morn madrid work day start",
    "tokens":[
      "back",
      "hotel",
      "late",
      "la",
      "earli",
      "morn",
      "madrid",
      "work",
      "day",
      "start"
    ],
    "token_count":10,
    "processed_text":"back hotel late la earli morn madrid work day start"
  },
  {
    "label":4,
    "text":"ok get new follow check recent tweet anyth find interest list great link",
    "cleaned_text":"ok get new follow check recent tweet anyth find interest list great link",
    "normalized_text":"ok get new follow check recent tweet anyth find interest list great link",
    "tokens":[
      "ok",
      "get",
      "new",
      "follow",
      "check",
      "recent",
      "tweet",
      "anyth",
      "find",
      "interest",
      "list",
      "great",
      "link"
    ],
    "token_count":13,
    "processed_text":"ok get new follow check recent tweet anyth find interest list great link"
  },
  {
    "label":4,
    "text":"that like first day fun though everyon sweet",
    "cleaned_text":"that like first day fun though everyon sweet",
    "normalized_text":"that like first day fun though everyon sweet",
    "tokens":[
      "like",
      "first",
      "day",
      "fun",
      "though",
      "everyon",
      "sweet"
    ],
    "token_count":7,
    "processed_text":"like first day fun though everyon sweet"
  },
  {
    "label":0,
    "text":"said thank tell x",
    "cleaned_text":"said thank tell x",
    "normalized_text":"said thank tell x",
    "tokens":[
      "said",
      "thank",
      "tell"
    ],
    "token_count":3,
    "processed_text":"said thank tell"
  },
  {
    "label":0,
    "text":"realli stop",
    "cleaned_text":"realli stop",
    "normalized_text":"realli stop",
    "tokens":[
      "realli",
      "stop"
    ],
    "token_count":2,
    "processed_text":"realli stop"
  },
  {
    "label":4,
    "text":"good time ny mani good run routespath",
    "cleaned_text":"good time ny mani good run routespath",
    "normalized_text":"good time ny mani good run routespath",
    "tokens":[
      "good",
      "time",
      "ny",
      "mani",
      "good",
      "run",
      "routespath"
    ],
    "token_count":7,
    "processed_text":"good time ny mani good run routespath"
  },
  {
    "label":0,
    "text":"send pic twitterberri send twitpic never sendsjust sit sit",
    "cleaned_text":"send pic twitterberri send twitpic never sendsjust sit sit",
    "normalized_text":"send pic twitterberri send twitpic never sendsjust sit sit",
    "tokens":[
      "send",
      "pic",
      "twitterberri",
      "send",
      "twitpic",
      "never",
      "sendsjust",
      "sit",
      "sit"
    ],
    "token_count":9,
    "processed_text":"send pic twitterberri send twitpic never sendsjust sit sit"
  },
  {
    "label":0,
    "text":"steve job wont speak wwdc",
    "cleaned_text":"steve job wont speak wwdc",
    "normalized_text":"steve job wont speak wwdc",
    "tokens":[
      "steve",
      "job",
      "wont",
      "speak",
      "wwdc"
    ],
    "token_count":5,
    "processed_text":"steve job wont speak wwdc"
  },
  {
    "label":0,
    "text":"ah lordyi h feel get hve mke decis like thisit horribl feel n pit stmch dlc",
    "cleaned_text":"ah lordyi h feel get hve mke decis like thisit horribl feel n pit stmch dlc",
    "normalized_text":"ah lordyi h feel get hve mke decis like thisit horribl feel n pit stmch dlc",
    "tokens":[
      "ah",
      "lordyi",
      "feel",
      "get",
      "hve",
      "mke",
      "deci",
      "like",
      "thisit",
      "horribl",
      "feel",
      "pit",
      "stmch",
      "dlc"
    ],
    "token_count":14,
    "processed_text":"ah lordyi feel get hve mke deci like thisit horribl feel pit stmch dlc"
  },
  {
    "label":0,
    "text":"want buy canon eo rebel ti",
    "cleaned_text":"want buy canon eo rebel ti",
    "normalized_text":"want buy canon eo rebel ti",
    "tokens":[
      "want",
      "buy",
      "canon",
      "eo",
      "rebel",
      "ti"
    ],
    "token_count":6,
    "processed_text":"want buy canon eo rebel ti"
  },
  {
    "label":4,
    "text":"r million whatno celebr congrat aston",
    "cleaned_text":"r million whatno celebr congrat aston",
    "normalized_text":"r million whatno celebr congrat aston",
    "tokens":[
      "million",
      "whatno",
      "celebr",
      "congrat",
      "aston"
    ],
    "token_count":5,
    "processed_text":"million whatno celebr congrat aston"
  },
  {
    "label":0,
    "text":"darn mine still",
    "cleaned_text":"darn mine still",
    "normalized_text":"darn mine still",
    "tokens":[
      "darn",
      "mine",
      "still"
    ],
    "token_count":3,
    "processed_text":"darn mine still"
  },
  {
    "label":4,
    "text":"youv gone independ produc new song bandmem garag",
    "cleaned_text":"youv gone independ produc new song bandmem garag",
    "normalized_text":"youv gone independ produc new song bandmem garag",
    "tokens":[
      "youv",
      "gone",
      "independ",
      "produc",
      "new",
      "song",
      "bandmem",
      "garag"
    ],
    "token_count":8,
    "processed_text":"youv gone independ produc new song bandmem garag"
  },
  {
    "label":4,
    "text":"thank awesom night beth victor et al guy rock world",
    "cleaned_text":"thank awesom night beth victor et al guy rock world",
    "normalized_text":"thank awesom night beth victor et al guy rock world",
    "tokens":[
      "thank",
      "awesom",
      "night",
      "beth",
      "victor",
      "et",
      "al",
      "guy",
      "rock",
      "world"
    ],
    "token_count":10,
    "processed_text":"thank awesom night beth victor et al guy rock world"
  },
  {
    "label":0,
    "text":"gym lot pack expect time morn plan work garden gone kaput due weather",
    "cleaned_text":"gym lot pack expect time morn plan work garden gone kaput due weather",
    "normalized_text":"gym lot pack expect time morn plan work garden gone kaput due weather",
    "tokens":[
      "gym",
      "lot",
      "pack",
      "expect",
      "time",
      "morn",
      "plan",
      "work",
      "garden",
      "gone",
      "kaput",
      "due",
      "weather"
    ],
    "token_count":13,
    "processed_text":"gym lot pack expect time morn plan work garden gone kaput due weather"
  },
  {
    "label":0,
    "text":"ohh want go see",
    "cleaned_text":"ohh want go see",
    "normalized_text":"ohh want go see",
    "tokens":[
      "ohh",
      "want",
      "go",
      "see"
    ],
    "token_count":4,
    "processed_text":"ohh want go see"
  },
  {
    "label":0,
    "text":"wish capabl speak mind",
    "cleaned_text":"wish capabl speak mind",
    "normalized_text":"wish capabl speak mind",
    "tokens":[
      "wish",
      "capabl",
      "speak",
      "mind"
    ],
    "token_count":4,
    "processed_text":"wish capabl speak mind"
  },
  {
    "label":0,
    "text":"awww poor babi im sorri u feel way shit wors come ill take care",
    "cleaned_text":"awww poor babi im sorri u feel way shit wors come ill take care",
    "normalized_text":"awww poor babi im sorri u feel way shit wors come ill take care",
    "tokens":[
      "awww",
      "poor",
      "babi",
      "im",
      "sorri",
      "feel",
      "way",
      "shit",
      "wor",
      "come",
      "ill",
      "take",
      "care"
    ],
    "token_count":13,
    "processed_text":"awww poor babi im sorri feel way shit wor come ill take care"
  },
  {
    "label":4,
    "text":"heh see youll get hang quotnon decor prettyquot soap thing yet",
    "cleaned_text":"heh see youll get hang quotnon decor prettyquot soap thing yet",
    "normalized_text":"heh see youll get hang quotnon decor prettyquot soap thing yet",
    "tokens":[
      "heh",
      "see",
      "youll",
      "get",
      "hang",
      "quotnon",
      "decor",
      "prettyquot",
      "soap",
      "thing",
      "yet"
    ],
    "token_count":11,
    "processed_text":"heh see youll get hang quotnon decor prettyquot soap thing yet"
  },
  {
    "label":4,
    "text":"hi bruce gonna michel show need help set url show technolog thing",
    "cleaned_text":"hi bruce gonna michel show need help set url show technolog thing",
    "normalized_text":"hi bruce gonna michel show need help set url show technolog thing",
    "tokens":[
      "hi",
      "bruce",
      "gon",
      "na",
      "michel",
      "show",
      "need",
      "help",
      "set",
      "url",
      "show",
      "technolog",
      "thing"
    ],
    "token_count":13,
    "processed_text":"hi bruce gon na michel show need help set url show technolog thing"
  },
  {
    "label":0,
    "text":"appar cant call marsh im pulsa know sumthin wanna tell miss",
    "cleaned_text":"appar cant call marsh im pulsa know sumthin wanna tell miss",
    "normalized_text":"appar cant call marsh im pulsa know sumthin wanna tell miss",
    "tokens":[
      "appar",
      "cant",
      "call",
      "marsh",
      "im",
      "pulsa",
      "know",
      "sumthin",
      "wan",
      "na",
      "tell",
      "miss"
    ],
    "token_count":12,
    "processed_text":"appar cant call marsh im pulsa know sumthin wan na tell miss"
  },
  {
    "label":4,
    "text":"wow what spam follow morn oh well",
    "cleaned_text":"wow what spam follow morn oh well",
    "normalized_text":"wow what spam follow morn oh well",
    "tokens":[
      "wow",
      "spam",
      "follow",
      "morn",
      "oh",
      "well"
    ],
    "token_count":6,
    "processed_text":"wow spam follow morn oh well"
  },
  {
    "label":0,
    "text":"bugger road close",
    "cleaned_text":"bugger road close",
    "normalized_text":"bugger road close",
    "tokens":[
      "bugger",
      "road",
      "close"
    ],
    "token_count":3,
    "processed_text":"bugger road close"
  },
  {
    "label":4,
    "text":"follow follow amp ill follow dday jayz robotpickuplin tetri sim palm pre england",
    "cleaned_text":"follow follow amp ill follow dday jayz robotpickuplin tetri sim palm pre england",
    "normalized_text":"follow follow amp ill follow dday jayz robotpickuplin tetri sim palm pre england",
    "tokens":[
      "follow",
      "follow",
      "amp",
      "ill",
      "follow",
      "dday",
      "jayz",
      "robotpickuplin",
      "tetri",
      "sim",
      "palm",
      "pre",
      "england"
    ],
    "token_count":13,
    "processed_text":"follow follow amp ill follow dday jayz robotpickuplin tetri sim palm pre england"
  },
  {
    "label":4,
    "text":"knoooow feel",
    "cleaned_text":"knoooow feel",
    "normalized_text":"knoooow feel",
    "tokens":[
      "knoooow",
      "feel"
    ],
    "token_count":2,
    "processed_text":"knoooow feel"
  },
  {
    "label":4,
    "text":"real account luci hale hayli duff",
    "cleaned_text":"real account luci hale hayli duff",
    "normalized_text":"real account luci hale hayli duff",
    "tokens":[
      "real",
      "account",
      "luci",
      "hale",
      "hayli",
      "duff"
    ],
    "token_count":6,
    "processed_text":"real account luci hale hayli duff"
  },
  {
    "label":4,
    "text":"shop soon anoth gig tonight look forward see alistair griffin regal room hammersmith",
    "cleaned_text":"shop soon anoth gig tonight look forward see alistair griffin regal room hammersmith",
    "normalized_text":"shop soon anoth gig tonight look forward see alistair griffin regal room hammersmith",
    "tokens":[
      "shop",
      "soon",
      "anoth",
      "gig",
      "tonight",
      "look",
      "forward",
      "see",
      "alistair",
      "griffin",
      "regal",
      "room",
      "hammersmith"
    ],
    "token_count":13,
    "processed_text":"shop soon anoth gig tonight look forward see alistair griffin regal room hammersmith"
  },
  {
    "label":0,
    "text":"wish richard wasnt sick would like see",
    "cleaned_text":"wish richard wasnt sick would like see",
    "normalized_text":"wish richard wasnt sick would like see",
    "tokens":[
      "wish",
      "richard",
      "wasnt",
      "sick",
      "like",
      "see"
    ],
    "token_count":6,
    "processed_text":"wish richard wasnt sick like see"
  },
  {
    "label":0,
    "text":"senior year come end",
    "cleaned_text":"senior year come end",
    "normalized_text":"senior year come end",
    "tokens":[
      "senior",
      "year",
      "come",
      "end"
    ],
    "token_count":4,
    "processed_text":"senior year come end"
  },
  {
    "label":4,
    "text":"im watch like mind tht kid black jacket mad learn first date bass today time perfect",
    "cleaned_text":"im watch like mind tht kid black jacket mad learn first date bass today time perfect",
    "normalized_text":"im watch like mind tht kid black jacket mad learn first date bass today time perfect",
    "tokens":[
      "im",
      "watch",
      "like",
      "mind",
      "tht",
      "kid",
      "black",
      "jacket",
      "mad",
      "learn",
      "first",
      "date",
      "bass",
      "today",
      "time",
      "perfect"
    ],
    "token_count":16,
    "processed_text":"im watch like mind tht kid black jacket mad learn first date bass today time perfect"
  },
  {
    "label":0,
    "text":"forgot surprisingli cute age",
    "cleaned_text":"forgot surprisingli cute age",
    "normalized_text":"forgot surprisingli cute age",
    "tokens":[
      "forgot",
      "surprisingli",
      "cute",
      "age"
    ],
    "token_count":4,
    "processed_text":"forgot surprisingli cute age"
  },
  {
    "label":4,
    "text":"chanc im watch new moon trailer th time",
    "cleaned_text":"chanc im watch new moon trailer th time",
    "normalized_text":"chanc im watch new moon trailer th time",
    "tokens":[
      "chanc",
      "im",
      "watch",
      "new",
      "moon",
      "trailer",
      "th",
      "time"
    ],
    "token_count":8,
    "processed_text":"chanc im watch new moon trailer th time"
  },
  {
    "label":4,
    "text":"u two fun amp think acha",
    "cleaned_text":"u two fun amp think acha",
    "normalized_text":"u two fun amp think acha",
    "tokens":[
      "two",
      "fun",
      "amp",
      "think",
      "acha"
    ],
    "token_count":5,
    "processed_text":"two fun amp think acha"
  },
  {
    "label":4,
    "text":"lolzzzz wow",
    "cleaned_text":"lolzzzz wow",
    "normalized_text":"lolzzzz wow",
    "tokens":[
      "lolzzzz",
      "wow"
    ],
    "token_count":2,
    "processed_text":"lolzzzz wow"
  },
  {
    "label":4,
    "text":"love old grey",
    "cleaned_text":"love old grey",
    "normalized_text":"love old grey",
    "tokens":[
      "love",
      "old",
      "grey"
    ],
    "token_count":3,
    "processed_text":"love old grey"
  },
  {
    "label":0,
    "text":"get option x",
    "cleaned_text":"get option x",
    "normalized_text":"get option x",
    "tokens":[
      "get",
      "option"
    ],
    "token_count":2,
    "processed_text":"get option"
  },
  {
    "label":4,
    "text":"pool tha crew",
    "cleaned_text":"pool tha crew",
    "normalized_text":"pool tha crew",
    "tokens":[
      "pool",
      "tha",
      "crew"
    ],
    "token_count":3,
    "processed_text":"pool tha crew"
  },
  {
    "label":0,
    "text":"get headach",
    "cleaned_text":"get headach",
    "normalized_text":"get headach",
    "tokens":[
      "get",
      "headach"
    ],
    "token_count":2,
    "processed_text":"get headach"
  },
  {
    "label":0,
    "text":"there noth tv",
    "cleaned_text":"there noth tv",
    "normalized_text":"there noth tv",
    "tokens":[
      "noth",
      "tv"
    ],
    "token_count":2,
    "processed_text":"noth tv"
  },
  {
    "label":0,
    "text":"tammi need good drug doin",
    "cleaned_text":"tammi need good drug doin",
    "normalized_text":"tammi need good drug doin",
    "tokens":[
      "tammi",
      "need",
      "good",
      "drug",
      "doin"
    ],
    "token_count":5,
    "processed_text":"tammi need good drug doin"
  },
  {
    "label":0,
    "text":"im bore go nan hous soon im gonna evan bore",
    "cleaned_text":"im bore go nan hous soon im gonna evan bore",
    "normalized_text":"im bore go nan hous soon im gonna evan bore",
    "tokens":[
      "im",
      "bore",
      "go",
      "nan",
      "hou",
      "soon",
      "im",
      "gon",
      "na",
      "evan",
      "bore"
    ],
    "token_count":11,
    "processed_text":"im bore go nan hou soon im gon na evan bore"
  },
  {
    "label":0,
    "text":"matter mani time tweet never replybum",
    "cleaned_text":"matter mani time tweet never replybum",
    "normalized_text":"matter mani time tweet never replybum",
    "tokens":[
      "matter",
      "mani",
      "time",
      "tweet",
      "never",
      "replybum"
    ],
    "token_count":6,
    "processed_text":"matter mani time tweet never replybum"
  },
  {
    "label":4,
    "text":"haha pic random dude thought cool took pic",
    "cleaned_text":"haha pic random dude thought cool took pic",
    "normalized_text":"haha pic random dude thought cool took pic",
    "tokens":[
      "haha",
      "pic",
      "random",
      "dude",
      "thought",
      "cool",
      "took",
      "pic"
    ],
    "token_count":8,
    "processed_text":"haha pic random dude thought cool took pic"
  },
  {
    "label":0,
    "text":"grabupcom unreach day",
    "cleaned_text":"grabupcom unreach day",
    "normalized_text":"grabupcom unreach day",
    "tokens":[
      "grabupcom",
      "unreach",
      "day"
    ],
    "token_count":3,
    "processed_text":"grabupcom unreach day"
  },
  {
    "label":4,
    "text":"happi birthday mate well least footbal last night town even rememb",
    "cleaned_text":"happi birthday mate well least footbal last night town even rememb",
    "normalized_text":"happi birthday mate well least footbal last night town even rememb",
    "tokens":[
      "happi",
      "birthday",
      "mate",
      "well",
      "least",
      "footbal",
      "last",
      "night",
      "town",
      "even",
      "rememb"
    ],
    "token_count":11,
    "processed_text":"happi birthday mate well least footbal last night town even rememb"
  },
  {
    "label":4,
    "text":"quick swim severn",
    "cleaned_text":"quick swim severn",
    "normalized_text":"quick swim severn",
    "tokens":[
      "quick",
      "swim",
      "severn"
    ],
    "token_count":3,
    "processed_text":"quick swim severn"
  },
  {
    "label":0,
    "text":"happen last week weather",
    "cleaned_text":"happen last week weather",
    "normalized_text":"happen last week weather",
    "tokens":[
      "happen",
      "last",
      "week",
      "weather"
    ],
    "token_count":4,
    "processed_text":"happen last week weather"
  },
  {
    "label":0,
    "text":"epic spam dad think serena ugli like quotwhi get old ladi play girl high schoolquot",
    "cleaned_text":"epic spam dad think serena ugli like quotwhi get old ladi play girl high schoolquot",
    "normalized_text":"epic spam dad think serena ugli like quotwhi get old ladi play girl high schoolquot",
    "tokens":[
      "epic",
      "spam",
      "dad",
      "think",
      "serena",
      "ugli",
      "like",
      "quotwhi",
      "get",
      "old",
      "ladi",
      "play",
      "girl",
      "high",
      "schoolquot"
    ],
    "token_count":15,
    "processed_text":"epic spam dad think serena ugli like quotwhi get old ladi play girl high schoolquot"
  },
  {
    "label":4,
    "text":"eatin oh boy",
    "cleaned_text":"eatin oh boy",
    "normalized_text":"eatin oh boy",
    "tokens":[
      "eatin",
      "oh",
      "boy"
    ],
    "token_count":3,
    "processed_text":"eatin oh boy"
  },
  {
    "label":4,
    "text":"earli gym day morrow sleepi time tomorrow crusad reach music supervisor weed begin believ shall",
    "cleaned_text":"earli gym day morrow sleepi time tomorrow crusad reach music supervisor weed begin believ shall",
    "normalized_text":"earli gym day morrow sleepi time tomorrow crusad reach music supervisor weed begin believ shall",
    "tokens":[
      "earli",
      "gym",
      "day",
      "morrow",
      "sleepi",
      "time",
      "tomorrow",
      "crusad",
      "reach",
      "music",
      "supervisor",
      "weed",
      "begin",
      "believ"
    ],
    "token_count":14,
    "processed_text":"earli gym day morrow sleepi time tomorrow crusad reach music supervisor weed begin believ"
  },
  {
    "label":4,
    "text":"love smiley face",
    "cleaned_text":"love smiley face",
    "normalized_text":"love smiley face",
    "tokens":[
      "love",
      "smiley",
      "face"
    ],
    "token_count":3,
    "processed_text":"love smiley face"
  },
  {
    "label":4,
    "text":"welcom littl twit",
    "cleaned_text":"welcom littl twit",
    "normalized_text":"welcom littl twit",
    "tokens":[
      "welcom",
      "littl",
      "twit"
    ],
    "token_count":3,
    "processed_text":"welcom littl twit"
  },
  {
    "label":4,
    "text":"hurndon hous go best buy",
    "cleaned_text":"hurndon hous go best buy",
    "normalized_text":"hurndon hous go best buy",
    "tokens":[
      "hurndon",
      "hou",
      "go",
      "best",
      "buy"
    ],
    "token_count":5,
    "processed_text":"hurndon hou go best buy"
  },
  {
    "label":0,
    "text":"awww boo miss young dracula co sleep",
    "cleaned_text":"awww boo miss young dracula co sleep",
    "normalized_text":"awww boo miss young dracula co sleep",
    "tokens":[
      "awww",
      "boo",
      "miss",
      "young",
      "dracula",
      "co",
      "sleep"
    ],
    "token_count":7,
    "processed_text":"awww boo miss young dracula co sleep"
  },
  {
    "label":4,
    "text":"ill put name say door need anymor xo",
    "cleaned_text":"ill put name say door need anymor xo",
    "normalized_text":"ill put name say door need anymor xo",
    "tokens":[
      "ill",
      "put",
      "name",
      "say",
      "door",
      "need",
      "anymor",
      "xo"
    ],
    "token_count":8,
    "processed_text":"ill put name say door need anymor xo"
  },
  {
    "label":4,
    "text":"im listen miley",
    "cleaned_text":"im listen miley",
    "normalized_text":"im listen miley",
    "tokens":[
      "im",
      "listen",
      "miley"
    ],
    "token_count":3,
    "processed_text":"im listen miley"
  },
  {
    "label":0,
    "text":"miss boyfriend love joshua daniel hawk",
    "cleaned_text":"miss boyfriend love joshua daniel hawk",
    "normalized_text":"miss boyfriend love joshua daniel hawk",
    "tokens":[
      "miss",
      "boyfriend",
      "love",
      "joshua",
      "daniel",
      "hawk"
    ],
    "token_count":6,
    "processed_text":"miss boyfriend love joshua daniel hawk"
  },
  {
    "label":4,
    "text":"eat ice cream bad doubl dutch alreadi eaten hehe",
    "cleaned_text":"eat ice cream bad doubl dutch alreadi eaten hehe",
    "normalized_text":"eat ice cream bad doubl dutch alreadi eaten hehe",
    "tokens":[
      "eat",
      "ice",
      "cream",
      "bad",
      "doubl",
      "dutch",
      "alreadi",
      "eaten",
      "hehe"
    ],
    "token_count":9,
    "processed_text":"eat ice cream bad doubl dutch alreadi eaten hehe"
  },
  {
    "label":4,
    "text":"happi pic everyon visit comment also better buy",
    "cleaned_text":"happi pic everyon visit comment also better buy",
    "normalized_text":"happi pic everyon visit comment also better buy",
    "tokens":[
      "happi",
      "pic",
      "everyon",
      "visit",
      "comment",
      "also",
      "better",
      "buy"
    ],
    "token_count":8,
    "processed_text":"happi pic everyon visit comment also better buy"
  },
  {
    "label":0,
    "text":"wish man dream would come pick put shoulder seem unlik day age",
    "cleaned_text":"wish man dream would come pick put shoulder seem unlik day age",
    "normalized_text":"wish man dream would come pick put shoulder seem unlik day age",
    "tokens":[
      "wish",
      "man",
      "dream",
      "come",
      "pick",
      "put",
      "shoulder",
      "seem",
      "unlik",
      "day",
      "age"
    ],
    "token_count":11,
    "processed_text":"wish man dream come pick put shoulder seem unlik day age"
  },
  {
    "label":4,
    "text":"happi mother day love mommi",
    "cleaned_text":"happi mother day love mommi",
    "normalized_text":"happi mother day love mommi",
    "tokens":[
      "happi",
      "mother",
      "day",
      "love",
      "mommi"
    ],
    "token_count":5,
    "processed_text":"happi mother day love mommi"
  },
  {
    "label":4,
    "text":"lol thought look gross",
    "cleaned_text":"lol thought look gross",
    "normalized_text":"lol thought look gross",
    "tokens":[
      "lol",
      "thought",
      "look",
      "gross"
    ],
    "token_count":4,
    "processed_text":"lol thought look gross"
  },
  {
    "label":4,
    "text":"morn tweet",
    "cleaned_text":"morn tweet",
    "normalized_text":"morn tweet",
    "tokens":[
      "morn",
      "tweet"
    ],
    "token_count":2,
    "processed_text":"morn tweet"
  },
  {
    "label":4,
    "text":"definit real man ooh song idea",
    "cleaned_text":"definit real man ooh song idea",
    "normalized_text":"definit real man ooh song idea",
    "tokens":[
      "definit",
      "real",
      "man",
      "ooh",
      "song",
      "idea"
    ],
    "token_count":6,
    "processed_text":"definit real man ooh song idea"
  },
  {
    "label":0,
    "text":"r u fun wit da babi kitti amp littl kid play wif babi around",
    "cleaned_text":"r u fun wit da babi kitti amp littl kid play wif babi around",
    "normalized_text":"r u fun wit da babi kitti amp littl kid play wif babi around",
    "tokens":[
      "fun",
      "wit",
      "da",
      "babi",
      "kitti",
      "amp",
      "littl",
      "kid",
      "play",
      "wif",
      "babi",
      "around"
    ],
    "token_count":12,
    "processed_text":"fun wit da babi kitti amp littl kid play wif babi around"
  },
  {
    "label":0,
    "text":"suppos ask type question ir audit",
    "cleaned_text":"suppos ask type question ir audit",
    "normalized_text":"suppos ask type question ir audit",
    "tokens":[
      "suppo",
      "ask",
      "type",
      "question",
      "ir",
      "audit"
    ],
    "token_count":6,
    "processed_text":"suppo ask type question ir audit"
  },
  {
    "label":4,
    "text":"snuggl new kitti",
    "cleaned_text":"snuggl new kitti",
    "normalized_text":"snuggl new kitti",
    "tokens":[
      "snuggl",
      "new",
      "kitti"
    ],
    "token_count":3,
    "processed_text":"snuggl new kitti"
  },
  {
    "label":4,
    "text":"lol aaw guess that still fun",
    "cleaned_text":"lol aaw guess that still fun",
    "normalized_text":"lol aaw guess that still fun",
    "tokens":[
      "lol",
      "aaw",
      "guess",
      "still",
      "fun"
    ],
    "token_count":5,
    "processed_text":"lol aaw guess still fun"
  },
  {
    "label":4,
    "text":"mention earlier danni hugger gosh wish hug someday seem realli huggabl",
    "cleaned_text":"mention earlier danni hugger gosh wish hug someday seem realli huggabl",
    "normalized_text":"mention earlier danni hugger gosh wish hug someday seem realli huggabl",
    "tokens":[
      "mention",
      "earlier",
      "danni",
      "hugger",
      "gosh",
      "wish",
      "hug",
      "someday",
      "seem",
      "realli",
      "huggabl"
    ],
    "token_count":11,
    "processed_text":"mention earlier danni hugger gosh wish hug someday seem realli huggabl"
  },
  {
    "label":0,
    "text":"yr old jump bed usand pretti much kiss good night sleep goodby",
    "cleaned_text":"yr old jump bed usand pretti much kiss good night sleep goodby",
    "normalized_text":"yr old jump bed usand pretti much kiss good night sleep goodby",
    "tokens":[
      "yr",
      "old",
      "jump",
      "bed",
      "usand",
      "pretti",
      "much",
      "kiss",
      "good",
      "night",
      "sleep",
      "goodbi"
    ],
    "token_count":12,
    "processed_text":"yr old jump bed usand pretti much kiss good night sleep goodbi"
  },
  {
    "label":4,
    "text":"wooo e awesom new psp go leak final news",
    "cleaned_text":"wooo e awesom new psp go leak final news",
    "normalized_text":"wooo e awesom new psp go leak final news",
    "tokens":[
      "wooo",
      "awesom",
      "new",
      "psp",
      "go",
      "leak",
      "final",
      "news"
    ],
    "token_count":8,
    "processed_text":"wooo awesom new psp go leak final news"
  },
  {
    "label":0,
    "text":"sleepless night need good book noth appeal right",
    "cleaned_text":"sleepless night need good book noth appeal right",
    "normalized_text":"sleepless night need good book noth appeal right",
    "tokens":[
      "sleepless",
      "night",
      "need",
      "good",
      "book",
      "noth",
      "appeal",
      "right"
    ],
    "token_count":8,
    "processed_text":"sleepless night need good book noth appeal right"
  },
  {
    "label":0,
    "text":"realli tomorrow though mayb free",
    "cleaned_text":"realli tomorrow though mayb free",
    "normalized_text":"realli tomorrow though mayb free",
    "tokens":[
      "realli",
      "tomorrow",
      "though",
      "mayb",
      "free"
    ],
    "token_count":5,
    "processed_text":"realli tomorrow though mayb free"
  },
  {
    "label":0,
    "text":"wake upahh headach",
    "cleaned_text":"wake upahh headach",
    "normalized_text":"wake upahh headach",
    "tokens":[
      "wake",
      "upahh",
      "headach"
    ],
    "token_count":3,
    "processed_text":"wake upahh headach"
  },
  {
    "label":4,
    "text":"home gonna hw could final go home lunch",
    "cleaned_text":"home gonna hw could final go home lunch",
    "normalized_text":"home gonna hw could final go home lunch",
    "tokens":[
      "home",
      "gon",
      "na",
      "hw",
      "final",
      "go",
      "home",
      "lunch"
    ],
    "token_count":8,
    "processed_text":"home gon na hw final go home lunch"
  },
  {
    "label":0,
    "text":"technic camera arent allow concert still id love photo nkotb cruis",
    "cleaned_text":"technic camera arent allow concert still id love photo nkotb cruis",
    "normalized_text":"technic camera arent allow concert still id love photo nkotb cruis",
    "tokens":[
      "technic",
      "camera",
      "arent",
      "allow",
      "concert",
      "still",
      "id",
      "love",
      "photo",
      "nkotb",
      "crui"
    ],
    "token_count":11,
    "processed_text":"technic camera arent allow concert still id love photo nkotb crui"
  },
  {
    "label":0,
    "text":"ive done ww need cold war that biggest topic",
    "cleaned_text":"ive done ww need cold war that biggest topic",
    "normalized_text":"ive done ww need cold war that biggest topic",
    "tokens":[
      "ive",
      "done",
      "ww",
      "need",
      "cold",
      "war",
      "biggest",
      "topic"
    ],
    "token_count":8,
    "processed_text":"ive done ww need cold war biggest topic"
  },
  {
    "label":0,
    "text":"stop play tweeti without fair stuck work",
    "cleaned_text":"stop play tweeti without fair stuck work",
    "normalized_text":"stop play tweeti without fair stuck work",
    "tokens":[
      "stop",
      "play",
      "tweeti",
      "without",
      "fair",
      "stuck",
      "work"
    ],
    "token_count":7,
    "processed_text":"stop play tweeti without fair stuck work"
  },
  {
    "label":4,
    "text":"keith jarret one seed quotconcentrationquot channel pandora write cant resist american rout",
    "cleaned_text":"keith jarret one seed quotconcentrationquot channel pandora write cant resist american rout",
    "normalized_text":"keith jarret one seed quotconcentrationquot channel pandora write cant resist american rout",
    "tokens":[
      "keith",
      "jarret",
      "one",
      "seed",
      "channel",
      "pandora",
      "write",
      "cant",
      "resist",
      "american",
      "rout"
    ],
    "token_count":11,
    "processed_text":"keith jarret one seed channel pandora write cant resist american rout"
  },
  {
    "label":0,
    "text":"sad tonight dont know want sammi ok",
    "cleaned_text":"sad tonight dont know want sammi ok",
    "normalized_text":"sad tonight dont know want sammi ok",
    "tokens":[
      "sad",
      "tonight",
      "dont",
      "know",
      "want",
      "sammi",
      "ok"
    ],
    "token_count":7,
    "processed_text":"sad tonight dont know want sammi ok"
  },
  {
    "label":4,
    "text":"air breath like pill know kind thing make chang daniel odonnel jim reev",
    "cleaned_text":"air breath like pill know kind thing make chang daniel odonnel jim reev",
    "normalized_text":"air breath like pill know kind thing make chang daniel odonnel jim reev",
    "tokens":[
      "air",
      "breath",
      "like",
      "pill",
      "know",
      "kind",
      "thing",
      "make",
      "chang",
      "daniel",
      "odonnel",
      "jim",
      "reev"
    ],
    "token_count":13,
    "processed_text":"air breath like pill know kind thing make chang daniel odonnel jim reev"
  },
  {
    "label":0,
    "text":"serious suck much want hang everyon right like use",
    "cleaned_text":"serious suck much want hang everyon right like use",
    "normalized_text":"serious suck much want hang everyon right like use",
    "tokens":[
      "seriou",
      "suck",
      "much",
      "want",
      "hang",
      "everyon",
      "right",
      "like",
      "use"
    ],
    "token_count":9,
    "processed_text":"seriou suck much want hang everyon right like use"
  },
  {
    "label":4,
    "text":"pretti nifti day today go emili housese arrongo arthursback mine",
    "cleaned_text":"pretti nifti day today go emili housese arrongo arthursback mine",
    "normalized_text":"pretti nifti day today go emili housese arrongo arthursback mine",
    "tokens":[
      "pretti",
      "nifti",
      "day",
      "today",
      "go",
      "emili",
      "houses",
      "arrongo",
      "arthursback",
      "mine"
    ],
    "token_count":10,
    "processed_text":"pretti nifti day today go emili houses arrongo arthursback mine"
  },
  {
    "label":0,
    "text":"work movi theatr toledo id let come see free",
    "cleaned_text":"work movi theatr toledo id let come see free",
    "normalized_text":"work movi theatr toledo id let come see free",
    "tokens":[
      "work",
      "movi",
      "theatr",
      "toledo",
      "id",
      "let",
      "come",
      "see",
      "free"
    ],
    "token_count":9,
    "processed_text":"work movi theatr toledo id let come see free"
  },
  {
    "label":4,
    "text":"beauti day southern ireland sun scorch",
    "cleaned_text":"beauti day southern ireland sun scorch",
    "normalized_text":"beauti day southern ireland sun scorch",
    "tokens":[
      "beauti",
      "day",
      "southern",
      "ireland",
      "sun",
      "scorch"
    ],
    "token_count":6,
    "processed_text":"beauti day southern ireland sun scorch"
  },
  {
    "label":0,
    "text":"think im gunna go play sim iphon stupid uk wish mac",
    "cleaned_text":"think im gunna go play sim iphon stupid uk wish mac",
    "normalized_text":"think im gunna go play sim iphon stupid uk wish mac",
    "tokens":[
      "think",
      "im",
      "gunna",
      "go",
      "play",
      "sim",
      "iphon",
      "stupid",
      "uk",
      "wish",
      "mac"
    ],
    "token_count":11,
    "processed_text":"think im gunna go play sim iphon stupid uk wish mac"
  },
  {
    "label":0,
    "text":"broken charger",
    "cleaned_text":"broken charger",
    "normalized_text":"broken charger",
    "tokens":[
      "broken",
      "charger"
    ],
    "token_count":2,
    "processed_text":"broken charger"
  },
  {
    "label":4,
    "text":"sound like fun place day mayb offic could requisit helmet",
    "cleaned_text":"sound like fun place day mayb offic could requisit helmet",
    "normalized_text":"sound like fun place day mayb offic could requisit helmet",
    "tokens":[
      "sound",
      "like",
      "fun",
      "place",
      "day",
      "mayb",
      "offic",
      "requisit",
      "helmet"
    ],
    "token_count":9,
    "processed_text":"sound like fun place day mayb offic requisit helmet"
  },
  {
    "label":4,
    "text":"hahaha iz okay funni read convers hahahaha mwah",
    "cleaned_text":"hahaha iz okay funni read convers hahahaha mwah",
    "normalized_text":"hahaha iz okay funni read convers hahahaha mwah",
    "tokens":[
      "hahaha",
      "iz",
      "okay",
      "funni",
      "read",
      "conver",
      "hahahaha",
      "mwah"
    ],
    "token_count":8,
    "processed_text":"hahaha iz okay funni read conver hahahaha mwah"
  },
  {
    "label":4,
    "text":"watch guy radio show entertain enjoy much",
    "cleaned_text":"watch guy radio show entertain enjoy much",
    "normalized_text":"watch guy radio show entertain enjoy much",
    "tokens":[
      "watch",
      "guy",
      "radio",
      "show",
      "entertain",
      "enjoy",
      "much"
    ],
    "token_count":7,
    "processed_text":"watch guy radio show entertain enjoy much"
  },
  {
    "label":0,
    "text":"sound like ultim result gun control",
    "cleaned_text":"sound like ultim result gun control",
    "normalized_text":"sound like ultim result gun control",
    "tokens":[
      "sound",
      "like",
      "ultim",
      "result",
      "gun",
      "control"
    ],
    "token_count":6,
    "processed_text":"sound like ultim result gun control"
  },
  {
    "label":0,
    "text":"work relat get soooo morn person",
    "cleaned_text":"work relat get soooo morn person",
    "normalized_text":"work relat get soooo morn person",
    "tokens":[
      "work",
      "relat",
      "get",
      "soooo",
      "morn",
      "person"
    ],
    "token_count":6,
    "processed_text":"work relat get soooo morn person"
  },
  {
    "label":0,
    "text":"hope photo work mine particular photo us didnt work",
    "cleaned_text":"hope photo work mine particular photo us didnt work",
    "normalized_text":"hope photo work mine particular photo us didnt work",
    "tokens":[
      "hope",
      "photo",
      "work",
      "mine",
      "particular",
      "photo",
      "us",
      "didnt",
      "work"
    ],
    "token_count":9,
    "processed_text":"hope photo work mine particular photo us didnt work"
  },
  {
    "label":0,
    "text":"haha kno",
    "cleaned_text":"haha kno",
    "normalized_text":"haha kno",
    "tokens":[
      "haha",
      "kno"
    ],
    "token_count":2,
    "processed_text":"haha kno"
  },
  {
    "label":4,
    "text":"long weekend sleep saturday move back room carpet done yay",
    "cleaned_text":"long weekend sleep saturday move back room carpet done yay",
    "normalized_text":"long weekend sleep saturday move back room carpet done yay",
    "tokens":[
      "long",
      "weekend",
      "sleep",
      "saturday",
      "move",
      "back",
      "room",
      "carpet",
      "done",
      "yay"
    ],
    "token_count":10,
    "processed_text":"long weekend sleep saturday move back room carpet done yay"
  },
  {
    "label":4,
    "text":"alway scare lol",
    "cleaned_text":"alway scare lol",
    "normalized_text":"alway scare lol",
    "tokens":[
      "alway",
      "scare",
      "lol"
    ],
    "token_count":3,
    "processed_text":"alway scare lol"
  },
  {
    "label":0,
    "text":"photo post graveyard concret block peopl live around kid play peopl take drug peopl sex",
    "cleaned_text":"photo post graveyard concret block peopl live around kid play peopl take drug peopl sex",
    "normalized_text":"photo post graveyard concret block peopl live around kid play peopl take drug peopl sex",
    "tokens":[
      "photo",
      "post",
      "graveyard",
      "concret",
      "block",
      "peopl",
      "live",
      "around",
      "kid",
      "play",
      "peopl",
      "take",
      "drug",
      "peopl",
      "sex"
    ],
    "token_count":15,
    "processed_text":"photo post graveyard concret block peopl live around kid play peopl take drug peopl sex"
  },
  {
    "label":4,
    "text":"graduat guin gallon challeng woo",
    "cleaned_text":"graduat guin gallon challeng woo",
    "normalized_text":"graduat guin gallon challeng woo",
    "tokens":[
      "graduat",
      "guin",
      "gallon",
      "challeng",
      "woo"
    ],
    "token_count":5,
    "processed_text":"graduat guin gallon challeng woo"
  },
  {
    "label":0,
    "text":"agin",
    "cleaned_text":"agin",
    "normalized_text":"agin",
    "tokens":[
      "agin"
    ],
    "token_count":1,
    "processed_text":"agin"
  },
  {
    "label":4,
    "text":"knowsimon softi",
    "cleaned_text":"knowsimon softi",
    "normalized_text":"knowsimon softi",
    "tokens":[
      "knowsimon",
      "softi"
    ],
    "token_count":2,
    "processed_text":"knowsimon softi"
  },
  {
    "label":4,
    "text":"great game golf nappi nap time roof sun",
    "cleaned_text":"great game golf nappi nap time roof sun",
    "normalized_text":"great game golf nappi nap time roof sun",
    "tokens":[
      "great",
      "game",
      "golf",
      "nappi",
      "nap",
      "time",
      "roof",
      "sun"
    ],
    "token_count":8,
    "processed_text":"great game golf nappi nap time roof sun"
  },
  {
    "label":0,
    "text":"cant resolv worldniccom network solut domain guess outag",
    "cleaned_text":"cant resolv worldniccom network solut domain guess outag",
    "normalized_text":"cant resolv worldniccom network solut domain guess outag",
    "tokens":[
      "cant",
      "resolv",
      "worldniccom",
      "network",
      "solut",
      "domain",
      "guess",
      "outag"
    ],
    "token_count":8,
    "processed_text":"cant resolv worldniccom network solut domain guess outag"
  },
  {
    "label":0,
    "text":"idea fo bff birthdaaay",
    "cleaned_text":"idea fo bff birthdaaay",
    "normalized_text":"idea fo bff birthdaaay",
    "tokens":[
      "idea",
      "fo",
      "bff",
      "birthdaaay"
    ],
    "token_count":4,
    "processed_text":"idea fo bff birthdaaay"
  },
  {
    "label":4,
    "text":"thank hope boy wear week",
    "cleaned_text":"thank hope boy wear week",
    "normalized_text":"thank hope boy wear week",
    "tokens":[
      "thank",
      "hope",
      "boy",
      "wear",
      "week"
    ],
    "token_count":5,
    "processed_text":"thank hope boy wear week"
  },
  {
    "label":0,
    "text":"numb begin wear mouth hurt im hungri terribl combin",
    "cleaned_text":"numb begin wear mouth hurt im hungri terribl combin",
    "normalized_text":"numb begin wear mouth hurt im hungri terribl combin",
    "tokens":[
      "numb",
      "begin",
      "wear",
      "mouth",
      "hurt",
      "im",
      "hungri",
      "terribl",
      "combin"
    ],
    "token_count":9,
    "processed_text":"numb begin wear mouth hurt im hungri terribl combin"
  },
  {
    "label":4,
    "text":"ah sorri hear oh couldnt fit fav drink lemonad especi real strawberri claim jumper",
    "cleaned_text":"ah sorri hear oh couldnt fit fav drink lemonad especi real strawberri claim jumper",
    "normalized_text":"ah sorri hear oh couldnt fit fav drink lemonad especi real strawberri claim jumper",
    "tokens":[
      "ah",
      "sorri",
      "hear",
      "oh",
      "couldnt",
      "fit",
      "fav",
      "drink",
      "lemonad",
      "especi",
      "real",
      "strawberri",
      "claim",
      "jumper"
    ],
    "token_count":14,
    "processed_text":"ah sorri hear oh couldnt fit fav drink lemonad especi real strawberri claim jumper"
  },
  {
    "label":4,
    "text":"hott love song cours love one ha",
    "cleaned_text":"hott love song cours love one ha",
    "normalized_text":"hott love song cours love one ha",
    "tokens":[
      "hott",
      "love",
      "song",
      "cour",
      "love",
      "one",
      "ha"
    ],
    "token_count":7,
    "processed_text":"hott love song cour love one ha"
  },
  {
    "label":0,
    "text":"omg ill tri later let know cant promis anyth im troubl well",
    "cleaned_text":"omg ill tri later let know cant promis anyth im troubl well",
    "normalized_text":"omg ill tri later let know cant promis anyth im troubl well",
    "tokens":[
      "omg",
      "ill",
      "tri",
      "later",
      "let",
      "know",
      "cant",
      "promi",
      "anyth",
      "im",
      "troubl",
      "well"
    ],
    "token_count":12,
    "processed_text":"omg ill tri later let know cant promi anyth im troubl well"
  },
  {
    "label":0,
    "text":"ugh dog run dream kick face complet awak bore",
    "cleaned_text":"ugh dog run dream kick face complet awak bore",
    "normalized_text":"ugh dog run dream kick face complet awak bore",
    "tokens":[
      "ugh",
      "dog",
      "run",
      "dream",
      "kick",
      "face",
      "complet",
      "awak",
      "bore"
    ],
    "token_count":9,
    "processed_text":"ugh dog run dream kick face complet awak bore"
  },
  {
    "label":0,
    "text":"okay prob talk u mrw then go bed soonwork mrw",
    "cleaned_text":"okay prob talk u mrw then go bed soonwork mrw",
    "normalized_text":"okay prob talk u mrw then go bed soonwork mrw",
    "tokens":[
      "okay",
      "prob",
      "talk",
      "mrw",
      "go",
      "bed",
      "soonwork",
      "mrw"
    ],
    "token_count":8,
    "processed_text":"okay prob talk mrw go bed soonwork mrw"
  },
  {
    "label":4,
    "text":"got freind world skype run busi paid version enjoy",
    "cleaned_text":"got freind world skype run busi paid version enjoy",
    "normalized_text":"got freind world skype run busi paid version enjoy",
    "tokens":[
      "got",
      "freind",
      "world",
      "skype",
      "run",
      "busi",
      "paid",
      "version",
      "enjoy"
    ],
    "token_count":9,
    "processed_text":"got freind world skype run busi paid version enjoy"
  },
  {
    "label":0,
    "text":"awww ur sweet sorri hear ur sf peep",
    "cleaned_text":"awww ur sweet sorri hear ur sf peep",
    "normalized_text":"awww ur sweet sorri hear ur sf peep",
    "tokens":[
      "awww",
      "ur",
      "sweet",
      "sorri",
      "hear",
      "ur",
      "sf",
      "peep"
    ],
    "token_count":8,
    "processed_text":"awww ur sweet sorri hear ur sf peep"
  },
  {
    "label":4,
    "text":"drive carefullyse get",
    "cleaned_text":"drive carefullyse get",
    "normalized_text":"drive carefullyse get",
    "tokens":[
      "drive",
      "carefullys",
      "get"
    ],
    "token_count":3,
    "processed_text":"drive carefullys get"
  },
  {
    "label":4,
    "text":"cia os arent smarter indonesian much better obvious",
    "cleaned_text":"cia os arent smarter indonesian much better obvious",
    "normalized_text":"cia os arent smarter indonesian much better obvious",
    "tokens":[
      "cia",
      "os",
      "arent",
      "smarter",
      "indonesian",
      "much",
      "better",
      "obviou"
    ],
    "token_count":8,
    "processed_text":"cia os arent smarter indonesian much better obviou"
  },
  {
    "label":0,
    "text":"anyon want buy new earphon ye",
    "cleaned_text":"anyon want buy new earphon ye",
    "normalized_text":"anyon want buy new earphon ye",
    "tokens":[
      "anyon",
      "want",
      "buy",
      "new",
      "earphon",
      "ye"
    ],
    "token_count":6,
    "processed_text":"anyon want buy new earphon ye"
  },
  {
    "label":0,
    "text":"twitter homepag mistyp correctli point left slash",
    "cleaned_text":"twitter homepag mistyp correctli point left slash",
    "normalized_text":"twitter homepag mistyp correctli point left slash",
    "tokens":[
      "twitter",
      "homepag",
      "mistyp",
      "correctli",
      "point",
      "left",
      "slash"
    ],
    "token_count":7,
    "processed_text":"twitter homepag mistyp correctli point left slash"
  },
  {
    "label":4,
    "text":"aww your star johnni thank",
    "cleaned_text":"aww your star johnni thank",
    "normalized_text":"aww your star johnni thank",
    "tokens":[
      "aww",
      "star",
      "johnni",
      "thank"
    ],
    "token_count":4,
    "processed_text":"aww star johnni thank"
  },
  {
    "label":0,
    "text":"adam version quotonequot like make cri joke ilhsfm",
    "cleaned_text":"adam version quotonequot like make cri joke ilhsfm",
    "normalized_text":"adam version quotonequot like make cri joke ilhsfm",
    "tokens":[
      "adam",
      "version",
      "quotonequot",
      "like",
      "make",
      "cri",
      "joke",
      "ilhsfm"
    ],
    "token_count":8,
    "processed_text":"adam version quotonequot like make cri joke ilhsfm"
  },
  {
    "label":4,
    "text":"yay im go see next month im obsess lol",
    "cleaned_text":"yay im go see next month im obsess lol",
    "normalized_text":"yay im go see next month im obsess lol",
    "tokens":[
      "yay",
      "im",
      "go",
      "see",
      "next",
      "month",
      "im",
      "obsess",
      "lol"
    ],
    "token_count":9,
    "processed_text":"yay im go see next month im obsess lol"
  },
  {
    "label":4,
    "text":"makanna myyi babyy hahahaa ye shake dat booti meat hahaha tutu bday funn",
    "cleaned_text":"makanna myyi babyy hahahaa ye shake dat booti meat hahaha tutu bday funn",
    "normalized_text":"makanna myyi babyy hahahaa ye shake dat booti meat hahaha tutu bday funn",
    "tokens":[
      "makanna",
      "myyi",
      "babyy",
      "hahahaa",
      "ye",
      "shake",
      "dat",
      "booti",
      "meat",
      "hahaha",
      "tutu",
      "bday",
      "funn"
    ],
    "token_count":13,
    "processed_text":"makanna myyi babyy hahahaa ye shake dat booti meat hahaha tutu bday funn"
  },
  {
    "label":4,
    "text":"hey ya",
    "cleaned_text":"hey ya",
    "normalized_text":"hey ya",
    "tokens":[
      "hey",
      "ya"
    ],
    "token_count":2,
    "processed_text":"hey ya"
  },
  {
    "label":4,
    "text":"best freind she tri win comp vote",
    "cleaned_text":"best freind she tri win comp vote",
    "normalized_text":"best freind she tri win comp vote",
    "tokens":[
      "best",
      "freind",
      "tri",
      "win",
      "comp",
      "vote"
    ],
    "token_count":6,
    "processed_text":"best freind tri win comp vote"
  },
  {
    "label":0,
    "text":"game rain read bedcap morn",
    "cleaned_text":"game rain read bedcap morn",
    "normalized_text":"game rain read bedcap morn",
    "tokens":[
      "game",
      "rain",
      "read",
      "bedcap",
      "morn"
    ],
    "token_count":5,
    "processed_text":"game rain read bedcap morn"
  },
  {
    "label":4,
    "text":"work day polish playtest tomorrow packag build tomorrow night time exhaust",
    "cleaned_text":"work day polish playtest tomorrow packag build tomorrow night time exhaust",
    "normalized_text":"work day polish playtest tomorrow packag build tomorrow night time exhaust",
    "tokens":[
      "work",
      "day",
      "polish",
      "playtest",
      "tomorrow",
      "packag",
      "build",
      "tomorrow",
      "night",
      "time",
      "exhaust"
    ],
    "token_count":11,
    "processed_text":"work day polish playtest tomorrow packag build tomorrow night time exhaust"
  },
  {
    "label":4,
    "text":"hope mini store safe ur garag",
    "cleaned_text":"hope mini store safe ur garag",
    "normalized_text":"hope mini store safe ur garag",
    "tokens":[
      "hope",
      "mini",
      "store",
      "safe",
      "ur",
      "garag"
    ],
    "token_count":6,
    "processed_text":"hope mini store safe ur garag"
  },
  {
    "label":4,
    "text":"first day fox",
    "cleaned_text":"first day fox",
    "normalized_text":"first day fox",
    "tokens":[
      "first",
      "day",
      "fox"
    ],
    "token_count":3,
    "processed_text":"first day fox"
  },
  {
    "label":0,
    "text":"although incred awkward scene fight enjoy expect shirtless shot chan though",
    "cleaned_text":"although incred awkward scene fight enjoy expect shirtless shot chan though",
    "normalized_text":"although incred awkward scene fight enjoy expect shirtless shot chan though",
    "tokens":[
      "although",
      "incr",
      "awkward",
      "scene",
      "fight",
      "enjoy",
      "expect",
      "shirtless",
      "shot",
      "chan",
      "though"
    ],
    "token_count":11,
    "processed_text":"although incr awkward scene fight enjoy expect shirtless shot chan though"
  },
  {
    "label":4,
    "text":"ye im sick anymor",
    "cleaned_text":"ye im sick anymor",
    "normalized_text":"ye im sick anymor",
    "tokens":[
      "ye",
      "im",
      "sick",
      "anymor"
    ],
    "token_count":4,
    "processed_text":"ye im sick anymor"
  },
  {
    "label":0,
    "text":"thank buddi didnt win though",
    "cleaned_text":"thank buddi didnt win though",
    "normalized_text":"thank buddi didnt win though",
    "tokens":[
      "thank",
      "buddi",
      "didnt",
      "win",
      "though"
    ],
    "token_count":5,
    "processed_text":"thank buddi didnt win though"
  },
  {
    "label":0,
    "text":"im tire night xo",
    "cleaned_text":"im tire night xo",
    "normalized_text":"im tire night xo",
    "tokens":[
      "im",
      "tire",
      "night",
      "xo"
    ],
    "token_count":4,
    "processed_text":"im tire night xo"
  },
  {
    "label":0,
    "text":"beach still work later class",
    "cleaned_text":"beach still work later class",
    "normalized_text":"beach still work later class",
    "tokens":[
      "beach",
      "still",
      "work",
      "later",
      "class"
    ],
    "token_count":5,
    "processed_text":"beach still work later class"
  },
  {
    "label":0,
    "text":"wwwaddictinggamescom im hella bore",
    "cleaned_text":"wwwaddictinggamescom im hella bore",
    "normalized_text":"wwwaddictinggamescom im hella bore",
    "tokens":[
      "im",
      "hella",
      "bore"
    ],
    "token_count":3,
    "processed_text":"im hella bore"
  },
  {
    "label":0,
    "text":"least rain",
    "cleaned_text":"least rain",
    "normalized_text":"least rain",
    "tokens":[
      "least",
      "rain"
    ],
    "token_count":2,
    "processed_text":"least rain"
  },
  {
    "label":4,
    "text":"yup would rather focu import thing",
    "cleaned_text":"yup would rather focu import thing",
    "normalized_text":"yup would rather focu import thing",
    "tokens":[
      "yup",
      "rather",
      "focu",
      "import",
      "thing"
    ],
    "token_count":5,
    "processed_text":"yup rather focu import thing"
  },
  {
    "label":4,
    "text":"deliv dsara utama what absolut musttri item menu",
    "cleaned_text":"deliv dsara utama what absolut musttri item menu",
    "normalized_text":"deliv dsara utama what absolut musttri item menu",
    "tokens":[
      "deliv",
      "dsara",
      "utama",
      "absolut",
      "musttri",
      "item",
      "menu"
    ],
    "token_count":7,
    "processed_text":"deliv dsara utama absolut musttri item menu"
  },
  {
    "label":0,
    "text":"hit gym damn lost check anyon seen haha",
    "cleaned_text":"hit gym damn lost check anyon seen haha",
    "normalized_text":"hit gym damn lost check anyon seen haha",
    "tokens":[
      "hit",
      "gym",
      "damn",
      "lost",
      "check",
      "anyon",
      "seen",
      "haha"
    ],
    "token_count":8,
    "processed_text":"hit gym damn lost check anyon seen haha"
  },
  {
    "label":0,
    "text":"tmobil alway take shit girl either phone servicealway bummer",
    "cleaned_text":"tmobil alway take shit girl either phone servicealway bummer",
    "normalized_text":"tmobil alway take shit girl either phone servicealway bummer",
    "tokens":[
      "tmobil",
      "alway",
      "take",
      "shit",
      "girl",
      "either",
      "phone",
      "servicealway",
      "bummer"
    ],
    "token_count":9,
    "processed_text":"tmobil alway take shit girl either phone servicealway bummer"
  },
  {
    "label":0,
    "text":"still sick hope today day recoveri ugh n stupid class",
    "cleaned_text":"still sick hope today day recoveri ugh n stupid class",
    "normalized_text":"still sick hope today day recoveri ugh n stupid class",
    "tokens":[
      "still",
      "sick",
      "hope",
      "today",
      "day",
      "recoveri",
      "ugh",
      "stupid",
      "class"
    ],
    "token_count":9,
    "processed_text":"still sick hope today day recoveri ugh stupid class"
  },
  {
    "label":0,
    "text":"thinkingwatz go becom clubhos nxt yr mean amp claud still ur ditchin us smh",
    "cleaned_text":"thinkingwatz go becom clubhos nxt yr mean amp claud still ur ditchin us smh",
    "normalized_text":"thinkingwatz go becom clubhos nxt yr mean amp claud still ur ditchin us smh",
    "tokens":[
      "thinkingwatz",
      "go",
      "becom",
      "clubho",
      "nxt",
      "yr",
      "mean",
      "amp",
      "claud",
      "still",
      "ur",
      "ditchin",
      "us",
      "smh"
    ],
    "token_count":14,
    "processed_text":"thinkingwatz go becom clubho nxt yr mean amp claud still ur ditchin us smh"
  },
  {
    "label":0,
    "text":"ive given",
    "cleaned_text":"ive given",
    "normalized_text":"ive given",
    "tokens":[
      "ive",
      "given"
    ],
    "token_count":2,
    "processed_text":"ive given"
  },
  {
    "label":4,
    "text":"nepsal jsem ze je novinka byl retweet",
    "cleaned_text":"nepsal jsem ze je novinka byl retweet",
    "normalized_text":"nepsal jsem ze je novinka byl retweet",
    "tokens":[
      "nepsal",
      "jsem",
      "ze",
      "je",
      "novinka",
      "byl",
      "retweet"
    ],
    "token_count":7,
    "processed_text":"nepsal jsem ze je novinka byl retweet"
  },
  {
    "label":4,
    "text":"italian nation holiday tomorrow parad beach",
    "cleaned_text":"italian nation holiday tomorrow parad beach",
    "normalized_text":"italian nation holiday tomorrow parad beach",
    "tokens":[
      "italian",
      "nation",
      "holiday",
      "tomorrow",
      "parad",
      "beach"
    ],
    "token_count":6,
    "processed_text":"italian nation holiday tomorrow parad beach"
  },
  {
    "label":0,
    "text":"lot like love one pebret oso",
    "cleaned_text":"lot like love one pebret oso",
    "normalized_text":"lot like love one pebret oso",
    "tokens":[
      "lot",
      "like",
      "love",
      "one",
      "pebret",
      "oso"
    ],
    "token_count":6,
    "processed_text":"lot like love one pebret oso"
  },
  {
    "label":4,
    "text":"could hold larger laptop im bike would big help im go take netbook bike",
    "cleaned_text":"could hold larger laptop im bike would big help im go take netbook bike",
    "normalized_text":"could hold larger laptop im bike would big help im go take netbook bike",
    "tokens":[
      "hold",
      "larger",
      "laptop",
      "im",
      "bike",
      "big",
      "help",
      "im",
      "go",
      "take",
      "netbook",
      "bike"
    ],
    "token_count":12,
    "processed_text":"hold larger laptop im bike big help im go take netbook bike"
  },
  {
    "label":4,
    "text":"could follow",
    "cleaned_text":"could follow",
    "normalized_text":"could follow",
    "tokens":[
      "follow"
    ],
    "token_count":1,
    "processed_text":"follow"
  },
  {
    "label":4,
    "text":"lol meant could twitter page gone funni wine howev would like pear cider",
    "cleaned_text":"lol meant could twitter page gone funni wine howev would like pear cider",
    "normalized_text":"lol meant could twitter page gone funni wine howev would like pear cider",
    "tokens":[
      "lol",
      "meant",
      "twitter",
      "page",
      "gone",
      "funni",
      "wine",
      "howev",
      "like",
      "pear",
      "cider"
    ],
    "token_count":11,
    "processed_text":"lol meant twitter page gone funni wine howev like pear cider"
  },
  {
    "label":0,
    "text":"grrr blogger editor screw format",
    "cleaned_text":"grrr blogger editor screw format",
    "normalized_text":"grrr blogger editor screw format",
    "tokens":[
      "grrr",
      "blogger",
      "editor",
      "screw",
      "format"
    ],
    "token_count":5,
    "processed_text":"grrr blogger editor screw format"
  },
  {
    "label":4,
    "text":"least wasnt much money amaz wife justin im sure alreadi know",
    "cleaned_text":"least wasnt much money amaz wife justin im sure alreadi know",
    "normalized_text":"least wasnt much money amaz wife justin im sure alreadi know",
    "tokens":[
      "least",
      "wasnt",
      "much",
      "money",
      "amaz",
      "wife",
      "justin",
      "im",
      "sure",
      "alreadi",
      "know"
    ],
    "token_count":11,
    "processed_text":"least wasnt much money amaz wife justin im sure alreadi know"
  },
  {
    "label":0,
    "text":"alreadi pack sunday",
    "cleaned_text":"alreadi pack sunday",
    "normalized_text":"alreadi pack sunday",
    "tokens":[
      "alreadi",
      "pack",
      "sunday"
    ],
    "token_count":3,
    "processed_text":"alreadi pack sunday"
  },
  {
    "label":4,
    "text":"oh forgot thank amp follow friday recommend",
    "cleaned_text":"oh forgot thank amp follow friday recommend",
    "normalized_text":"oh forgot thank amp follow friday recommend",
    "tokens":[
      "oh",
      "forgot",
      "thank",
      "amp",
      "follow",
      "friday",
      "recommend"
    ],
    "token_count":7,
    "processed_text":"oh forgot thank amp follow friday recommend"
  },
  {
    "label":0,
    "text":"hate reread tweet post bberri misspel cant delet repost phone",
    "cleaned_text":"hate reread tweet post bberri misspel cant delet repost phone",
    "normalized_text":"hate reread tweet post bberri misspel cant delet repost phone",
    "tokens":[
      "hate",
      "reread",
      "tweet",
      "post",
      "bberri",
      "misspel",
      "cant",
      "delet",
      "repost",
      "phone"
    ],
    "token_count":10,
    "processed_text":"hate reread tweet post bberri misspel cant delet repost phone"
  },
  {
    "label":0,
    "text":"look horribl hill mile",
    "cleaned_text":"look horribl hill mile",
    "normalized_text":"look horribl hill mile",
    "tokens":[
      "look",
      "horribl",
      "hill",
      "mile"
    ],
    "token_count":4,
    "processed_text":"look horribl hill mile"
  },
  {
    "label":4,
    "text":"wait nurs take iv siss hand wheelchairampthen home",
    "cleaned_text":"wait nurs take iv siss hand wheelchairampthen home",
    "normalized_text":"wait nurs take iv siss hand wheelchairampthen home",
    "tokens":[
      "wait",
      "nur",
      "take",
      "iv",
      "siss",
      "hand",
      "home"
    ],
    "token_count":7,
    "processed_text":"wait nur take iv siss hand home"
  },
  {
    "label":4,
    "text":"problemo im glad found tbh didnt still wouldnt heard lol",
    "cleaned_text":"problemo im glad found tbh didnt still wouldnt heard lol",
    "normalized_text":"problemo im glad found tbh didnt still wouldnt heard lol",
    "tokens":[
      "problemo",
      "im",
      "glad",
      "found",
      "tbh",
      "didnt",
      "still",
      "wouldnt",
      "heard",
      "lol"
    ],
    "token_count":10,
    "processed_text":"problemo im glad found tbh didnt still wouldnt heard lol"
  },
  {
    "label":4,
    "text":"time work",
    "cleaned_text":"time work",
    "normalized_text":"time work",
    "tokens":[
      "time",
      "work"
    ],
    "token_count":2,
    "processed_text":"time work"
  },
  {
    "label":0,
    "text":"excit spend time amaz mom world mom bestest soon amp cant wait golf though",
    "cleaned_text":"excit spend time amaz mom world mom bestest soon amp cant wait golf though",
    "normalized_text":"excit spend time amaz mom world mom bestest soon amp cant wait golf though",
    "tokens":[
      "excit",
      "spend",
      "time",
      "amaz",
      "mom",
      "world",
      "mom",
      "bestest",
      "soon",
      "amp",
      "cant",
      "wait",
      "golf",
      "though"
    ],
    "token_count":14,
    "processed_text":"excit spend time amaz mom world mom bestest soon amp cant wait golf though"
  },
  {
    "label":0,
    "text":"whyy live uk",
    "cleaned_text":"whyy live uk",
    "normalized_text":"whyy live uk",
    "tokens":[
      "whyy",
      "live",
      "uk"
    ],
    "token_count":3,
    "processed_text":"whyy live uk"
  },
  {
    "label":4,
    "text":"couldnt ask better boyfriend friend weekend",
    "cleaned_text":"couldnt ask better boyfriend friend weekend",
    "normalized_text":"couldnt ask better boyfriend friend weekend",
    "tokens":[
      "couldnt",
      "ask",
      "better",
      "boyfriend",
      "friend",
      "weekend"
    ],
    "token_count":6,
    "processed_text":"couldnt ask better boyfriend friend weekend"
  },
  {
    "label":4,
    "text":"spottedon left fake guy fawk fur",
    "cleaned_text":"spottedon left fake guy fawk fur",
    "normalized_text":"spottedon left fake guy fawk fur",
    "tokens":[
      "spottedon",
      "left",
      "fake",
      "guy",
      "fawk",
      "fur"
    ],
    "token_count":6,
    "processed_text":"spottedon left fake guy fawk fur"
  },
  {
    "label":4,
    "text":"wimp amp slept hr someth drive sleep beauti day",
    "cleaned_text":"wimp amp slept hr someth drive sleep beauti day",
    "normalized_text":"wimp amp slept hr someth drive sleep beauti day",
    "tokens":[
      "wimp",
      "amp",
      "slept",
      "hr",
      "someth",
      "drive",
      "sleep",
      "beauti",
      "day"
    ],
    "token_count":9,
    "processed_text":"wimp amp slept hr someth drive sleep beauti day"
  },
  {
    "label":0,
    "text":"tattoo hilari dare hung glass wine im mess next day x",
    "cleaned_text":"tattoo hilari dare hung glass wine im mess next day x",
    "normalized_text":"tattoo hilari dare hung glass wine im mess next day x",
    "tokens":[
      "tattoo",
      "hilari",
      "dare",
      "hung",
      "glass",
      "wine",
      "im",
      "mess",
      "next",
      "day"
    ],
    "token_count":10,
    "processed_text":"tattoo hilari dare hung glass wine im mess next day"
  },
  {
    "label":0,
    "text":"ugh envi come june",
    "cleaned_text":"ugh envi come june",
    "normalized_text":"ugh envi come june",
    "tokens":[
      "ugh",
      "envi",
      "come",
      "june"
    ],
    "token_count":4,
    "processed_text":"ugh envi come june"
  },
  {
    "label":0,
    "text":"make sad look sad face gt",
    "cleaned_text":"make sad look sad face gt",
    "normalized_text":"make sad look sad face gt",
    "tokens":[
      "make",
      "sad",
      "look",
      "sad",
      "face",
      "gt"
    ],
    "token_count":6,
    "processed_text":"make sad look sad face gt"
  },
  {
    "label":0,
    "text":"work dont realli know",
    "cleaned_text":"work dont realli know",
    "normalized_text":"work dont realli know",
    "tokens":[
      "work",
      "dont",
      "realli",
      "know"
    ],
    "token_count":4,
    "processed_text":"work dont realli know"
  },
  {
    "label":4,
    "text":"finish watch walli old man cute",
    "cleaned_text":"finish watch walli old man cute",
    "normalized_text":"finish watch walli old man cute",
    "tokens":[
      "finish",
      "watch",
      "walli",
      "old",
      "man",
      "cute"
    ],
    "token_count":6,
    "processed_text":"finish watch walli old man cute"
  },
  {
    "label":0,
    "text":"go take cat nap mambo fight bad day today hate work core realli somebodi save",
    "cleaned_text":"go take cat nap mambo fight bad day today hate work core realli somebodi save",
    "normalized_text":"go take cat nap mambo fight bad day today hate work core realli somebodi save",
    "tokens":[
      "go",
      "take",
      "cat",
      "nap",
      "mambo",
      "fight",
      "bad",
      "day",
      "today",
      "hate",
      "work",
      "core",
      "realli",
      "somebodi",
      "save"
    ],
    "token_count":15,
    "processed_text":"go take cat nap mambo fight bad day today hate work core realli somebodi save"
  },
  {
    "label":4,
    "text":"texa ranger beat oakland athlet im proud ranger fan",
    "cleaned_text":"texa ranger beat oakland athlet im proud ranger fan",
    "normalized_text":"texa ranger beat oakland athlet im proud ranger fan",
    "tokens":[
      "texa",
      "ranger",
      "beat",
      "oakland",
      "athlet",
      "im",
      "proud",
      "ranger",
      "fan"
    ],
    "token_count":9,
    "processed_text":"texa ranger beat oakland athlet im proud ranger fan"
  },
  {
    "label":0,
    "text":"still miss besti",
    "cleaned_text":"still miss besti",
    "normalized_text":"still miss besti",
    "tokens":[
      "still",
      "miss",
      "besti"
    ],
    "token_count":3,
    "processed_text":"still miss besti"
  },
  {
    "label":0,
    "text":"ur crazi anyon eat like insan haha lonelyandroid",
    "cleaned_text":"ur crazi anyon eat like insan haha lonelyandroid",
    "normalized_text":"ur crazi anyon eat like insan haha lonelyandroid",
    "tokens":[
      "ur",
      "crazi",
      "anyon",
      "eat",
      "like",
      "insan",
      "haha",
      "lonelyandroid"
    ],
    "token_count":8,
    "processed_text":"ur crazi anyon eat like insan haha lonelyandroid"
  },
  {
    "label":0,
    "text":"hurri come ny lol im good fl either im want list",
    "cleaned_text":"hurri come ny lol im good fl either im want list",
    "normalized_text":"hurri come ny lol im good fl either im want list",
    "tokens":[
      "hurri",
      "come",
      "ny",
      "lol",
      "im",
      "good",
      "fl",
      "either",
      "im",
      "want",
      "list"
    ],
    "token_count":11,
    "processed_text":"hurri come ny lol im good fl either im want list"
  },
  {
    "label":0,
    "text":"solid state drive awesom perform alon doesnt justifi pay around twiceasmor get consider less space",
    "cleaned_text":"solid state drive awesom perform alon doesnt justifi pay around twiceasmor get consider less space",
    "normalized_text":"solid state drive awesom perform alon doesnt justifi pay around twiceasmor get consider less space",
    "tokens":[
      "solid",
      "state",
      "drive",
      "awesom",
      "perform",
      "alon",
      "doesnt",
      "justifi",
      "pay",
      "around",
      "twiceasmor",
      "get",
      "consid",
      "less",
      "space"
    ],
    "token_count":15,
    "processed_text":"solid state drive awesom perform alon doesnt justifi pay around twiceasmor get consid less space"
  },
  {
    "label":4,
    "text":"oh u creat new build still look new button facebook pingfm",
    "cleaned_text":"oh u creat new build still look new button facebook pingfm",
    "normalized_text":"oh u creat new build still look new button facebook pingfm",
    "tokens":[
      "oh",
      "creat",
      "new",
      "build",
      "still",
      "look",
      "new",
      "button",
      "facebook",
      "pingfm"
    ],
    "token_count":10,
    "processed_text":"oh creat new build still look new button facebook pingfm"
  },
  {
    "label":0,
    "text":"love song without bracket bit",
    "cleaned_text":"love song without bracket bit",
    "normalized_text":"love song without bracket bit",
    "tokens":[
      "love",
      "song",
      "without",
      "bracket",
      "bit"
    ],
    "token_count":5,
    "processed_text":"love song without bracket bit"
  },
  {
    "label":0,
    "text":"isnt pay park near hous condo around us wait list spot",
    "cleaned_text":"isnt pay park near hous condo around us wait list spot",
    "normalized_text":"isnt pay park near hous condo around us wait list spot",
    "tokens":[
      "isnt",
      "pay",
      "park",
      "near",
      "hou",
      "condo",
      "around",
      "us",
      "wait",
      "list",
      "spot"
    ],
    "token_count":11,
    "processed_text":"isnt pay park near hou condo around us wait list spot"
  },
  {
    "label":0,
    "text":"need florida week long",
    "cleaned_text":"need florida week long",
    "normalized_text":"need florida week long",
    "tokens":[
      "need",
      "florida",
      "week",
      "long"
    ],
    "token_count":4,
    "processed_text":"need florida week long"
  },
  {
    "label":4,
    "text":"homework done",
    "cleaned_text":"homework done",
    "normalized_text":"homework done",
    "tokens":[
      "homework",
      "done"
    ],
    "token_count":2,
    "processed_text":"homework done"
  },
  {
    "label":0,
    "text":"batal bikin supris buat papa",
    "cleaned_text":"batal bikin supris buat papa",
    "normalized_text":"batal bikin supris buat papa",
    "tokens":[
      "batal",
      "bikin",
      "supri",
      "buat",
      "papa"
    ],
    "token_count":5,
    "processed_text":"batal bikin supri buat papa"
  },
  {
    "label":0,
    "text":"p came today irrit tummi hurt",
    "cleaned_text":"p came today irrit tummi hurt",
    "normalized_text":"p came today irrit tummi hurt",
    "tokens":[
      "came",
      "today",
      "irrit",
      "tummi",
      "hurt"
    ],
    "token_count":5,
    "processed_text":"came today irrit tummi hurt"
  },
  {
    "label":0,
    "text":"valencia magic mountain still go",
    "cleaned_text":"valencia magic mountain still go",
    "normalized_text":"valencia magic mountain still go",
    "tokens":[
      "valencia",
      "magic",
      "mountain",
      "still",
      "go"
    ],
    "token_count":5,
    "processed_text":"valencia magic mountain still go"
  },
  {
    "label":0,
    "text":"perfect hr commut would undoubt drain life alan",
    "cleaned_text":"perfect hr commut would undoubt drain life alan",
    "normalized_text":"perfect hr commut would undoubt drain life alan",
    "tokens":[
      "perfect",
      "hr",
      "commut",
      "undoubt",
      "drain",
      "life",
      "alan"
    ],
    "token_count":7,
    "processed_text":"perfect hr commut undoubt drain life alan"
  },
  {
    "label":0,
    "text":"sunday alreadi",
    "cleaned_text":"sunday alreadi",
    "normalized_text":"sunday alreadi",
    "tokens":[
      "sunday",
      "alreadi"
    ],
    "token_count":2,
    "processed_text":"sunday alreadi"
  },
  {
    "label":0,
    "text":"oh crap screw draw paper",
    "cleaned_text":"oh crap screw draw paper",
    "normalized_text":"oh crap screw draw paper",
    "tokens":[
      "oh",
      "crap",
      "screw",
      "draw",
      "paper"
    ],
    "token_count":5,
    "processed_text":"oh crap screw draw paper"
  },
  {
    "label":0,
    "text":"took burger last bun",
    "cleaned_text":"took burger last bun",
    "normalized_text":"took burger last bun",
    "tokens":[
      "took",
      "burger",
      "last",
      "bun"
    ],
    "token_count":4,
    "processed_text":"took burger last bun"
  },
  {
    "label":0,
    "text":"joey imancip peter disown two parent serpeat opposit form",
    "cleaned_text":"joey imancip peter disown two parent serpeat opposit form",
    "normalized_text":"joey imancip peter disown two parent serpeat opposit form",
    "tokens":[
      "joey",
      "imancip",
      "peter",
      "disown",
      "two",
      "parent",
      "serpeat",
      "opposit",
      "form"
    ],
    "token_count":9,
    "processed_text":"joey imancip peter disown two parent serpeat opposit form"
  },
  {
    "label":0,
    "text":"rainalway rainingthi town seem blade runner movi",
    "cleaned_text":"rainalway rainingthi town seem blade runner movi",
    "normalized_text":"rainalway rainingthi town seem blade runner movi",
    "tokens":[
      "rainalway",
      "rainingthi",
      "town",
      "seem",
      "blade",
      "runner",
      "movi"
    ],
    "token_count":7,
    "processed_text":"rainalway rainingthi town seem blade runner movi"
  },
  {
    "label":0,
    "text":"facup tweet soon lost stream video feed",
    "cleaned_text":"facup tweet soon lost stream video feed",
    "normalized_text":"facup tweet soon lost stream video feed",
    "tokens":[
      "facup",
      "tweet",
      "soon",
      "lost",
      "stream",
      "video",
      "feed"
    ],
    "token_count":7,
    "processed_text":"facup tweet soon lost stream video feed"
  },
  {
    "label":0,
    "text":"rememb feefer broke leg hous lole saw fell",
    "cleaned_text":"rememb feefer broke leg hous lole saw fell",
    "normalized_text":"rememb feefer broke leg hous lole saw fell",
    "tokens":[
      "rememb",
      "feefer",
      "broke",
      "leg",
      "hou",
      "lole",
      "saw",
      "fell"
    ],
    "token_count":8,
    "processed_text":"rememb feefer broke leg hou lole saw fell"
  },
  {
    "label":0,
    "text":"writer block tell bout",
    "cleaned_text":"writer block tell bout",
    "normalized_text":"writer block tell bout",
    "tokens":[
      "writer",
      "block",
      "tell",
      "bout"
    ],
    "token_count":4,
    "processed_text":"writer block tell bout"
  },
  {
    "label":4,
    "text":"thank know appreci wish",
    "cleaned_text":"thank know appreci wish",
    "normalized_text":"thank know appreci wish",
    "tokens":[
      "thank",
      "know",
      "appreci",
      "wish"
    ],
    "token_count":4,
    "processed_text":"thank know appreci wish"
  },
  {
    "label":0,
    "text":"would come way get",
    "cleaned_text":"would come way get",
    "normalized_text":"would come way get",
    "tokens":[
      "come",
      "way",
      "get"
    ],
    "token_count":3,
    "processed_text":"come way get"
  },
  {
    "label":4,
    "text":"good idea let week school reopen okay",
    "cleaned_text":"good idea let week school reopen okay",
    "normalized_text":"good idea let week school reopen okay",
    "tokens":[
      "good",
      "idea",
      "let",
      "week",
      "school",
      "reopen",
      "okay"
    ],
    "token_count":7,
    "processed_text":"good idea let week school reopen okay"
  },
  {
    "label":0,
    "text":"thought twitter suppos help feel alon still pretti cool anyway",
    "cleaned_text":"thought twitter suppos help feel alon still pretti cool anyway",
    "normalized_text":"thought twitter suppos help feel alon still pretti cool anyway",
    "tokens":[
      "thought",
      "twitter",
      "suppo",
      "help",
      "feel",
      "alon",
      "still",
      "pretti",
      "cool",
      "anyway"
    ],
    "token_count":10,
    "processed_text":"thought twitter suppo help feel alon still pretti cool anyway"
  },
  {
    "label":0,
    "text":"cant open facebook multipli",
    "cleaned_text":"cant open facebook multipli",
    "normalized_text":"cant open facebook multipli",
    "tokens":[
      "cant",
      "open",
      "facebook",
      "multipli"
    ],
    "token_count":4,
    "processed_text":"cant open facebook multipli"
  },
  {
    "label":0,
    "text":"nake snow flurri ga chrissak",
    "cleaned_text":"nake snow flurri ga chrissak",
    "normalized_text":"nake snow flurri ga chrissak",
    "tokens":[
      "nake",
      "snow",
      "flurri",
      "ga",
      "chrissak"
    ],
    "token_count":5,
    "processed_text":"nake snow flurri ga chrissak"
  },
  {
    "label":0,
    "text":"never put phone silent coulda babi right hope see later miss much",
    "cleaned_text":"never put phone silent coulda babi right hope see later miss much",
    "normalized_text":"never put phone silent coulda babi right hope see later miss much",
    "tokens":[
      "never",
      "put",
      "phone",
      "silent",
      "coulda",
      "babi",
      "right",
      "hope",
      "see",
      "later",
      "miss",
      "much"
    ],
    "token_count":12,
    "processed_text":"never put phone silent coulda babi right hope see later miss much"
  },
  {
    "label":4,
    "text":"script im listen guitar soooo nice want learn im go learn",
    "cleaned_text":"script im listen guitar soooo nice want learn im go learn",
    "normalized_text":"script im listen guitar soooo nice want learn im go learn",
    "tokens":[
      "script",
      "im",
      "listen",
      "guitar",
      "soooo",
      "nice",
      "want",
      "learn",
      "im",
      "go",
      "learn"
    ],
    "token_count":11,
    "processed_text":"script im listen guitar soooo nice want learn im go learn"
  },
  {
    "label":4,
    "text":"thank yesterday went best night life thank",
    "cleaned_text":"thank yesterday went best night life thank",
    "normalized_text":"thank yesterday went best night life thank",
    "tokens":[
      "thank",
      "yesterday",
      "went",
      "best",
      "night",
      "life",
      "thank"
    ],
    "token_count":7,
    "processed_text":"thank yesterday went best night life thank"
  },
  {
    "label":4,
    "text":"bow consid point question mark felt far subtl",
    "cleaned_text":"bow consid point question mark felt far subtl",
    "normalized_text":"bow consid point question mark felt far subtl",
    "tokens":[
      "bow",
      "consid",
      "point",
      "question",
      "mark",
      "felt",
      "far",
      "subtl"
    ],
    "token_count":8,
    "processed_text":"bow consid point question mark felt far subtl"
  },
  {
    "label":0,
    "text":"worri douchebag factor",
    "cleaned_text":"worri douchebag factor",
    "normalized_text":"worri douchebag factor",
    "tokens":[
      "worri",
      "douchebag",
      "factor"
    ],
    "token_count":3,
    "processed_text":"worri douchebag factor"
  },
  {
    "label":0,
    "text":"mean youll leav alon",
    "cleaned_text":"mean youll leav alon",
    "normalized_text":"mean youll leav alon",
    "tokens":[
      "mean",
      "youll",
      "leav",
      "alon"
    ],
    "token_count":4,
    "processed_text":"mean youll leav alon"
  },
  {
    "label":0,
    "text":"love parti wish could stay longer",
    "cleaned_text":"love parti wish could stay longer",
    "normalized_text":"love parti wish could stay longer",
    "tokens":[
      "love",
      "parti",
      "wish",
      "stay",
      "longer"
    ],
    "token_count":5,
    "processed_text":"love parti wish stay longer"
  },
  {
    "label":0,
    "text":"yeahh",
    "cleaned_text":"yeahh",
    "normalized_text":"yeahh",
    "tokens":[
      "yeahh"
    ],
    "token_count":1,
    "processed_text":"yeahh"
  },
  {
    "label":0,
    "text":"zant awsom prob cost fortun feel like ive age miss car sub",
    "cleaned_text":"zant awsom prob cost fortun feel like ive age miss car sub",
    "normalized_text":"zant awsom prob cost fortun feel like ive age miss car sub",
    "tokens":[
      "zant",
      "awsom",
      "prob",
      "cost",
      "fortun",
      "feel",
      "like",
      "ive",
      "age",
      "miss",
      "car",
      "sub"
    ],
    "token_count":12,
    "processed_text":"zant awsom prob cost fortun feel like ive age miss car sub"
  },
  {
    "label":4,
    "text":"mmm cheesssi potato good idea bed",
    "cleaned_text":"mmm cheesssi potato good idea bed",
    "normalized_text":"mmm cheesssi potato good idea bed",
    "tokens":[
      "mmm",
      "cheesssi",
      "potato",
      "good",
      "idea",
      "bed"
    ],
    "token_count":6,
    "processed_text":"mmm cheesssi potato good idea bed"
  },
  {
    "label":4,
    "text":"think na cut haha insid ocean park",
    "cleaned_text":"think na cut haha insid ocean park",
    "normalized_text":"think na cut haha insid ocean park",
    "tokens":[
      "think",
      "na",
      "cut",
      "haha",
      "insid",
      "ocean",
      "park"
    ],
    "token_count":7,
    "processed_text":"think na cut haha insid ocean park"
  },
  {
    "label":0,
    "text":"wonder big name dj start develop th danc music scene asturia spain isnt one",
    "cleaned_text":"wonder big name dj start develop th danc music scene asturia spain isnt one",
    "normalized_text":"wonder big name dj start develop th danc music scene asturia spain isnt one",
    "tokens":[
      "wonder",
      "big",
      "name",
      "dj",
      "start",
      "develop",
      "th",
      "danc",
      "music",
      "scene",
      "asturia",
      "spain",
      "isnt",
      "one"
    ],
    "token_count":14,
    "processed_text":"wonder big name dj start develop th danc music scene asturia spain isnt one"
  },
  {
    "label":0,
    "text":"came back beachit realli crowd",
    "cleaned_text":"came back beachit realli crowd",
    "normalized_text":"came back beachit realli crowd",
    "tokens":[
      "came",
      "back",
      "beachit",
      "realli",
      "crowd"
    ],
    "token_count":5,
    "processed_text":"came back beachit realli crowd"
  },
  {
    "label":4,
    "text":"well fun",
    "cleaned_text":"well fun",
    "normalized_text":"well fun",
    "tokens":[
      "well",
      "fun"
    ],
    "token_count":2,
    "processed_text":"well fun"
  },
  {
    "label":0,
    "text":"ohgodohgodohgod want",
    "cleaned_text":"ohgodohgodohgod want",
    "normalized_text":"ohgodohgodohgod want",
    "tokens":[
      "ohgodohgodohgod",
      "want"
    ],
    "token_count":2,
    "processed_text":"ohgodohgodohgod want"
  },
  {
    "label":0,
    "text":"damn aunti flo birthday birthday sex",
    "cleaned_text":"damn aunti flo birthday birthday sex",
    "normalized_text":"damn aunti flo birthday birthday sex",
    "tokens":[
      "damn",
      "aunti",
      "flo",
      "birthday",
      "birthday",
      "sex"
    ],
    "token_count":6,
    "processed_text":"damn aunti flo birthday birthday sex"
  },
  {
    "label":4,
    "text":"god day exam",
    "cleaned_text":"god day exam",
    "normalized_text":"god day exam",
    "tokens":[
      "god",
      "day",
      "exam"
    ],
    "token_count":3,
    "processed_text":"god day exam"
  },
  {
    "label":4,
    "text":"work lot offic",
    "cleaned_text":"work lot offic",
    "normalized_text":"work lot offic",
    "tokens":[
      "work",
      "lot",
      "offic"
    ],
    "token_count":3,
    "processed_text":"work lot offic"
  },
  {
    "label":0,
    "text":"damn even swallow pain",
    "cleaned_text":"damn even swallow pain",
    "normalized_text":"damn even swallow pain",
    "tokens":[
      "damn",
      "even",
      "swallow",
      "pain"
    ],
    "token_count":4,
    "processed_text":"damn even swallow pain"
  },
  {
    "label":4,
    "text":"let us go yon pedohood hide bush togeth",
    "cleaned_text":"let us go yon pedohood hide bush togeth",
    "normalized_text":"let us go yon pedohood hide bush togeth",
    "tokens":[
      "let",
      "us",
      "go",
      "yon",
      "pedohood",
      "hide",
      "bush",
      "togeth"
    ],
    "token_count":8,
    "processed_text":"let us go yon pedohood hide bush togeth"
  },
  {
    "label":0,
    "text":"signatur rub mondayey bracelet least plenti autograph still sad day",
    "cleaned_text":"signatur rub mondayey bracelet least plenti autograph still sad day",
    "normalized_text":"signatur rub mondayey bracelet least plenti autograph still sad day",
    "tokens":[
      "signatur",
      "rub",
      "mondayey",
      "bracelet",
      "least",
      "plenti",
      "autograph",
      "still",
      "sad",
      "day"
    ],
    "token_count":10,
    "processed_text":"signatur rub mondayey bracelet least plenti autograph still sad day"
  },
  {
    "label":0,
    "text":"much littl timehous clean start start babi room mayb shop trip week",
    "cleaned_text":"much littl timehous clean start start babi room mayb shop trip week",
    "normalized_text":"much littl timehous clean start start babi room mayb shop trip week",
    "tokens":[
      "much",
      "littl",
      "timeh",
      "clean",
      "start",
      "start",
      "babi",
      "room",
      "mayb",
      "shop",
      "trip",
      "week"
    ],
    "token_count":12,
    "processed_text":"much littl timeh clean start start babi room mayb shop trip week"
  },
  {
    "label":4,
    "text":"great time mpf",
    "cleaned_text":"great time mpf",
    "normalized_text":"great time mpf",
    "tokens":[
      "great",
      "time",
      "mpf"
    ],
    "token_count":3,
    "processed_text":"great time mpf"
  },
  {
    "label":4,
    "text":"neither confirm deni os version iphon adp headphon problem gone away",
    "cleaned_text":"neither confirm deni os version iphon adp headphon problem gone away",
    "normalized_text":"neither confirm deni os version iphon adp headphon problem gone away",
    "tokens":[
      "neither",
      "confirm",
      "deni",
      "os",
      "version",
      "iphon",
      "adp",
      "headphon",
      "problem",
      "gone",
      "away"
    ],
    "token_count":11,
    "processed_text":"neither confirm deni os version iphon adp headphon problem gone away"
  },
  {
    "label":4,
    "text":"lol soooo see u doug cheesi old wed dress",
    "cleaned_text":"lol soooo see u doug cheesi old wed dress",
    "normalized_text":"lol soooo see u doug cheesi old wed dress",
    "tokens":[
      "lol",
      "soooo",
      "see",
      "doug",
      "cheesi",
      "old",
      "wed",
      "dress"
    ],
    "token_count":8,
    "processed_text":"lol soooo see doug cheesi old wed dress"
  },
  {
    "label":0,
    "text":"everyth fine flash flood im tire live like",
    "cleaned_text":"everyth fine flash flood im tire live like",
    "normalized_text":"everyth fine flash flood im tire live like",
    "tokens":[
      "everyth",
      "fine",
      "flash",
      "flood",
      "im",
      "tire",
      "live",
      "like"
    ],
    "token_count":8,
    "processed_text":"everyth fine flash flood im tire live like"
  },
  {
    "label":0,
    "text":"man want back chapel hill entir school time hate ctop group everyon obnoxi",
    "cleaned_text":"man want back chapel hill entir school time hate ctop group everyon obnoxi",
    "normalized_text":"man want back chapel hill entir school time hate ctop group everyon obnoxi",
    "tokens":[
      "man",
      "want",
      "back",
      "chapel",
      "hill",
      "entir",
      "school",
      "time",
      "hate",
      "ctop",
      "group",
      "everyon",
      "obnoxi"
    ],
    "token_count":13,
    "processed_text":"man want back chapel hill entir school time hate ctop group everyon obnoxi"
  },
  {
    "label":4,
    "text":"good isnt",
    "cleaned_text":"good isnt",
    "normalized_text":"good isnt",
    "tokens":[
      "good",
      "isnt"
    ],
    "token_count":2,
    "processed_text":"good isnt"
  },
  {
    "label":4,
    "text":"monday morn work weather terribl traffic much better thing improv",
    "cleaned_text":"monday morn work weather terribl traffic much better thing improv",
    "normalized_text":"monday morn work weather terribl traffic much better thing improv",
    "tokens":[
      "monday",
      "morn",
      "work",
      "weather",
      "terribl",
      "traffic",
      "much",
      "better",
      "thing",
      "improv"
    ],
    "token_count":10,
    "processed_text":"monday morn work weather terribl traffic much better thing improv"
  },
  {
    "label":4,
    "text":"classi expect wayn",
    "cleaned_text":"classi expect wayn",
    "normalized_text":"classi expect wayn",
    "tokens":[
      "classi",
      "expect",
      "wayn"
    ],
    "token_count":3,
    "processed_text":"classi expect wayn"
  },
  {
    "label":0,
    "text":"wish could",
    "cleaned_text":"wish could",
    "normalized_text":"wish could",
    "tokens":[
      "wish"
    ],
    "token_count":1,
    "processed_text":"wish"
  },
  {
    "label":0,
    "text":"yay glass arriv eastgarden toe much pain abl go get",
    "cleaned_text":"yay glass arriv eastgarden toe much pain abl go get",
    "normalized_text":"yay glass arriv eastgarden toe much pain abl go get",
    "tokens":[
      "yay",
      "glass",
      "arriv",
      "eastgarden",
      "toe",
      "much",
      "pain",
      "abl",
      "go",
      "get"
    ],
    "token_count":10,
    "processed_text":"yay glass arriv eastgarden toe much pain abl go get"
  },
  {
    "label":0,
    "text":"charli roastbeef mostaza manejamo wtf",
    "cleaned_text":"charli roastbeef mostaza manejamo wtf",
    "normalized_text":"charli roastbeef mostaza manejamo wtf",
    "tokens":[
      "charli",
      "roastbeef",
      "mostaza",
      "manejamo",
      "wtf"
    ],
    "token_count":5,
    "processed_text":"charli roastbeef mostaza manejamo wtf"
  },
  {
    "label":4,
    "text":"westfield centrelondonhow hard find sunglass heeeeug full free wifi",
    "cleaned_text":"westfield centrelondonhow hard find sunglass heeeeug full free wifi",
    "normalized_text":"westfield centrelondonhow hard find sunglass heeeeug full free wifi",
    "tokens":[
      "westfield",
      "centrelondonhow",
      "hard",
      "find",
      "sunglass",
      "heeeeug",
      "full",
      "free",
      "wifi"
    ],
    "token_count":9,
    "processed_text":"westfield centrelondonhow hard find sunglass heeeeug full free wifi"
  },
  {
    "label":0,
    "text":"see everyon talk",
    "cleaned_text":"see everyon talk",
    "normalized_text":"see everyon talk",
    "tokens":[
      "see",
      "everyon",
      "talk"
    ],
    "token_count":3,
    "processed_text":"see everyon talk"
  },
  {
    "label":4,
    "text":"hahay crack thx followingbtw weird ralph love bio name",
    "cleaned_text":"hahay crack thx followingbtw weird ralph love bio name",
    "normalized_text":"hahay crack thx followingbtw weird ralph love bio name",
    "tokens":[
      "hahay",
      "crack",
      "thx",
      "followingbtw",
      "weird",
      "ralph",
      "love",
      "bio",
      "name"
    ],
    "token_count":9,
    "processed_text":"hahay crack thx followingbtw weird ralph love bio name"
  },
  {
    "label":4,
    "text":"warmbut like get email friend she spoke",
    "cleaned_text":"warmbut like get email friend she spoke",
    "normalized_text":"warmbut like get email friend she spoke",
    "tokens":[
      "warmbut",
      "like",
      "get",
      "email",
      "friend",
      "spoke"
    ],
    "token_count":6,
    "processed_text":"warmbut like get email friend spoke"
  },
  {
    "label":0,
    "text":"mother keep hound assign leav alon never tell theyr due",
    "cleaned_text":"mother keep hound assign leav alon never tell theyr due",
    "normalized_text":"mother keep hound assign leav alon never tell theyr due",
    "tokens":[
      "mother",
      "keep",
      "hound",
      "assign",
      "leav",
      "alon",
      "never",
      "tell",
      "theyr",
      "due"
    ],
    "token_count":10,
    "processed_text":"mother keep hound assign leav alon never tell theyr due"
  },
  {
    "label":4,
    "text":"elect todayoff vote",
    "cleaned_text":"elect todayoff vote",
    "normalized_text":"elect todayoff vote",
    "tokens":[
      "elect",
      "todayoff",
      "vote"
    ],
    "token_count":3,
    "processed_text":"elect todayoff vote"
  },
  {
    "label":4,
    "text":"happi new boldi berri father",
    "cleaned_text":"happi new boldi berri father",
    "normalized_text":"happi new boldi berri father",
    "tokens":[
      "happi",
      "new",
      "boldi",
      "berri",
      "father"
    ],
    "token_count":5,
    "processed_text":"happi new boldi berri father"
  },
  {
    "label":0,
    "text":"mg cod kill vertigo kid id give barbi crewcut play gi joe",
    "cleaned_text":"mg cod kill vertigo kid id give barbi crewcut play gi joe",
    "normalized_text":"mg cod kill vertigo kid id give barbi crewcut play gi joe",
    "tokens":[
      "mg",
      "cod",
      "kill",
      "vertigo",
      "kid",
      "id",
      "give",
      "barbi",
      "crewcut",
      "play",
      "gi",
      "joe"
    ],
    "token_count":12,
    "processed_text":"mg cod kill vertigo kid id give barbi crewcut play gi joe"
  },
  {
    "label":0,
    "text":"read peopl tweet everyon pair convers want pair",
    "cleaned_text":"read peopl tweet everyon pair convers want pair",
    "normalized_text":"read peopl tweet everyon pair convers want pair",
    "tokens":[
      "read",
      "peopl",
      "tweet",
      "everyon",
      "pair",
      "conver",
      "want",
      "pair"
    ],
    "token_count":8,
    "processed_text":"read peopl tweet everyon pair conver want pair"
  },
  {
    "label":0,
    "text":"thee worst headach",
    "cleaned_text":"thee worst headach",
    "normalized_text":"thee worst headach",
    "tokens":[
      "thee",
      "worst",
      "headach"
    ],
    "token_count":3,
    "processed_text":"thee worst headach"
  },
  {
    "label":0,
    "text":"yeah well im bore",
    "cleaned_text":"yeah well im bore",
    "normalized_text":"yeah well im bore",
    "tokens":[
      "yeah",
      "well",
      "im",
      "bore"
    ],
    "token_count":4,
    "processed_text":"yeah well im bore"
  },
  {
    "label":0,
    "text":"neopoint get ad earn game section score straight k pt play record pt count neopet",
    "cleaned_text":"neopoint get ad earn game section score straight k pt play record pt count neopet",
    "normalized_text":"neopoint get ad earn game section score straight k pt play record pt count neopet",
    "tokens":[
      "neopoint",
      "get",
      "ad",
      "earn",
      "game",
      "section",
      "score",
      "straight",
      "pt",
      "play",
      "record",
      "pt",
      "count",
      "neopet"
    ],
    "token_count":14,
    "processed_text":"neopoint get ad earn game section score straight pt play record pt count neopet"
  },
  {
    "label":0,
    "text":"kerri got flat come visit theyr repair free wtf must hot shit lol",
    "cleaned_text":"kerri got flat come visit theyr repair free wtf must hot shit lol",
    "normalized_text":"kerri got flat come visit theyr repair free wtf must hot shit lol",
    "tokens":[
      "kerri",
      "got",
      "flat",
      "come",
      "visit",
      "theyr",
      "repair",
      "free",
      "wtf",
      "hot",
      "shit",
      "lol"
    ],
    "token_count":12,
    "processed_text":"kerri got flat come visit theyr repair free wtf hot shit lol"
  },
  {
    "label":4,
    "text":"crappi movi great night",
    "cleaned_text":"crappi movi great night",
    "normalized_text":"crappi movi great night",
    "tokens":[
      "crappi",
      "movi",
      "great",
      "night"
    ],
    "token_count":4,
    "processed_text":"crappi movi great night"
  },
  {
    "label":4,
    "text":"new bug vw",
    "cleaned_text":"new bug vw",
    "normalized_text":"new bug vw",
    "tokens":[
      "new",
      "bug",
      "vw"
    ],
    "token_count":3,
    "processed_text":"new bug vw"
  },
  {
    "label":4,
    "text":"wow nice",
    "cleaned_text":"wow nice",
    "normalized_text":"wow nice",
    "tokens":[
      "wow",
      "nice"
    ],
    "token_count":2,
    "processed_text":"wow nice"
  },
  {
    "label":4,
    "text":"well fulli rest readi go need lot advic affil market etc newbi techi stuff xxx",
    "cleaned_text":"well fulli rest readi go need lot advic affil market etc newbi techi stuff xxx",
    "normalized_text":"well fulli rest readi go need lot advic affil market etc newbi techi stuff xxx",
    "tokens":[
      "well",
      "fulli",
      "rest",
      "readi",
      "go",
      "need",
      "lot",
      "advic",
      "affil",
      "market",
      "etc",
      "newbi",
      "techi",
      "stuff",
      "xxx"
    ],
    "token_count":15,
    "processed_text":"well fulli rest readi go need lot advic affil market etc newbi techi stuff xxx"
  },
  {
    "label":0,
    "text":"horni littl dog room",
    "cleaned_text":"horni littl dog room",
    "normalized_text":"horni littl dog room",
    "tokens":[
      "horni",
      "littl",
      "dog",
      "room"
    ],
    "token_count":4,
    "processed_text":"horni littl dog room"
  },
  {
    "label":0,
    "text":"p amp c still awak drive nut night bear wake amp drive crazi",
    "cleaned_text":"p amp c still awak drive nut night bear wake amp drive crazi",
    "normalized_text":"p amp c still awak drive nut night bear wake amp drive crazi",
    "tokens":[
      "amp",
      "still",
      "awak",
      "drive",
      "nut",
      "night",
      "bear",
      "wake",
      "amp",
      "drive",
      "crazi"
    ],
    "token_count":11,
    "processed_text":"amp still awak drive nut night bear wake amp drive crazi"
  },
  {
    "label":4,
    "text":"could follow",
    "cleaned_text":"could follow",
    "normalized_text":"could follow",
    "tokens":[
      "follow"
    ],
    "token_count":1,
    "processed_text":"follow"
  },
  {
    "label":4,
    "text":"get jason fix night long thing itun tomorrow",
    "cleaned_text":"get jason fix night long thing itun tomorrow",
    "normalized_text":"get jason fix night long thing itun tomorrow",
    "tokens":[
      "get",
      "jason",
      "fix",
      "night",
      "long",
      "thing",
      "itun",
      "tomorrow"
    ],
    "token_count":8,
    "processed_text":"get jason fix night long thing itun tomorrow"
  },
  {
    "label":0,
    "text":"wanna go home cant caus custom wont leav boss wont let leav either",
    "cleaned_text":"wanna go home cant caus custom wont leav boss wont let leav either",
    "normalized_text":"wanna go home cant caus custom wont leav boss wont let leav either",
    "tokens":[
      "wan",
      "na",
      "go",
      "home",
      "cant",
      "cau",
      "custom",
      "wont",
      "leav",
      "boss",
      "wont",
      "let",
      "leav",
      "either"
    ],
    "token_count":14,
    "processed_text":"wan na go home cant cau custom wont leav boss wont let leav either"
  },
  {
    "label":4,
    "text":"work mama crayon luff boyfriend btw",
    "cleaned_text":"work mama crayon luff boyfriend btw",
    "normalized_text":"work mama crayon luff boyfriend btw",
    "tokens":[
      "work",
      "mama",
      "crayon",
      "luff",
      "boyfriend",
      "btw"
    ],
    "token_count":6,
    "processed_text":"work mama crayon luff boyfriend btw"
  },
  {
    "label":0,
    "text":"bore oh sadd",
    "cleaned_text":"bore oh sadd",
    "normalized_text":"bore oh sadd",
    "tokens":[
      "bore",
      "oh",
      "sadd"
    ],
    "token_count":3,
    "processed_text":"bore oh sadd"
  },
  {
    "label":4,
    "text":"havent heard j minu shame tsk tsk awesom band",
    "cleaned_text":"havent heard j minu shame tsk tsk awesom band",
    "normalized_text":"havent heard j minu shame tsk tsk awesom band",
    "tokens":[
      "havent",
      "heard",
      "minu",
      "shame",
      "tsk",
      "tsk",
      "awesom",
      "band"
    ],
    "token_count":8,
    "processed_text":"havent heard minu shame tsk tsk awesom band"
  },
  {
    "label":0,
    "text":"got aw news cousin husband pancreat cancer prognosi good",
    "cleaned_text":"got aw news cousin husband pancreat cancer prognosi good",
    "normalized_text":"got aw news cousin husband pancreat cancer prognosi good",
    "tokens":[
      "got",
      "aw",
      "news",
      "cousin",
      "husband",
      "pancreat",
      "cancer",
      "prognosi",
      "good"
    ],
    "token_count":9,
    "processed_text":"got aw news cousin husband pancreat cancer prognosi good"
  },
  {
    "label":0,
    "text":"workingoutsid sun laugh",
    "cleaned_text":"workingoutsid sun laugh",
    "normalized_text":"workingoutsid sun laugh",
    "tokens":[
      "workingoutsid",
      "sun",
      "laugh"
    ],
    "token_count":3,
    "processed_text":"workingoutsid sun laugh"
  },
  {
    "label":4,
    "text":"emma london one u talk phone u go follow like u said love u mandi xx",
    "cleaned_text":"emma london one u talk phone u go follow like u said love u mandi xx",
    "normalized_text":"emma london one u talk phone u go follow like u said love u mandi xx",
    "tokens":[
      "emma",
      "london",
      "one",
      "talk",
      "phone",
      "go",
      "follow",
      "like",
      "said",
      "love",
      "mandi",
      "xx"
    ],
    "token_count":12,
    "processed_text":"emma london one talk phone go follow like said love mandi xx"
  },
  {
    "label":4,
    "text":"impressiveif deliv promo stuff show itll amaz that big though",
    "cleaned_text":"impressiveif deliv promo stuff show itll amaz that big though",
    "normalized_text":"impressiveif deliv promo stuff show itll amaz that big though",
    "tokens":[
      "impressiveif",
      "deliv",
      "promo",
      "stuff",
      "show",
      "itll",
      "amaz",
      "big",
      "though"
    ],
    "token_count":9,
    "processed_text":"impressiveif deliv promo stuff show itll amaz big though"
  },
  {
    "label":4,
    "text":"vote em cmon tell cn vote xcept sumthin wiv bein twittish vice presid that",
    "cleaned_text":"vote em cmon tell cn vote xcept sumthin wiv bein twittish vice presid that",
    "normalized_text":"vote em cmon tell cn vote xcept sumthin wiv bein twittish vice presid that",
    "tokens":[
      "vote",
      "em",
      "cmon",
      "tell",
      "cn",
      "vote",
      "xcept",
      "sumthin",
      "wiv",
      "bein",
      "twittish",
      "vice",
      "presid"
    ],
    "token_count":13,
    "processed_text":"vote em cmon tell cn vote xcept sumthin wiv bein twittish vice presid"
  },
  {
    "label":4,
    "text":"day left school count today",
    "cleaned_text":"day left school count today",
    "normalized_text":"day left school count today",
    "tokens":[
      "day",
      "left",
      "school",
      "count",
      "today"
    ],
    "token_count":5,
    "processed_text":"day left school count today"
  },
  {
    "label":4,
    "text":"playin onlin sinc hoursmin th chip almost left hurri pocher bern",
    "cleaned_text":"playin onlin sinc hoursmin th chip almost left hurri pocher bern",
    "normalized_text":"playin onlin sinc hoursmin th chip almost left hurri pocher bern",
    "tokens":[
      "playin",
      "onlin",
      "sinc",
      "hoursmin",
      "th",
      "chip",
      "almost",
      "left",
      "hurri",
      "pocher",
      "bern"
    ],
    "token_count":11,
    "processed_text":"playin onlin sinc hoursmin th chip almost left hurri pocher bern"
  },
  {
    "label":0,
    "text":"that cute im switchin co lil bro n wasnt der haha arent self involv bitch lol",
    "cleaned_text":"that cute im switchin co lil bro n wasnt der haha arent self involv bitch lol",
    "normalized_text":"that cute im switchin co lil bro n wasnt der haha arent self involv bitch lol",
    "tokens":[
      "cute",
      "im",
      "switchin",
      "co",
      "lil",
      "bro",
      "wasnt",
      "der",
      "haha",
      "arent",
      "self",
      "involv",
      "bitch",
      "lol"
    ],
    "token_count":14,
    "processed_text":"cute im switchin co lil bro wasnt der haha arent self involv bitch lol"
  },
  {
    "label":4,
    "text":"graduat want detail",
    "cleaned_text":"graduat want detail",
    "normalized_text":"graduat want detail",
    "tokens":[
      "graduat",
      "want",
      "detail"
    ],
    "token_count":3,
    "processed_text":"graduat want detail"
  },
  {
    "label":0,
    "text":"realiz squid better rob",
    "cleaned_text":"realiz squid better rob",
    "normalized_text":"realiz squid better rob",
    "tokens":[
      "realiz",
      "squid",
      "better",
      "rob"
    ],
    "token_count":4,
    "processed_text":"realiz squid better rob"
  },
  {
    "label":0,
    "text":"mom went job delhi yesterdayi cri thw whole weekeneverytim door bell ring think",
    "cleaned_text":"mom went job delhi yesterdayi cri thw whole weekeneverytim door bell ring think",
    "normalized_text":"mom went job delhi yesterdayi cri thw whole weekeneverytim door bell ring think",
    "tokens":[
      "mom",
      "went",
      "job",
      "delhi",
      "yesterdayi",
      "cri",
      "thw",
      "whole",
      "weekeneverytim",
      "door",
      "bell",
      "ring",
      "think"
    ],
    "token_count":13,
    "processed_text":"mom went job delhi yesterdayi cri thw whole weekeneverytim door bell ring think"
  },
  {
    "label":4,
    "text":"wwwfoxnewscomstrategyroom still go strong increas skill public interact make point known worldrecord",
    "cleaned_text":"wwwfoxnewscomstrategyroom still go strong increas skill public interact make point known worldrecord",
    "normalized_text":"wwwfoxnewscomstrategyroom still go strong increas skill public interact make point known worldrecord",
    "tokens":[
      "still",
      "go",
      "strong",
      "increa",
      "skill",
      "public",
      "interact",
      "make",
      "point",
      "known",
      "worldrecord"
    ],
    "token_count":11,
    "processed_text":"still go strong increa skill public interact make point known worldrecord"
  },
  {
    "label":0,
    "text":"zoe sinc wont sleep ahhhhh",
    "cleaned_text":"zoe sinc wont sleep ahhhhh",
    "normalized_text":"zoe sinc wont sleep ahhhhh",
    "tokens":[
      "zoe",
      "sinc",
      "wont",
      "sleep",
      "ahhhhh"
    ],
    "token_count":5,
    "processed_text":"zoe sinc wont sleep ahhhhh"
  },
  {
    "label":4,
    "text":"bring around week night",
    "cleaned_text":"bring around week night",
    "normalized_text":"bring around week night",
    "tokens":[
      "bring",
      "around",
      "week",
      "night"
    ],
    "token_count":4,
    "processed_text":"bring around week night"
  },
  {
    "label":4,
    "text":"wake vega hop number billboard top weeee",
    "cleaned_text":"wake vega hop number billboard top weeee",
    "normalized_text":"wake vega hop number billboard top weeee",
    "tokens":[
      "wake",
      "vega",
      "hop",
      "number",
      "billboard",
      "top",
      "weeee"
    ],
    "token_count":7,
    "processed_text":"wake vega hop number billboard top weeee"
  },
  {
    "label":4,
    "text":"one shown cartoon network like kid isp hate enough man p",
    "cleaned_text":"one shown cartoon network like kid isp hate enough man p",
    "normalized_text":"one shown cartoon network like kid isp hate enough man p",
    "tokens":[
      "one",
      "shown",
      "cartoon",
      "network",
      "like",
      "kid",
      "isp",
      "hate",
      "enough",
      "man"
    ],
    "token_count":10,
    "processed_text":"one shown cartoon network like kid isp hate enough man"
  },
  {
    "label":4,
    "text":"happi birthday twitterworld truli bless anoth wonder year life",
    "cleaned_text":"happi birthday twitterworld truli bless anoth wonder year life",
    "normalized_text":"happi birthday twitterworld truli bless anoth wonder year life",
    "tokens":[
      "happi",
      "birthday",
      "twitterworld",
      "truli",
      "bless",
      "anoth",
      "wonder",
      "year",
      "life"
    ],
    "token_count":9,
    "processed_text":"happi birthday twitterworld truli bless anoth wonder year life"
  },
  {
    "label":0,
    "text":"oh time get alreadi",
    "cleaned_text":"oh time get alreadi",
    "normalized_text":"oh time get alreadi",
    "tokens":[
      "oh",
      "time",
      "get",
      "alreadi"
    ],
    "token_count":4,
    "processed_text":"oh time get alreadi"
  },
  {
    "label":4,
    "text":"tri go sleep feel like kid christma eve cant wait go dr morn",
    "cleaned_text":"tri go sleep feel like kid christma eve cant wait go dr morn",
    "normalized_text":"tri go sleep feel like kid christma eve cant wait go dr morn",
    "tokens":[
      "tri",
      "go",
      "sleep",
      "feel",
      "like",
      "kid",
      "christma",
      "eve",
      "cant",
      "wait",
      "go",
      "dr",
      "morn"
    ],
    "token_count":13,
    "processed_text":"tri go sleep feel like kid christma eve cant wait go dr morn"
  },
  {
    "label":4,
    "text":"say quoth quit dead yetquot",
    "cleaned_text":"say quoth quit dead yetquot",
    "normalized_text":"say quoth quit dead yetquot",
    "tokens":[
      "say",
      "quoth",
      "quit",
      "dead",
      "yetquot"
    ],
    "token_count":5,
    "processed_text":"say quoth quit dead yetquot"
  },
  {
    "label":4,
    "text":"twiter person weekend",
    "cleaned_text":"twiter person weekend",
    "normalized_text":"twiter person weekend",
    "tokens":[
      "twiter",
      "person",
      "weekend"
    ],
    "token_count":3,
    "processed_text":"twiter person weekend"
  },
  {
    "label":4,
    "text":"ahhhh gotta love summer night",
    "cleaned_text":"ahhhh gotta love summer night",
    "normalized_text":"ahhhh gotta love summer night",
    "tokens":[
      "ahhhh",
      "got",
      "ta",
      "love",
      "summer",
      "night"
    ],
    "token_count":6,
    "processed_text":"ahhhh got ta love summer night"
  },
  {
    "label":4,
    "text":"plan mucho parti also went celtic pub hear gini brother play gig great pubish",
    "cleaned_text":"plan mucho parti also went celtic pub hear gini brother play gig great pubish",
    "normalized_text":"plan mucho parti also went celtic pub hear gini brother play gig great pubish",
    "tokens":[
      "plan",
      "mucho",
      "parti",
      "also",
      "went",
      "celtic",
      "pub",
      "hear",
      "gini",
      "brother",
      "play",
      "gig",
      "great",
      "pubish"
    ],
    "token_count":14,
    "processed_text":"plan mucho parti also went celtic pub hear gini brother play gig great pubish"
  },
  {
    "label":4,
    "text":"hmm mayb two comput lol",
    "cleaned_text":"hmm mayb two comput lol",
    "normalized_text":"hmm mayb two comput lol",
    "tokens":[
      "hmm",
      "mayb",
      "two",
      "comput",
      "lol"
    ],
    "token_count":5,
    "processed_text":"hmm mayb two comput lol"
  },
  {
    "label":4,
    "text":"keep mind heart",
    "cleaned_text":"keep mind heart",
    "normalized_text":"keep mind heart",
    "tokens":[
      "keep",
      "mind",
      "heart"
    ],
    "token_count":3,
    "processed_text":"keep mind heart"
  },
  {
    "label":0,
    "text":"think have sore thoart comin still listenin music",
    "cleaned_text":"think have sore thoart comin still listenin music",
    "normalized_text":"think have sore thoart comin still listenin music",
    "tokens":[
      "think",
      "sore",
      "thoart",
      "comin",
      "still",
      "listenin",
      "music"
    ],
    "token_count":7,
    "processed_text":"think sore thoart comin still listenin music"
  },
  {
    "label":4,
    "text":"long fruit sunday full worship word mission meet elder meet youth group god good",
    "cleaned_text":"long fruit sunday full worship word mission meet elder meet youth group god good",
    "normalized_text":"long fruit sunday full worship word mission meet elder meet youth group god good",
    "tokens":[
      "long",
      "fruit",
      "sunday",
      "full",
      "worship",
      "word",
      "mission",
      "meet",
      "elder",
      "meet",
      "youth",
      "group",
      "god",
      "good"
    ],
    "token_count":14,
    "processed_text":"long fruit sunday full worship word mission meet elder meet youth group god good"
  },
  {
    "label":0,
    "text":"work day huray",
    "cleaned_text":"work day huray",
    "normalized_text":"work day huray",
    "tokens":[
      "work",
      "day",
      "huray"
    ],
    "token_count":3,
    "processed_text":"work day huray"
  },
  {
    "label":4,
    "text":"want give guess mani complaint oftcom get mention guess",
    "cleaned_text":"want give guess mani complaint oftcom get mention guess",
    "normalized_text":"want give guess mani complaint oftcom get mention guess",
    "tokens":[
      "want",
      "give",
      "guess",
      "mani",
      "complaint",
      "oftcom",
      "get",
      "mention",
      "guess"
    ],
    "token_count":9,
    "processed_text":"want give guess mani complaint oftcom get mention guess"
  },
  {
    "label":0,
    "text":"thigh hurt alot thank sister makijng lung around pool lt",
    "cleaned_text":"thigh hurt alot thank sister makijng lung around pool lt",
    "normalized_text":"thigh hurt alot thank sister makijng lung around pool lt",
    "tokens":[
      "thigh",
      "hurt",
      "alot",
      "thank",
      "sister",
      "makijng",
      "lung",
      "around",
      "pool",
      "lt"
    ],
    "token_count":10,
    "processed_text":"thigh hurt alot thank sister makijng lung around pool lt"
  },
  {
    "label":4,
    "text":"make video youtub workvirtu tour stand inventori home",
    "cleaned_text":"make video youtub workvirtu tour stand inventori home",
    "normalized_text":"make video youtub workvirtu tour stand inventori home",
    "tokens":[
      "make",
      "video",
      "youtub",
      "workvirtu",
      "tour",
      "stand",
      "inventori",
      "home"
    ],
    "token_count":8,
    "processed_text":"make video youtub workvirtu tour stand inventori home"
  },
  {
    "label":0,
    "text":"tire want vacat winter",
    "cleaned_text":"tire want vacat winter",
    "normalized_text":"tire want vacat winter",
    "tokens":[
      "tire",
      "want",
      "vacat",
      "winter"
    ],
    "token_count":4,
    "processed_text":"tire want vacat winter"
  },
  {
    "label":0,
    "text":"awk he nicest guy lol hateperez",
    "cleaned_text":"awk he nicest guy lol hateperez",
    "normalized_text":"awk he nicest guy lol hateperez",
    "tokens":[
      "awk",
      "nicest",
      "guy",
      "lol",
      "hateperez"
    ],
    "token_count":5,
    "processed_text":"awk nicest guy lol hateperez"
  },
  {
    "label":0,
    "text":"dear world im break consid dump dana via",
    "cleaned_text":"dear world im break consid dump dana via",
    "normalized_text":"dear world im break consid dump dana via",
    "tokens":[
      "dear",
      "world",
      "im",
      "break",
      "consid",
      "dump",
      "dana",
      "via"
    ],
    "token_count":8,
    "processed_text":"dear world im break consid dump dana via"
  },
  {
    "label":4,
    "text":"total bet theyd itd fun cliqueoftwitt",
    "cleaned_text":"total bet theyd itd fun cliqueoftwitt",
    "normalized_text":"total bet theyd itd fun cliqueoftwitt",
    "tokens":[
      "total",
      "bet",
      "theyd",
      "itd",
      "fun",
      "cliqueoftwitt"
    ],
    "token_count":6,
    "processed_text":"total bet theyd itd fun cliqueoftwitt"
  },
  {
    "label":0,
    "text":"rain atlanta past weekend rain decid follow back floridait pour",
    "cleaned_text":"rain atlanta past weekend rain decid follow back floridait pour",
    "normalized_text":"rain atlanta past weekend rain decid follow back floridait pour",
    "tokens":[
      "rain",
      "atlanta",
      "past",
      "weekend",
      "rain",
      "decid",
      "follow",
      "back",
      "floridait",
      "pour"
    ],
    "token_count":10,
    "processed_text":"rain atlanta past weekend rain decid follow back floridait pour"
  },
  {
    "label":0,
    "text":"understand get european fund polit parti",
    "cleaned_text":"understand get european fund polit parti",
    "normalized_text":"understand get european fund polit parti",
    "tokens":[
      "understand",
      "get",
      "european",
      "fund",
      "polit",
      "parti"
    ],
    "token_count":6,
    "processed_text":"understand get european fund polit parti"
  },
  {
    "label":0,
    "text":"kinda bum mah baybay never redial boo",
    "cleaned_text":"kinda bum mah baybay never redial boo",
    "normalized_text":"kinda bum mah baybay never redial boo",
    "tokens":[
      "kinda",
      "bum",
      "mah",
      "baybay",
      "never",
      "redial",
      "boo"
    ],
    "token_count":7,
    "processed_text":"kinda bum mah baybay never redial boo"
  },
  {
    "label":0,
    "text":"look like well excit sunday yay us youll probabl sambuca u clean anyway",
    "cleaned_text":"look like well excit sunday yay us youll probabl sambuca u clean anyway",
    "normalized_text":"look like well excit sunday yay us youll probabl sambuca u clean anyway",
    "tokens":[
      "look",
      "like",
      "well",
      "excit",
      "sunday",
      "yay",
      "us",
      "youll",
      "probabl",
      "sambuca",
      "clean",
      "anyway"
    ],
    "token_count":12,
    "processed_text":"look like well excit sunday yay us youll probabl sambuca clean anyway"
  },
  {
    "label":0,
    "text":"hour nashvil hous hunt later luck",
    "cleaned_text":"hour nashvil hous hunt later luck",
    "normalized_text":"hour nashvil hous hunt later luck",
    "tokens":[
      "hour",
      "nashvil",
      "hou",
      "hunt",
      "later",
      "luck"
    ],
    "token_count":6,
    "processed_text":"hour nashvil hou hunt later luck"
  },
  {
    "label":0,
    "text":"sad look like ur wit fever",
    "cleaned_text":"sad look like ur wit fever",
    "normalized_text":"sad look like ur wit fever",
    "tokens":[
      "sad",
      "look",
      "like",
      "ur",
      "wit",
      "fever"
    ],
    "token_count":6,
    "processed_text":"sad look like ur wit fever"
  },
  {
    "label":4,
    "text":"yeah sem hol enjoy",
    "cleaned_text":"yeah sem hol enjoy",
    "normalized_text":"yeah sem hol enjoy",
    "tokens":[
      "yeah",
      "sem",
      "hol",
      "enjoy"
    ],
    "token_count":4,
    "processed_text":"yeah sem hol enjoy"
  },
  {
    "label":0,
    "text":"laptop still dead",
    "cleaned_text":"laptop still dead",
    "normalized_text":"laptop still dead",
    "tokens":[
      "laptop",
      "still",
      "dead"
    ],
    "token_count":3,
    "processed_text":"laptop still dead"
  },
  {
    "label":4,
    "text":"bore awesom time happi someth",
    "cleaned_text":"bore awesom time happi someth",
    "normalized_text":"bore awesom time happi someth",
    "tokens":[
      "bore",
      "awesom",
      "time",
      "happi",
      "someth"
    ],
    "token_count":5,
    "processed_text":"bore awesom time happi someth"
  },
  {
    "label":0,
    "text":"p worth keep real use plenti real life exampl make subject easier digest wish",
    "cleaned_text":"p worth keep real use plenti real life exampl make subject easier digest wish",
    "normalized_text":"p worth keep real use plenti real life exampl make subject easier digest wish",
    "tokens":[
      "worth",
      "keep",
      "real",
      "use",
      "plenti",
      "real",
      "life",
      "exampl",
      "make",
      "subject",
      "easier",
      "digest",
      "wish"
    ],
    "token_count":13,
    "processed_text":"worth keep real use plenti real life exampl make subject easier digest wish"
  },
  {
    "label":4,
    "text":"heck ye first saw link thought stalk took pictur first grade window lol",
    "cleaned_text":"heck ye first saw link thought stalk took pictur first grade window lol",
    "normalized_text":"heck ye first saw link thought stalk took pictur first grade window lol",
    "tokens":[
      "heck",
      "ye",
      "first",
      "saw",
      "link",
      "thought",
      "stalk",
      "took",
      "pictur",
      "first",
      "grade",
      "window",
      "lol"
    ],
    "token_count":13,
    "processed_text":"heck ye first saw link thought stalk took pictur first grade window lol"
  },
  {
    "label":0,
    "text":"cant dm discount ticket dont follow",
    "cleaned_text":"cant dm discount ticket dont follow",
    "normalized_text":"cant dm discount ticket dont follow",
    "tokens":[
      "cant",
      "dm",
      "discount",
      "ticket",
      "dont",
      "follow"
    ],
    "token_count":6,
    "processed_text":"cant dm discount ticket dont follow"
  },
  {
    "label":4,
    "text":"call answer shall tri tomorrow",
    "cleaned_text":"call answer shall tri tomorrow",
    "normalized_text":"call answer shall tri tomorrow",
    "tokens":[
      "call",
      "answer",
      "tri",
      "tomorrow"
    ],
    "token_count":4,
    "processed_text":"call answer tri tomorrow"
  },
  {
    "label":0,
    "text":"gone feel bad",
    "cleaned_text":"gone feel bad",
    "normalized_text":"gone feel bad",
    "tokens":[
      "gone",
      "feel",
      "bad"
    ],
    "token_count":3,
    "processed_text":"gone feel bad"
  },
  {
    "label":4,
    "text":"yep one two besti",
    "cleaned_text":"yep one two besti",
    "normalized_text":"yep one two besti",
    "tokens":[
      "yep",
      "one",
      "two",
      "besti"
    ],
    "token_count":4,
    "processed_text":"yep one two besti"
  },
  {
    "label":4,
    "text":"forget delici",
    "cleaned_text":"forget delici",
    "normalized_text":"forget delici",
    "tokens":[
      "forget",
      "delici"
    ],
    "token_count":2,
    "processed_text":"forget delici"
  },
  {
    "label":4,
    "text":"got ticket well one actual see newquay yayi",
    "cleaned_text":"got ticket well one actual see newquay yayi",
    "normalized_text":"got ticket well one actual see newquay yayi",
    "tokens":[
      "got",
      "ticket",
      "well",
      "one",
      "actual",
      "see",
      "newquay",
      "yayi"
    ],
    "token_count":8,
    "processed_text":"got ticket well one actual see newquay yayi"
  },
  {
    "label":4,
    "text":"add us myspac youtub mle",
    "cleaned_text":"add us myspac youtub mle",
    "normalized_text":"add us myspac youtub mle",
    "tokens":[
      "add",
      "us",
      "myspac",
      "youtub",
      "mle"
    ],
    "token_count":5,
    "processed_text":"add us myspac youtub mle"
  },
  {
    "label":4,
    "text":"excit new harri potter movi trailer b look amaz p wish juli alreadi",
    "cleaned_text":"excit new harri potter movi trailer b look amaz p wish juli alreadi",
    "normalized_text":"excit new harri potter movi trailer b look amaz p wish juli alreadi",
    "tokens":[
      "excit",
      "new",
      "harri",
      "potter",
      "movi",
      "trailer",
      "look",
      "amaz",
      "wish",
      "juli",
      "alreadi"
    ],
    "token_count":11,
    "processed_text":"excit new harri potter movi trailer look amaz wish juli alreadi"
  },
  {
    "label":4,
    "text":"lie watch rwrr challeng bed",
    "cleaned_text":"lie watch rwrr challeng bed",
    "normalized_text":"lie watch rwrr challeng bed",
    "tokens":[
      "lie",
      "watch",
      "rwrr",
      "challeng",
      "bed"
    ],
    "token_count":5,
    "processed_text":"lie watch rwrr challeng bed"
  },
  {
    "label":4,
    "text":"ill right deep heart",
    "cleaned_text":"ill right deep heart",
    "normalized_text":"ill right deep heart",
    "tokens":[
      "ill",
      "right",
      "deep",
      "heart"
    ],
    "token_count":4,
    "processed_text":"ill right deep heart"
  },
  {
    "label":4,
    "text":"went saw coupl flat today morn famili plan move like build castl air",
    "cleaned_text":"went saw coupl flat today morn famili plan move like build castl air",
    "normalized_text":"went saw coupl flat today morn famili plan move like build castl air",
    "tokens":[
      "went",
      "saw",
      "coupl",
      "flat",
      "today",
      "morn",
      "famili",
      "plan",
      "move",
      "like",
      "build",
      "castl",
      "air"
    ],
    "token_count":13,
    "processed_text":"went saw coupl flat today morn famili plan move like build castl air"
  },
  {
    "label":0,
    "text":"im sorri sad",
    "cleaned_text":"im sorri sad",
    "normalized_text":"im sorri sad",
    "tokens":[
      "im",
      "sorri",
      "sad"
    ],
    "token_count":3,
    "processed_text":"im sorri sad"
  },
  {
    "label":0,
    "text":"lol thank know suppos like liter dont mm",
    "cleaned_text":"lol thank know suppos like liter dont mm",
    "normalized_text":"lol thank know suppos like liter dont mm",
    "tokens":[
      "lol",
      "thank",
      "know",
      "suppo",
      "like",
      "liter",
      "dont",
      "mm"
    ],
    "token_count":8,
    "processed_text":"lol thank know suppo like liter dont mm"
  },
  {
    "label":4,
    "text":"stole name compani use parti cuz effin dope save hip hop buy shirt",
    "cleaned_text":"stole name compani use parti cuz effin dope save hip hop buy shirt",
    "normalized_text":"stole name compani use parti cuz effin dope save hip hop buy shirt",
    "tokens":[
      "stole",
      "name",
      "compani",
      "use",
      "parti",
      "cuz",
      "effin",
      "dope",
      "save",
      "hip",
      "hop",
      "buy",
      "shirt"
    ],
    "token_count":13,
    "processed_text":"stole name compani use parti cuz effin dope save hip hop buy shirt"
  },
  {
    "label":4,
    "text":"si back enjoy nice chees french sausag big oliv",
    "cleaned_text":"si back enjoy nice chees french sausag big oliv",
    "normalized_text":"si back enjoy nice chees french sausag big oliv",
    "tokens":[
      "si",
      "back",
      "enjoy",
      "nice",
      "chee",
      "french",
      "sausag",
      "big",
      "oliv"
    ],
    "token_count":9,
    "processed_text":"si back enjoy nice chee french sausag big oliv"
  },
  {
    "label":0,
    "text":"yep good feel didnt last back sleep",
    "cleaned_text":"yep good feel didnt last back sleep",
    "normalized_text":"yep good feel didnt last back sleep",
    "tokens":[
      "yep",
      "good",
      "feel",
      "didnt",
      "last",
      "back",
      "sleep"
    ],
    "token_count":7,
    "processed_text":"yep good feel didnt last back sleep"
  },
  {
    "label":4,
    "text":"space shuttl star trek etc love never got brick build anyth fabul admir",
    "cleaned_text":"space shuttl star trek etc love never got brick build anyth fabul admir",
    "normalized_text":"space shuttl star trek etc love never got brick build anyth fabul admir",
    "tokens":[
      "space",
      "shuttl",
      "star",
      "trek",
      "etc",
      "love",
      "never",
      "got",
      "brick",
      "build",
      "anyth",
      "fabul",
      "admir"
    ],
    "token_count":13,
    "processed_text":"space shuttl star trek etc love never got brick build anyth fabul admir"
  },
  {
    "label":4,
    "text":"hope youll visit countri eat mango haha",
    "cleaned_text":"hope youll visit countri eat mango haha",
    "normalized_text":"hope youll visit countri eat mango haha",
    "tokens":[
      "hope",
      "youll",
      "visit",
      "countri",
      "eat",
      "mango",
      "haha"
    ],
    "token_count":7,
    "processed_text":"hope youll visit countri eat mango haha"
  },
  {
    "label":0,
    "text":"brain candi found sleep instead",
    "cleaned_text":"brain candi found sleep instead",
    "normalized_text":"brain candi found sleep instead",
    "tokens":[
      "brain",
      "candi",
      "found",
      "sleep",
      "instead"
    ],
    "token_count":5,
    "processed_text":"brain candi found sleep instead"
  },
  {
    "label":4,
    "text":"still want silli thing let know guy hang",
    "cleaned_text":"still want silli thing let know guy hang",
    "normalized_text":"still want silli thing let know guy hang",
    "tokens":[
      "still",
      "want",
      "silli",
      "thing",
      "let",
      "know",
      "guy",
      "hang"
    ],
    "token_count":8,
    "processed_text":"still want silli thing let know guy hang"
  },
  {
    "label":0,
    "text":"dress readi hope get weed hr le sad lol",
    "cleaned_text":"dress readi hope get weed hr le sad lol",
    "normalized_text":"dress readi hope get weed hr le sad lol",
    "tokens":[
      "dress",
      "readi",
      "hope",
      "get",
      "weed",
      "hr",
      "le",
      "sad",
      "lol"
    ],
    "token_count":9,
    "processed_text":"dress readi hope get weed hr le sad lol"
  },
  {
    "label":0,
    "text":"camera need new batteri",
    "cleaned_text":"camera need new batteri",
    "normalized_text":"camera need new batteri",
    "tokens":[
      "camera",
      "need",
      "new",
      "batteri"
    ],
    "token_count":4,
    "processed_text":"camera need new batteri"
  },
  {
    "label":0,
    "text":"oh im work wish could join xx",
    "cleaned_text":"oh im work wish could join xx",
    "normalized_text":"oh im work wish could join xx",
    "tokens":[
      "oh",
      "im",
      "work",
      "wish",
      "join",
      "xx"
    ],
    "token_count":6,
    "processed_text":"oh im work wish join xx"
  },
  {
    "label":0,
    "text":"client deadlin lunch",
    "cleaned_text":"client deadlin lunch",
    "normalized_text":"client deadlin lunch",
    "tokens":[
      "client",
      "deadlin",
      "lunch"
    ],
    "token_count":3,
    "processed_text":"client deadlin lunch"
  },
  {
    "label":4,
    "text":"board",
    "cleaned_text":"board",
    "normalized_text":"board",
    "tokens":[
      "board"
    ],
    "token_count":1,
    "processed_text":"board"
  },
  {
    "label":4,
    "text":"gratitud sent back ya sue thank",
    "cleaned_text":"gratitud sent back ya sue thank",
    "normalized_text":"gratitud sent back ya sue thank",
    "tokens":[
      "gratitud",
      "sent",
      "back",
      "ya",
      "sue",
      "thank"
    ],
    "token_count":6,
    "processed_text":"gratitud sent back ya sue thank"
  },
  {
    "label":4,
    "text":"hello lee thank welcom girl",
    "cleaned_text":"hello lee thank welcom girl",
    "normalized_text":"hello lee thank welcom girl",
    "tokens":[
      "hello",
      "lee",
      "thank",
      "welcom",
      "girl"
    ],
    "token_count":5,
    "processed_text":"hello lee thank welcom girl"
  },
  {
    "label":4,
    "text":"hey karen tonight",
    "cleaned_text":"hey karen tonight",
    "normalized_text":"hey karen tonight",
    "tokens":[
      "hey",
      "karen",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"hey karen tonight"
  },
  {
    "label":0,
    "text":"love fact employ sit stare window",
    "cleaned_text":"love fact employ sit stare window",
    "normalized_text":"love fact employ sit stare window",
    "tokens":[
      "love",
      "fact",
      "employ",
      "sit",
      "stare",
      "window"
    ],
    "token_count":6,
    "processed_text":"love fact employ sit stare window"
  },
  {
    "label":0,
    "text":"real though feel sorri poor bacon babi",
    "cleaned_text":"real though feel sorri poor bacon babi",
    "normalized_text":"real though feel sorri poor bacon babi",
    "tokens":[
      "real",
      "though",
      "feel",
      "sorri",
      "poor",
      "bacon",
      "babi"
    ],
    "token_count":7,
    "processed_text":"real though feel sorri poor bacon babi"
  },
  {
    "label":0,
    "text":"yeayeayea az alway",
    "cleaned_text":"yeayeayea az alway",
    "normalized_text":"yeayeayea az alway",
    "tokens":[
      "yeayeayea",
      "az",
      "alway"
    ],
    "token_count":3,
    "processed_text":"yeayeayea az alway"
  },
  {
    "label":4,
    "text":"earl twitit save name earl via hmm twitit nickchang",
    "cleaned_text":"earl twitit save name earl via hmm twitit nickchang",
    "normalized_text":"earl twitit save name earl via hmm twitit nickchang",
    "tokens":[
      "earl",
      "twitit",
      "save",
      "name",
      "earl",
      "via",
      "hmm",
      "twitit",
      "nickchang"
    ],
    "token_count":9,
    "processed_text":"earl twitit save name earl via hmm twitit nickchang"
  },
  {
    "label":0,
    "text":"probabl visit chichen itza impress got half way pyramid",
    "cleaned_text":"probabl visit chichen itza impress got half way pyramid",
    "normalized_text":"probabl visit chichen itza impress got half way pyramid",
    "tokens":[
      "probabl",
      "visit",
      "chichen",
      "itza",
      "impress",
      "got",
      "half",
      "way",
      "pyramid"
    ],
    "token_count":9,
    "processed_text":"probabl visit chichen itza impress got half way pyramid"
  },
  {
    "label":0,
    "text":"say goodby grandmoth hard never see one thing said quotdont forget mequot",
    "cleaned_text":"say goodby grandmoth hard never see one thing said quotdont forget mequot",
    "normalized_text":"say goodby grandmoth hard never see one thing said quotdont forget mequot",
    "tokens":[
      "say",
      "goodbi",
      "grandmoth",
      "hard",
      "never",
      "see",
      "one",
      "thing",
      "said",
      "quotdont",
      "forget",
      "mequot"
    ],
    "token_count":12,
    "processed_text":"say goodbi grandmoth hard never see one thing said quotdont forget mequot"
  },
  {
    "label":0,
    "text":"leav beach",
    "cleaned_text":"leav beach",
    "normalized_text":"leav beach",
    "tokens":[
      "leav",
      "beach"
    ],
    "token_count":2,
    "processed_text":"leav beach"
  },
  {
    "label":4,
    "text":"hi ami",
    "cleaned_text":"hi ami",
    "normalized_text":"hi ami",
    "tokens":[
      "hi",
      "ami"
    ],
    "token_count":2,
    "processed_text":"hi ami"
  },
  {
    "label":4,
    "text":"thank u follow ad u love background ur page",
    "cleaned_text":"thank u follow ad u love background ur page",
    "normalized_text":"thank u follow ad u love background ur page",
    "tokens":[
      "thank",
      "follow",
      "ad",
      "love",
      "background",
      "ur",
      "page"
    ],
    "token_count":7,
    "processed_text":"thank follow ad love background ur page"
  },
  {
    "label":0,
    "text":"keep put go back queen hate hour train ride peopl jitney wors though obnoxi",
    "cleaned_text":"keep put go back queen hate hour train ride peopl jitney wors though obnoxi",
    "normalized_text":"keep put go back queen hate hour train ride peopl jitney wors though obnoxi",
    "tokens":[
      "keep",
      "put",
      "go",
      "back",
      "queen",
      "hate",
      "hour",
      "train",
      "ride",
      "peopl",
      "jitney",
      "wor",
      "though",
      "obnoxi"
    ],
    "token_count":14,
    "processed_text":"keep put go back queen hate hour train ride peopl jitney wor though obnoxi"
  },
  {
    "label":4,
    "text":"make sure keep cowboy jersey closet till see team year good bab",
    "cleaned_text":"make sure keep cowboy jersey closet till see team year good bab",
    "normalized_text":"make sure keep cowboy jersey closet till see team year good bab",
    "tokens":[
      "make",
      "sure",
      "keep",
      "cowboy",
      "jersey",
      "closet",
      "till",
      "see",
      "team",
      "year",
      "good",
      "bab"
    ],
    "token_count":12,
    "processed_text":"make sure keep cowboy jersey closet till see team year good bab"
  },
  {
    "label":0,
    "text":"technic u longer excus der tonit",
    "cleaned_text":"technic u longer excus der tonit",
    "normalized_text":"technic u longer excus der tonit",
    "tokens":[
      "technic",
      "longer",
      "excu",
      "der",
      "tonit"
    ],
    "token_count":5,
    "processed_text":"technic longer excu der tonit"
  },
  {
    "label":0,
    "text":"back revis hate bore",
    "cleaned_text":"back revis hate bore",
    "normalized_text":"back revis hate bore",
    "tokens":[
      "back",
      "revi",
      "hate",
      "bore"
    ],
    "token_count":4,
    "processed_text":"back revi hate bore"
  },
  {
    "label":0,
    "text":"wow feel place brookdal ghetto peopl onlynot singl race besid barn nobl shut",
    "cleaned_text":"wow feel place brookdal ghetto peopl onlynot singl race besid barn nobl shut",
    "normalized_text":"wow feel place brookdal ghetto peopl onlynot singl race besid barn nobl shut",
    "tokens":[
      "wow",
      "feel",
      "place",
      "brookdal",
      "ghetto",
      "peopl",
      "onlynot",
      "singl",
      "race",
      "besid",
      "barn",
      "nobl",
      "shut"
    ],
    "token_count":13,
    "processed_text":"wow feel place brookdal ghetto peopl onlynot singl race besid barn nobl shut"
  },
  {
    "label":0,
    "text":"dont feel well morn",
    "cleaned_text":"dont feel well morn",
    "normalized_text":"dont feel well morn",
    "tokens":[
      "dont",
      "feel",
      "well",
      "morn"
    ],
    "token_count":4,
    "processed_text":"dont feel well morn"
  },
  {
    "label":0,
    "text":"haha week left",
    "cleaned_text":"haha week left",
    "normalized_text":"haha week left",
    "tokens":[
      "haha",
      "week",
      "left"
    ],
    "token_count":3,
    "processed_text":"haha week left"
  },
  {
    "label":0,
    "text":"that definit sudden chang someon day friend brother pass away",
    "cleaned_text":"that definit sudden chang someon day friend brother pass away",
    "normalized_text":"that definit sudden chang someon day friend brother pass away",
    "tokens":[
      "definit",
      "sudden",
      "chang",
      "someon",
      "day",
      "friend",
      "brother",
      "pass",
      "away"
    ],
    "token_count":9,
    "processed_text":"definit sudden chang someon day friend brother pass away"
  },
  {
    "label":0,
    "text":"go bed",
    "cleaned_text":"go bed",
    "normalized_text":"go bed",
    "tokens":[
      "go",
      "bed"
    ],
    "token_count":2,
    "processed_text":"go bed"
  },
  {
    "label":0,
    "text":"draft thought suck icould bought much fee owe",
    "cleaned_text":"draft thought suck icould bought much fee owe",
    "normalized_text":"draft thought suck icould bought much fee owe",
    "tokens":[
      "draft",
      "thought",
      "suck",
      "icould",
      "bought",
      "much",
      "fee",
      "owe"
    ],
    "token_count":8,
    "processed_text":"draft thought suck icould bought much fee owe"
  },
  {
    "label":4,
    "text":"london bound exactli week",
    "cleaned_text":"london bound exactli week",
    "normalized_text":"london bound exactli week",
    "tokens":[
      "london",
      "bound",
      "exactli",
      "week"
    ],
    "token_count":4,
    "processed_text":"london bound exactli week"
  },
  {
    "label":4,
    "text":"pass undangundang test luck side",
    "cleaned_text":"pass undangundang test luck side",
    "normalized_text":"pass undangundang test luck side",
    "tokens":[
      "pass",
      "undangundang",
      "test",
      "luck",
      "side"
    ],
    "token_count":5,
    "processed_text":"pass undangundang test luck side"
  },
  {
    "label":4,
    "text":"final done even readi answer prmedia question morn",
    "cleaned_text":"final done even readi answer prmedia question morn",
    "normalized_text":"final done even readi answer prmedia question morn",
    "tokens":[
      "final",
      "done",
      "even",
      "readi",
      "answer",
      "prmedia",
      "question",
      "morn"
    ],
    "token_count":8,
    "processed_text":"final done even readi answer prmedia question morn"
  },
  {
    "label":0,
    "text":"put fleatick ointment yorki jimmi choonow hous smell like citronella yuck",
    "cleaned_text":"put fleatick ointment yorki jimmi choonow hous smell like citronella yuck",
    "normalized_text":"put fleatick ointment yorki jimmi choonow hous smell like citronella yuck",
    "tokens":[
      "put",
      "fleatick",
      "ointment",
      "yorki",
      "jimmi",
      "choonow",
      "hou",
      "smell",
      "like",
      "citronella",
      "yuck"
    ],
    "token_count":11,
    "processed_text":"put fleatick ointment yorki jimmi choonow hou smell like citronella yuck"
  },
  {
    "label":0,
    "text":"awww poor babi",
    "cleaned_text":"awww poor babi",
    "normalized_text":"awww poor babi",
    "tokens":[
      "awww",
      "poor",
      "babi"
    ],
    "token_count":3,
    "processed_text":"awww poor babi"
  },
  {
    "label":0,
    "text":"piti thought serious awesom support act",
    "cleaned_text":"piti thought serious awesom support act",
    "normalized_text":"piti thought serious awesom support act",
    "tokens":[
      "piti",
      "thought",
      "seriou",
      "awesom",
      "support",
      "act"
    ],
    "token_count":6,
    "processed_text":"piti thought seriou awesom support act"
  },
  {
    "label":0,
    "text":"kitti woke sleep quit soundli",
    "cleaned_text":"kitti woke sleep quit soundli",
    "normalized_text":"kitti woke sleep quit soundli",
    "tokens":[
      "kitti",
      "woke",
      "sleep",
      "quit",
      "soundli"
    ],
    "token_count":5,
    "processed_text":"kitti woke sleep quit soundli"
  },
  {
    "label":4,
    "text":"wek holiday u beliv ohhh lol",
    "cleaned_text":"wek holiday u beliv ohhh lol",
    "normalized_text":"wek holiday u beliv ohhh lol",
    "tokens":[
      "wek",
      "holiday",
      "beliv",
      "ohhh",
      "lol"
    ],
    "token_count":5,
    "processed_text":"wek holiday beliv ohhh lol"
  },
  {
    "label":0,
    "text":"good night twittervil hope fall asleep soon lol",
    "cleaned_text":"good night twittervil hope fall asleep soon lol",
    "normalized_text":"good night twittervil hope fall asleep soon lol",
    "tokens":[
      "good",
      "night",
      "twittervil",
      "hope",
      "fall",
      "asleep",
      "soon",
      "lol"
    ],
    "token_count":8,
    "processed_text":"good night twittervil hope fall asleep soon lol"
  },
  {
    "label":4,
    "text":"amc make crap show opinion",
    "cleaned_text":"amc make crap show opinion",
    "normalized_text":"amc make crap show opinion",
    "tokens":[
      "amc",
      "make",
      "crap",
      "show",
      "opinion"
    ],
    "token_count":5,
    "processed_text":"amc make crap show opinion"
  },
  {
    "label":4,
    "text":"what grade yeah well share stuff math gcse tomorrow good luck us lol ye",
    "cleaned_text":"what grade yeah well share stuff math gcse tomorrow good luck us lol ye",
    "normalized_text":"what grade yeah well share stuff math gcse tomorrow good luck us lol ye",
    "tokens":[
      "grade",
      "yeah",
      "well",
      "share",
      "stuff",
      "math",
      "gcse",
      "tomorrow",
      "good",
      "luck",
      "us",
      "lol",
      "ye"
    ],
    "token_count":13,
    "processed_text":"grade yeah well share stuff math gcse tomorrow good luck us lol ye"
  },
  {
    "label":0,
    "text":"bye littl houseil miss u",
    "cleaned_text":"bye littl houseil miss u",
    "normalized_text":"bye littl houseil miss u",
    "tokens":[
      "bye",
      "littl",
      "houseil",
      "miss"
    ],
    "token_count":4,
    "processed_text":"bye littl houseil miss"
  },
  {
    "label":4,
    "text":"want quotperfect couch comput devicequot",
    "cleaned_text":"want quotperfect couch comput devicequot",
    "normalized_text":"want quotperfect couch comput devicequot",
    "tokens":[
      "want",
      "quotperfect",
      "couch",
      "comput",
      "devicequot"
    ],
    "token_count":5,
    "processed_text":"want quotperfect couch comput devicequot"
  },
  {
    "label":0,
    "text":"great work tomorrow",
    "cleaned_text":"great work tomorrow",
    "normalized_text":"great work tomorrow",
    "tokens":[
      "great",
      "work",
      "tomorrow"
    ],
    "token_count":3,
    "processed_text":"great work tomorrow"
  },
  {
    "label":0,
    "text":"tweet today took day tomorrow got hit back school",
    "cleaned_text":"tweet today took day tomorrow got hit back school",
    "normalized_text":"tweet today took day tomorrow got hit back school",
    "tokens":[
      "tweet",
      "today",
      "took",
      "day",
      "tomorrow",
      "got",
      "hit",
      "back",
      "school"
    ],
    "token_count":9,
    "processed_text":"tweet today took day tomorrow got hit back school"
  },
  {
    "label":4,
    "text":"well guess definit make way",
    "cleaned_text":"well guess definit make way",
    "normalized_text":"well guess definit make way",
    "tokens":[
      "well",
      "guess",
      "definit",
      "make",
      "way"
    ],
    "token_count":5,
    "processed_text":"well guess definit make way"
  },
  {
    "label":4,
    "text":"happi friday",
    "cleaned_text":"happi friday",
    "normalized_text":"happi friday",
    "tokens":[
      "happi",
      "friday"
    ],
    "token_count":2,
    "processed_text":"happi friday"
  },
  {
    "label":0,
    "text":"life becom one pile electron amp physcial realli import pile may actual get address sorri email",
    "cleaned_text":"life becom one pile electron amp physcial realli import pile may actual get address sorri email",
    "normalized_text":"life becom one pile electron amp physcial realli import pile may actual get address sorri email",
    "tokens":[
      "life",
      "becom",
      "one",
      "pile",
      "electron",
      "amp",
      "physcial",
      "realli",
      "import",
      "pile",
      "may",
      "actual",
      "get",
      "address",
      "sorri",
      "email"
    ],
    "token_count":16,
    "processed_text":"life becom one pile electron amp physcial realli import pile may actual get address sorri email"
  },
  {
    "label":4,
    "text":"good thought come way",
    "cleaned_text":"good thought come way",
    "normalized_text":"good thought come way",
    "tokens":[
      "good",
      "thought",
      "come",
      "way"
    ],
    "token_count":4,
    "processed_text":"good thought come way"
  },
  {
    "label":0,
    "text":"need cellular devic mother took hah she quotbusyquot give back lol grr",
    "cleaned_text":"need cellular devic mother took hah she quotbusyquot give back lol grr",
    "normalized_text":"need cellular devic mother took hah she quotbusyquot give back lol grr",
    "tokens":[
      "need",
      "cellular",
      "devic",
      "mother",
      "took",
      "hah",
      "quotbusyquot",
      "give",
      "back",
      "lol",
      "grr"
    ],
    "token_count":11,
    "processed_text":"need cellular devic mother took hah quotbusyquot give back lol grr"
  },
  {
    "label":4,
    "text":"look like beauti day day day call chill day santa monica quad ride",
    "cleaned_text":"look like beauti day day day call chill day santa monica quad ride",
    "normalized_text":"look like beauti day day day call chill day santa monica quad ride",
    "tokens":[
      "look",
      "like",
      "beauti",
      "day",
      "day",
      "day",
      "call",
      "chill",
      "day",
      "santa",
      "monica",
      "quad",
      "ride"
    ],
    "token_count":13,
    "processed_text":"look like beauti day day day call chill day santa monica quad ride"
  },
  {
    "label":0,
    "text":"idina menzel actual love",
    "cleaned_text":"idina menzel actual love",
    "normalized_text":"idina menzel actual love",
    "tokens":[
      "idina",
      "menzel",
      "actual",
      "love"
    ],
    "token_count":4,
    "processed_text":"idina menzel actual love"
  },
  {
    "label":0,
    "text":"farrah breast cancer final succumb",
    "cleaned_text":"farrah breast cancer final succumb",
    "normalized_text":"farrah breast cancer final succumb",
    "tokens":[
      "farrah",
      "breast",
      "cancer",
      "final",
      "succumb"
    ],
    "token_count":5,
    "processed_text":"farrah breast cancer final succumb"
  },
  {
    "label":4,
    "text":"awww ador still ador know luv ya",
    "cleaned_text":"awww ador still ador know luv ya",
    "normalized_text":"awww ador still ador know luv ya",
    "tokens":[
      "awww",
      "ador",
      "still",
      "ador",
      "know",
      "luv",
      "ya"
    ],
    "token_count":7,
    "processed_text":"awww ador still ador know luv ya"
  },
  {
    "label":0,
    "text":"good bye richmond nova traffic suck damn love k perri new singl tho",
    "cleaned_text":"good bye richmond nova traffic suck damn love k perri new singl tho",
    "normalized_text":"good bye richmond nova traffic suck damn love k perri new singl tho",
    "tokens":[
      "good",
      "bye",
      "richmond",
      "nova",
      "traffic",
      "suck",
      "damn",
      "love",
      "perri",
      "new",
      "singl",
      "tho"
    ],
    "token_count":12,
    "processed_text":"good bye richmond nova traffic suck damn love perri new singl tho"
  },
  {
    "label":0,
    "text":"awww cant block work go listen song call trail lie awesom song",
    "cleaned_text":"awww cant block work go listen song call trail lie awesom song",
    "normalized_text":"awww cant block work go listen song call trail lie awesom song",
    "tokens":[
      "awww",
      "cant",
      "block",
      "work",
      "go",
      "listen",
      "song",
      "call",
      "trail",
      "lie",
      "awesom",
      "song"
    ],
    "token_count":12,
    "processed_text":"awww cant block work go listen song call trail lie awesom song"
  },
  {
    "label":0,
    "text":"say pleas save grace",
    "cleaned_text":"say pleas save grace",
    "normalized_text":"say pleas save grace",
    "tokens":[
      "say",
      "plea",
      "save",
      "grace"
    ],
    "token_count":4,
    "processed_text":"say plea save grace"
  },
  {
    "label":0,
    "text":"wish could listen servic buck month get phone",
    "cleaned_text":"wish could listen servic buck month get phone",
    "normalized_text":"wish could listen servic buck month get phone",
    "tokens":[
      "wish",
      "listen",
      "servic",
      "buck",
      "month",
      "get",
      "phone"
    ],
    "token_count":7,
    "processed_text":"wish listen servic buck month get phone"
  },
  {
    "label":0,
    "text":"cant wait new iphon heck confirm shouldv wait bought ny appl store",
    "cleaned_text":"cant wait new iphon heck confirm shouldv wait bought ny appl store",
    "normalized_text":"cant wait new iphon heck confirm shouldv wait bought ny appl store",
    "tokens":[
      "cant",
      "wait",
      "new",
      "iphon",
      "heck",
      "confirm",
      "shouldv",
      "wait",
      "bought",
      "ny",
      "appl",
      "store"
    ],
    "token_count":12,
    "processed_text":"cant wait new iphon heck confirm shouldv wait bought ny appl store"
  },
  {
    "label":0,
    "text":"rbd spilt best mexican pop group everi random",
    "cleaned_text":"rbd spilt best mexican pop group everi random",
    "normalized_text":"rbd spilt best mexican pop group everi random",
    "tokens":[
      "rbd",
      "spilt",
      "best",
      "mexican",
      "pop",
      "group",
      "everi",
      "random"
    ],
    "token_count":8,
    "processed_text":"rbd spilt best mexican pop group everi random"
  },
  {
    "label":4,
    "text":"presid would make happen haha",
    "cleaned_text":"presid would make happen haha",
    "normalized_text":"presid would make happen haha",
    "tokens":[
      "presid",
      "make",
      "happen",
      "haha"
    ],
    "token_count":4,
    "processed_text":"presid make happen haha"
  },
  {
    "label":4,
    "text":"home right",
    "cleaned_text":"home right",
    "normalized_text":"home right",
    "tokens":[
      "home",
      "right"
    ],
    "token_count":2,
    "processed_text":"home right"
  },
  {
    "label":0,
    "text":"everyon asleep earli",
    "cleaned_text":"everyon asleep earli",
    "normalized_text":"everyon asleep earli",
    "tokens":[
      "everyon",
      "asleep",
      "earli"
    ],
    "token_count":3,
    "processed_text":"everyon asleep earli"
  },
  {
    "label":0,
    "text":"lmbo ima bring marketin scheme cuz u right behind n ur nd page",
    "cleaned_text":"lmbo ima bring marketin scheme cuz u right behind n ur nd page",
    "normalized_text":"lmbo ima bring marketin scheme cuz u right behind n ur nd page",
    "tokens":[
      "lmbo",
      "ima",
      "bring",
      "marketin",
      "scheme",
      "cuz",
      "right",
      "behind",
      "ur",
      "nd",
      "page"
    ],
    "token_count":11,
    "processed_text":"lmbo ima bring marketin scheme cuz right behind ur nd page"
  },
  {
    "label":0,
    "text":"new post tumblr didnt tweet boo",
    "cleaned_text":"new post tumblr didnt tweet boo",
    "normalized_text":"new post tumblr didnt tweet boo",
    "tokens":[
      "new",
      "post",
      "tumblr",
      "didnt",
      "tweet",
      "boo"
    ],
    "token_count":6,
    "processed_text":"new post tumblr didnt tweet boo"
  },
  {
    "label":0,
    "text":"want car back",
    "cleaned_text":"want car back",
    "normalized_text":"want car back",
    "tokens":[
      "want",
      "car",
      "back"
    ],
    "token_count":3,
    "processed_text":"want car back"
  },
  {
    "label":0,
    "text":"im bawl goodnight everyon load pray want proactiv",
    "cleaned_text":"im bawl goodnight everyon load pray want proactiv",
    "normalized_text":"im bawl goodnight everyon load pray want proactiv",
    "tokens":[
      "im",
      "bawl",
      "goodnight",
      "everyon",
      "load",
      "pray",
      "want",
      "proactiv"
    ],
    "token_count":8,
    "processed_text":"im bawl goodnight everyon load pray want proactiv"
  },
  {
    "label":4,
    "text":"your hyster thank morn laugh via must mean moron laugh",
    "cleaned_text":"your hyster thank morn laugh via must mean moron laugh",
    "normalized_text":"your hyster thank morn laugh via must mean moron laugh",
    "tokens":[
      "hyster",
      "thank",
      "morn",
      "laugh",
      "via",
      "mean",
      "moron",
      "laugh"
    ],
    "token_count":8,
    "processed_text":"hyster thank morn laugh via mean moron laugh"
  },
  {
    "label":4,
    "text":"bitchass soo annoy parent unit numero uno dad offici gone good move florida btw conform",
    "cleaned_text":"bitchass soo annoy parent unit numero uno dad offici gone good move florida btw conform",
    "normalized_text":"bitchass soo annoy parent unit numero uno dad offici gone good move florida btw conform",
    "tokens":[
      "bitchass",
      "soo",
      "annoy",
      "parent",
      "unit",
      "numero",
      "uno",
      "dad",
      "offici",
      "gone",
      "good",
      "move",
      "florida",
      "btw",
      "conform"
    ],
    "token_count":15,
    "processed_text":"bitchass soo annoy parent unit numero uno dad offici gone good move florida btw conform"
  },
  {
    "label":4,
    "text":"train love crew",
    "cleaned_text":"train love crew",
    "normalized_text":"train love crew",
    "tokens":[
      "train",
      "love",
      "crew"
    ],
    "token_count":3,
    "processed_text":"train love crew"
  },
  {
    "label":4,
    "text":"crm amp social media need follow scrm amp profil",
    "cleaned_text":"crm amp social media need follow scrm amp profil",
    "normalized_text":"crm amp social media need follow scrm amp profil",
    "tokens":[
      "crm",
      "amp",
      "social",
      "media",
      "need",
      "follow",
      "scrm",
      "amp",
      "profil"
    ],
    "token_count":9,
    "processed_text":"crm amp social media need follow scrm amp profil"
  },
  {
    "label":4,
    "text":"andyclemmensen whoo got trend topic rule",
    "cleaned_text":"andyclemmensen whoo got trend topic rule",
    "normalized_text":"andyclemmensen whoo got trend topic rule",
    "tokens":[
      "andyclemmensen",
      "whoo",
      "got",
      "trend",
      "topic",
      "rule"
    ],
    "token_count":6,
    "processed_text":"andyclemmensen whoo got trend topic rule"
  },
  {
    "label":4,
    "text":"accord marco smoke weed make philosoph consid philosoph shower",
    "cleaned_text":"accord marco smoke weed make philosoph consid philosoph shower",
    "normalized_text":"accord marco smoke weed make philosoph consid philosoph shower",
    "tokens":[
      "accord",
      "marco",
      "smoke",
      "weed",
      "make",
      "philosoph",
      "consid",
      "philosoph",
      "shower"
    ],
    "token_count":9,
    "processed_text":"accord marco smoke weed make philosoph consid philosoph shower"
  },
  {
    "label":4,
    "text":"ok rainbow sesam street one rock lil world young",
    "cleaned_text":"ok rainbow sesam street one rock lil world young",
    "normalized_text":"ok rainbow sesam street one rock lil world young",
    "tokens":[
      "ok",
      "rainbow",
      "sesam",
      "street",
      "one",
      "rock",
      "lil",
      "world",
      "young"
    ],
    "token_count":9,
    "processed_text":"ok rainbow sesam street one rock lil world young"
  },
  {
    "label":0,
    "text":"didnt see ur face today ill tomorrow day let catch",
    "cleaned_text":"didnt see ur face today ill tomorrow day let catch",
    "normalized_text":"didnt see ur face today ill tomorrow day let catch",
    "tokens":[
      "didnt",
      "see",
      "ur",
      "face",
      "today",
      "ill",
      "tomorrow",
      "day",
      "let",
      "catch"
    ],
    "token_count":10,
    "processed_text":"didnt see ur face today ill tomorrow day let catch"
  },
  {
    "label":4,
    "text":"go drive shop list whilst cigarett amp someth dinner",
    "cleaned_text":"go drive shop list whilst cigarett amp someth dinner",
    "normalized_text":"go drive shop list whilst cigarett amp someth dinner",
    "tokens":[
      "go",
      "drive",
      "shop",
      "list",
      "whilst",
      "cigarett",
      "amp",
      "someth",
      "dinner"
    ],
    "token_count":9,
    "processed_text":"go drive shop list whilst cigarett amp someth dinner"
  },
  {
    "label":4,
    "text":"iz jealou hope your fun",
    "cleaned_text":"iz jealou hope your fun",
    "normalized_text":"iz jealou hope your fun",
    "tokens":[
      "iz",
      "jealou",
      "hope",
      "fun"
    ],
    "token_count":4,
    "processed_text":"iz jealou hope fun"
  },
  {
    "label":0,
    "text":"fresno andor bakersfield pleas havent seen guy jerri pizza sinc mc tour mani moon ago",
    "cleaned_text":"fresno andor bakersfield pleas havent seen guy jerri pizza sinc mc tour mani moon ago",
    "normalized_text":"fresno andor bakersfield pleas havent seen guy jerri pizza sinc mc tour mani moon ago",
    "tokens":[
      "fresno",
      "andor",
      "bakersfield",
      "plea",
      "havent",
      "seen",
      "guy",
      "jerri",
      "pizza",
      "sinc",
      "mc",
      "tour",
      "mani",
      "moon",
      "ago"
    ],
    "token_count":15,
    "processed_text":"fresno andor bakersfield plea havent seen guy jerri pizza sinc mc tour mani moon ago"
  },
  {
    "label":0,
    "text":"also user wont ever find set user assum set isnt cool",
    "cleaned_text":"also user wont ever find set user assum set isnt cool",
    "normalized_text":"also user wont ever find set user assum set isnt cool",
    "tokens":[
      "also",
      "user",
      "wont",
      "ever",
      "find",
      "set",
      "user",
      "assum",
      "set",
      "isnt",
      "cool"
    ],
    "token_count":11,
    "processed_text":"also user wont ever find set user assum set isnt cool"
  },
  {
    "label":0,
    "text":"well live second away go anytim realli pretti",
    "cleaned_text":"well live second away go anytim realli pretti",
    "normalized_text":"well live second away go anytim realli pretti",
    "tokens":[
      "well",
      "live",
      "second",
      "away",
      "go",
      "anytim",
      "realli",
      "pretti"
    ],
    "token_count":8,
    "processed_text":"well live second away go anytim realli pretti"
  },
  {
    "label":4,
    "text":"cant wait trip europ day go",
    "cleaned_text":"cant wait trip europ day go",
    "normalized_text":"cant wait trip europ day go",
    "tokens":[
      "cant",
      "wait",
      "trip",
      "europ",
      "day",
      "go"
    ],
    "token_count":6,
    "processed_text":"cant wait trip europ day go"
  },
  {
    "label":0,
    "text":"last minut revis gonna help",
    "cleaned_text":"last minut revis gonna help",
    "normalized_text":"last minut revis gonna help",
    "tokens":[
      "last",
      "minut",
      "revi",
      "gon",
      "na",
      "help"
    ],
    "token_count":6,
    "processed_text":"last minut revi gon na help"
  },
  {
    "label":0,
    "text":"coldston sushi break amazingli good need love baldwin happi becca paint came soo good lab till",
    "cleaned_text":"coldston sushi break amazingli good need love baldwin happi becca paint came soo good lab till",
    "normalized_text":"coldston sushi break amazingli good need love baldwin happi becca paint came soo good lab till",
    "tokens":[
      "coldston",
      "sushi",
      "break",
      "amazingli",
      "good",
      "need",
      "love",
      "baldwin",
      "happi",
      "becca",
      "paint",
      "came",
      "soo",
      "good",
      "lab",
      "till"
    ],
    "token_count":16,
    "processed_text":"coldston sushi break amazingli good need love baldwin happi becca paint came soo good lab till"
  },
  {
    "label":0,
    "text":"ahw that beyond sweet want",
    "cleaned_text":"ahw that beyond sweet want",
    "normalized_text":"ahw that beyond sweet want",
    "tokens":[
      "ahw",
      "beyond",
      "sweet",
      "want"
    ],
    "token_count":4,
    "processed_text":"ahw beyond sweet want"
  },
  {
    "label":4,
    "text":"carniv maggi",
    "cleaned_text":"carniv maggi",
    "normalized_text":"carniv maggi",
    "tokens":[
      "carniv",
      "maggi"
    ],
    "token_count":2,
    "processed_text":"carniv maggi"
  },
  {
    "label":4,
    "text":"go goood day im mood snocon",
    "cleaned_text":"go goood day im mood snocon",
    "normalized_text":"go goood day im mood snocon",
    "tokens":[
      "go",
      "goood",
      "day",
      "im",
      "mood",
      "snocon"
    ],
    "token_count":6,
    "processed_text":"go goood day im mood snocon"
  },
  {
    "label":4,
    "text":"eh send email salescom talk",
    "cleaned_text":"eh send email salescom talk",
    "normalized_text":"eh send email salescom talk",
    "tokens":[
      "eh",
      "send",
      "email",
      "salescom",
      "talk"
    ],
    "token_count":5,
    "processed_text":"eh send email salescom talk"
  },
  {
    "label":0,
    "text":"fake tweet err wake slice garlic",
    "cleaned_text":"fake tweet err wake slice garlic",
    "normalized_text":"fake tweet err wake slice garlic",
    "tokens":[
      "fake",
      "tweet",
      "err",
      "wake",
      "slice",
      "garlic"
    ],
    "token_count":6,
    "processed_text":"fake tweet err wake slice garlic"
  },
  {
    "label":4,
    "text":"read write cping",
    "cleaned_text":"read write cping",
    "normalized_text":"read write cping",
    "tokens":[
      "read",
      "write",
      "cping"
    ],
    "token_count":3,
    "processed_text":"read write cping"
  },
  {
    "label":0,
    "text":"rain rain go away pleas come back day",
    "cleaned_text":"rain rain go away pleas come back day",
    "normalized_text":"rain rain go away pleas come back day",
    "tokens":[
      "rain",
      "rain",
      "go",
      "away",
      "plea",
      "come",
      "back",
      "day"
    ],
    "token_count":8,
    "processed_text":"rain rain go away plea come back day"
  },
  {
    "label":0,
    "text":"horribl dream last night",
    "cleaned_text":"horribl dream last night",
    "normalized_text":"horribl dream last night",
    "tokens":[
      "horribl",
      "dream",
      "last",
      "night"
    ],
    "token_count":4,
    "processed_text":"horribl dream last night"
  },
  {
    "label":0,
    "text":"kid use ask radio control car everi christma alway got cheap remot control car",
    "cleaned_text":"kid use ask radio control car everi christma alway got cheap remot control car",
    "normalized_text":"kid use ask radio control car everi christma alway got cheap remot control car",
    "tokens":[
      "kid",
      "use",
      "ask",
      "radio",
      "control",
      "car",
      "everi",
      "christma",
      "alway",
      "got",
      "cheap",
      "remot",
      "control",
      "car"
    ],
    "token_count":14,
    "processed_text":"kid use ask radio control car everi christma alway got cheap remot control car"
  },
  {
    "label":4,
    "text":"least saw got chanc part haha",
    "cleaned_text":"least saw got chanc part haha",
    "normalized_text":"least saw got chanc part haha",
    "tokens":[
      "least",
      "saw",
      "got",
      "chanc",
      "part",
      "haha"
    ],
    "token_count":6,
    "processed_text":"least saw got chanc part haha"
  },
  {
    "label":4,
    "text":"well back realityi love realiti though",
    "cleaned_text":"well back realityi love realiti though",
    "normalized_text":"well back realityi love realiti though",
    "tokens":[
      "well",
      "back",
      "realityi",
      "love",
      "realiti",
      "though"
    ],
    "token_count":6,
    "processed_text":"well back realityi love realiti though"
  },
  {
    "label":4,
    "text":"love bo sing well damn talent bastard",
    "cleaned_text":"love bo sing well damn talent bastard",
    "normalized_text":"love bo sing well damn talent bastard",
    "tokens":[
      "love",
      "bo",
      "sing",
      "well",
      "damn",
      "talent",
      "bastard"
    ],
    "token_count":7,
    "processed_text":"love bo sing well damn talent bastard"
  },
  {
    "label":4,
    "text":"thank mush rolando game cool funi what exactli day rolando come best",
    "cleaned_text":"thank mush rolando game cool funi what exactli day rolando come best",
    "normalized_text":"thank mush rolando game cool funi what exactli day rolando come best",
    "tokens":[
      "thank",
      "mush",
      "rolando",
      "game",
      "cool",
      "funi",
      "exactli",
      "day",
      "rolando",
      "come",
      "best"
    ],
    "token_count":11,
    "processed_text":"thank mush rolando game cool funi exactli day rolando come best"
  },
  {
    "label":4,
    "text":"could supris littl mic pick could pretend make bird nois",
    "cleaned_text":"could supris littl mic pick could pretend make bird nois",
    "normalized_text":"could supris littl mic pick could pretend make bird nois",
    "tokens":[
      "supri",
      "littl",
      "mic",
      "pick",
      "pretend",
      "make",
      "bird",
      "noi"
    ],
    "token_count":8,
    "processed_text":"supri littl mic pick pretend make bird noi"
  },
  {
    "label":0,
    "text":"live version much better",
    "cleaned_text":"live version much better",
    "normalized_text":"live version much better",
    "tokens":[
      "live",
      "version",
      "much",
      "better"
    ],
    "token_count":4,
    "processed_text":"live version much better"
  },
  {
    "label":4,
    "text":"cash cash",
    "cleaned_text":"cash cash",
    "normalized_text":"cash cash",
    "tokens":[
      "cash",
      "cash"
    ],
    "token_count":2,
    "processed_text":"cash cash"
  },
  {
    "label":4,
    "text":"sing",
    "cleaned_text":"sing",
    "normalized_text":"sing",
    "tokens":[
      "sing"
    ],
    "token_count":1,
    "processed_text":"sing"
  },
  {
    "label":0,
    "text":"uh oh doesnt mean think",
    "cleaned_text":"uh oh doesnt mean think",
    "normalized_text":"uh oh doesnt mean think",
    "tokens":[
      "uh",
      "oh",
      "doesnt",
      "mean",
      "think"
    ],
    "token_count":5,
    "processed_text":"uh oh doesnt mean think"
  },
  {
    "label":0,
    "text":"done haha smell holiday skin howev happi angri shade pinkr",
    "cleaned_text":"done haha smell holiday skin howev happi angri shade pinkr",
    "normalized_text":"done haha smell holiday skin howev happi angri shade pinkr",
    "tokens":[
      "done",
      "haha",
      "smell",
      "holiday",
      "skin",
      "howev",
      "happi",
      "angri",
      "shade",
      "pinkr"
    ],
    "token_count":10,
    "processed_text":"done haha smell holiday skin howev happi angri shade pinkr"
  },
  {
    "label":0,
    "text":"last week elect start seem paster war goe time",
    "cleaned_text":"last week elect start seem paster war goe time",
    "normalized_text":"last week elect start seem paster war goe time",
    "tokens":[
      "last",
      "week",
      "elect",
      "start",
      "seem",
      "paster",
      "war",
      "goe",
      "time"
    ],
    "token_count":9,
    "processed_text":"last week elect start seem paster war goe time"
  },
  {
    "label":0,
    "text":"someth seem wrong applecom itun app store inaccess need game",
    "cleaned_text":"someth seem wrong applecom itun app store inaccess need game",
    "normalized_text":"someth seem wrong applecom itun app store inaccess need game",
    "tokens":[
      "someth",
      "seem",
      "wrong",
      "applecom",
      "itun",
      "app",
      "store",
      "inaccess",
      "need",
      "game"
    ],
    "token_count":10,
    "processed_text":"someth seem wrong applecom itun app store inaccess need game"
  },
  {
    "label":4,
    "text":"nice talk",
    "cleaned_text":"nice talk",
    "normalized_text":"nice talk",
    "tokens":[
      "nice",
      "talk"
    ],
    "token_count":2,
    "processed_text":"nice talk"
  },
  {
    "label":0,
    "text":"mai meano today",
    "cleaned_text":"mai meano today",
    "normalized_text":"mai meano today",
    "tokens":[
      "mai",
      "meano",
      "today"
    ],
    "token_count":3,
    "processed_text":"mai meano today"
  },
  {
    "label":4,
    "text":"saw glad surviv storm goodi store",
    "cleaned_text":"saw glad surviv storm goodi store",
    "normalized_text":"saw glad surviv storm goodi store",
    "tokens":[
      "saw",
      "glad",
      "surviv",
      "storm",
      "goodi",
      "store"
    ],
    "token_count":6,
    "processed_text":"saw glad surviv storm goodi store"
  },
  {
    "label":0,
    "text":"bodi sore",
    "cleaned_text":"bodi sore",
    "normalized_text":"bodi sore",
    "tokens":[
      "bodi",
      "sore"
    ],
    "token_count":2,
    "processed_text":"bodi sore"
  },
  {
    "label":4,
    "text":"hmmm one",
    "cleaned_text":"hmmm one",
    "normalized_text":"hmmm one",
    "tokens":[
      "hmmm",
      "one"
    ],
    "token_count":2,
    "processed_text":"hmmm one"
  },
  {
    "label":0,
    "text":"wackest guess gotta ju wait til rain stop go jupit",
    "cleaned_text":"wackest guess gotta ju wait til rain stop go jupit",
    "normalized_text":"wackest guess gotta ju wait til rain stop go jupit",
    "tokens":[
      "wackest",
      "guess",
      "got",
      "ta",
      "ju",
      "wait",
      "til",
      "rain",
      "stop",
      "go",
      "jupit"
    ],
    "token_count":11,
    "processed_text":"wackest guess got ta ju wait til rain stop go jupit"
  },
  {
    "label":0,
    "text":"head still pound",
    "cleaned_text":"head still pound",
    "normalized_text":"head still pound",
    "tokens":[
      "head",
      "still",
      "pound"
    ],
    "token_count":3,
    "processed_text":"head still pound"
  },
  {
    "label":0,
    "text":"fuck trackbal stuck",
    "cleaned_text":"fuck trackbal stuck",
    "normalized_text":"fuck trackbal stuck",
    "tokens":[
      "fuck",
      "trackbal",
      "stuck"
    ],
    "token_count":3,
    "processed_text":"fuck trackbal stuck"
  },
  {
    "label":0,
    "text":"fuckit tix jkimmel show sold rd longer",
    "cleaned_text":"fuckit tix jkimmel show sold rd longer",
    "normalized_text":"fuckit tix jkimmel show sold rd longer",
    "tokens":[
      "fuckit",
      "tix",
      "jkimmel",
      "show",
      "sold",
      "rd",
      "longer"
    ],
    "token_count":7,
    "processed_text":"fuckit tix jkimmel show sold rd longer"
  },
  {
    "label":0,
    "text":"iphon broken keyboard wont popup cant repli text",
    "cleaned_text":"iphon broken keyboard wont popup cant repli text",
    "normalized_text":"iphon broken keyboard wont popup cant repli text",
    "tokens":[
      "iphon",
      "broken",
      "keyboard",
      "wont",
      "popup",
      "cant",
      "repli",
      "text"
    ],
    "token_count":8,
    "processed_text":"iphon broken keyboard wont popup cant repli text"
  },
  {
    "label":0,
    "text":"gotta help mum wash bad time",
    "cleaned_text":"gotta help mum wash bad time",
    "normalized_text":"gotta help mum wash bad time",
    "tokens":[
      "got",
      "ta",
      "help",
      "mum",
      "wash",
      "bad",
      "time"
    ],
    "token_count":7,
    "processed_text":"got ta help mum wash bad time"
  },
  {
    "label":0,
    "text":"brew saturday morn coffe listen child toy sing farmer delli miss cool june weather last week",
    "cleaned_text":"brew saturday morn coffe listen child toy sing farmer delli miss cool june weather last week",
    "normalized_text":"brew saturday morn coffe listen child toy sing farmer delli miss cool june weather last week",
    "tokens":[
      "brew",
      "saturday",
      "morn",
      "coff",
      "listen",
      "child",
      "toy",
      "sing",
      "farmer",
      "delli",
      "miss",
      "cool",
      "june",
      "weather",
      "last",
      "week"
    ],
    "token_count":16,
    "processed_text":"brew saturday morn coff listen child toy sing farmer delli miss cool june weather last week"
  },
  {
    "label":4,
    "text":"ur welcomeanytim",
    "cleaned_text":"ur welcomeanytim",
    "normalized_text":"ur welcomeanytim",
    "tokens":[
      "ur",
      "welcomeanytim"
    ],
    "token_count":2,
    "processed_text":"ur welcomeanytim"
  },
  {
    "label":4,
    "text":"well wellwel look found",
    "cleaned_text":"well wellwel look found",
    "normalized_text":"well wellwel look found",
    "tokens":[
      "well",
      "wellwel",
      "look",
      "found"
    ],
    "token_count":4,
    "processed_text":"well wellwel look found"
  },
  {
    "label":4,
    "text":"texa babi repres what",
    "cleaned_text":"texa babi repres what",
    "normalized_text":"texa babi repres what",
    "tokens":[
      "texa",
      "babi",
      "repr"
    ],
    "token_count":3,
    "processed_text":"texa babi repr"
  },
  {
    "label":0,
    "text":"countdown begun day go",
    "cleaned_text":"countdown begun day go",
    "normalized_text":"countdown begun day go",
    "tokens":[
      "countdown",
      "begun",
      "day",
      "go"
    ],
    "token_count":4,
    "processed_text":"countdown begun day go"
  },
  {
    "label":4,
    "text":"work sunshin g dongl great way work weather",
    "cleaned_text":"work sunshin g dongl great way work weather",
    "normalized_text":"work sunshin g dongl great way work weather",
    "tokens":[
      "work",
      "sunshin",
      "dongl",
      "great",
      "way",
      "work",
      "weather"
    ],
    "token_count":7,
    "processed_text":"work sunshin dongl great way work weather"
  },
  {
    "label":0,
    "text":"want make chocol cover banana settl banana split instead",
    "cleaned_text":"want make chocol cover banana settl banana split instead",
    "normalized_text":"want make chocol cover banana settl banana split instead",
    "tokens":[
      "want",
      "make",
      "chocol",
      "cover",
      "banana",
      "settl",
      "banana",
      "split",
      "instead"
    ],
    "token_count":9,
    "processed_text":"want make chocol cover banana settl banana split instead"
  },
  {
    "label":0,
    "text":"go back work today",
    "cleaned_text":"go back work today",
    "normalized_text":"go back work today",
    "tokens":[
      "go",
      "back",
      "work",
      "today"
    ],
    "token_count":4,
    "processed_text":"go back work today"
  },
  {
    "label":4,
    "text":"awe thank sweetheart tell green favorit color",
    "cleaned_text":"awe thank sweetheart tell green favorit color",
    "normalized_text":"awe thank sweetheart tell green favorit color",
    "tokens":[
      "awe",
      "thank",
      "sweetheart",
      "tell",
      "green",
      "favorit",
      "color"
    ],
    "token_count":7,
    "processed_text":"awe thank sweetheart tell green favorit color"
  },
  {
    "label":0,
    "text":"well celebr father day day earli sinc hubbi work sundayit th wed anniversari",
    "cleaned_text":"well celebr father day day earli sinc hubbi work sundayit th wed anniversari",
    "normalized_text":"well celebr father day day earli sinc hubbi work sundayit th wed anniversari",
    "tokens":[
      "well",
      "celebr",
      "father",
      "day",
      "day",
      "earli",
      "sinc",
      "hubbi",
      "work",
      "sundayit",
      "th",
      "wed",
      "anniversari"
    ],
    "token_count":13,
    "processed_text":"well celebr father day day earli sinc hubbi work sundayit th wed anniversari"
  },
  {
    "label":0,
    "text":"weather cold june",
    "cleaned_text":"weather cold june",
    "normalized_text":"weather cold june",
    "tokens":[
      "weather",
      "cold",
      "june"
    ],
    "token_count":3,
    "processed_text":"weather cold june"
  },
  {
    "label":4,
    "text":"awak time go shop taylor swift amaz last night",
    "cleaned_text":"awak time go shop taylor swift amaz last night",
    "normalized_text":"awak time go shop taylor swift amaz last night",
    "tokens":[
      "awak",
      "time",
      "go",
      "shop",
      "taylor",
      "swift",
      "amaz",
      "last",
      "night"
    ],
    "token_count":9,
    "processed_text":"awak time go shop taylor swift amaz last night"
  },
  {
    "label":0,
    "text":"that offer",
    "cleaned_text":"that offer",
    "normalized_text":"that offer",
    "tokens":[
      "offer"
    ],
    "token_count":1,
    "processed_text":"offer"
  },
  {
    "label":4,
    "text":"glad tdrb sort ticket dont think jane would enjoy go",
    "cleaned_text":"glad tdrb sort ticket dont think jane would enjoy go",
    "normalized_text":"glad tdrb sort ticket dont think jane would enjoy go",
    "tokens":[
      "glad",
      "tdrb",
      "sort",
      "ticket",
      "dont",
      "think",
      "jane",
      "enjoy",
      "go"
    ],
    "token_count":9,
    "processed_text":"glad tdrb sort ticket dont think jane enjoy go"
  },
  {
    "label":4,
    "text":"hmm macbook biasa aja sih susah pakenya im use window program gttt",
    "cleaned_text":"hmm macbook biasa aja sih susah pakenya im use window program gttt",
    "normalized_text":"hmm macbook biasa aja sih susah pakenya im use window program gttt",
    "tokens":[
      "hmm",
      "macbook",
      "biasa",
      "aja",
      "sih",
      "susah",
      "pakenya",
      "im",
      "use",
      "window",
      "program",
      "gttt"
    ],
    "token_count":12,
    "processed_text":"hmm macbook biasa aja sih susah pakenya im use window program gttt"
  },
  {
    "label":4,
    "text":"nite least pointtonightgoodnight",
    "cleaned_text":"nite least pointtonightgoodnight",
    "normalized_text":"nite least pointtonightgoodnight",
    "tokens":[
      "nite",
      "least"
    ],
    "token_count":2,
    "processed_text":"nite least"
  },
  {
    "label":4,
    "text":"impress cracker actual good flavour",
    "cleaned_text":"impress cracker actual good flavour",
    "normalized_text":"impress cracker actual good flavour",
    "tokens":[
      "impress",
      "cracker",
      "actual",
      "good",
      "flavour"
    ],
    "token_count":5,
    "processed_text":"impress cracker actual good flavour"
  },
  {
    "label":0,
    "text":"glad spendin night sad tomorrow ill home",
    "cleaned_text":"glad spendin night sad tomorrow ill home",
    "normalized_text":"glad spendin night sad tomorrow ill home",
    "tokens":[
      "glad",
      "spendin",
      "night",
      "sad",
      "tomorrow",
      "ill",
      "home"
    ],
    "token_count":7,
    "processed_text":"glad spendin night sad tomorrow ill home"
  },
  {
    "label":0,
    "text":"damn time anoth pedicur chip toenail open cabinet shit happen",
    "cleaned_text":"damn time anoth pedicur chip toenail open cabinet shit happen",
    "normalized_text":"damn time anoth pedicur chip toenail open cabinet shit happen",
    "tokens":[
      "damn",
      "time",
      "anoth",
      "pedicur",
      "chip",
      "toenail",
      "open",
      "cabinet",
      "shit",
      "happen"
    ],
    "token_count":10,
    "processed_text":"damn time anoth pedicur chip toenail open cabinet shit happen"
  },
  {
    "label":0,
    "text":"cours car start fuck plan parti fml",
    "cleaned_text":"cours car start fuck plan parti fml",
    "normalized_text":"cours car start fuck plan parti fml",
    "tokens":[
      "cour",
      "car",
      "start",
      "fuck",
      "plan",
      "parti",
      "fml"
    ],
    "token_count":7,
    "processed_text":"cour car start fuck plan parti fml"
  },
  {
    "label":4,
    "text":"im tire unbeliev tire children amaz first photograph playschool",
    "cleaned_text":"im tire unbeliev tire children amaz first photograph playschool",
    "normalized_text":"im tire unbeliev tire children amaz first photograph playschool",
    "tokens":[
      "im",
      "tire",
      "unbeliev",
      "tire",
      "children",
      "amaz",
      "first",
      "photograph",
      "playschool"
    ],
    "token_count":9,
    "processed_text":"im tire unbeliev tire children amaz first photograph playschool"
  },
  {
    "label":4,
    "text":"hang around hous today movi tonight",
    "cleaned_text":"hang around hous today movi tonight",
    "normalized_text":"hang around hous today movi tonight",
    "tokens":[
      "hang",
      "around",
      "hou",
      "today",
      "movi",
      "tonight"
    ],
    "token_count":6,
    "processed_text":"hang around hou today movi tonight"
  },
  {
    "label":0,
    "text":"get readi class day miss pool lake husb kid fun without",
    "cleaned_text":"get readi class day miss pool lake husb kid fun without",
    "normalized_text":"get readi class day miss pool lake husb kid fun without",
    "tokens":[
      "get",
      "readi",
      "class",
      "day",
      "miss",
      "pool",
      "lake",
      "husb",
      "kid",
      "fun",
      "without"
    ],
    "token_count":11,
    "processed_text":"get readi class day miss pool lake husb kid fun without"
  },
  {
    "label":0,
    "text":"miss sobe lol televis",
    "cleaned_text":"miss sobe lol televis",
    "normalized_text":"miss sobe lol televis",
    "tokens":[
      "miss",
      "sobe",
      "lol",
      "televi"
    ],
    "token_count":4,
    "processed_text":"miss sobe lol televi"
  },
  {
    "label":0,
    "text":"final made offic realli make effort time sinc im known late",
    "cleaned_text":"final made offic realli make effort time sinc im known late",
    "normalized_text":"final made offic realli make effort time sinc im known late",
    "tokens":[
      "final",
      "made",
      "offic",
      "realli",
      "make",
      "effort",
      "time",
      "sinc",
      "im",
      "known",
      "late"
    ],
    "token_count":11,
    "processed_text":"final made offic realli make effort time sinc im known late"
  },
  {
    "label":0,
    "text":"someon need call depress sink soon",
    "cleaned_text":"someon need call depress sink soon",
    "normalized_text":"someon need call depress sink soon",
    "tokens":[
      "someon",
      "need",
      "call",
      "depress",
      "sink",
      "soon"
    ],
    "token_count":6,
    "processed_text":"someon need call depress sink soon"
  },
  {
    "label":4,
    "text":"happi birthday hope wonder day",
    "cleaned_text":"happi birthday hope wonder day",
    "normalized_text":"happi birthday hope wonder day",
    "tokens":[
      "happi",
      "birthday",
      "hope",
      "wonder",
      "day"
    ],
    "token_count":5,
    "processed_text":"happi birthday hope wonder day"
  },
  {
    "label":4,
    "text":"luvli sarah amp abi cute poni she happi us come time take lill maisi wk luv horsi like",
    "cleaned_text":"luvli sarah amp abi cute poni she happi us come time take lill maisi wk luv horsi like",
    "normalized_text":"luvli sarah amp abi cute poni she happi us come time take lill maisi wk luv horsi like",
    "tokens":[
      "luvli",
      "sarah",
      "amp",
      "abi",
      "cute",
      "poni",
      "happi",
      "us",
      "come",
      "time",
      "take",
      "lill",
      "maisi",
      "wk",
      "luv",
      "horsi",
      "like"
    ],
    "token_count":17,
    "processed_text":"luvli sarah amp abi cute poni happi us come time take lill maisi wk luv horsi like"
  },
  {
    "label":0,
    "text":"aww im send mine look aw ye need ash show ly xxx",
    "cleaned_text":"aww im send mine look aw ye need ash show ly xxx",
    "normalized_text":"aww im send mine look aw ye need ash show ly xxx",
    "tokens":[
      "aww",
      "im",
      "send",
      "mine",
      "look",
      "aw",
      "ye",
      "need",
      "ash",
      "show",
      "ly",
      "xxx"
    ],
    "token_count":12,
    "processed_text":"aww im send mine look aw ye need ash show ly xxx"
  },
  {
    "label":4,
    "text":"musicmonday didnt amp emili osment",
    "cleaned_text":"musicmonday didnt amp emili osment",
    "normalized_text":"musicmonday didnt amp emili osment",
    "tokens":[
      "musicmonday",
      "didnt",
      "amp",
      "emili",
      "osment"
    ],
    "token_count":5,
    "processed_text":"musicmonday didnt amp emili osment"
  },
  {
    "label":4,
    "text":"got concert ticket guitar pick hangin around mirror",
    "cleaned_text":"got concert ticket guitar pick hangin around mirror",
    "normalized_text":"got concert ticket guitar pick hangin around mirror",
    "tokens":[
      "got",
      "concert",
      "ticket",
      "guitar",
      "pick",
      "hangin",
      "around",
      "mirror"
    ],
    "token_count":8,
    "processed_text":"got concert ticket guitar pick hangin around mirror"
  },
  {
    "label":0,
    "text":"sorri couldnt hang u town great second leg tour",
    "cleaned_text":"sorri couldnt hang u town great second leg tour",
    "normalized_text":"sorri couldnt hang u town great second leg tour",
    "tokens":[
      "sorri",
      "couldnt",
      "hang",
      "town",
      "great",
      "second",
      "leg",
      "tour"
    ],
    "token_count":8,
    "processed_text":"sorri couldnt hang town great second leg tour"
  },
  {
    "label":4,
    "text":"get roch onlin least earn month year form",
    "cleaned_text":"get roch onlin least earn month year form",
    "normalized_text":"get roch onlin least earn month year form",
    "tokens":[
      "get",
      "roch",
      "onlin",
      "least",
      "earn",
      "month",
      "year",
      "form"
    ],
    "token_count":8,
    "processed_text":"get roch onlin least earn month year form"
  },
  {
    "label":0,
    "text":"aaaah scare fs actual split two im kind excit lot worri",
    "cleaned_text":"aaaah scare fs actual split two im kind excit lot worri",
    "normalized_text":"aaaah scare fs actual split two im kind excit lot worri",
    "tokens":[
      "aaaah",
      "scare",
      "fs",
      "actual",
      "split",
      "two",
      "im",
      "kind",
      "excit",
      "lot",
      "worri"
    ],
    "token_count":11,
    "processed_text":"aaaah scare fs actual split two im kind excit lot worri"
  },
  {
    "label":0,
    "text":"back wed drink coke think tuition tomorrow suck",
    "cleaned_text":"back wed drink coke think tuition tomorrow suck",
    "normalized_text":"back wed drink coke think tuition tomorrow suck",
    "tokens":[
      "back",
      "wed",
      "drink",
      "coke",
      "think",
      "tuition",
      "tomorrow",
      "suck"
    ],
    "token_count":8,
    "processed_text":"back wed drink coke think tuition tomorrow suck"
  },
  {
    "label":0,
    "text":"spent yesterday bikini sundresstoday wonder whether im go abl hang wash",
    "cleaned_text":"spent yesterday bikini sundresstoday wonder whether im go abl hang wash",
    "normalized_text":"spent yesterday bikini sundresstoday wonder whether im go abl hang wash",
    "tokens":[
      "spent",
      "yesterday",
      "bikini",
      "sundresstoday",
      "wonder",
      "whether",
      "im",
      "go",
      "abl",
      "hang",
      "wash"
    ],
    "token_count":11,
    "processed_text":"spent yesterday bikini sundresstoday wonder whether im go abl hang wash"
  },
  {
    "label":4,
    "text":"chocol banana pancak breakfast today think ill top appl chunk tasti guiltfre indulg",
    "cleaned_text":"chocol banana pancak breakfast today think ill top appl chunk tasti guiltfre indulg",
    "normalized_text":"chocol banana pancak breakfast today think ill top appl chunk tasti guiltfre indulg",
    "tokens":[
      "chocol",
      "banana",
      "pancak",
      "breakfast",
      "today",
      "think",
      "ill",
      "top",
      "appl",
      "chunk",
      "tasti",
      "guiltfr",
      "indulg"
    ],
    "token_count":13,
    "processed_text":"chocol banana pancak breakfast today think ill top appl chunk tasti guiltfr indulg"
  },
  {
    "label":0,
    "text":"go bed even funish miss like crazi mother f love",
    "cleaned_text":"go bed even funish miss like crazi mother f love",
    "normalized_text":"go bed even funish miss like crazi mother f love",
    "tokens":[
      "go",
      "bed",
      "even",
      "funish",
      "miss",
      "like",
      "crazi",
      "mother",
      "love"
    ],
    "token_count":9,
    "processed_text":"go bed even funish miss like crazi mother love"
  },
  {
    "label":0,
    "text":"npr contest open peopl us",
    "cleaned_text":"npr contest open peopl us",
    "normalized_text":"npr contest open peopl us",
    "tokens":[
      "npr",
      "contest",
      "open",
      "peopl",
      "us"
    ],
    "token_count":5,
    "processed_text":"npr contest open peopl us"
  },
  {
    "label":0,
    "text":"tri find someth wear tonight",
    "cleaned_text":"tri find someth wear tonight",
    "normalized_text":"tri find someth wear tonight",
    "tokens":[
      "tri",
      "find",
      "someth",
      "wear",
      "tonight"
    ],
    "token_count":5,
    "processed_text":"tri find someth wear tonight"
  },
  {
    "label":0,
    "text":"got stuck behind tractor full cow poo",
    "cleaned_text":"got stuck behind tractor full cow poo",
    "normalized_text":"got stuck behind tractor full cow poo",
    "tokens":[
      "got",
      "stuck",
      "behind",
      "tractor",
      "full",
      "cow",
      "poo"
    ],
    "token_count":7,
    "processed_text":"got stuck behind tractor full cow poo"
  },
  {
    "label":0,
    "text":"nice relax sunday catch motogp action later even though alreadi know result",
    "cleaned_text":"nice relax sunday catch motogp action later even though alreadi know result",
    "normalized_text":"nice relax sunday catch motogp action later even though alreadi know result",
    "tokens":[
      "nice",
      "relax",
      "sunday",
      "catch",
      "motogp",
      "action",
      "later",
      "even",
      "though",
      "alreadi",
      "know",
      "result"
    ],
    "token_count":12,
    "processed_text":"nice relax sunday catch motogp action later even though alreadi know result"
  },
  {
    "label":4,
    "text":"pnk awesom quotthi use funhous full evil clown time start countdown im gonna burn downquot",
    "cleaned_text":"pnk awesom quotthi use funhous full evil clown time start countdown im gonna burn downquot",
    "normalized_text":"pnk awesom quotthi use funhous full evil clown time start countdown im gonna burn downquot",
    "tokens":[
      "pnk",
      "awesom",
      "quotthi",
      "use",
      "funhou",
      "full",
      "evil",
      "clown",
      "time",
      "start",
      "countdown",
      "im",
      "gon",
      "na",
      "burn",
      "downquot"
    ],
    "token_count":16,
    "processed_text":"pnk awesom quotthi use funhou full evil clown time start countdown im gon na burn downquot"
  },
  {
    "label":4,
    "text":"shush",
    "cleaned_text":"shush",
    "normalized_text":"shush",
    "tokens":[
      "shush"
    ],
    "token_count":1,
    "processed_text":"shush"
  },
  {
    "label":0,
    "text":"sniffl sniffl cough cough",
    "cleaned_text":"sniffl sniffl cough cough",
    "normalized_text":"sniffl sniffl cough cough",
    "tokens":[
      "sniffl",
      "sniffl",
      "cough",
      "cough"
    ],
    "token_count":4,
    "processed_text":"sniffl sniffl cough cough"
  },
  {
    "label":0,
    "text":"want gym stupid ankl",
    "cleaned_text":"want gym stupid ankl",
    "normalized_text":"want gym stupid ankl",
    "tokens":[
      "want",
      "gym",
      "stupid",
      "ankl"
    ],
    "token_count":4,
    "processed_text":"want gym stupid ankl"
  },
  {
    "label":0,
    "text":"one mine bcomgradu exam cachart account exam clashingwhich one choosei confus",
    "cleaned_text":"one mine bcomgradu exam cachart account exam clashingwhich one choosei confus",
    "normalized_text":"one mine bcomgradu exam cachart account exam clashingwhich one choosei confus",
    "tokens":[
      "one",
      "mine",
      "bcomgradu",
      "exam",
      "cachart",
      "account",
      "exam",
      "clashingwhich",
      "one",
      "choosei",
      "confu"
    ],
    "token_count":11,
    "processed_text":"one mine bcomgradu exam cachart account exam clashingwhich one choosei confu"
  },
  {
    "label":4,
    "text":"want labradoodl",
    "cleaned_text":"want labradoodl",
    "normalized_text":"want labradoodl",
    "tokens":[
      "want",
      "labradoodl"
    ],
    "token_count":2,
    "processed_text":"want labradoodl"
  },
  {
    "label":0,
    "text":"lanc miss shay get pick much cri itcom home",
    "cleaned_text":"lanc miss shay get pick much cri itcom home",
    "normalized_text":"lanc miss shay get pick much cri itcom home",
    "tokens":[
      "lanc",
      "miss",
      "shay",
      "get",
      "pick",
      "much",
      "cri",
      "itcom",
      "home"
    ],
    "token_count":9,
    "processed_text":"lanc miss shay get pick much cri itcom home"
  },
  {
    "label":0,
    "text":"sorri know english suck iam portugues",
    "cleaned_text":"sorri know english suck iam portugues",
    "normalized_text":"sorri know english suck iam portugues",
    "tokens":[
      "sorri",
      "know",
      "english",
      "suck",
      "iam",
      "portugu"
    ],
    "token_count":6,
    "processed_text":"sorri know english suck iam portugu"
  },
  {
    "label":4,
    "text":"oh well go fave fun",
    "cleaned_text":"oh well go fave fun",
    "normalized_text":"oh well go fave fun",
    "tokens":[
      "oh",
      "well",
      "go",
      "fave",
      "fun"
    ],
    "token_count":5,
    "processed_text":"oh well go fave fun"
  },
  {
    "label":0,
    "text":"theyr horribl ill go gokey offend ear w scream gokeyisadouch",
    "cleaned_text":"theyr horribl ill go gokey offend ear w scream gokeyisadouch",
    "normalized_text":"theyr horribl ill go gokey offend ear w scream gokeyisadouch",
    "tokens":[
      "theyr",
      "horribl",
      "ill",
      "go",
      "gokey",
      "offend",
      "ear",
      "scream",
      "gokeyisadouch"
    ],
    "token_count":9,
    "processed_text":"theyr horribl ill go gokey offend ear scream gokeyisadouch"
  },
  {
    "label":0,
    "text":"realli get butt gear get work done hard nice day",
    "cleaned_text":"realli get butt gear get work done hard nice day",
    "normalized_text":"realli get butt gear get work done hard nice day",
    "tokens":[
      "realli",
      "get",
      "butt",
      "gear",
      "get",
      "work",
      "done",
      "hard",
      "nice",
      "day"
    ],
    "token_count":10,
    "processed_text":"realli get butt gear get work done hard nice day"
  },
  {
    "label":4,
    "text":"far greater trust know",
    "cleaned_text":"far greater trust know",
    "normalized_text":"far greater trust know",
    "tokens":[
      "far",
      "greater",
      "trust",
      "know"
    ],
    "token_count":4,
    "processed_text":"far greater trust know"
  },
  {
    "label":4,
    "text":"dont take book beg start get readi make vlog",
    "cleaned_text":"dont take book beg start get readi make vlog",
    "normalized_text":"dont take book beg start get readi make vlog",
    "tokens":[
      "dont",
      "take",
      "book",
      "beg",
      "start",
      "get",
      "readi",
      "make",
      "vlog"
    ],
    "token_count":9,
    "processed_text":"dont take book beg start get readi make vlog"
  },
  {
    "label":4,
    "text":"im go home take doggi parkwil twitter park",
    "cleaned_text":"im go home take doggi parkwil twitter park",
    "normalized_text":"im go home take doggi parkwil twitter park",
    "tokens":[
      "im",
      "go",
      "home",
      "take",
      "doggi",
      "parkwil",
      "twitter",
      "park"
    ],
    "token_count":8,
    "processed_text":"im go home take doggi parkwil twitter park"
  },
  {
    "label":4,
    "text":"lolll bloodi good excus gorra make havent ya haha",
    "cleaned_text":"lolll bloodi good excus gorra make havent ya haha",
    "normalized_text":"lolll bloodi good excus gorra make havent ya haha",
    "tokens":[
      "lolll",
      "bloodi",
      "good",
      "excu",
      "gorra",
      "make",
      "havent",
      "ya",
      "haha"
    ],
    "token_count":9,
    "processed_text":"lolll bloodi good excu gorra make havent ya haha"
  },
  {
    "label":4,
    "text":"temitracedemihahaha luv coupleand think theyr realli date",
    "cleaned_text":"temitracedemihahaha luv coupleand think theyr realli date",
    "normalized_text":"temitracedemihahaha luv coupleand think theyr realli date",
    "tokens":[
      "luv",
      "coupleand",
      "think",
      "theyr",
      "realli",
      "date"
    ],
    "token_count":6,
    "processed_text":"luv coupleand think theyr realli date"
  },
  {
    "label":0,
    "text":"got hung member custom servic",
    "cleaned_text":"got hung member custom servic",
    "normalized_text":"got hung member custom servic",
    "tokens":[
      "got",
      "hung",
      "member",
      "custom",
      "servic"
    ],
    "token_count":5,
    "processed_text":"got hung member custom servic"
  },
  {
    "label":0,
    "text":"awww im sorri hear hun last kitti heart break never forget",
    "cleaned_text":"awww im sorri hear hun last kitti heart break never forget",
    "normalized_text":"awww im sorri hear hun last kitti heart break never forget",
    "tokens":[
      "awww",
      "im",
      "sorri",
      "hear",
      "hun",
      "last",
      "kitti",
      "heart",
      "break",
      "never",
      "forget"
    ],
    "token_count":11,
    "processed_text":"awww im sorri hear hun last kitti heart break never forget"
  },
  {
    "label":4,
    "text":"crazi video timelaps style perspect make peopl look like stopmot claymat figur",
    "cleaned_text":"crazi video timelaps style perspect make peopl look like stopmot claymat figur",
    "normalized_text":"crazi video timelaps style perspect make peopl look like stopmot claymat figur",
    "tokens":[
      "crazi",
      "video",
      "timelap",
      "style",
      "perspect",
      "make",
      "peopl",
      "look",
      "like",
      "stopmot",
      "claymat",
      "figur"
    ],
    "token_count":12,
    "processed_text":"crazi video timelap style perspect make peopl look like stopmot claymat figur"
  },
  {
    "label":0,
    "text":"hate live england sometim hangov doesnt open till next week",
    "cleaned_text":"hate live england sometim hangov doesnt open till next week",
    "normalized_text":"hate live england sometim hangov doesnt open till next week",
    "tokens":[
      "hate",
      "live",
      "england",
      "sometim",
      "hangov",
      "doesnt",
      "open",
      "till",
      "next",
      "week"
    ],
    "token_count":10,
    "processed_text":"hate live england sometim hangov doesnt open till next week"
  },
  {
    "label":0,
    "text":"your jerk xd",
    "cleaned_text":"your jerk xd",
    "normalized_text":"your jerk xd",
    "tokens":[
      "jerk",
      "xd"
    ],
    "token_count":2,
    "processed_text":"jerk xd"
  },
  {
    "label":0,
    "text":"hahahah snayup soz",
    "cleaned_text":"hahahah snayup soz",
    "normalized_text":"hahahah snayup soz",
    "tokens":[
      "hahahah",
      "snayup",
      "soz"
    ],
    "token_count":3,
    "processed_text":"hahahah snayup soz"
  },
  {
    "label":4,
    "text":"mysteri trip uncov geneva switzerland amp french riviera amp lourd franc",
    "cleaned_text":"mysteri trip uncov geneva switzerland amp french riviera amp lourd franc",
    "normalized_text":"mysteri trip uncov geneva switzerland amp french riviera amp lourd franc",
    "tokens":[
      "mysteri",
      "trip",
      "uncov",
      "geneva",
      "switzerland",
      "amp",
      "french",
      "riviera",
      "amp",
      "lourd",
      "franc"
    ],
    "token_count":11,
    "processed_text":"mysteri trip uncov geneva switzerland amp french riviera amp lourd franc"
  },
  {
    "label":4,
    "text":"well im shower friend jacki sleep she bring movi watch",
    "cleaned_text":"well im shower friend jacki sleep she bring movi watch",
    "normalized_text":"well im shower friend jacki sleep she bring movi watch",
    "tokens":[
      "well",
      "im",
      "shower",
      "friend",
      "jacki",
      "sleep",
      "bring",
      "movi",
      "watch"
    ],
    "token_count":9,
    "processed_text":"well im shower friend jacki sleep bring movi watch"
  },
  {
    "label":4,
    "text":"lol love best friend make irl gif face amp read groupi stori pinkberri middl night",
    "cleaned_text":"lol love best friend make irl gif face amp read groupi stori pinkberri middl night",
    "normalized_text":"lol love best friend make irl gif face amp read groupi stori pinkberri middl night",
    "tokens":[
      "lol",
      "love",
      "best",
      "friend",
      "make",
      "irl",
      "gif",
      "face",
      "amp",
      "read",
      "groupi",
      "stori",
      "pinkberri",
      "middl",
      "night"
    ],
    "token_count":15,
    "processed_text":"lol love best friend make irl gif face amp read groupi stori pinkberri middl night"
  },
  {
    "label":0,
    "text":"dont wanna wake earli josh",
    "cleaned_text":"dont wanna wake earli josh",
    "normalized_text":"dont wanna wake earli josh",
    "tokens":[
      "dont",
      "wan",
      "na",
      "wake",
      "earli",
      "josh"
    ],
    "token_count":6,
    "processed_text":"dont wan na wake earli josh"
  },
  {
    "label":4,
    "text":"im greenhous open bar",
    "cleaned_text":"im greenhous open bar",
    "normalized_text":"im greenhous open bar",
    "tokens":[
      "im",
      "greenhou",
      "open",
      "bar"
    ],
    "token_count":4,
    "processed_text":"im greenhou open bar"
  },
  {
    "label":4,
    "text":"got tattoo yesterday amp im go boston afternoon hello urban outfitt",
    "cleaned_text":"got tattoo yesterday amp im go boston afternoon hello urban outfitt",
    "normalized_text":"got tattoo yesterday amp im go boston afternoon hello urban outfitt",
    "tokens":[
      "got",
      "tattoo",
      "yesterday",
      "amp",
      "im",
      "go",
      "boston",
      "afternoon",
      "hello",
      "urban",
      "outfitt"
    ],
    "token_count":11,
    "processed_text":"got tattoo yesterday amp im go boston afternoon hello urban outfitt"
  },
  {
    "label":4,
    "text":"got baltimor",
    "cleaned_text":"got baltimor",
    "normalized_text":"got baltimor",
    "tokens":[
      "got",
      "baltimor"
    ],
    "token_count":2,
    "processed_text":"got baltimor"
  },
  {
    "label":4,
    "text":"sweet",
    "cleaned_text":"sweet",
    "normalized_text":"sweet",
    "tokens":[
      "sweet"
    ],
    "token_count":1,
    "processed_text":"sweet"
  },
  {
    "label":0,
    "text":"stuck work",
    "cleaned_text":"stuck work",
    "normalized_text":"stuck work",
    "tokens":[
      "stuck",
      "work"
    ],
    "token_count":2,
    "processed_text":"stuck work"
  },
  {
    "label":0,
    "text":"u add u near pc",
    "cleaned_text":"u add u near pc",
    "normalized_text":"u add u near pc",
    "tokens":[
      "add",
      "near",
      "pc"
    ],
    "token_count":3,
    "processed_text":"add near pc"
  },
  {
    "label":0,
    "text":"bore day watch histori ice cream",
    "cleaned_text":"bore day watch histori ice cream",
    "normalized_text":"bore day watch histori ice cream",
    "tokens":[
      "bore",
      "day",
      "watch",
      "histori",
      "ice",
      "cream"
    ],
    "token_count":6,
    "processed_text":"bore day watch histori ice cream"
  },
  {
    "label":4,
    "text":"epic inde",
    "cleaned_text":"epic inde",
    "normalized_text":"epic inde",
    "tokens":[
      "epic",
      "ind"
    ],
    "token_count":2,
    "processed_text":"epic ind"
  },
  {
    "label":4,
    "text":"oh lawd total pass night twitter",
    "cleaned_text":"oh lawd total pass night twitter",
    "normalized_text":"oh lawd total pass night twitter",
    "tokens":[
      "oh",
      "lawd",
      "total",
      "pass",
      "night",
      "twitter"
    ],
    "token_count":6,
    "processed_text":"oh lawd total pass night twitter"
  },
  {
    "label":4,
    "text":"im go school today",
    "cleaned_text":"im go school today",
    "normalized_text":"im go school today",
    "tokens":[
      "im",
      "go",
      "school",
      "today"
    ],
    "token_count":4,
    "processed_text":"im go school today"
  },
  {
    "label":0,
    "text":"im disgust bad yesterday",
    "cleaned_text":"im disgust bad yesterday",
    "normalized_text":"im disgust bad yesterday",
    "tokens":[
      "im",
      "disgust",
      "bad",
      "yesterday"
    ],
    "token_count":4,
    "processed_text":"im disgust bad yesterday"
  },
  {
    "label":0,
    "text":"soo bore work",
    "cleaned_text":"soo bore work",
    "normalized_text":"soo bore work",
    "tokens":[
      "soo",
      "bore",
      "work"
    ],
    "token_count":3,
    "processed_text":"soo bore work"
  },
  {
    "label":4,
    "text":"twin love youuuuuuu keep tri monday k k im pump next weekend",
    "cleaned_text":"twin love youuuuuuu keep tri monday k k im pump next weekend",
    "normalized_text":"twin love youuuuuuu keep tri monday k k im pump next weekend",
    "tokens":[
      "twin",
      "love",
      "youuuuuuu",
      "keep",
      "tri",
      "monday",
      "im",
      "pump",
      "next",
      "weekend"
    ],
    "token_count":10,
    "processed_text":"twin love youuuuuuu keep tri monday im pump next weekend"
  },
  {
    "label":4,
    "text":"back london today thank dinner mum",
    "cleaned_text":"back london today thank dinner mum",
    "normalized_text":"back london today thank dinner mum",
    "tokens":[
      "back",
      "london",
      "today",
      "thank",
      "dinner",
      "mum"
    ],
    "token_count":6,
    "processed_text":"back london today thank dinner mum"
  },
  {
    "label":0,
    "text":"jr saw dr pa bronchiol weigh",
    "cleaned_text":"jr saw dr pa bronchiol weigh",
    "normalized_text":"jr saw dr pa bronchiol weigh",
    "tokens":[
      "jr",
      "saw",
      "dr",
      "pa",
      "bronchiol",
      "weigh"
    ],
    "token_count":6,
    "processed_text":"jr saw dr pa bronchiol weigh"
  },
  {
    "label":0,
    "text":"sniffl want gizmo",
    "cleaned_text":"sniffl want gizmo",
    "normalized_text":"sniffl want gizmo",
    "tokens":[
      "sniffl",
      "want",
      "gizmo"
    ],
    "token_count":3,
    "processed_text":"sniffl want gizmo"
  },
  {
    "label":0,
    "text":"also wan ler stella would scare watch",
    "cleaned_text":"also wan ler stella would scare watch",
    "normalized_text":"also wan ler stella would scare watch",
    "tokens":[
      "also",
      "wan",
      "ler",
      "stella",
      "scare",
      "watch"
    ],
    "token_count":6,
    "processed_text":"also wan ler stella scare watch"
  },
  {
    "label":4,
    "text":"awak nice sleep",
    "cleaned_text":"awak nice sleep",
    "normalized_text":"awak nice sleep",
    "tokens":[
      "awak",
      "nice",
      "sleep"
    ],
    "token_count":3,
    "processed_text":"awak nice sleep"
  },
  {
    "label":4,
    "text":"prohibit parti ss dress",
    "cleaned_text":"prohibit parti ss dress",
    "normalized_text":"prohibit parti ss dress",
    "tokens":[
      "prohibit",
      "parti",
      "ss",
      "dress"
    ],
    "token_count":4,
    "processed_text":"prohibit parti ss dress"
  },
  {
    "label":4,
    "text":"got yesterday",
    "cleaned_text":"got yesterday",
    "normalized_text":"got yesterday",
    "tokens":[
      "got",
      "yesterday"
    ],
    "token_count":2,
    "processed_text":"got yesterday"
  },
  {
    "label":4,
    "text":"go start quotwould ratherquot day",
    "cleaned_text":"go start quotwould ratherquot day",
    "normalized_text":"go start quotwould ratherquot day",
    "tokens":[
      "go",
      "start",
      "quotwould",
      "ratherquot",
      "day"
    ],
    "token_count":5,
    "processed_text":"go start quotwould ratherquot day"
  },
  {
    "label":4,
    "text":"total im sure would love proper weegi night could match drink drink",
    "cleaned_text":"total im sure would love proper weegi night could match drink drink",
    "normalized_text":"total im sure would love proper weegi night could match drink drink",
    "tokens":[
      "total",
      "im",
      "sure",
      "love",
      "proper",
      "weegi",
      "night",
      "match",
      "drink",
      "drink"
    ],
    "token_count":10,
    "processed_text":"total im sure love proper weegi night match drink drink"
  },
  {
    "label":0,
    "text":"still write essay church new drum set wait box unopen",
    "cleaned_text":"still write essay church new drum set wait box unopen",
    "normalized_text":"still write essay church new drum set wait box unopen",
    "tokens":[
      "still",
      "write",
      "essay",
      "church",
      "new",
      "drum",
      "set",
      "wait",
      "box",
      "unopen"
    ],
    "token_count":10,
    "processed_text":"still write essay church new drum set wait box unopen"
  },
  {
    "label":0,
    "text":"lol search lili allen remov peep follow wont let follow rpatz",
    "cleaned_text":"lol search lili allen remov peep follow wont let follow rpatz",
    "normalized_text":"lol search lili allen remov peep follow wont let follow rpatz",
    "tokens":[
      "lol",
      "search",
      "lili",
      "allen",
      "remov",
      "peep",
      "follow",
      "wont",
      "let",
      "follow",
      "rpatz"
    ],
    "token_count":11,
    "processed_text":"lol search lili allen remov peep follow wont let follow rpatz"
  },
  {
    "label":4,
    "text":"st time ive stoke opportun awhil quit time work far",
    "cleaned_text":"st time ive stoke opportun awhil quit time work far",
    "normalized_text":"st time ive stoke opportun awhil quit time work far",
    "tokens":[
      "st",
      "time",
      "ive",
      "stoke",
      "opportun",
      "awhil",
      "quit",
      "time",
      "work",
      "far"
    ],
    "token_count":10,
    "processed_text":"st time ive stoke opportun awhil quit time work far"
  },
  {
    "label":0,
    "text":"cant sleep bc wrist hurt much took pain medicin help",
    "cleaned_text":"cant sleep bc wrist hurt much took pain medicin help",
    "normalized_text":"cant sleep bc wrist hurt much took pain medicin help",
    "tokens":[
      "cant",
      "sleep",
      "bc",
      "wrist",
      "hurt",
      "much",
      "took",
      "pain",
      "medicin",
      "help"
    ],
    "token_count":10,
    "processed_text":"cant sleep bc wrist hurt much took pain medicin help"
  },
  {
    "label":4,
    "text":"dont sass ye better",
    "cleaned_text":"dont sass ye better",
    "normalized_text":"dont sass ye better",
    "tokens":[
      "dont",
      "sass",
      "ye",
      "better"
    ],
    "token_count":4,
    "processed_text":"dont sass ye better"
  },
  {
    "label":0,
    "text":"kinda nice head still hurt",
    "cleaned_text":"kinda nice head still hurt",
    "normalized_text":"kinda nice head still hurt",
    "tokens":[
      "kinda",
      "nice",
      "head",
      "still",
      "hurt"
    ],
    "token_count":5,
    "processed_text":"kinda nice head still hurt"
  },
  {
    "label":4,
    "text":"retweet dm got dont subscrib anyth tweet r tweet",
    "cleaned_text":"retweet dm got dont subscrib anyth tweet r tweet",
    "normalized_text":"retweet dm got dont subscrib anyth tweet r tweet",
    "tokens":[
      "retweet",
      "dm",
      "got",
      "dont",
      "subscrib",
      "anyth",
      "tweet",
      "tweet"
    ],
    "token_count":8,
    "processed_text":"retweet dm got dont subscrib anyth tweet tweet"
  },
  {
    "label":0,
    "text":"facebook msn bebo caus im well harcor see fuck bore x",
    "cleaned_text":"facebook msn bebo caus im well harcor see fuck bore x",
    "normalized_text":"facebook msn bebo caus im well harcor see fuck bore x",
    "tokens":[
      "facebook",
      "msn",
      "bebo",
      "cau",
      "im",
      "well",
      "harcor",
      "see",
      "fuck",
      "bore"
    ],
    "token_count":10,
    "processed_text":"facebook msn bebo cau im well harcor see fuck bore"
  },
  {
    "label":0,
    "text":"she upset came home didnt clean head pound couldnt anyth like",
    "cleaned_text":"she upset came home didnt clean head pound couldnt anyth like",
    "normalized_text":"she upset came home didnt clean head pound couldnt anyth like",
    "tokens":[
      "upset",
      "came",
      "home",
      "didnt",
      "clean",
      "head",
      "pound",
      "couldnt",
      "anyth",
      "like"
    ],
    "token_count":10,
    "processed_text":"upset came home didnt clean head pound couldnt anyth like"
  },
  {
    "label":4,
    "text":"home casino good day left extra pocket found read glass today",
    "cleaned_text":"home casino good day left extra pocket found read glass today",
    "normalized_text":"home casino good day left extra pocket found read glass today",
    "tokens":[
      "home",
      "casino",
      "good",
      "day",
      "left",
      "extra",
      "pocket",
      "found",
      "read",
      "glass",
      "today"
    ],
    "token_count":11,
    "processed_text":"home casino good day left extra pocket found read glass today"
  },
  {
    "label":4,
    "text":"wow great weather f",
    "cleaned_text":"wow great weather f",
    "normalized_text":"wow great weather f",
    "tokens":[
      "wow",
      "great",
      "weather"
    ],
    "token_count":3,
    "processed_text":"wow great weather"
  },
  {
    "label":4,
    "text":"want dslr",
    "cleaned_text":"want dslr",
    "normalized_text":"want dslr",
    "tokens":[
      "want",
      "dslr"
    ],
    "token_count":2,
    "processed_text":"want dslr"
  },
  {
    "label":0,
    "text":"someon tell googl quotfli speechquot quottitu andronicusquot tri seem she unfriend",
    "cleaned_text":"someon tell googl quotfli speechquot quottitu andronicusquot tri seem she unfriend",
    "normalized_text":"someon tell googl quotfli speechquot quottitu andronicusquot tri seem she unfriend",
    "tokens":[
      "someon",
      "tell",
      "googl",
      "quotfli",
      "speechquot",
      "quottitu",
      "andronicusquot",
      "tri",
      "seem",
      "unfriend"
    ],
    "token_count":10,
    "processed_text":"someon tell googl quotfli speechquot quottitu andronicusquot tri seem unfriend"
  },
  {
    "label":0,
    "text":"youd expect realli well went last week least theyv gone ive got hour two spare",
    "cleaned_text":"youd expect realli well went last week least theyv gone ive got hour two spare",
    "normalized_text":"youd expect realli well went last week least theyv gone ive got hour two spare",
    "tokens":[
      "youd",
      "expect",
      "realli",
      "well",
      "went",
      "last",
      "week",
      "least",
      "theyv",
      "gone",
      "ive",
      "got",
      "hour",
      "two",
      "spare"
    ],
    "token_count":15,
    "processed_text":"youd expect realli well went last week least theyv gone ive got hour two spare"
  },
  {
    "label":0,
    "text":"id argu actual go apolog dayof sinc dont get one",
    "cleaned_text":"id argu actual go apolog dayof sinc dont get one",
    "normalized_text":"id argu actual go apolog dayof sinc dont get one",
    "tokens":[
      "id",
      "argu",
      "actual",
      "go",
      "apolog",
      "dayof",
      "sinc",
      "dont",
      "get",
      "one"
    ],
    "token_count":10,
    "processed_text":"id argu actual go apolog dayof sinc dont get one"
  },
  {
    "label":4,
    "text":"lol ill def help",
    "cleaned_text":"lol ill def help",
    "normalized_text":"lol ill def help",
    "tokens":[
      "lol",
      "ill",
      "def",
      "help"
    ],
    "token_count":4,
    "processed_text":"lol ill def help"
  },
  {
    "label":0,
    "text":"cant sleep shoulder killin",
    "cleaned_text":"cant sleep shoulder killin",
    "normalized_text":"cant sleep shoulder killin",
    "tokens":[
      "cant",
      "sleep",
      "shoulder",
      "killin"
    ],
    "token_count":4,
    "processed_text":"cant sleep shoulder killin"
  },
  {
    "label":0,
    "text":"that hot still ear though",
    "cleaned_text":"that hot still ear though",
    "normalized_text":"that hot still ear though",
    "tokens":[
      "hot",
      "still",
      "ear",
      "though"
    ],
    "token_count":4,
    "processed_text":"hot still ear though"
  },
  {
    "label":4,
    "text":"didnt first export blog though memori issu yet work fine",
    "cleaned_text":"didnt first export blog though memori issu yet work fine",
    "normalized_text":"didnt first export blog though memori issu yet work fine",
    "tokens":[
      "didnt",
      "first",
      "export",
      "blog",
      "though",
      "memori",
      "issu",
      "yet",
      "work",
      "fine"
    ],
    "token_count":10,
    "processed_text":"didnt first export blog though memori issu yet work fine"
  },
  {
    "label":4,
    "text":"sound like pile fun",
    "cleaned_text":"sound like pile fun",
    "normalized_text":"sound like pile fun",
    "tokens":[
      "sound",
      "like",
      "pile",
      "fun"
    ],
    "token_count":4,
    "processed_text":"sound like pile fun"
  },
  {
    "label":0,
    "text":"feel good hislt",
    "cleaned_text":"feel good hislt",
    "normalized_text":"feel good hislt",
    "tokens":[
      "feel",
      "good",
      "hislt"
    ],
    "token_count":3,
    "processed_text":"feel good hislt"
  },
  {
    "label":4,
    "text":"peopl fuck werid",
    "cleaned_text":"peopl fuck werid",
    "normalized_text":"peopl fuck werid",
    "tokens":[
      "peopl",
      "fuck",
      "werid"
    ],
    "token_count":3,
    "processed_text":"peopl fuck werid"
  },
  {
    "label":0,
    "text":"found server last night good",
    "cleaned_text":"found server last night good",
    "normalized_text":"found server last night good",
    "tokens":[
      "found",
      "server",
      "last",
      "night",
      "good"
    ],
    "token_count":5,
    "processed_text":"found server last night good"
  },
  {
    "label":4,
    "text":"watch previous er amazingggg",
    "cleaned_text":"watch previous er amazingggg",
    "normalized_text":"watch previous er amazingggg",
    "tokens":[
      "watch",
      "previou",
      "er",
      "amazingggg"
    ],
    "token_count":4,
    "processed_text":"watch previou er amazingggg"
  },
  {
    "label":4,
    "text":"name lion club leo",
    "cleaned_text":"name lion club leo",
    "normalized_text":"name lion club leo",
    "tokens":[
      "name",
      "lion",
      "club",
      "leo"
    ],
    "token_count":4,
    "processed_text":"name lion club leo"
  },
  {
    "label":0,
    "text":"still dont understand twitter much",
    "cleaned_text":"still dont understand twitter much",
    "normalized_text":"still dont understand twitter much",
    "tokens":[
      "still",
      "dont",
      "understand",
      "twitter",
      "much"
    ],
    "token_count":5,
    "processed_text":"still dont understand twitter much"
  },
  {
    "label":4,
    "text":"still tri figur twitter ish",
    "cleaned_text":"still tri figur twitter ish",
    "normalized_text":"still tri figur twitter ish",
    "tokens":[
      "still",
      "tri",
      "figur",
      "twitter",
      "ish"
    ],
    "token_count":5,
    "processed_text":"still tri figur twitter ish"
  },
  {
    "label":0,
    "text":"head back saintu lucia harsh breed uni",
    "cleaned_text":"head back saintu lucia harsh breed uni",
    "normalized_text":"head back saintu lucia harsh breed uni",
    "tokens":[
      "head",
      "back",
      "saintu",
      "lucia",
      "harsh",
      "breed",
      "uni"
    ],
    "token_count":7,
    "processed_text":"head back saintu lucia harsh breed uni"
  },
  {
    "label":4,
    "text":"hello hello tweet tweet say",
    "cleaned_text":"hello hello tweet tweet say",
    "normalized_text":"hello hello tweet tweet say",
    "tokens":[
      "hello",
      "hello",
      "tweet",
      "tweet",
      "say"
    ],
    "token_count":5,
    "processed_text":"hello hello tweet tweet say"
  },
  {
    "label":4,
    "text":"guy absolut amaz today there better way say",
    "cleaned_text":"guy absolut amaz today there better way say",
    "normalized_text":"guy absolut amaz today there better way say",
    "tokens":[
      "guy",
      "absolut",
      "amaz",
      "today",
      "better",
      "way",
      "say"
    ],
    "token_count":7,
    "processed_text":"guy absolut amaz today better way say"
  },
  {
    "label":4,
    "text":"babi couch potato love",
    "cleaned_text":"babi couch potato love",
    "normalized_text":"babi couch potato love",
    "tokens":[
      "babi",
      "couch",
      "potato",
      "love"
    ],
    "token_count":4,
    "processed_text":"babi couch potato love"
  },
  {
    "label":0,
    "text":"hope youll fantast summer summer end hahah hope repli",
    "cleaned_text":"hope youll fantast summer summer end hahah hope repli",
    "normalized_text":"hope youll fantast summer summer end hahah hope repli",
    "tokens":[
      "hope",
      "youll",
      "fantast",
      "summer",
      "summer",
      "end",
      "hahah",
      "hope",
      "repli"
    ],
    "token_count":9,
    "processed_text":"hope youll fantast summer summer end hahah hope repli"
  },
  {
    "label":0,
    "text":"sim work new mac went spent hope would possibl work doesnt",
    "cleaned_text":"sim work new mac went spent hope would possibl work doesnt",
    "normalized_text":"sim work new mac went spent hope would possibl work doesnt",
    "tokens":[
      "sim",
      "work",
      "new",
      "mac",
      "went",
      "spent",
      "hope",
      "possibl",
      "work",
      "doesnt"
    ],
    "token_count":10,
    "processed_text":"sim work new mac went spent hope possibl work doesnt"
  },
  {
    "label":4,
    "text":"awww your cute thank sweeti ye everybodi love itor els",
    "cleaned_text":"awww your cute thank sweeti ye everybodi love itor els",
    "normalized_text":"awww your cute thank sweeti ye everybodi love itor els",
    "tokens":[
      "awww",
      "cute",
      "thank",
      "sweeti",
      "ye",
      "everybodi",
      "love",
      "itor",
      "el"
    ],
    "token_count":9,
    "processed_text":"awww cute thank sweeti ye everybodi love itor el"
  },
  {
    "label":4,
    "text":"nice chat web mktg team terri colleg busi uga brought coffe",
    "cleaned_text":"nice chat web mktg team terri colleg busi uga brought coffe",
    "normalized_text":"nice chat web mktg team terri colleg busi uga brought coffe",
    "tokens":[
      "nice",
      "chat",
      "web",
      "mktg",
      "team",
      "terri",
      "colleg",
      "busi",
      "uga",
      "brought",
      "coff"
    ],
    "token_count":11,
    "processed_text":"nice chat web mktg team terri colleg busi uga brought coff"
  },
  {
    "label":4,
    "text":"brighton hope gonna good day",
    "cleaned_text":"brighton hope gonna good day",
    "normalized_text":"brighton hope gonna good day",
    "tokens":[
      "brighton",
      "hope",
      "gon",
      "na",
      "good",
      "day"
    ],
    "token_count":6,
    "processed_text":"brighton hope gon na good day"
  },
  {
    "label":4,
    "text":"ugh evil plan get rid sister go work never home sleep oh well time go see",
    "cleaned_text":"ugh evil plan get rid sister go work never home sleep oh well time go see",
    "normalized_text":"ugh evil plan get rid sister go work never home sleep oh well time go see",
    "tokens":[
      "ugh",
      "evil",
      "plan",
      "get",
      "rid",
      "sister",
      "go",
      "work",
      "never",
      "home",
      "sleep",
      "oh",
      "well",
      "time",
      "go",
      "see"
    ],
    "token_count":16,
    "processed_text":"ugh evil plan get rid sister go work never home sleep oh well time go see"
  },
  {
    "label":0,
    "text":"cant revis whyyi rain miss ziiheathhrayarachel oh fo fizzl",
    "cleaned_text":"cant revis whyyi rain miss ziiheathhrayarachel oh fo fizzl",
    "normalized_text":"cant revis whyyi rain miss ziiheathhrayarachel oh fo fizzl",
    "tokens":[
      "cant",
      "revi",
      "whyyi",
      "rain",
      "miss",
      "oh",
      "fo",
      "fizzl"
    ],
    "token_count":8,
    "processed_text":"cant revi whyyi rain miss oh fo fizzl"
  },
  {
    "label":4,
    "text":"cooli would prob okay need someon",
    "cleaned_text":"cooli would prob okay need someon",
    "normalized_text":"cooli would prob okay need someon",
    "tokens":[
      "cooli",
      "prob",
      "okay",
      "need",
      "someon"
    ],
    "token_count":5,
    "processed_text":"cooli prob okay need someon"
  },
  {
    "label":4,
    "text":"good luck migrat dreamhost look like popular site still use dh site",
    "cleaned_text":"good luck migrat dreamhost look like popular site still use dh site",
    "normalized_text":"good luck migrat dreamhost look like popular site still use dh site",
    "tokens":[
      "good",
      "luck",
      "migrat",
      "dreamhost",
      "look",
      "like",
      "popular",
      "site",
      "still",
      "use",
      "dh",
      "site"
    ],
    "token_count":12,
    "processed_text":"good luck migrat dreamhost look like popular site still use dh site"
  },
  {
    "label":0,
    "text":"play quarter final mess knee",
    "cleaned_text":"play quarter final mess knee",
    "normalized_text":"play quarter final mess knee",
    "tokens":[
      "play",
      "quarter",
      "final",
      "mess",
      "knee"
    ],
    "token_count":5,
    "processed_text":"play quarter final mess knee"
  },
  {
    "label":0,
    "text":"im freak roast courthous",
    "cleaned_text":"im freak roast courthous",
    "normalized_text":"im freak roast courthous",
    "tokens":[
      "im",
      "freak",
      "roast",
      "courthou"
    ],
    "token_count":4,
    "processed_text":"im freak roast courthou"
  },
  {
    "label":4,
    "text":"im write script php watch video ted sum pretti good compani",
    "cleaned_text":"im write script php watch video ted sum pretti good compani",
    "normalized_text":"im write script php watch video ted sum pretti good compani",
    "tokens":[
      "im",
      "write",
      "script",
      "php",
      "watch",
      "video",
      "ted",
      "sum",
      "pretti",
      "good",
      "compani"
    ],
    "token_count":11,
    "processed_text":"im write script php watch video ted sum pretti good compani"
  },
  {
    "label":0,
    "text":"cat huf puf weather babi kick enthusiast long sleep today",
    "cleaned_text":"cat huf puf weather babi kick enthusiast long sleep today",
    "normalized_text":"cat huf puf weather babi kick enthusiast long sleep today",
    "tokens":[
      "cat",
      "huf",
      "puf",
      "weather",
      "babi",
      "kick",
      "enthusiast",
      "long",
      "sleep",
      "today"
    ],
    "token_count":10,
    "processed_text":"cat huf puf weather babi kick enthusiast long sleep today"
  },
  {
    "label":4,
    "text":"zs want go night twittervers",
    "cleaned_text":"zs want go night twittervers",
    "normalized_text":"zs want go night twittervers",
    "tokens":[
      "zs",
      "want",
      "go",
      "night",
      "twitterv"
    ],
    "token_count":5,
    "processed_text":"zs want go night twitterv"
  },
  {
    "label":0,
    "text":"last day kn tomorrow go wierd see everyon time",
    "cleaned_text":"last day kn tomorrow go wierd see everyon time",
    "normalized_text":"last day kn tomorrow go wierd see everyon time",
    "tokens":[
      "last",
      "day",
      "kn",
      "tomorrow",
      "go",
      "wierd",
      "see",
      "everyon",
      "time"
    ],
    "token_count":9,
    "processed_text":"last day kn tomorrow go wierd see everyon time"
  },
  {
    "label":4,
    "text":"new updat onlin check",
    "cleaned_text":"new updat onlin check",
    "normalized_text":"new updat onlin check",
    "tokens":[
      "new",
      "updat",
      "onlin",
      "check"
    ],
    "token_count":4,
    "processed_text":"new updat onlin check"
  },
  {
    "label":4,
    "text":"belong song",
    "cleaned_text":"belong song",
    "normalized_text":"belong song",
    "tokens":[
      "belong",
      "song"
    ],
    "token_count":2,
    "processed_text":"belong song"
  },
  {
    "label":4,
    "text":"officialtila dont need man make feel complet anyth gurl singl happi",
    "cleaned_text":"officialtila dont need man make feel complet anyth gurl singl happi",
    "normalized_text":"officialtila dont need man make feel complet anyth gurl singl happi",
    "tokens":[
      "officialtila",
      "dont",
      "need",
      "man",
      "make",
      "feel",
      "complet",
      "anyth",
      "gurl",
      "singl",
      "happi"
    ],
    "token_count":11,
    "processed_text":"officialtila dont need man make feel complet anyth gurl singl happi"
  },
  {
    "label":0,
    "text":"mitchel",
    "cleaned_text":"mitchel",
    "normalized_text":"mitchel",
    "tokens":[
      "mitchel"
    ],
    "token_count":1,
    "processed_text":"mitchel"
  },
  {
    "label":0,
    "text":"wasnt today",
    "cleaned_text":"wasnt today",
    "normalized_text":"wasnt today",
    "tokens":[
      "wasnt",
      "today"
    ],
    "token_count":2,
    "processed_text":"wasnt today"
  },
  {
    "label":0,
    "text":"haha laugh britt say yeah suck esp caus prob wont go sleep till",
    "cleaned_text":"haha laugh britt say yeah suck esp caus prob wont go sleep till",
    "normalized_text":"haha laugh britt say yeah suck esp caus prob wont go sleep till",
    "tokens":[
      "haha",
      "laugh",
      "britt",
      "say",
      "yeah",
      "suck",
      "esp",
      "cau",
      "prob",
      "wont",
      "go",
      "sleep",
      "till"
    ],
    "token_count":13,
    "processed_text":"haha laugh britt say yeah suck esp cau prob wont go sleep till"
  },
  {
    "label":0,
    "text":"gf use sonic warfar car make listen garbag music like peach cream",
    "cleaned_text":"gf use sonic warfar car make listen garbag music like peach cream",
    "normalized_text":"gf use sonic warfar car make listen garbag music like peach cream",
    "tokens":[
      "gf",
      "use",
      "sonic",
      "warfar",
      "car",
      "make",
      "listen",
      "garbag",
      "music",
      "like",
      "peach",
      "cream"
    ],
    "token_count":12,
    "processed_text":"gf use sonic warfar car make listen garbag music like peach cream"
  },
  {
    "label":4,
    "text":"congrat bigwig offici forb mention",
    "cleaned_text":"congrat bigwig offici forb mention",
    "normalized_text":"congrat bigwig offici forb mention",
    "tokens":[
      "congrat",
      "bigwig",
      "offici",
      "forb",
      "mention"
    ],
    "token_count":5,
    "processed_text":"congrat bigwig offici forb mention"
  },
  {
    "label":4,
    "text":"chariti softbal game today diablo stadiumbr good get free seee",
    "cleaned_text":"chariti softbal game today diablo stadiumbr good get free seee",
    "normalized_text":"chariti softbal game today diablo stadiumbr good get free seee",
    "tokens":[
      "chariti",
      "softbal",
      "game",
      "today",
      "diablo",
      "stadiumbr",
      "good",
      "get",
      "free",
      "seee"
    ],
    "token_count":10,
    "processed_text":"chariti softbal game today diablo stadiumbr good get free seee"
  },
  {
    "label":4,
    "text":"lol one eye bleed sound pretti bad alright mention code sound bad enough good luck",
    "cleaned_text":"lol one eye bleed sound pretti bad alright mention code sound bad enough good luck",
    "normalized_text":"lol one eye bleed sound pretti bad alright mention code sound bad enough good luck",
    "tokens":[
      "lol",
      "one",
      "eye",
      "bleed",
      "sound",
      "pretti",
      "bad",
      "alright",
      "mention",
      "code",
      "sound",
      "bad",
      "enough",
      "good",
      "luck"
    ],
    "token_count":15,
    "processed_text":"lol one eye bleed sound pretti bad alright mention code sound bad enough good luck"
  },
  {
    "label":0,
    "text":"power wtf middl scene",
    "cleaned_text":"power wtf middl scene",
    "normalized_text":"power wtf middl scene",
    "tokens":[
      "power",
      "wtf",
      "middl",
      "scene"
    ],
    "token_count":4,
    "processed_text":"power wtf middl scene"
  },
  {
    "label":4,
    "text":"na na nanana hey hey carri away sean aka total melt heart miss show",
    "cleaned_text":"na na nanana hey hey carri away sean aka total melt heart miss show",
    "normalized_text":"na na nanana hey hey carri away sean aka total melt heart miss show",
    "tokens":[
      "na",
      "na",
      "nanana",
      "hey",
      "hey",
      "carri",
      "away",
      "sean",
      "aka",
      "total",
      "melt",
      "heart",
      "miss",
      "show"
    ],
    "token_count":14,
    "processed_text":"na na nanana hey hey carri away sean aka total melt heart miss show"
  },
  {
    "label":0,
    "text":"miss chat",
    "cleaned_text":"miss chat",
    "normalized_text":"miss chat",
    "tokens":[
      "miss",
      "chat"
    ],
    "token_count":2,
    "processed_text":"miss chat"
  },
  {
    "label":0,
    "text":"properti photographi real estat photographi im googl canberra properti photographi im get traffic",
    "cleaned_text":"properti photographi real estat photographi im googl canberra properti photographi im get traffic",
    "normalized_text":"properti photographi real estat photographi im googl canberra properti photographi im get traffic",
    "tokens":[
      "properti",
      "photographi",
      "real",
      "estat",
      "photographi",
      "im",
      "googl",
      "canberra",
      "properti",
      "photographi",
      "im",
      "get",
      "traffic"
    ],
    "token_count":13,
    "processed_text":"properti photographi real estat photographi im googl canberra properti photographi im get traffic"
  },
  {
    "label":0,
    "text":"second night apart man bed go get bed trudg floor furnitur shop next check",
    "cleaned_text":"second night apart man bed go get bed trudg floor furnitur shop next check",
    "normalized_text":"second night apart man bed go get bed trudg floor furnitur shop next check",
    "tokens":[
      "second",
      "night",
      "apart",
      "man",
      "bed",
      "go",
      "get",
      "bed",
      "trudg",
      "floor",
      "furnitur",
      "shop",
      "next",
      "check"
    ],
    "token_count":14,
    "processed_text":"second night apart man bed go get bed trudg floor furnitur shop next check"
  },
  {
    "label":0,
    "text":"boy bath bed cant wait bloomin fest tomorrowhop weather good look bad sunday",
    "cleaned_text":"boy bath bed cant wait bloomin fest tomorrowhop weather good look bad sunday",
    "normalized_text":"boy bath bed cant wait bloomin fest tomorrowhop weather good look bad sunday",
    "tokens":[
      "boy",
      "bath",
      "bed",
      "cant",
      "wait",
      "bloomin",
      "fest",
      "tomorrowhop",
      "weather",
      "good",
      "look",
      "bad",
      "sunday"
    ],
    "token_count":13,
    "processed_text":"boy bath bed cant wait bloomin fest tomorrowhop weather good look bad sunday"
  },
  {
    "label":0,
    "text":"dont wanna go work",
    "cleaned_text":"dont wanna go work",
    "normalized_text":"dont wanna go work",
    "tokens":[
      "dont",
      "wan",
      "na",
      "go",
      "work"
    ],
    "token_count":5,
    "processed_text":"dont wan na go work"
  },
  {
    "label":0,
    "text":"upset",
    "cleaned_text":"upset",
    "normalized_text":"upset",
    "tokens":[
      "upset"
    ],
    "token_count":1,
    "processed_text":"upset"
  },
  {
    "label":0,
    "text":"oh wait get go concert septemb mine isnt octob least go one",
    "cleaned_text":"oh wait get go concert septemb mine isnt octob least go one",
    "normalized_text":"oh wait get go concert septemb mine isnt octob least go one",
    "tokens":[
      "oh",
      "wait",
      "get",
      "go",
      "concert",
      "septemb",
      "mine",
      "isnt",
      "octob",
      "least",
      "go",
      "one"
    ],
    "token_count":12,
    "processed_text":"oh wait get go concert septemb mine isnt octob least go one"
  },
  {
    "label":4,
    "text":"still starbuck talk michael stickam",
    "cleaned_text":"still starbuck talk michael stickam",
    "normalized_text":"still starbuck talk michael stickam",
    "tokens":[
      "still",
      "starbuck",
      "talk",
      "michael",
      "stickam"
    ],
    "token_count":5,
    "processed_text":"still starbuck talk michael stickam"
  },
  {
    "label":0,
    "text":"back hurt need watch sit build model hour straight type rehears report",
    "cleaned_text":"back hurt need watch sit build model hour straight type rehears report",
    "normalized_text":"back hurt need watch sit build model hour straight type rehears report",
    "tokens":[
      "back",
      "hurt",
      "need",
      "watch",
      "sit",
      "build",
      "model",
      "hour",
      "straight",
      "type",
      "rehear",
      "report"
    ],
    "token_count":12,
    "processed_text":"back hurt need watch sit build model hour straight type rehear report"
  },
  {
    "label":0,
    "text":"pretti picim sooo jealousim go anywher good summer",
    "cleaned_text":"pretti picim sooo jealousim go anywher good summer",
    "normalized_text":"pretti picim sooo jealousim go anywher good summer",
    "tokens":[
      "pretti",
      "picim",
      "sooo",
      "jealousim",
      "go",
      "anywh",
      "good",
      "summer"
    ],
    "token_count":8,
    "processed_text":"pretti picim sooo jealousim go anywh good summer"
  },
  {
    "label":4,
    "text":"listen kennst du die star bushido feat oliv pocher lt",
    "cleaned_text":"listen kennst du die star bushido feat oliv pocher lt",
    "normalized_text":"listen kennst du die star bushido feat oliv pocher lt",
    "tokens":[
      "listen",
      "kennst",
      "du",
      "die",
      "star",
      "bushido",
      "feat",
      "oliv",
      "pocher",
      "lt"
    ],
    "token_count":10,
    "processed_text":"listen kennst du die star bushido feat oliv pocher lt"
  },
  {
    "label":0,
    "text":"ahhh well thank support anyway",
    "cleaned_text":"ahhh well thank support anyway",
    "normalized_text":"ahhh well thank support anyway",
    "tokens":[
      "ahhh",
      "well",
      "thank",
      "support",
      "anyway"
    ],
    "token_count":5,
    "processed_text":"ahhh well thank support anyway"
  },
  {
    "label":4,
    "text":"happi three year anniversari thank much wonder three year",
    "cleaned_text":"happi three year anniversari thank much wonder three year",
    "normalized_text":"happi three year anniversari thank much wonder three year",
    "tokens":[
      "happi",
      "three",
      "year",
      "anniversari",
      "thank",
      "much",
      "wonder",
      "three",
      "year"
    ],
    "token_count":9,
    "processed_text":"happi three year anniversari thank much wonder three year"
  },
  {
    "label":4,
    "text":"sleep ani didnt sleep work day went dinner mexican famili",
    "cleaned_text":"sleep ani didnt sleep work day went dinner mexican famili",
    "normalized_text":"sleep ani didnt sleep work day went dinner mexican famili",
    "tokens":[
      "sleep",
      "ani",
      "didnt",
      "sleep",
      "work",
      "day",
      "went",
      "dinner",
      "mexican",
      "famili"
    ],
    "token_count":10,
    "processed_text":"sleep ani didnt sleep work day went dinner mexican famili"
  },
  {
    "label":4,
    "text":"cricket bring best beast us",
    "cleaned_text":"cricket bring best beast us",
    "normalized_text":"cricket bring best beast us",
    "tokens":[
      "cricket",
      "bring",
      "best",
      "beast",
      "us"
    ],
    "token_count":5,
    "processed_text":"cricket bring best beast us"
  },
  {
    "label":4,
    "text":"ill right overwith video cam",
    "cleaned_text":"ill right overwith video cam",
    "normalized_text":"ill right overwith video cam",
    "tokens":[
      "ill",
      "right",
      "overwith",
      "video",
      "cam"
    ],
    "token_count":5,
    "processed_text":"ill right overwith video cam"
  },
  {
    "label":4,
    "text":"woke play guitar hero ill make video im dress lol",
    "cleaned_text":"woke play guitar hero ill make video im dress lol",
    "normalized_text":"woke play guitar hero ill make video im dress lol",
    "tokens":[
      "woke",
      "play",
      "guitar",
      "hero",
      "ill",
      "make",
      "video",
      "im",
      "dress",
      "lol"
    ],
    "token_count":10,
    "processed_text":"woke play guitar hero ill make video im dress lol"
  },
  {
    "label":4,
    "text":"there guy bunni suit new v ad say",
    "cleaned_text":"there guy bunni suit new v ad say",
    "normalized_text":"there guy bunni suit new v ad say",
    "tokens":[
      "guy",
      "bunni",
      "suit",
      "new",
      "ad",
      "say"
    ],
    "token_count":6,
    "processed_text":"guy bunni suit new ad say"
  },
  {
    "label":0,
    "text":"odd reason twitter dont feel right feel well homo",
    "cleaned_text":"odd reason twitter dont feel right feel well homo",
    "normalized_text":"odd reason twitter dont feel right feel well homo",
    "tokens":[
      "odd",
      "reason",
      "twitter",
      "dont",
      "feel",
      "right",
      "feel",
      "well",
      "homo"
    ],
    "token_count":9,
    "processed_text":"odd reason twitter dont feel right feel well homo"
  },
  {
    "label":0,
    "text":"fear may kick without notic disappear randomli know got boot tweet much",
    "cleaned_text":"fear may kick without notic disappear randomli know got boot tweet much",
    "normalized_text":"fear may kick without notic disappear randomli know got boot tweet much",
    "tokens":[
      "fear",
      "may",
      "kick",
      "without",
      "notic",
      "disappear",
      "randomli",
      "know",
      "got",
      "boot",
      "tweet",
      "much"
    ],
    "token_count":12,
    "processed_text":"fear may kick without notic disappear randomli know got boot tweet much"
  },
  {
    "label":4,
    "text":"im go tv watch nbc philli im wear red top gold headband",
    "cleaned_text":"im go tv watch nbc philli im wear red top gold headband",
    "normalized_text":"im go tv watch nbc philli im wear red top gold headband",
    "tokens":[
      "im",
      "go",
      "tv",
      "watch",
      "nbc",
      "philli",
      "im",
      "wear",
      "red",
      "top",
      "gold",
      "headband"
    ],
    "token_count":12,
    "processed_text":"im go tv watch nbc philli im wear red top gold headband"
  },
  {
    "label":4,
    "text":"watch show",
    "cleaned_text":"watch show",
    "normalized_text":"watch show",
    "tokens":[
      "watch",
      "show"
    ],
    "token_count":2,
    "processed_text":"watch show"
  },
  {
    "label":4,
    "text":"record",
    "cleaned_text":"record",
    "normalized_text":"record",
    "tokens":[
      "record"
    ],
    "token_count":1,
    "processed_text":"record"
  },
  {
    "label":4,
    "text":"twitter oddli enough iwont name shame cock like canari clit im alway vile sorri",
    "cleaned_text":"twitter oddli enough iwont name shame cock like canari clit im alway vile sorri",
    "normalized_text":"twitter oddli enough iwont name shame cock like canari clit im alway vile sorri",
    "tokens":[
      "twitter",
      "oddli",
      "enough",
      "iwont",
      "name",
      "shame",
      "cock",
      "like",
      "canari",
      "clit",
      "im",
      "alway",
      "vile",
      "sorri"
    ],
    "token_count":14,
    "processed_text":"twitter oddli enough iwont name shame cock like canari clit im alway vile sorri"
  },
  {
    "label":4,
    "text":"eek im excit instal steam mac counter strike half life heart content",
    "cleaned_text":"eek im excit instal steam mac counter strike half life heart content",
    "normalized_text":"eek im excit instal steam mac counter strike half life heart content",
    "tokens":[
      "eek",
      "im",
      "excit",
      "instal",
      "steam",
      "mac",
      "counter",
      "strike",
      "half",
      "life",
      "heart",
      "content"
    ],
    "token_count":12,
    "processed_text":"eek im excit instal steam mac counter strike half life heart content"
  },
  {
    "label":0,
    "text":"poor chino dog hurt foot he limp",
    "cleaned_text":"poor chino dog hurt foot he limp",
    "normalized_text":"poor chino dog hurt foot he limp",
    "tokens":[
      "poor",
      "chino",
      "dog",
      "hurt",
      "foot",
      "limp"
    ],
    "token_count":6,
    "processed_text":"poor chino dog hurt foot limp"
  },
  {
    "label":0,
    "text":"imag look kanji dictionari unless know your realli confus simeonhobb",
    "cleaned_text":"imag look kanji dictionari unless know your realli confus simeonhobb",
    "normalized_text":"imag look kanji dictionari unless know your realli confus simeonhobb",
    "tokens":[
      "imag",
      "look",
      "kanji",
      "dictionari",
      "unless",
      "know",
      "realli",
      "confu",
      "simeonhobb"
    ],
    "token_count":9,
    "processed_text":"imag look kanji dictionari unless know realli confu simeonhobb"
  },
  {
    "label":4,
    "text":"robert pattinson",
    "cleaned_text":"robert pattinson",
    "normalized_text":"robert pattinson",
    "tokens":[
      "robert",
      "pattinson"
    ],
    "token_count":2,
    "processed_text":"robert pattinson"
  },
  {
    "label":4,
    "text":"troubl host seem complet solv sorri issu everyth return normal",
    "cleaned_text":"troubl host seem complet solv sorri issu everyth return normal",
    "normalized_text":"troubl host seem complet solv sorri issu everyth return normal",
    "tokens":[
      "troubl",
      "host",
      "seem",
      "complet",
      "solv",
      "sorri",
      "issu",
      "everyth",
      "return",
      "normal"
    ],
    "token_count":10,
    "processed_text":"troubl host seem complet solv sorri issu everyth return normal"
  },
  {
    "label":4,
    "text":"oh wow didnt know work radio learn someth new everyday huh",
    "cleaned_text":"oh wow didnt know work radio learn someth new everyday huh",
    "normalized_text":"oh wow didnt know work radio learn someth new everyday huh",
    "tokens":[
      "oh",
      "wow",
      "didnt",
      "know",
      "work",
      "radio",
      "learn",
      "someth",
      "new",
      "everyday",
      "huh"
    ],
    "token_count":11,
    "processed_text":"oh wow didnt know work radio learn someth new everyday huh"
  },
  {
    "label":4,
    "text":"like previous said aint hurt much today wonder day im gonna play spyro laura",
    "cleaned_text":"like previous said aint hurt much today wonder day im gonna play spyro laura",
    "normalized_text":"like previous said aint hurt much today wonder day im gonna play spyro laura",
    "tokens":[
      "like",
      "previou",
      "said",
      "aint",
      "hurt",
      "much",
      "today",
      "wonder",
      "day",
      "im",
      "gon",
      "na",
      "play",
      "spyro",
      "laura"
    ],
    "token_count":15,
    "processed_text":"like previou said aint hurt much today wonder day im gon na play spyro laura"
  },
  {
    "label":0,
    "text":"english paper quotbquot dissapoint",
    "cleaned_text":"english paper quotbquot dissapoint",
    "normalized_text":"english paper quotbquot dissapoint",
    "tokens":[
      "english",
      "paper",
      "quotbquot",
      "dissapoint"
    ],
    "token_count":4,
    "processed_text":"english paper quotbquot dissapoint"
  },
  {
    "label":4,
    "text":"look forward see share",
    "cleaned_text":"look forward see share",
    "normalized_text":"look forward see share",
    "tokens":[
      "look",
      "forward",
      "see",
      "share"
    ],
    "token_count":4,
    "processed_text":"look forward see share"
  },
  {
    "label":4,
    "text":"good need get housework done sit without interrupt cours tivo paus good part",
    "cleaned_text":"good need get housework done sit without interrupt cours tivo paus good part",
    "normalized_text":"good need get housework done sit without interrupt cours tivo paus good part",
    "tokens":[
      "good",
      "need",
      "get",
      "housework",
      "done",
      "sit",
      "without",
      "interrupt",
      "cour",
      "tivo",
      "pau",
      "good",
      "part"
    ],
    "token_count":13,
    "processed_text":"good need get housework done sit without interrupt cour tivo pau good part"
  },
  {
    "label":0,
    "text":"yet r u gunna meet tonit fuuuun miss oh la la time",
    "cleaned_text":"yet r u gunna meet tonit fuuuun miss oh la la time",
    "normalized_text":"yet r u gunna meet tonit fuuuun miss oh la la time",
    "tokens":[
      "yet",
      "gunna",
      "meet",
      "tonit",
      "fuuuun",
      "miss",
      "oh",
      "la",
      "la",
      "time"
    ],
    "token_count":10,
    "processed_text":"yet gunna meet tonit fuuuun miss oh la la time"
  },
  {
    "label":0,
    "text":"think im get sick",
    "cleaned_text":"think im get sick",
    "normalized_text":"think im get sick",
    "tokens":[
      "think",
      "im",
      "get",
      "sick"
    ],
    "token_count":4,
    "processed_text":"think im get sick"
  },
  {
    "label":4,
    "text":"agre",
    "cleaned_text":"agre",
    "normalized_text":"agre",
    "tokens":[
      "agr"
    ],
    "token_count":1,
    "processed_text":"agr"
  },
  {
    "label":0,
    "text":"school tomorrow got homework week oh well",
    "cleaned_text":"school tomorrow got homework week oh well",
    "normalized_text":"school tomorrow got homework week oh well",
    "tokens":[
      "school",
      "tomorrow",
      "got",
      "homework",
      "week",
      "oh",
      "well"
    ],
    "token_count":7,
    "processed_text":"school tomorrow got homework week oh well"
  },
  {
    "label":0,
    "text":"reciev email oberlin colleg titl quotth tall short oberlin collegequot tri tell",
    "cleaned_text":"reciev email oberlin colleg titl quotth tall short oberlin collegequot tri tell",
    "normalized_text":"reciev email oberlin colleg titl quotth tall short oberlin collegequot tri tell",
    "tokens":[
      "reciev",
      "email",
      "oberlin",
      "colleg",
      "titl",
      "quotth",
      "tall",
      "short",
      "oberlin",
      "collegequot",
      "tri",
      "tell"
    ],
    "token_count":12,
    "processed_text":"reciev email oberlin colleg titl quotth tall short oberlin collegequot tri tell"
  },
  {
    "label":0,
    "text":"chest hurt im go lie bed",
    "cleaned_text":"chest hurt im go lie bed",
    "normalized_text":"chest hurt im go lie bed",
    "tokens":[
      "chest",
      "hurt",
      "im",
      "go",
      "lie",
      "bed"
    ],
    "token_count":6,
    "processed_text":"chest hurt im go lie bed"
  },
  {
    "label":4,
    "text":"oh fab thank much realli appreci",
    "cleaned_text":"oh fab thank much realli appreci",
    "normalized_text":"oh fab thank much realli appreci",
    "tokens":[
      "oh",
      "fab",
      "thank",
      "much",
      "realli",
      "appreci"
    ],
    "token_count":6,
    "processed_text":"oh fab thank much realli appreci"
  },
  {
    "label":0,
    "text":"im work hate monday",
    "cleaned_text":"im work hate monday",
    "normalized_text":"im work hate monday",
    "tokens":[
      "im",
      "work",
      "hate",
      "monday"
    ],
    "token_count":4,
    "processed_text":"im work hate monday"
  },
  {
    "label":4,
    "text":"one planet one world one peopl one unifi support iranelect",
    "cleaned_text":"one planet one world one peopl one unifi support iranelect",
    "normalized_text":"one planet one world one peopl one unifi support iranelect",
    "tokens":[
      "one",
      "planet",
      "one",
      "world",
      "one",
      "peopl",
      "one",
      "unifi",
      "support",
      "iranelect"
    ],
    "token_count":10,
    "processed_text":"one planet one world one peopl one unifi support iranelect"
  },
  {
    "label":0,
    "text":"darn wont get time",
    "cleaned_text":"darn wont get time",
    "normalized_text":"darn wont get time",
    "tokens":[
      "darn",
      "wont",
      "get",
      "time"
    ],
    "token_count":4,
    "processed_text":"darn wont get time"
  },
  {
    "label":4,
    "text":"alfresco friday festiv dtown outsid plaza theatr free play till come play",
    "cleaned_text":"alfresco friday festiv dtown outsid plaza theatr free play till come play",
    "normalized_text":"alfresco friday festiv dtown outsid plaza theatr free play till come play",
    "tokens":[
      "alfresco",
      "friday",
      "festiv",
      "dtown",
      "outsid",
      "plaza",
      "theatr",
      "free",
      "play",
      "till",
      "come",
      "play"
    ],
    "token_count":12,
    "processed_text":"alfresco friday festiv dtown outsid plaza theatr free play till come play"
  },
  {
    "label":0,
    "text":"ahaa awh someon school got bu remind run joke funni funni miss school peopl",
    "cleaned_text":"ahaa awh someon school got bu remind run joke funni funni miss school peopl",
    "normalized_text":"ahaa awh someon school got bu remind run joke funni funni miss school peopl",
    "tokens":[
      "ahaa",
      "awh",
      "someon",
      "school",
      "got",
      "bu",
      "remind",
      "run",
      "joke",
      "funni",
      "funni",
      "miss",
      "school",
      "peopl"
    ],
    "token_count":14,
    "processed_text":"ahaa awh someon school got bu remind run joke funni funni miss school peopl"
  },
  {
    "label":0,
    "text":"sound good butter rice cinnamon dutch appl pie healthi",
    "cleaned_text":"sound good butter rice cinnamon dutch appl pie healthi",
    "normalized_text":"sound good butter rice cinnamon dutch appl pie healthi",
    "tokens":[
      "sound",
      "good",
      "butter",
      "rice",
      "cinnamon",
      "dutch",
      "appl",
      "pie",
      "healthi"
    ],
    "token_count":9,
    "processed_text":"sound good butter rice cinnamon dutch appl pie healthi"
  },
  {
    "label":4,
    "text":"saturday night live tonight",
    "cleaned_text":"saturday night live tonight",
    "normalized_text":"saturday night live tonight",
    "tokens":[
      "saturday",
      "night",
      "live",
      "tonight"
    ],
    "token_count":4,
    "processed_text":"saturday night live tonight"
  },
  {
    "label":4,
    "text":"hahaha hmm alum im go thursday",
    "cleaned_text":"hahaha hmm alum im go thursday",
    "normalized_text":"hahaha hmm alum im go thursday",
    "tokens":[
      "hahaha",
      "hmm",
      "alum",
      "im",
      "go",
      "thursday"
    ],
    "token_count":6,
    "processed_text":"hahaha hmm alum im go thursday"
  },
  {
    "label":4,
    "text":"see grey anatomi favourit seri",
    "cleaned_text":"see grey anatomi favourit seri",
    "normalized_text":"see grey anatomi favourit seri",
    "tokens":[
      "see",
      "grey",
      "anatomi",
      "favourit",
      "seri"
    ],
    "token_count":5,
    "processed_text":"see grey anatomi favourit seri"
  },
  {
    "label":0,
    "text":"lie bed crazi sick",
    "cleaned_text":"lie bed crazi sick",
    "normalized_text":"lie bed crazi sick",
    "tokens":[
      "lie",
      "bed",
      "crazi",
      "sick"
    ],
    "token_count":4,
    "processed_text":"lie bed crazi sick"
  },
  {
    "label":4,
    "text":"least quot thingsquot came within hour",
    "cleaned_text":"least quot thingsquot came within hour",
    "normalized_text":"least quot thingsquot came within hour",
    "tokens":[
      "least",
      "quot",
      "thingsquot",
      "came",
      "within",
      "hour"
    ],
    "token_count":6,
    "processed_text":"least quot thingsquot came within hour"
  },
  {
    "label":4,
    "text":"much expect much buy email matter catchm praval dot com thank",
    "cleaned_text":"much expect much buy email matter catchm praval dot com thank",
    "normalized_text":"much expect much buy email matter catchm praval dot com thank",
    "tokens":[
      "much",
      "expect",
      "much",
      "buy",
      "email",
      "matter",
      "catchm",
      "praval",
      "dot",
      "com",
      "thank"
    ],
    "token_count":11,
    "processed_text":"much expect much buy email matter catchm praval dot com thank"
  },
  {
    "label":4,
    "text":"flexibl one thing realli could work nearli disloc shoulder play golf wii lol get old think",
    "cleaned_text":"flexibl one thing realli could work nearli disloc shoulder play golf wii lol get old think",
    "normalized_text":"flexibl one thing realli could work nearli disloc shoulder play golf wii lol get old think",
    "tokens":[
      "flexibl",
      "one",
      "thing",
      "realli",
      "work",
      "nearli",
      "disloc",
      "shoulder",
      "play",
      "golf",
      "wii",
      "lol",
      "get",
      "old",
      "think"
    ],
    "token_count":15,
    "processed_text":"flexibl one thing realli work nearli disloc shoulder play golf wii lol get old think"
  },
  {
    "label":4,
    "text":"morn twitter everi one sunni cold day england",
    "cleaned_text":"morn twitter everi one sunni cold day england",
    "normalized_text":"morn twitter everi one sunni cold day england",
    "tokens":[
      "morn",
      "twitter",
      "everi",
      "one",
      "sunni",
      "cold",
      "day",
      "england"
    ],
    "token_count":8,
    "processed_text":"morn twitter everi one sunni cold day england"
  },
  {
    "label":4,
    "text":"thank like youtub video fish",
    "cleaned_text":"thank like youtub video fish",
    "normalized_text":"thank like youtub video fish",
    "tokens":[
      "thank",
      "like",
      "youtub",
      "video",
      "fish"
    ],
    "token_count":5,
    "processed_text":"thank like youtub video fish"
  },
  {
    "label":0,
    "text":"cant dm",
    "cleaned_text":"cant dm",
    "normalized_text":"cant dm",
    "tokens":[
      "cant",
      "dm"
    ],
    "token_count":2,
    "processed_text":"cant dm"
  },
  {
    "label":4,
    "text":"lucki estonia small countri one pool go beach summer",
    "cleaned_text":"lucki estonia small countri one pool go beach summer",
    "normalized_text":"lucki estonia small countri one pool go beach summer",
    "tokens":[
      "lucki",
      "estonia",
      "small",
      "countri",
      "one",
      "pool",
      "go",
      "beach",
      "summer"
    ],
    "token_count":9,
    "processed_text":"lucki estonia small countri one pool go beach summer"
  },
  {
    "label":0,
    "text":"didnt even end go heard blast bad aye",
    "cleaned_text":"didnt even end go heard blast bad aye",
    "normalized_text":"didnt even end go heard blast bad aye",
    "tokens":[
      "didnt",
      "even",
      "end",
      "go",
      "heard",
      "blast",
      "bad",
      "aye"
    ],
    "token_count":8,
    "processed_text":"didnt even end go heard blast bad aye"
  },
  {
    "label":4,
    "text":"web cellphon",
    "cleaned_text":"web cellphon",
    "normalized_text":"web cellphon",
    "tokens":[
      "web",
      "cellphon"
    ],
    "token_count":2,
    "processed_text":"web cellphon"
  },
  {
    "label":4,
    "text":"anyon know theyr make anoth xmen wolverin cannot get enough movi",
    "cleaned_text":"anyon know theyr make anoth xmen wolverin cannot get enough movi",
    "normalized_text":"anyon know theyr make anoth xmen wolverin cannot get enough movi",
    "tokens":[
      "anyon",
      "know",
      "theyr",
      "make",
      "anoth",
      "xmen",
      "wolverin",
      "get",
      "enough",
      "movi"
    ],
    "token_count":10,
    "processed_text":"anyon know theyr make anoth xmen wolverin get enough movi"
  },
  {
    "label":4,
    "text":"ive tip twice today haa",
    "cleaned_text":"ive tip twice today haa",
    "normalized_text":"ive tip twice today haa",
    "tokens":[
      "ive",
      "tip",
      "twice",
      "today",
      "haa"
    ],
    "token_count":5,
    "processed_text":"ive tip twice today haa"
  },
  {
    "label":4,
    "text":"love stay late",
    "cleaned_text":"love stay late",
    "normalized_text":"love stay late",
    "tokens":[
      "love",
      "stay",
      "late"
    ],
    "token_count":3,
    "processed_text":"love stay late"
  },
  {
    "label":0,
    "text":"yeah cpw told sunday dont know network get tho",
    "cleaned_text":"yeah cpw told sunday dont know network get tho",
    "normalized_text":"yeah cpw told sunday dont know network get tho",
    "tokens":[
      "yeah",
      "cpw",
      "told",
      "sunday",
      "dont",
      "know",
      "network",
      "get",
      "tho"
    ],
    "token_count":9,
    "processed_text":"yeah cpw told sunday dont know network get tho"
  },
  {
    "label":4,
    "text":"final everyth fix edit side whew multipl episod come way week thank hang",
    "cleaned_text":"final everyth fix edit side whew multipl episod come way week thank hang",
    "normalized_text":"final everyth fix edit side whew multipl episod come way week thank hang",
    "tokens":[
      "final",
      "everyth",
      "fix",
      "edit",
      "side",
      "whew",
      "multipl",
      "episod",
      "come",
      "way",
      "week",
      "thank",
      "hang"
    ],
    "token_count":13,
    "processed_text":"final everyth fix edit side whew multipl episod come way week thank hang"
  },
  {
    "label":4,
    "text":"woah woah woah back photoshoot photograph",
    "cleaned_text":"woah woah woah back photoshoot photograph",
    "normalized_text":"woah woah woah back photoshoot photograph",
    "tokens":[
      "woah",
      "woah",
      "woah",
      "back",
      "photoshoot",
      "photograph"
    ],
    "token_count":6,
    "processed_text":"woah woah woah back photoshoot photograph"
  },
  {
    "label":0,
    "text":"sick june uh oh there practic tomorrow",
    "cleaned_text":"sick june uh oh there practic tomorrow",
    "normalized_text":"sick june uh oh there practic tomorrow",
    "tokens":[
      "sick",
      "june",
      "uh",
      "oh",
      "practic",
      "tomorrow"
    ],
    "token_count":6,
    "processed_text":"sick june uh oh practic tomorrow"
  },
  {
    "label":4,
    "text":"littl shop see movi kassi mayb hit coupl bar",
    "cleaned_text":"littl shop see movi kassi mayb hit coupl bar",
    "normalized_text":"littl shop see movi kassi mayb hit coupl bar",
    "tokens":[
      "littl",
      "shop",
      "see",
      "movi",
      "kassi",
      "mayb",
      "hit",
      "coupl",
      "bar"
    ],
    "token_count":9,
    "processed_text":"littl shop see movi kassi mayb hit coupl bar"
  },
  {
    "label":0,
    "text":"fanci starbuck read beth updat haha pc kill speed boo wish laptop charger",
    "cleaned_text":"fanci starbuck read beth updat haha pc kill speed boo wish laptop charger",
    "normalized_text":"fanci starbuck read beth updat haha pc kill speed boo wish laptop charger",
    "tokens":[
      "fanci",
      "starbuck",
      "read",
      "beth",
      "updat",
      "haha",
      "pc",
      "kill",
      "speed",
      "boo",
      "wish",
      "laptop",
      "charger"
    ],
    "token_count":13,
    "processed_text":"fanci starbuck read beth updat haha pc kill speed boo wish laptop charger"
  },
  {
    "label":4,
    "text":"clean room read",
    "cleaned_text":"clean room read",
    "normalized_text":"clean room read",
    "tokens":[
      "clean",
      "room",
      "read"
    ],
    "token_count":3,
    "processed_text":"clean room read"
  },
  {
    "label":4,
    "text":"goodnight everyon",
    "cleaned_text":"goodnight everyon",
    "normalized_text":"goodnight everyon",
    "tokens":[
      "goodnight",
      "everyon"
    ],
    "token_count":2,
    "processed_text":"goodnight everyon"
  },
  {
    "label":0,
    "text":"think needless say ambientchilloutdowntempo music call whatev u like much underr",
    "cleaned_text":"think needless say ambientchilloutdowntempo music call whatev u like much underr",
    "normalized_text":"think needless say ambientchilloutdowntempo music call whatev u like much underr",
    "tokens":[
      "think",
      "needless",
      "say",
      "music",
      "call",
      "whatev",
      "like",
      "much",
      "underr"
    ],
    "token_count":9,
    "processed_text":"think needless say music call whatev like much underr"
  },
  {
    "label":4,
    "text":"fo sho gnite",
    "cleaned_text":"fo sho gnite",
    "normalized_text":"fo sho gnite",
    "tokens":[
      "fo",
      "sho",
      "gnite"
    ],
    "token_count":3,
    "processed_text":"fo sho gnite"
  },
  {
    "label":0,
    "text":"realli need follow",
    "cleaned_text":"realli need follow",
    "normalized_text":"realli need follow",
    "tokens":[
      "realli",
      "need",
      "follow"
    ],
    "token_count":3,
    "processed_text":"realli need follow"
  },
  {
    "label":4,
    "text":"way freakin jealou like right hate memba lol fun time take pictur",
    "cleaned_text":"way freakin jealou like right hate memba lol fun time take pictur",
    "normalized_text":"way freakin jealou like right hate memba lol fun time take pictur",
    "tokens":[
      "way",
      "freakin",
      "jealou",
      "like",
      "right",
      "hate",
      "memba",
      "lol",
      "fun",
      "time",
      "take",
      "pictur"
    ],
    "token_count":12,
    "processed_text":"way freakin jealou like right hate memba lol fun time take pictur"
  },
  {
    "label":4,
    "text":"hahahaha face im home listen jona brother",
    "cleaned_text":"hahahaha face im home listen jona brother",
    "normalized_text":"hahahaha face im home listen jona brother",
    "tokens":[
      "hahahaha",
      "face",
      "im",
      "home",
      "listen",
      "jona",
      "brother"
    ],
    "token_count":7,
    "processed_text":"hahahaha face im home listen jona brother"
  },
  {
    "label":0,
    "text":"start get headach",
    "cleaned_text":"start get headach",
    "normalized_text":"start get headach",
    "tokens":[
      "start",
      "get",
      "headach"
    ],
    "token_count":3,
    "processed_text":"start get headach"
  },
  {
    "label":4,
    "text":"actual manoeuvr correct british english tweet origin esa europ",
    "cleaned_text":"actual manoeuvr correct british english tweet origin esa europ",
    "normalized_text":"actual manoeuvr correct british english tweet origin esa europ",
    "tokens":[
      "actual",
      "manoeuvr",
      "correct",
      "british",
      "english",
      "tweet",
      "origin",
      "esa",
      "europ"
    ],
    "token_count":9,
    "processed_text":"actual manoeuvr correct british english tweet origin esa europ"
  },
  {
    "label":0,
    "text":"gahh im mad miss ill wait tomorroww",
    "cleaned_text":"gahh im mad miss ill wait tomorroww",
    "normalized_text":"gahh im mad miss ill wait tomorroww",
    "tokens":[
      "gahh",
      "im",
      "mad",
      "miss",
      "ill",
      "wait",
      "tomorroww"
    ],
    "token_count":7,
    "processed_text":"gahh im mad miss ill wait tomorroww"
  },
  {
    "label":0,
    "text":"hey doesnt alertm get messag",
    "cleaned_text":"hey doesnt alertm get messag",
    "normalized_text":"hey doesnt alertm get messag",
    "tokens":[
      "hey",
      "doesnt",
      "alertm",
      "get",
      "messag"
    ],
    "token_count":5,
    "processed_text":"hey doesnt alertm get messag"
  },
  {
    "label":0,
    "text":"thing maid meant",
    "cleaned_text":"thing maid meant",
    "normalized_text":"thing maid meant",
    "tokens":[
      "thing",
      "maid",
      "meant"
    ],
    "token_count":3,
    "processed_text":"thing maid meant"
  },
  {
    "label":4,
    "text":"remov unus applic game remov ton file need clean comput sim",
    "cleaned_text":"remov unus applic game remov ton file need clean comput sim",
    "normalized_text":"remov unus applic game remov ton file need clean comput sim",
    "tokens":[
      "remov",
      "unu",
      "applic",
      "game",
      "remov",
      "ton",
      "file",
      "need",
      "clean",
      "comput",
      "sim"
    ],
    "token_count":11,
    "processed_text":"remov unu applic game remov ton file need clean comput sim"
  },
  {
    "label":0,
    "text":"omg one schoolfre day",
    "cleaned_text":"omg one schoolfre day",
    "normalized_text":"omg one schoolfre day",
    "tokens":[
      "omg",
      "one",
      "schoolfr",
      "day"
    ],
    "token_count":4,
    "processed_text":"omg one schoolfr day"
  },
  {
    "label":4,
    "text":"hey get least one donnieklang live gt",
    "cleaned_text":"hey get least one donnieklang live gt",
    "normalized_text":"hey get least one donnieklang live gt",
    "tokens":[
      "hey",
      "get",
      "least",
      "one",
      "donnieklang",
      "live",
      "gt"
    ],
    "token_count":7,
    "processed_text":"hey get least one donnieklang live gt"
  },
  {
    "label":4,
    "text":"frangipani wild scent gt thankyou much gorgeou wildand devin beast youmwah mwah",
    "cleaned_text":"frangipani wild scent gt thankyou much gorgeou wildand devin beast youmwah mwah",
    "normalized_text":"frangipani wild scent gt thankyou much gorgeou wildand devin beast youmwah mwah",
    "tokens":[
      "frangipani",
      "wild",
      "scent",
      "gt",
      "thankyou",
      "much",
      "gorgeou",
      "wildand",
      "devin",
      "beast",
      "youmwah",
      "mwah"
    ],
    "token_count":12,
    "processed_text":"frangipani wild scent gt thankyou much gorgeou wildand devin beast youmwah mwah"
  },
  {
    "label":4,
    "text":"cant wait hear new tune mc hope visit toronto soon xoxo",
    "cleaned_text":"cant wait hear new tune mc hope visit toronto soon xoxo",
    "normalized_text":"cant wait hear new tune mc hope visit toronto soon xoxo",
    "tokens":[
      "cant",
      "wait",
      "hear",
      "new",
      "tune",
      "mc",
      "hope",
      "visit",
      "toronto",
      "soon",
      "xoxo"
    ],
    "token_count":11,
    "processed_text":"cant wait hear new tune mc hope visit toronto soon xoxo"
  },
  {
    "label":4,
    "text":"inde clearli share stupid humour p",
    "cleaned_text":"inde clearli share stupid humour p",
    "normalized_text":"inde clearli share stupid humour p",
    "tokens":[
      "ind",
      "clearli",
      "share",
      "stupid",
      "humour"
    ],
    "token_count":5,
    "processed_text":"ind clearli share stupid humour"
  },
  {
    "label":4,
    "text":"saigon intern airport await pal flight back home",
    "cleaned_text":"saigon intern airport await pal flight back home",
    "normalized_text":"saigon intern airport await pal flight back home",
    "tokens":[
      "saigon",
      "intern",
      "airport",
      "await",
      "pal",
      "flight",
      "back",
      "home"
    ],
    "token_count":8,
    "processed_text":"saigon intern airport await pal flight back home"
  },
  {
    "label":4,
    "text":"wonder time tonight",
    "cleaned_text":"wonder time tonight",
    "normalized_text":"wonder time tonight",
    "tokens":[
      "wonder",
      "time",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"wonder time tonight"
  },
  {
    "label":4,
    "text":"im park learn drive car",
    "cleaned_text":"im park learn drive car",
    "normalized_text":"im park learn drive car",
    "tokens":[
      "im",
      "park",
      "learn",
      "drive",
      "car"
    ],
    "token_count":5,
    "processed_text":"im park learn drive car"
  },
  {
    "label":0,
    "text":"justin get readi last time",
    "cleaned_text":"justin get readi last time",
    "normalized_text":"justin get readi last time",
    "tokens":[
      "justin",
      "get",
      "readi",
      "last",
      "time"
    ],
    "token_count":5,
    "processed_text":"justin get readi last time"
  },
  {
    "label":0,
    "text":"yeahat work",
    "cleaned_text":"yeahat work",
    "normalized_text":"yeahat work",
    "tokens":[
      "yeahat",
      "work"
    ],
    "token_count":2,
    "processed_text":"yeahat work"
  },
  {
    "label":4,
    "text":"reliev mother day franc",
    "cleaned_text":"reliev mother day franc",
    "normalized_text":"reliev mother day franc",
    "tokens":[
      "reliev",
      "mother",
      "day",
      "franc"
    ],
    "token_count":4,
    "processed_text":"reliev mother day franc"
  },
  {
    "label":0,
    "text":"obtain null locat didnt work",
    "cleaned_text":"obtain null locat didnt work",
    "normalized_text":"obtain null locat didnt work",
    "tokens":[
      "obtain",
      "null",
      "locat",
      "didnt",
      "work"
    ],
    "token_count":5,
    "processed_text":"obtain null locat didnt work"
  },
  {
    "label":0,
    "text":"get sick thank talli ahah jk stupid allergi",
    "cleaned_text":"get sick thank talli ahah jk stupid allergi",
    "normalized_text":"get sick thank talli ahah jk stupid allergi",
    "tokens":[
      "get",
      "sick",
      "thank",
      "talli",
      "ahah",
      "jk",
      "stupid",
      "allergi"
    ],
    "token_count":8,
    "processed_text":"get sick thank talli ahah jk stupid allergi"
  },
  {
    "label":4,
    "text":"fir appoint move start",
    "cleaned_text":"fir appoint move start",
    "normalized_text":"fir appoint move start",
    "tokens":[
      "fir",
      "appoint",
      "move",
      "start"
    ],
    "token_count":4,
    "processed_text":"fir appoint move start"
  },
  {
    "label":0,
    "text":"iphon app purchas download transfer devic aw appl fail",
    "cleaned_text":"iphon app purchas download transfer devic aw appl fail",
    "normalized_text":"iphon app purchas download transfer devic aw appl fail",
    "tokens":[
      "iphon",
      "app",
      "purcha",
      "download",
      "transfer",
      "devic",
      "aw",
      "appl",
      "fail"
    ],
    "token_count":9,
    "processed_text":"iphon app purcha download transfer devic aw appl fail"
  },
  {
    "label":0,
    "text":"problem steam commun could server ill go leftdead mod tool",
    "cleaned_text":"problem steam commun could server ill go leftdead mod tool",
    "normalized_text":"problem steam commun could server ill go leftdead mod tool",
    "tokens":[
      "problem",
      "steam",
      "commun",
      "server",
      "ill",
      "go",
      "leftdead",
      "mod",
      "tool"
    ],
    "token_count":9,
    "processed_text":"problem steam commun server ill go leftdead mod tool"
  },
  {
    "label":0,
    "text":"fwk camera wont work develop diari today guess mumbl",
    "cleaned_text":"fwk camera wont work develop diari today guess mumbl",
    "normalized_text":"fwk camera wont work develop diari today guess mumbl",
    "tokens":[
      "fwk",
      "camera",
      "wont",
      "work",
      "develop",
      "diari",
      "today",
      "guess",
      "mumbl"
    ],
    "token_count":9,
    "processed_text":"fwk camera wont work develop diari today guess mumbl"
  },
  {
    "label":4,
    "text":"think okjust go leav hubsh handymanawwwsh littl outgrow cot",
    "cleaned_text":"think okjust go leav hubsh handymanawwwsh littl outgrow cot",
    "normalized_text":"think okjust go leav hubsh handymanawwwsh littl outgrow cot",
    "tokens":[
      "think",
      "okjust",
      "go",
      "leav",
      "hubsh",
      "handymanawwwsh",
      "littl",
      "outgrow",
      "cot"
    ],
    "token_count":9,
    "processed_text":"think okjust go leav hubsh handymanawwwsh littl outgrow cot"
  },
  {
    "label":0,
    "text":"finger bleed",
    "cleaned_text":"finger bleed",
    "normalized_text":"finger bleed",
    "tokens":[
      "finger",
      "bleed"
    ],
    "token_count":2,
    "processed_text":"finger bleed"
  },
  {
    "label":0,
    "text":"kimbi mark wait aaa holli wont start",
    "cleaned_text":"kimbi mark wait aaa holli wont start",
    "normalized_text":"kimbi mark wait aaa holli wont start",
    "tokens":[
      "kimbi",
      "mark",
      "wait",
      "aaa",
      "holli",
      "wont",
      "start"
    ],
    "token_count":7,
    "processed_text":"kimbi mark wait aaa holli wont start"
  },
  {
    "label":4,
    "text":"love gettogeth",
    "cleaned_text":"love gettogeth",
    "normalized_text":"love gettogeth",
    "tokens":[
      "love",
      "gettogeth"
    ],
    "token_count":2,
    "processed_text":"love gettogeth"
  },
  {
    "label":0,
    "text":"hey one babi left im go withdraw hawkcam live gt",
    "cleaned_text":"hey one babi left im go withdraw hawkcam live gt",
    "normalized_text":"hey one babi left im go withdraw hawkcam live gt",
    "tokens":[
      "hey",
      "one",
      "babi",
      "left",
      "im",
      "go",
      "withdraw",
      "hawkcam",
      "live",
      "gt"
    ],
    "token_count":10,
    "processed_text":"hey one babi left im go withdraw hawkcam live gt"
  },
  {
    "label":0,
    "text":"watch quot week laterquot good day",
    "cleaned_text":"watch quot week laterquot good day",
    "normalized_text":"watch quot week laterquot good day",
    "tokens":[
      "watch",
      "quot",
      "week",
      "laterquot",
      "good",
      "day"
    ],
    "token_count":6,
    "processed_text":"watch quot week laterquot good day"
  },
  {
    "label":4,
    "text":"yo mimi thank kind word check messag",
    "cleaned_text":"yo mimi thank kind word check messag",
    "normalized_text":"yo mimi thank kind word check messag",
    "tokens":[
      "yo",
      "mimi",
      "thank",
      "kind",
      "word",
      "check",
      "messag"
    ],
    "token_count":7,
    "processed_text":"yo mimi thank kind word check messag"
  },
  {
    "label":0,
    "text":"ampnd still cant hear right ear",
    "cleaned_text":"ampnd still cant hear right ear",
    "normalized_text":"ampnd still cant hear right ear",
    "tokens":[
      "ampnd",
      "still",
      "cant",
      "hear",
      "right",
      "ear"
    ],
    "token_count":6,
    "processed_text":"ampnd still cant hear right ear"
  },
  {
    "label":4,
    "text":"rootkit detector use unhid stuff anti viru softwar found load stuff",
    "cleaned_text":"rootkit detector use unhid stuff anti viru softwar found load stuff",
    "normalized_text":"rootkit detector use unhid stuff anti viru softwar found load stuff",
    "tokens":[
      "rootkit",
      "detector",
      "use",
      "unhid",
      "stuff",
      "anti",
      "viru",
      "softwar",
      "found",
      "load",
      "stuff"
    ],
    "token_count":11,
    "processed_text":"rootkit detector use unhid stuff anti viru softwar found load stuff"
  },
  {
    "label":4,
    "text":"eeeeeeeeeeeeeeeeeeeeeep let hope team jona announc june th meet greet tomoroww badli wanna win",
    "cleaned_text":"eeeeeeeeeeeeeeeeeeeeeep let hope team jona announc june th meet greet tomoroww badli wanna win",
    "normalized_text":"eeeeeeeeeeeeeeeeeeeeeep let hope team jona announc june th meet greet tomoroww badli wanna win",
    "tokens":[
      "let",
      "hope",
      "team",
      "jona",
      "announc",
      "june",
      "th",
      "meet",
      "greet",
      "tomoroww",
      "badli",
      "wan",
      "na",
      "win"
    ],
    "token_count":14,
    "processed_text":"let hope team jona announc june th meet greet tomoroww badli wan na win"
  },
  {
    "label":4,
    "text":"listen chri brown keri hilson superhuman bloodi love song",
    "cleaned_text":"listen chri brown keri hilson superhuman bloodi love song",
    "normalized_text":"listen chri brown keri hilson superhuman bloodi love song",
    "tokens":[
      "listen",
      "chri",
      "brown",
      "keri",
      "hilson",
      "superhuman",
      "bloodi",
      "love",
      "song"
    ],
    "token_count":9,
    "processed_text":"listen chri brown keri hilson superhuman bloodi love song"
  },
  {
    "label":4,
    "text":"sam hot got booti fo day lafayett sexyso your true blood convert",
    "cleaned_text":"sam hot got booti fo day lafayett sexyso your true blood convert",
    "normalized_text":"sam hot got booti fo day lafayett sexyso your true blood convert",
    "tokens":[
      "sam",
      "hot",
      "got",
      "booti",
      "fo",
      "day",
      "lafayett",
      "sexyso",
      "true",
      "blood",
      "convert"
    ],
    "token_count":11,
    "processed_text":"sam hot got booti fo day lafayett sexyso true blood convert"
  },
  {
    "label":4,
    "text":"awwwww good luck babe youll great like alway xx lt",
    "cleaned_text":"awwwww good luck babe youll great like alway xx lt",
    "normalized_text":"awwwww good luck babe youll great like alway xx lt",
    "tokens":[
      "awwwww",
      "good",
      "luck",
      "babe",
      "youll",
      "great",
      "like",
      "alway",
      "xx",
      "lt"
    ],
    "token_count":10,
    "processed_text":"awwwww good luck babe youll great like alway xx lt"
  },
  {
    "label":4,
    "text":"know nearli fell backward stair anyth event happen day dearest",
    "cleaned_text":"know nearli fell backward stair anyth event happen day dearest",
    "normalized_text":"know nearli fell backward stair anyth event happen day dearest",
    "tokens":[
      "know",
      "nearli",
      "fell",
      "backward",
      "stair",
      "anyth",
      "event",
      "happen",
      "day",
      "dearest"
    ],
    "token_count":10,
    "processed_text":"know nearli fell backward stair anyth event happen day dearest"
  },
  {
    "label":0,
    "text":"grrrrr urgent piec work end april get feedback",
    "cleaned_text":"grrrrr urgent piec work end april get feedback",
    "normalized_text":"grrrrr urgent piec work end april get feedback",
    "tokens":[
      "grrrrr",
      "urgent",
      "piec",
      "work",
      "end",
      "april",
      "get",
      "feedback"
    ],
    "token_count":8,
    "processed_text":"grrrrr urgent piec work end april get feedback"
  },
  {
    "label":4,
    "text":"amaz sometim stranger reach amp touch heart ltcant wait mommi",
    "cleaned_text":"amaz sometim stranger reach amp touch heart ltcant wait mommi",
    "normalized_text":"amaz sometim stranger reach amp touch heart ltcant wait mommi",
    "tokens":[
      "amaz",
      "sometim",
      "stranger",
      "reach",
      "amp",
      "touch",
      "heart",
      "ltcant",
      "wait",
      "mommi"
    ],
    "token_count":10,
    "processed_text":"amaz sometim stranger reach amp touch heart ltcant wait mommi"
  },
  {
    "label":0,
    "text":"note buy run shoe need massag bad",
    "cleaned_text":"note buy run shoe need massag bad",
    "normalized_text":"note buy run shoe need massag bad",
    "tokens":[
      "note",
      "buy",
      "run",
      "shoe",
      "need",
      "massag",
      "bad"
    ],
    "token_count":7,
    "processed_text":"note buy run shoe need massag bad"
  },
  {
    "label":4,
    "text":"look ur updatesquit thought one",
    "cleaned_text":"look ur updatesquit thought one",
    "normalized_text":"look ur updatesquit thought one",
    "tokens":[
      "look",
      "ur",
      "updatesquit",
      "thought",
      "one"
    ],
    "token_count":5,
    "processed_text":"look ur updatesquit thought one"
  },
  {
    "label":0,
    "text":"busi day time make dinner put kid bed need wok app fall class",
    "cleaned_text":"busi day time make dinner put kid bed need wok app fall class",
    "normalized_text":"busi day time make dinner put kid bed need wok app fall class",
    "tokens":[
      "busi",
      "day",
      "time",
      "make",
      "dinner",
      "put",
      "kid",
      "bed",
      "need",
      "wok",
      "app",
      "fall",
      "class"
    ],
    "token_count":13,
    "processed_text":"busi day time make dinner put kid bed need wok app fall class"
  },
  {
    "label":4,
    "text":"final done lab cannot put till last minut next wknd sasquatch amp nin",
    "cleaned_text":"final done lab cannot put till last minut next wknd sasquatch amp nin",
    "normalized_text":"final done lab cannot put till last minut next wknd sasquatch amp nin",
    "tokens":[
      "final",
      "done",
      "lab",
      "put",
      "till",
      "last",
      "minut",
      "next",
      "wknd",
      "sasquatch",
      "amp",
      "nin"
    ],
    "token_count":12,
    "processed_text":"final done lab put till last minut next wknd sasquatch amp nin"
  },
  {
    "label":0,
    "text":"think im gonna cri page hate teacher sooooooooooooo much",
    "cleaned_text":"think im gonna cri page hate teacher sooooooooooooo much",
    "normalized_text":"think im gonna cri page hate teacher sooooooooooooo much",
    "tokens":[
      "think",
      "im",
      "gon",
      "na",
      "cri",
      "page",
      "hate",
      "teacher",
      "sooooooooooooo",
      "much"
    ],
    "token_count":10,
    "processed_text":"think im gon na cri page hate teacher sooooooooooooo much"
  },
  {
    "label":4,
    "text":"aricella leah say bore tha phone kumpettetryna find somethin parti hit ttli",
    "cleaned_text":"aricella leah say bore tha phone kumpettetryna find somethin parti hit ttli",
    "normalized_text":"aricella leah say bore tha phone kumpettetryna find somethin parti hit ttli",
    "tokens":[
      "aricella",
      "leah",
      "say",
      "bore",
      "tha",
      "phone",
      "kumpettetryna",
      "find",
      "somethin",
      "parti",
      "hit",
      "ttli"
    ],
    "token_count":12,
    "processed_text":"aricella leah say bore tha phone kumpettetryna find somethin parti hit ttli"
  },
  {
    "label":4,
    "text":"mhm gw favorit",
    "cleaned_text":"mhm gw favorit",
    "normalized_text":"mhm gw favorit",
    "tokens":[
      "mhm",
      "gw",
      "favorit"
    ],
    "token_count":3,
    "processed_text":"mhm gw favorit"
  },
  {
    "label":0,
    "text":"wish still get point get level",
    "cleaned_text":"wish still get point get level",
    "normalized_text":"wish still get point get level",
    "tokens":[
      "wish",
      "still",
      "get",
      "point",
      "get",
      "level"
    ],
    "token_count":6,
    "processed_text":"wish still get point get level"
  },
  {
    "label":0,
    "text":"feel uneasi unknown reason",
    "cleaned_text":"feel uneasi unknown reason",
    "normalized_text":"feel uneasi unknown reason",
    "tokens":[
      "feel",
      "uneasi",
      "unknown",
      "reason"
    ],
    "token_count":4,
    "processed_text":"feel uneasi unknown reason"
  },
  {
    "label":4,
    "text":"looool hey bb im good havent soooo long how go u",
    "cleaned_text":"looool hey bb im good havent soooo long how go u",
    "normalized_text":"looool hey bb im good havent soooo long how go u",
    "tokens":[
      "looool",
      "hey",
      "bb",
      "im",
      "good",
      "havent",
      "soooo",
      "long",
      "go"
    ],
    "token_count":9,
    "processed_text":"looool hey bb im good havent soooo long go"
  },
  {
    "label":0,
    "text":"kill sunflow work look sad almost buri",
    "cleaned_text":"kill sunflow work look sad almost buri",
    "normalized_text":"kill sunflow work look sad almost buri",
    "tokens":[
      "kill",
      "sunflow",
      "work",
      "look",
      "sad",
      "almost",
      "buri"
    ],
    "token_count":7,
    "processed_text":"kill sunflow work look sad almost buri"
  },
  {
    "label":0,
    "text":"oh daughter sign im alon",
    "cleaned_text":"oh daughter sign im alon",
    "normalized_text":"oh daughter sign im alon",
    "tokens":[
      "oh",
      "daughter",
      "sign",
      "im",
      "alon"
    ],
    "token_count":5,
    "processed_text":"oh daughter sign im alon"
  },
  {
    "label":4,
    "text":"awesom day got bampb pictur look awesom got promis back track figur tri",
    "cleaned_text":"awesom day got bampb pictur look awesom got promis back track figur tri",
    "normalized_text":"awesom day got bampb pictur look awesom got promis back track figur tri",
    "tokens":[
      "awesom",
      "day",
      "got",
      "bampb",
      "pictur",
      "look",
      "awesom",
      "got",
      "promi",
      "back",
      "track",
      "figur",
      "tri"
    ],
    "token_count":13,
    "processed_text":"awesom day got bampb pictur look awesom got promi back track figur tri"
  },
  {
    "label":4,
    "text":"suck wang wimp where parti huh",
    "cleaned_text":"suck wang wimp where parti huh",
    "normalized_text":"suck wang wimp where parti huh",
    "tokens":[
      "suck",
      "wang",
      "wimp",
      "parti",
      "huh"
    ],
    "token_count":5,
    "processed_text":"suck wang wimp parti huh"
  },
  {
    "label":0,
    "text":"would come skype im friend hous doesnt cant",
    "cleaned_text":"would come skype im friend hous doesnt cant",
    "normalized_text":"would come skype im friend hous doesnt cant",
    "tokens":[
      "come",
      "skype",
      "im",
      "friend",
      "hou",
      "doesnt",
      "cant"
    ],
    "token_count":7,
    "processed_text":"come skype im friend hou doesnt cant"
  },
  {
    "label":0,
    "text":"mmm food that better hour pool home lunch until nearli oclock",
    "cleaned_text":"mmm food that better hour pool home lunch until nearli oclock",
    "normalized_text":"mmm food that better hour pool home lunch until nearli oclock",
    "tokens":[
      "mmm",
      "food",
      "better",
      "hour",
      "pool",
      "home",
      "lunch",
      "nearli",
      "oclock"
    ],
    "token_count":9,
    "processed_text":"mmm food better hour pool home lunch nearli oclock"
  },
  {
    "label":0,
    "text":"ugh today go bad",
    "cleaned_text":"ugh today go bad",
    "normalized_text":"ugh today go bad",
    "tokens":[
      "ugh",
      "today",
      "go",
      "bad"
    ],
    "token_count":4,
    "processed_text":"ugh today go bad"
  },
  {
    "label":0,
    "text":"ice coffe",
    "cleaned_text":"ice coffe",
    "normalized_text":"ice coffe",
    "tokens":[
      "ice",
      "coff"
    ],
    "token_count":2,
    "processed_text":"ice coff"
  },
  {
    "label":4,
    "text":"ye second comic much",
    "cleaned_text":"ye second comic much",
    "normalized_text":"ye second comic much",
    "tokens":[
      "ye",
      "second",
      "comic",
      "much"
    ],
    "token_count":4,
    "processed_text":"ye second comic much"
  },
  {
    "label":0,
    "text":"amaz weekend miss much",
    "cleaned_text":"amaz weekend miss much",
    "normalized_text":"amaz weekend miss much",
    "tokens":[
      "amaz",
      "weekend",
      "miss",
      "much"
    ],
    "token_count":4,
    "processed_text":"amaz weekend miss much"
  },
  {
    "label":4,
    "text":"night saturday noth els watch wcg ultim gamer usa network start weekend",
    "cleaned_text":"night saturday noth els watch wcg ultim gamer usa network start weekend",
    "normalized_text":"night saturday noth els watch wcg ultim gamer usa network start weekend",
    "tokens":[
      "night",
      "saturday",
      "noth",
      "el",
      "watch",
      "wcg",
      "ultim",
      "gamer",
      "usa",
      "network",
      "start",
      "weekend"
    ],
    "token_count":12,
    "processed_text":"night saturday noth el watch wcg ultim gamer usa network start weekend"
  },
  {
    "label":4,
    "text":"happi found warren elli twitter one els get away say good morn sinner quit like",
    "cleaned_text":"happi found warren elli twitter one els get away say good morn sinner quit like",
    "normalized_text":"happi found warren elli twitter one els get away say good morn sinner quit like",
    "tokens":[
      "happi",
      "found",
      "warren",
      "elli",
      "twitter",
      "one",
      "el",
      "get",
      "away",
      "say",
      "good",
      "morn",
      "sinner",
      "quit",
      "like"
    ],
    "token_count":15,
    "processed_text":"happi found warren elli twitter one el get away say good morn sinner quit like"
  },
  {
    "label":0,
    "text":"one word footi let know later doctor hospit may broken wrist",
    "cleaned_text":"one word footi let know later doctor hospit may broken wrist",
    "normalized_text":"one word footi let know later doctor hospit may broken wrist",
    "tokens":[
      "one",
      "word",
      "footi",
      "let",
      "know",
      "later",
      "doctor",
      "hospit",
      "may",
      "broken",
      "wrist"
    ],
    "token_count":11,
    "processed_text":"one word footi let know later doctor hospit may broken wrist"
  },
  {
    "label":0,
    "text":"stuck bethnal green liverpool street age best place scorch day",
    "cleaned_text":"stuck bethnal green liverpool street age best place scorch day",
    "normalized_text":"stuck bethnal green liverpool street age best place scorch day",
    "tokens":[
      "stuck",
      "bethnal",
      "green",
      "liverpool",
      "street",
      "age",
      "best",
      "place",
      "scorch",
      "day"
    ],
    "token_count":10,
    "processed_text":"stuck bethnal green liverpool street age best place scorch day"
  },
  {
    "label":4,
    "text":"gm coffe turn",
    "cleaned_text":"gm coffe turn",
    "normalized_text":"gm coffe turn",
    "tokens":[
      "gm",
      "coff",
      "turn"
    ],
    "token_count":3,
    "processed_text":"gm coff turn"
  },
  {
    "label":0,
    "text":"wish warp tour australia instead drink listen ipod",
    "cleaned_text":"wish warp tour australia instead drink listen ipod",
    "normalized_text":"wish warp tour australia instead drink listen ipod",
    "tokens":[
      "wish",
      "warp",
      "tour",
      "australia",
      "instead",
      "drink",
      "listen",
      "ipod"
    ],
    "token_count":8,
    "processed_text":"wish warp tour australia instead drink listen ipod"
  },
  {
    "label":4,
    "text":"wowhungarynic know u",
    "cleaned_text":"wowhungarynic know u",
    "normalized_text":"wowhungarynic know u",
    "tokens":[
      "wowhungaryn",
      "know"
    ],
    "token_count":2,
    "processed_text":"wowhungaryn know"
  },
  {
    "label":4,
    "text":"happyexperi design present happi let happi flow throughout day night",
    "cleaned_text":"happyexperi design present happi let happi flow throughout day night",
    "normalized_text":"happyexperi design present happi let happi flow throughout day night",
    "tokens":[
      "happyexperi",
      "design",
      "present",
      "happi",
      "let",
      "happi",
      "flow",
      "throughout",
      "day",
      "night"
    ],
    "token_count":10,
    "processed_text":"happyexperi design present happi let happi flow throughout day night"
  },
  {
    "label":4,
    "text":"young leo dicaprio soooo dreami",
    "cleaned_text":"young leo dicaprio soooo dreami",
    "normalized_text":"young leo dicaprio soooo dreami",
    "tokens":[
      "young",
      "leo",
      "dicaprio",
      "soooo",
      "dreami"
    ],
    "token_count":5,
    "processed_text":"young leo dicaprio soooo dreami"
  },
  {
    "label":4,
    "text":"new arriv",
    "cleaned_text":"new arriv",
    "normalized_text":"new arriv",
    "tokens":[
      "new",
      "arriv"
    ],
    "token_count":2,
    "processed_text":"new arriv"
  },
  {
    "label":0,
    "text":"last year sad day",
    "cleaned_text":"last year sad day",
    "normalized_text":"last year sad day",
    "tokens":[
      "last",
      "year",
      "sad",
      "day"
    ],
    "token_count":4,
    "processed_text":"last year sad day"
  },
  {
    "label":0,
    "text":"britain got talent susanboyl finish second shell still great career",
    "cleaned_text":"britain got talent susanboyl finish second shell still great career",
    "normalized_text":"britain got talent susanboyl finish second shell still great career",
    "tokens":[
      "britain",
      "got",
      "talent",
      "susanboyl",
      "finish",
      "second",
      "shell",
      "still",
      "great",
      "career"
    ],
    "token_count":10,
    "processed_text":"britain got talent susanboyl finish second shell still great career"
  },
  {
    "label":0,
    "text":"couldnt finish",
    "cleaned_text":"couldnt finish",
    "normalized_text":"couldnt finish",
    "tokens":[
      "couldnt",
      "finish"
    ],
    "token_count":2,
    "processed_text":"couldnt finish"
  },
  {
    "label":4,
    "text":"sierra comment third time she awesom",
    "cleaned_text":"sierra comment third time she awesom",
    "normalized_text":"sierra comment third time she awesom",
    "tokens":[
      "sierra",
      "comment",
      "third",
      "time",
      "awesom"
    ],
    "token_count":5,
    "processed_text":"sierra comment third time awesom"
  },
  {
    "label":4,
    "text":"awesom quotopenli gay teen vote prom queen la high schoolquot",
    "cleaned_text":"awesom quotopenli gay teen vote prom queen la high schoolquot",
    "normalized_text":"awesom quotopenli gay teen vote prom queen la high schoolquot",
    "tokens":[
      "awesom",
      "quotopenli",
      "gay",
      "teen",
      "vote",
      "prom",
      "queen",
      "la",
      "high",
      "schoolquot"
    ],
    "token_count":10,
    "processed_text":"awesom quotopenli gay teen vote prom queen la high schoolquot"
  },
  {
    "label":0,
    "text":"brother wont stop throw",
    "cleaned_text":"brother wont stop throw",
    "normalized_text":"brother wont stop throw",
    "tokens":[
      "brother",
      "wont",
      "stop",
      "throw"
    ],
    "token_count":4,
    "processed_text":"brother wont stop throw"
  },
  {
    "label":4,
    "text":"feel im go feel amazingggg thursday",
    "cleaned_text":"feel im go feel amazingggg thursday",
    "normalized_text":"feel im go feel amazingggg thursday",
    "tokens":[
      "feel",
      "im",
      "go",
      "feel",
      "amazingggg",
      "thursday"
    ],
    "token_count":6,
    "processed_text":"feel im go feel amazingggg thursday"
  },
  {
    "label":4,
    "text":"thank ff dont realli get work yet still thank",
    "cleaned_text":"thank ff dont realli get work yet still thank",
    "normalized_text":"thank ff dont realli get work yet still thank",
    "tokens":[
      "thank",
      "ff",
      "dont",
      "realli",
      "get",
      "work",
      "yet",
      "still",
      "thank"
    ],
    "token_count":9,
    "processed_text":"thank ff dont realli get work yet still thank"
  },
  {
    "label":0,
    "text":"go swim anyon wanna join",
    "cleaned_text":"go swim anyon wanna join",
    "normalized_text":"go swim anyon wanna join",
    "tokens":[
      "go",
      "swim",
      "anyon",
      "wan",
      "na",
      "join"
    ],
    "token_count":6,
    "processed_text":"go swim anyon wan na join"
  },
  {
    "label":4,
    "text":"bbq hope great saturday",
    "cleaned_text":"bbq hope great saturday",
    "normalized_text":"bbq hope great saturday",
    "tokens":[
      "bbq",
      "hope",
      "great",
      "saturday"
    ],
    "token_count":4,
    "processed_text":"bbq hope great saturday"
  },
  {
    "label":4,
    "text":"sent first articl miss",
    "cleaned_text":"sent first articl miss",
    "normalized_text":"sent first articl miss",
    "tokens":[
      "sent",
      "first",
      "articl",
      "miss"
    ],
    "token_count":4,
    "processed_text":"sent first articl miss"
  },
  {
    "label":4,
    "text":"want la la land mtl suck right",
    "cleaned_text":"want la la land mtl suck right",
    "normalized_text":"want la la land mtl suck right",
    "tokens":[
      "want",
      "la",
      "la",
      "land",
      "mtl",
      "suck",
      "right"
    ],
    "token_count":7,
    "processed_text":"want la la land mtl suck right"
  },
  {
    "label":0,
    "text":"that feel like injur hamstr least got cross thatll learn warm properli",
    "cleaned_text":"that feel like injur hamstr least got cross thatll learn warm properli",
    "normalized_text":"that feel like injur hamstr least got cross thatll learn warm properli",
    "tokens":[
      "feel",
      "like",
      "injur",
      "hamstr",
      "least",
      "got",
      "cross",
      "thatll",
      "learn",
      "warm",
      "properli"
    ],
    "token_count":11,
    "processed_text":"feel like injur hamstr least got cross thatll learn warm properli"
  },
  {
    "label":4,
    "text":"thank dawn arznova",
    "cleaned_text":"thank dawn arznova",
    "normalized_text":"thank dawn arznova",
    "tokens":[
      "thank",
      "dawn",
      "arznova"
    ],
    "token_count":3,
    "processed_text":"thank dawn arznova"
  },
  {
    "label":0,
    "text":"lol help",
    "cleaned_text":"lol help",
    "normalized_text":"lol help",
    "tokens":[
      "lol",
      "help"
    ],
    "token_count":2,
    "processed_text":"lol help"
  },
  {
    "label":0,
    "text":"well hafta soon",
    "cleaned_text":"well hafta soon",
    "normalized_text":"well hafta soon",
    "tokens":[
      "well",
      "hafta",
      "soon"
    ],
    "token_count":3,
    "processed_text":"well hafta soon"
  },
  {
    "label":0,
    "text":"love fuckin bullshit swear thing peopl make shit",
    "cleaned_text":"love fuckin bullshit swear thing peopl make shit",
    "normalized_text":"love fuckin bullshit swear thing peopl make shit",
    "tokens":[
      "love",
      "fuckin",
      "bullshit",
      "swear",
      "thing",
      "peopl",
      "make",
      "shit"
    ],
    "token_count":8,
    "processed_text":"love fuckin bullshit swear thing peopl make shit"
  },
  {
    "label":0,
    "text":"think im gonna tri get look wavi vacat",
    "cleaned_text":"think im gonna tri get look wavi vacat",
    "normalized_text":"think im gonna tri get look wavi vacat",
    "tokens":[
      "think",
      "im",
      "gon",
      "na",
      "tri",
      "get",
      "look",
      "wavi",
      "vacat"
    ],
    "token_count":9,
    "processed_text":"think im gon na tri get look wavi vacat"
  },
  {
    "label":4,
    "text":"im workin itim transit super broke student life full fledg paid career life ill catch soon",
    "cleaned_text":"im workin itim transit super broke student life full fledg paid career life ill catch soon",
    "normalized_text":"im workin itim transit super broke student life full fledg paid career life ill catch soon",
    "tokens":[
      "im",
      "workin",
      "itim",
      "transit",
      "super",
      "broke",
      "student",
      "life",
      "full",
      "fledg",
      "paid",
      "career",
      "life",
      "ill",
      "catch",
      "soon"
    ],
    "token_count":16,
    "processed_text":"im workin itim transit super broke student life full fledg paid career life ill catch soon"
  },
  {
    "label":0,
    "text":"alcioneg love amaz weather great food greatbut cuti",
    "cleaned_text":"alcioneg love amaz weather great food greatbut cuti",
    "normalized_text":"alcioneg love amaz weather great food greatbut cuti",
    "tokens":[
      "alcioneg",
      "love",
      "amaz",
      "weather",
      "great",
      "food",
      "greatbut",
      "cuti"
    ],
    "token_count":8,
    "processed_text":"alcioneg love amaz weather great food greatbut cuti"
  },
  {
    "label":4,
    "text":"hush",
    "cleaned_text":"hush",
    "normalized_text":"hush",
    "tokens":[
      "hush"
    ],
    "token_count":1,
    "processed_text":"hush"
  },
  {
    "label":4,
    "text":"ipod touch user test new app pl tk",
    "cleaned_text":"ipod touch user test new app pl tk",
    "normalized_text":"ipod touch user test new app pl tk",
    "tokens":[
      "ipod",
      "touch",
      "user",
      "test",
      "new",
      "app",
      "pl",
      "tk"
    ],
    "token_count":8,
    "processed_text":"ipod touch user test new app pl tk"
  },
  {
    "label":0,
    "text":"time next mtng",
    "cleaned_text":"time next mtng",
    "normalized_text":"time next mtng",
    "tokens":[
      "time",
      "next",
      "mtng"
    ],
    "token_count":3,
    "processed_text":"time next mtng"
  },
  {
    "label":0,
    "text":"yay he bit mess",
    "cleaned_text":"yay he bit mess",
    "normalized_text":"yay he bit mess",
    "tokens":[
      "yay",
      "bit",
      "mess"
    ],
    "token_count":3,
    "processed_text":"yay bit mess"
  },
  {
    "label":0,
    "text":"internet connect experienc problem withdrawsc get facebook",
    "cleaned_text":"internet connect experienc problem withdrawsc get facebook",
    "normalized_text":"internet connect experienc problem withdrawsc get facebook",
    "tokens":[
      "internet",
      "connect",
      "experienc",
      "problem",
      "withdrawsc",
      "get",
      "facebook"
    ],
    "token_count":7,
    "processed_text":"internet connect experienc problem withdrawsc get facebook"
  },
  {
    "label":0,
    "text":"miss ami amp scooti cri west palm beach fl",
    "cleaned_text":"miss ami amp scooti cri west palm beach fl",
    "normalized_text":"miss ami amp scooti cri west palm beach fl",
    "tokens":[
      "miss",
      "ami",
      "amp",
      "scooti",
      "cri",
      "west",
      "palm",
      "beach",
      "fl"
    ],
    "token_count":9,
    "processed_text":"miss ami amp scooti cri west palm beach fl"
  },
  {
    "label":4,
    "text":"hey follow twitter jail account chuckmemonday",
    "cleaned_text":"hey follow twitter jail account chuckmemonday",
    "normalized_text":"hey follow twitter jail account chuckmemonday",
    "tokens":[
      "hey",
      "follow",
      "twitter",
      "jail",
      "account",
      "chuckmemonday"
    ],
    "token_count":6,
    "processed_text":"hey follow twitter jail account chuckmemonday"
  },
  {
    "label":0,
    "text":"right field roof bar opposit side",
    "cleaned_text":"right field roof bar opposit side",
    "normalized_text":"right field roof bar opposit side",
    "tokens":[
      "right",
      "field",
      "roof",
      "bar",
      "opposit",
      "side"
    ],
    "token_count":6,
    "processed_text":"right field roof bar opposit side"
  },
  {
    "label":0,
    "text":"raini day",
    "cleaned_text":"raini day",
    "normalized_text":"raini day",
    "tokens":[
      "raini",
      "day"
    ],
    "token_count":2,
    "processed_text":"raini day"
  },
  {
    "label":4,
    "text":"ye life would definit fit charact lol",
    "cleaned_text":"ye life would definit fit charact lol",
    "normalized_text":"ye life would definit fit charact lol",
    "tokens":[
      "ye",
      "life",
      "definit",
      "fit",
      "charact",
      "lol"
    ],
    "token_count":6,
    "processed_text":"ye life definit fit charact lol"
  },
  {
    "label":4,
    "text":"that lot peopl question anoth one",
    "cleaned_text":"that lot peopl question anoth one",
    "normalized_text":"that lot peopl question anoth one",
    "tokens":[
      "lot",
      "peopl",
      "question",
      "anoth",
      "one"
    ],
    "token_count":5,
    "processed_text":"lot peopl question anoth one"
  },
  {
    "label":0,
    "text":"cant sleep button damn thing",
    "cleaned_text":"cant sleep button damn thing",
    "normalized_text":"cant sleep button damn thing",
    "tokens":[
      "cant",
      "sleep",
      "button",
      "damn",
      "thing"
    ],
    "token_count":5,
    "processed_text":"cant sleep button damn thing"
  },
  {
    "label":0,
    "text":"nooo meet huhhh",
    "cleaned_text":"nooo meet huhhh",
    "normalized_text":"nooo meet huhhh",
    "tokens":[
      "nooo",
      "meet",
      "huhhh"
    ],
    "token_count":3,
    "processed_text":"nooo meet huhhh"
  },
  {
    "label":0,
    "text":"thinkinglast time back flat back coupl day welcom late suck",
    "cleaned_text":"thinkinglast time back flat back coupl day welcom late suck",
    "normalized_text":"thinkinglast time back flat back coupl day welcom late suck",
    "tokens":[
      "thinkinglast",
      "time",
      "back",
      "flat",
      "back",
      "coupl",
      "day",
      "welcom",
      "late",
      "suck"
    ],
    "token_count":10,
    "processed_text":"thinkinglast time back flat back coupl day welcom late suck"
  },
  {
    "label":0,
    "text":"ye deposit go",
    "cleaned_text":"ye deposit go",
    "normalized_text":"ye deposit go",
    "tokens":[
      "ye",
      "deposit",
      "go"
    ],
    "token_count":3,
    "processed_text":"ye deposit go"
  },
  {
    "label":4,
    "text":"wow ortho shame",
    "cleaned_text":"wow ortho shame",
    "normalized_text":"wow ortho shame",
    "tokens":[
      "wow",
      "ortho",
      "shame"
    ],
    "token_count":3,
    "processed_text":"wow ortho shame"
  },
  {
    "label":0,
    "text":"davepecken never receiv dm last night twitter hack lung someth",
    "cleaned_text":"davepecken never receiv dm last night twitter hack lung someth",
    "normalized_text":"davepecken never receiv dm last night twitter hack lung someth",
    "tokens":[
      "davepecken",
      "never",
      "receiv",
      "dm",
      "last",
      "night",
      "twitter",
      "hack",
      "lung",
      "someth"
    ],
    "token_count":10,
    "processed_text":"davepecken never receiv dm last night twitter hack lung someth"
  },
  {
    "label":0,
    "text":"feel way go build ikea stuff",
    "cleaned_text":"feel way go build ikea stuff",
    "normalized_text":"feel way go build ikea stuff",
    "tokens":[
      "feel",
      "way",
      "go",
      "build",
      "ikea",
      "stuff"
    ],
    "token_count":6,
    "processed_text":"feel way go build ikea stuff"
  },
  {
    "label":4,
    "text":"new twitter account pleas follow wwwtwittercombeckymaria thank",
    "cleaned_text":"new twitter account pleas follow wwwtwittercombeckymaria thank",
    "normalized_text":"new twitter account pleas follow wwwtwittercombeckymaria thank",
    "tokens":[
      "new",
      "twitter",
      "account",
      "plea",
      "follow",
      "thank"
    ],
    "token_count":6,
    "processed_text":"new twitter account plea follow thank"
  },
  {
    "label":0,
    "text":"hate im gonna emo day",
    "cleaned_text":"hate im gonna emo day",
    "normalized_text":"hate im gonna emo day",
    "tokens":[
      "hate",
      "im",
      "gon",
      "na",
      "emo",
      "day"
    ],
    "token_count":6,
    "processed_text":"hate im gon na emo day"
  },
  {
    "label":0,
    "text":"ive got charg laptop amp stolen internet connect world oyster except coffe includ oyster im sad",
    "cleaned_text":"ive got charg laptop amp stolen internet connect world oyster except coffe includ oyster im sad",
    "normalized_text":"ive got charg laptop amp stolen internet connect world oyster except coffe includ oyster im sad",
    "tokens":[
      "ive",
      "got",
      "charg",
      "laptop",
      "amp",
      "stolen",
      "internet",
      "connect",
      "world",
      "oyster",
      "except",
      "coff",
      "includ",
      "oyster",
      "im",
      "sad"
    ],
    "token_count":16,
    "processed_text":"ive got charg laptop amp stolen internet connect world oyster except coff includ oyster im sad"
  },
  {
    "label":0,
    "text":"alway fuck thing everyth good dont know appreci good thing anymor",
    "cleaned_text":"alway fuck thing everyth good dont know appreci good thing anymor",
    "normalized_text":"alway fuck thing everyth good dont know appreci good thing anymor",
    "tokens":[
      "alway",
      "fuck",
      "thing",
      "everyth",
      "good",
      "dont",
      "know",
      "appreci",
      "good",
      "thing",
      "anymor"
    ],
    "token_count":11,
    "processed_text":"alway fuck thing everyth good dont know appreci good thing anymor"
  },
  {
    "label":4,
    "text":"danggg look cover old one ur nvr gna open huh u shud frame hahah",
    "cleaned_text":"danggg look cover old one ur nvr gna open huh u shud frame hahah",
    "normalized_text":"danggg look cover old one ur nvr gna open huh u shud frame hahah",
    "tokens":[
      "danggg",
      "look",
      "cover",
      "old",
      "one",
      "ur",
      "nvr",
      "gna",
      "open",
      "huh",
      "shud",
      "frame",
      "hahah"
    ],
    "token_count":13,
    "processed_text":"danggg look cover old one ur nvr gna open huh shud frame hahah"
  },
  {
    "label":4,
    "text":"weekend begin",
    "cleaned_text":"weekend begin",
    "normalized_text":"weekend begin",
    "tokens":[
      "weekend",
      "begin"
    ],
    "token_count":2,
    "processed_text":"weekend begin"
  },
  {
    "label":4,
    "text":"ill pour wine rock wall open hous next week cant wait one one one one",
    "cleaned_text":"ill pour wine rock wall open hous next week cant wait one one one one",
    "normalized_text":"ill pour wine rock wall open hous next week cant wait one one one one",
    "tokens":[
      "ill",
      "pour",
      "wine",
      "rock",
      "wall",
      "open",
      "hou",
      "next",
      "week",
      "cant",
      "wait",
      "one",
      "one",
      "one",
      "one"
    ],
    "token_count":15,
    "processed_text":"ill pour wine rock wall open hou next week cant wait one one one one"
  },
  {
    "label":4,
    "text":"chanc hear song day boker show great show",
    "cleaned_text":"chanc hear song day boker show great show",
    "normalized_text":"chanc hear song day boker show great show",
    "tokens":[
      "chanc",
      "hear",
      "song",
      "day",
      "boker",
      "show",
      "great",
      "show"
    ],
    "token_count":8,
    "processed_text":"chanc hear song day boker show great show"
  },
  {
    "label":4,
    "text":"like start delet key",
    "cleaned_text":"like start delet key",
    "normalized_text":"like start delet key",
    "tokens":[
      "like",
      "start",
      "delet",
      "key"
    ],
    "token_count":4,
    "processed_text":"like start delet key"
  },
  {
    "label":0,
    "text":"wanna go bed finish stupid scienc lab",
    "cleaned_text":"wanna go bed finish stupid scienc lab",
    "normalized_text":"wanna go bed finish stupid scienc lab",
    "tokens":[
      "wan",
      "na",
      "go",
      "bed",
      "finish",
      "stupid",
      "scienc",
      "lab"
    ],
    "token_count":8,
    "processed_text":"wan na go bed finish stupid scienc lab"
  },
  {
    "label":0,
    "text":"ive still slept",
    "cleaned_text":"ive still slept",
    "normalized_text":"ive still slept",
    "tokens":[
      "ive",
      "still",
      "slept"
    ],
    "token_count":3,
    "processed_text":"ive still slept"
  },
  {
    "label":0,
    "text":"class tom im tamad want go tom afternoon",
    "cleaned_text":"class tom im tamad want go tom afternoon",
    "normalized_text":"class tom im tamad want go tom afternoon",
    "tokens":[
      "class",
      "tom",
      "im",
      "tamad",
      "want",
      "go",
      "tom",
      "afternoon"
    ],
    "token_count":8,
    "processed_text":"class tom im tamad want go tom afternoon"
  },
  {
    "label":4,
    "text":"got new york get readi shop",
    "cleaned_text":"got new york get readi shop",
    "normalized_text":"got new york get readi shop",
    "tokens":[
      "got",
      "new",
      "york",
      "get",
      "readi",
      "shop"
    ],
    "token_count":6,
    "processed_text":"got new york get readi shop"
  },
  {
    "label":0,
    "text":"omg sister slick molest wtf",
    "cleaned_text":"omg sister slick molest wtf",
    "normalized_text":"omg sister slick molest wtf",
    "tokens":[
      "omg",
      "sister",
      "slick",
      "molest",
      "wtf"
    ],
    "token_count":5,
    "processed_text":"omg sister slick molest wtf"
  },
  {
    "label":4,
    "text":"would amaz plu like month summer holiday year want decent weather",
    "cleaned_text":"would amaz plu like month summer holiday year want decent weather",
    "normalized_text":"would amaz plu like month summer holiday year want decent weather",
    "tokens":[
      "amaz",
      "plu",
      "like",
      "month",
      "summer",
      "holiday",
      "year",
      "want",
      "decent",
      "weather"
    ],
    "token_count":10,
    "processed_text":"amaz plu like month summer holiday year want decent weather"
  },
  {
    "label":4,
    "text":"awesom good soo looong",
    "cleaned_text":"awesom good soo looong",
    "normalized_text":"awesom good soo looong",
    "tokens":[
      "awesom",
      "good",
      "soo",
      "looong"
    ],
    "token_count":4,
    "processed_text":"awesom good soo looong"
  },
  {
    "label":0,
    "text":"irl sorri go ill tri come later sorri",
    "cleaned_text":"irl sorri go ill tri come later sorri",
    "normalized_text":"irl sorri go ill tri come later sorri",
    "tokens":[
      "irl",
      "sorri",
      "go",
      "ill",
      "tri",
      "come",
      "later",
      "sorri"
    ],
    "token_count":8,
    "processed_text":"irl sorri go ill tri come later sorri"
  },
  {
    "label":0,
    "text":"three day three week",
    "cleaned_text":"three day three week",
    "normalized_text":"three day three week",
    "tokens":[
      "three",
      "day",
      "three",
      "week"
    ],
    "token_count":4,
    "processed_text":"three day three week"
  },
  {
    "label":0,
    "text":"madd nasti outsid ewww hate havin put hair ponytail realli choic",
    "cleaned_text":"madd nasti outsid ewww hate havin put hair ponytail realli choic",
    "normalized_text":"madd nasti outsid ewww hate havin put hair ponytail realli choic",
    "tokens":[
      "madd",
      "nasti",
      "outsid",
      "ewww",
      "hate",
      "havin",
      "put",
      "hair",
      "ponytail",
      "realli",
      "choic"
    ],
    "token_count":11,
    "processed_text":"madd nasti outsid ewww hate havin put hair ponytail realli choic"
  },
  {
    "label":0,
    "text":"whhhaaat tre jealou mine doesnt go th found chocol much better thank x",
    "cleaned_text":"whhhaaat tre jealou mine doesnt go th found chocol much better thank x",
    "normalized_text":"whhhaaat tre jealou mine doesnt go th found chocol much better thank x",
    "tokens":[
      "whhhaaat",
      "tre",
      "jealou",
      "mine",
      "doesnt",
      "go",
      "th",
      "found",
      "chocol",
      "much",
      "better",
      "thank"
    ],
    "token_count":12,
    "processed_text":"whhhaaat tre jealou mine doesnt go th found chocol much better thank"
  },
  {
    "label":0,
    "text":"alot friend stab dont get reason",
    "cleaned_text":"alot friend stab dont get reason",
    "normalized_text":"alot friend stab dont get reason",
    "tokens":[
      "alot",
      "friend",
      "stab",
      "dont",
      "get",
      "reason"
    ],
    "token_count":6,
    "processed_text":"alot friend stab dont get reason"
  },
  {
    "label":0,
    "text":"wish tshert n saudi arabia",
    "cleaned_text":"wish tshert n saudi arabia",
    "normalized_text":"wish tshert n saudi arabia",
    "tokens":[
      "wish",
      "tshert",
      "saudi",
      "arabia"
    ],
    "token_count":4,
    "processed_text":"wish tshert saudi arabia"
  },
  {
    "label":0,
    "text":"want shamwow",
    "cleaned_text":"want shamwow",
    "normalized_text":"want shamwow",
    "tokens":[
      "want",
      "shamwow"
    ],
    "token_count":2,
    "processed_text":"want shamwow"
  },
  {
    "label":0,
    "text":"felt sad dc wini hate see prieti zinta win",
    "cleaned_text":"felt sad dc wini hate see prieti zinta win",
    "normalized_text":"felt sad dc wini hate see prieti zinta win",
    "tokens":[
      "felt",
      "sad",
      "dc",
      "wini",
      "hate",
      "see",
      "prieti",
      "zinta",
      "win"
    ],
    "token_count":9,
    "processed_text":"felt sad dc wini hate see prieti zinta win"
  },
  {
    "label":0,
    "text":"birthday happi father day",
    "cleaned_text":"birthday happi father day",
    "normalized_text":"birthday happi father day",
    "tokens":[
      "birthday",
      "happi",
      "father",
      "day"
    ],
    "token_count":4,
    "processed_text":"birthday happi father day"
  },
  {
    "label":0,
    "text":"dread tomorrow hate hospit especi babi patient",
    "cleaned_text":"dread tomorrow hate hospit especi babi patient",
    "normalized_text":"dread tomorrow hate hospit especi babi patient",
    "tokens":[
      "dread",
      "tomorrow",
      "hate",
      "hospit",
      "especi",
      "babi",
      "patient"
    ],
    "token_count":7,
    "processed_text":"dread tomorrow hate hospit especi babi patient"
  },
  {
    "label":4,
    "text":"day stuff happen day stuff happen day stuff happen squarespac like stuff",
    "cleaned_text":"day stuff happen day stuff happen day stuff happen squarespac like stuff",
    "normalized_text":"day stuff happen day stuff happen day stuff happen squarespac like stuff",
    "tokens":[
      "day",
      "stuff",
      "happen",
      "day",
      "stuff",
      "happen",
      "day",
      "stuff",
      "happen",
      "squarespac",
      "like",
      "stuff"
    ],
    "token_count":12,
    "processed_text":"day stuff happen day stuff happen day stuff happen squarespac like stuff"
  },
  {
    "label":4,
    "text":"dont want pay download somethin good game impress",
    "cleaned_text":"dont want pay download somethin good game impress",
    "normalized_text":"dont want pay download somethin good game impress",
    "tokens":[
      "dont",
      "want",
      "pay",
      "download",
      "somethin",
      "good",
      "game",
      "impress"
    ],
    "token_count":8,
    "processed_text":"dont want pay download somethin good game impress"
  },
  {
    "label":0,
    "text":"face com past hour hate pack parcel",
    "cleaned_text":"face com past hour hate pack parcel",
    "normalized_text":"face com past hour hate pack parcel",
    "tokens":[
      "face",
      "com",
      "past",
      "hour",
      "hate",
      "pack",
      "parcel"
    ],
    "token_count":7,
    "processed_text":"face com past hour hate pack parcel"
  },
  {
    "label":0,
    "text":"even aust gov roll bln mb bband well still way behind rest world bband speed",
    "cleaned_text":"even aust gov roll bln mb bband well still way behind rest world bband speed",
    "normalized_text":"even aust gov roll bln mb bband well still way behind rest world bband speed",
    "tokens":[
      "even",
      "aust",
      "gov",
      "roll",
      "bln",
      "mb",
      "bband",
      "well",
      "still",
      "way",
      "behind",
      "rest",
      "world",
      "bband",
      "speed"
    ],
    "token_count":15,
    "processed_text":"even aust gov roll bln mb bband well still way behind rest world bband speed"
  },
  {
    "label":0,
    "text":"want sommmee",
    "cleaned_text":"want sommmee",
    "normalized_text":"want sommmee",
    "tokens":[
      "want",
      "sommme"
    ],
    "token_count":2,
    "processed_text":"want sommme"
  },
  {
    "label":0,
    "text":"freakn piss",
    "cleaned_text":"freakn piss",
    "normalized_text":"freakn piss",
    "tokens":[
      "freakn",
      "piss"
    ],
    "token_count":2,
    "processed_text":"freakn piss"
  },
  {
    "label":0,
    "text":"need credit blackberri",
    "cleaned_text":"need credit blackberri",
    "normalized_text":"need credit blackberri",
    "tokens":[
      "need",
      "credit",
      "blackberri"
    ],
    "token_count":3,
    "processed_text":"need credit blackberri"
  },
  {
    "label":0,
    "text":"back work tomorrow",
    "cleaned_text":"back work tomorrow",
    "normalized_text":"back work tomorrow",
    "tokens":[
      "back",
      "work",
      "tomorrow"
    ],
    "token_count":3,
    "processed_text":"back work tomorrow"
  },
  {
    "label":0,
    "text":"want live yellow submarin",
    "cleaned_text":"want live yellow submarin",
    "normalized_text":"want live yellow submarin",
    "tokens":[
      "want",
      "live",
      "yellow",
      "submarin"
    ],
    "token_count":4,
    "processed_text":"want live yellow submarin"
  },
  {
    "label":0,
    "text":"kati perri gig cancel barrowland tonight gut go interview",
    "cleaned_text":"kati perri gig cancel barrowland tonight gut go interview",
    "normalized_text":"kati perri gig cancel barrowland tonight gut go interview",
    "tokens":[
      "kati",
      "perri",
      "gig",
      "cancel",
      "barrowland",
      "tonight",
      "gut",
      "go",
      "interview"
    ],
    "token_count":9,
    "processed_text":"kati perri gig cancel barrowland tonight gut go interview"
  },
  {
    "label":0,
    "text":"welcom home vietnam vet walk put hand shake say welcom home never got militarymon",
    "cleaned_text":"welcom home vietnam vet walk put hand shake say welcom home never got militarymon",
    "normalized_text":"welcom home vietnam vet walk put hand shake say welcom home never got militarymon",
    "tokens":[
      "welcom",
      "home",
      "vietnam",
      "vet",
      "walk",
      "put",
      "hand",
      "shake",
      "say",
      "welcom",
      "home",
      "never",
      "got",
      "militarymon"
    ],
    "token_count":14,
    "processed_text":"welcom home vietnam vet walk put hand shake say welcom home never got militarymon"
  },
  {
    "label":0,
    "text":"im im finland dont know iphon",
    "cleaned_text":"im im finland dont know iphon",
    "normalized_text":"im im finland dont know iphon",
    "tokens":[
      "im",
      "im",
      "finland",
      "dont",
      "know",
      "iphon"
    ],
    "token_count":6,
    "processed_text":"im im finland dont know iphon"
  },
  {
    "label":0,
    "text":"cant follow quottwidroidquot sing beautifullllllli goodtim",
    "cleaned_text":"cant follow quottwidroidquot sing beautifullllllli goodtim",
    "normalized_text":"cant follow quottwidroidquot sing beautifullllllli goodtim",
    "tokens":[
      "cant",
      "follow",
      "sing",
      "goodtim"
    ],
    "token_count":4,
    "processed_text":"cant follow sing goodtim"
  },
  {
    "label":0,
    "text":"knooow im scare",
    "cleaned_text":"knooow im scare",
    "normalized_text":"knooow im scare",
    "tokens":[
      "knooow",
      "im",
      "scare"
    ],
    "token_count":3,
    "processed_text":"knooow im scare"
  },
  {
    "label":0,
    "text":"acura dealer get oil chang theater bad close cant twitpic",
    "cleaned_text":"acura dealer get oil chang theater bad close cant twitpic",
    "normalized_text":"acura dealer get oil chang theater bad close cant twitpic",
    "tokens":[
      "acura",
      "dealer",
      "get",
      "oil",
      "chang",
      "theater",
      "bad",
      "close",
      "cant",
      "twitpic"
    ],
    "token_count":10,
    "processed_text":"acura dealer get oil chang theater bad close cant twitpic"
  },
  {
    "label":0,
    "text":"cant co download link expir",
    "cleaned_text":"cant co download link expir",
    "normalized_text":"cant co download link expir",
    "tokens":[
      "cant",
      "co",
      "download",
      "link",
      "expir"
    ],
    "token_count":5,
    "processed_text":"cant co download link expir"
  },
  {
    "label":0,
    "text":"cancel oz tour devist pleas reconsid",
    "cleaned_text":"cancel oz tour devist pleas reconsid",
    "normalized_text":"cancel oz tour devist pleas reconsid",
    "tokens":[
      "cancel",
      "oz",
      "tour",
      "devist",
      "plea",
      "reconsid"
    ],
    "token_count":6,
    "processed_text":"cancel oz tour devist plea reconsid"
  },
  {
    "label":0,
    "text":"gosh miss stetson much hope get text back soon wana cri",
    "cleaned_text":"gosh miss stetson much hope get text back soon wana cri",
    "normalized_text":"gosh miss stetson much hope get text back soon wana cri",
    "tokens":[
      "gosh",
      "miss",
      "stetson",
      "much",
      "hope",
      "get",
      "text",
      "back",
      "soon",
      "wana",
      "cri"
    ],
    "token_count":11,
    "processed_text":"gosh miss stetson much hope get text back soon wana cri"
  },
  {
    "label":0,
    "text":"afraid swine flu",
    "cleaned_text":"afraid swine flu",
    "normalized_text":"afraid swine flu",
    "tokens":[
      "afraid",
      "swine",
      "flu"
    ],
    "token_count":3,
    "processed_text":"afraid swine flu"
  },
  {
    "label":0,
    "text":"fuckin bb pissin offshit wipe everythin dayday good day",
    "cleaned_text":"fuckin bb pissin offshit wipe everythin dayday good day",
    "normalized_text":"fuckin bb pissin offshit wipe everythin dayday good day",
    "tokens":[
      "fuckin",
      "bb",
      "pissin",
      "offshit",
      "wipe",
      "everythin",
      "dayday",
      "good",
      "day"
    ],
    "token_count":9,
    "processed_text":"fuckin bb pissin offshit wipe everythin dayday good day"
  },
  {
    "label":4,
    "text":"typic women uh",
    "cleaned_text":"typic women uh",
    "normalized_text":"typic women uh",
    "tokens":[
      "typic",
      "women",
      "uh"
    ],
    "token_count":3,
    "processed_text":"typic women uh"
  },
  {
    "label":0,
    "text":"im attent whore want peopl pay attent time im needi today someon pay lot attent",
    "cleaned_text":"im attent whore want peopl pay attent time im needi today someon pay lot attent",
    "normalized_text":"im attent whore want peopl pay attent time im needi today someon pay lot attent",
    "tokens":[
      "im",
      "attent",
      "whore",
      "want",
      "peopl",
      "pay",
      "attent",
      "time",
      "im",
      "needi",
      "today",
      "someon",
      "pay",
      "lot",
      "attent"
    ],
    "token_count":15,
    "processed_text":"im attent whore want peopl pay attent time im needi today someon pay lot attent"
  },
  {
    "label":4,
    "text":"im regist dont worri term new guy event",
    "cleaned_text":"im regist dont worri term new guy event",
    "normalized_text":"im regist dont worri term new guy event",
    "tokens":[
      "im",
      "regist",
      "dont",
      "worri",
      "term",
      "new",
      "guy",
      "event"
    ],
    "token_count":8,
    "processed_text":"im regist dont worri term new guy event"
  },
  {
    "label":0,
    "text":"neighbour new apart came quotcould sing basement loud truli shock cant bear itquot lol sigh",
    "cleaned_text":"neighbour new apart came quotcould sing basement loud truli shock cant bear itquot lol sigh",
    "normalized_text":"neighbour new apart came quotcould sing basement loud truli shock cant bear itquot lol sigh",
    "tokens":[
      "neighbour",
      "new",
      "apart",
      "came",
      "quotcould",
      "sing",
      "basement",
      "loud",
      "truli",
      "shock",
      "cant",
      "bear",
      "itquot",
      "lol",
      "sigh"
    ],
    "token_count":15,
    "processed_text":"neighbour new apart came quotcould sing basement loud truli shock cant bear itquot lol sigh"
  },
  {
    "label":4,
    "text":"repeat im watch quotoliverquot watch",
    "cleaned_text":"repeat im watch quotoliverquot watch",
    "normalized_text":"repeat im watch quotoliverquot watch",
    "tokens":[
      "repeat",
      "im",
      "watch",
      "quotoliverquot",
      "watch"
    ],
    "token_count":5,
    "processed_text":"repeat im watch quotoliverquot watch"
  },
  {
    "label":4,
    "text":"conserv point poll earlier point also show conserv twa fun",
    "cleaned_text":"conserv point poll earlier point also show conserv twa fun",
    "normalized_text":"conserv point poll earlier point also show conserv twa fun",
    "tokens":[
      "conserv",
      "point",
      "poll",
      "earlier",
      "point",
      "also",
      "show",
      "conserv",
      "twa",
      "fun"
    ],
    "token_count":10,
    "processed_text":"conserv point poll earlier point also show conserv twa fun"
  },
  {
    "label":0,
    "text":"tawfiq arif ahmad sangat nak pergi dive tapi ada sangat banyak kerja nak kena buat sorri exceed maximum",
    "cleaned_text":"tawfiq arif ahmad sangat nak pergi dive tapi ada sangat banyak kerja nak kena buat sorri exceed maximum",
    "normalized_text":"tawfiq arif ahmad sangat nak pergi dive tapi ada sangat banyak kerja nak kena buat sorri exceed maximum",
    "tokens":[
      "tawfiq",
      "arif",
      "ahmad",
      "sangat",
      "nak",
      "pergi",
      "dive",
      "tapi",
      "ada",
      "sangat",
      "banyak",
      "kerja",
      "nak",
      "kena",
      "buat",
      "sorri",
      "exceed",
      "maximum"
    ],
    "token_count":18,
    "processed_text":"tawfiq arif ahmad sangat nak pergi dive tapi ada sangat banyak kerja nak kena buat sorri exceed maximum"
  },
  {
    "label":4,
    "text":"write load letter gtgt autograph request got littl dri spell atm thumbsdown",
    "cleaned_text":"write load letter gtgt autograph request got littl dri spell atm thumbsdown",
    "normalized_text":"write load letter gtgt autograph request got littl dri spell atm thumbsdown",
    "tokens":[
      "write",
      "load",
      "letter",
      "gtgt",
      "autograph",
      "request",
      "got",
      "littl",
      "dri",
      "spell",
      "atm",
      "thumbsdown"
    ],
    "token_count":12,
    "processed_text":"write load letter gtgt autograph request got littl dri spell atm thumbsdown"
  },
  {
    "label":4,
    "text":"enjoy gather far thank travel prayer well wish",
    "cleaned_text":"enjoy gather far thank travel prayer well wish",
    "normalized_text":"enjoy gather far thank travel prayer well wish",
    "tokens":[
      "enjoy",
      "gather",
      "far",
      "thank",
      "travel",
      "prayer",
      "well",
      "wish"
    ],
    "token_count":8,
    "processed_text":"enjoy gather far thank travel prayer well wish"
  },
  {
    "label":4,
    "text":"actual sound bad impli bad cook someth intent",
    "cleaned_text":"actual sound bad impli bad cook someth intent",
    "normalized_text":"actual sound bad impli bad cook someth intent",
    "tokens":[
      "actual",
      "sound",
      "bad",
      "impli",
      "bad",
      "cook",
      "someth",
      "intent"
    ],
    "token_count":8,
    "processed_text":"actual sound bad impli bad cook someth intent"
  },
  {
    "label":4,
    "text":"book romant like quotlov storyquot quotreal diaryquot tour life ever thought",
    "cleaned_text":"book romant like quotlov storyquot quotreal diaryquot tour life ever thought",
    "normalized_text":"book romant like quotlov storyquot quotreal diaryquot tour life ever thought",
    "tokens":[
      "book",
      "romant",
      "like",
      "quotlov",
      "storyquot",
      "quotreal",
      "diaryquot",
      "tour",
      "life",
      "ever",
      "thought"
    ],
    "token_count":11,
    "processed_text":"book romant like quotlov storyquot quotreal diaryquot tour life ever thought"
  },
  {
    "label":4,
    "text":"gonna go play pool warm spring eastern want join",
    "cleaned_text":"gonna go play pool warm spring eastern want join",
    "normalized_text":"gonna go play pool warm spring eastern want join",
    "tokens":[
      "gon",
      "na",
      "go",
      "play",
      "pool",
      "warm",
      "spring",
      "eastern",
      "want",
      "join"
    ],
    "token_count":10,
    "processed_text":"gon na go play pool warm spring eastern want join"
  },
  {
    "label":0,
    "text":"wander round brighton found lego shop expens",
    "cleaned_text":"wander round brighton found lego shop expens",
    "normalized_text":"wander round brighton found lego shop expens",
    "tokens":[
      "wander",
      "round",
      "brighton",
      "found",
      "lego",
      "shop",
      "expen"
    ],
    "token_count":7,
    "processed_text":"wander round brighton found lego shop expen"
  },
  {
    "label":0,
    "text":"otw mangga unfortun shop",
    "cleaned_text":"otw mangga unfortun shop",
    "normalized_text":"otw mangga unfortun shop",
    "tokens":[
      "otw",
      "mangga",
      "unfortun",
      "shop"
    ],
    "token_count":4,
    "processed_text":"otw mangga unfortun shop"
  },
  {
    "label":4,
    "text":"gonna watch quotlov siamquot bet good movi",
    "cleaned_text":"gonna watch quotlov siamquot bet good movi",
    "normalized_text":"gonna watch quotlov siamquot bet good movi",
    "tokens":[
      "gon",
      "na",
      "watch",
      "quotlov",
      "siamquot",
      "bet",
      "good",
      "movi"
    ],
    "token_count":8,
    "processed_text":"gon na watch quotlov siamquot bet good movi"
  },
  {
    "label":4,
    "text":"welcom im gonna call saynow number time tonight leav messag what number",
    "cleaned_text":"welcom im gonna call saynow number time tonight leav messag what number",
    "normalized_text":"welcom im gonna call saynow number time tonight leav messag what number",
    "tokens":[
      "welcom",
      "im",
      "gon",
      "na",
      "call",
      "saynow",
      "number",
      "time",
      "tonight",
      "leav",
      "messag",
      "number"
    ],
    "token_count":12,
    "processed_text":"welcom im gon na call saynow number time tonight leav messag number"
  },
  {
    "label":4,
    "text":"put new desk togeth gotta love ikea",
    "cleaned_text":"put new desk togeth gotta love ikea",
    "normalized_text":"put new desk togeth gotta love ikea",
    "tokens":[
      "put",
      "new",
      "desk",
      "togeth",
      "got",
      "ta",
      "love",
      "ikea"
    ],
    "token_count":8,
    "processed_text":"put new desk togeth got ta love ikea"
  },
  {
    "label":4,
    "text":"yay cant wait see play guy rock",
    "cleaned_text":"yay cant wait see play guy rock",
    "normalized_text":"yay cant wait see play guy rock",
    "tokens":[
      "yay",
      "cant",
      "wait",
      "see",
      "play",
      "guy",
      "rock"
    ],
    "token_count":7,
    "processed_text":"yay cant wait see play guy rock"
  },
  {
    "label":4,
    "text":"finish scienc exam easi",
    "cleaned_text":"finish scienc exam easi",
    "normalized_text":"finish scienc exam easi",
    "tokens":[
      "finish",
      "scienc",
      "exam",
      "easi"
    ],
    "token_count":4,
    "processed_text":"finish scienc exam easi"
  },
  {
    "label":4,
    "text":"great workout today sore throat",
    "cleaned_text":"great workout today sore throat",
    "normalized_text":"great workout today sore throat",
    "tokens":[
      "great",
      "workout",
      "today",
      "sore",
      "throat"
    ],
    "token_count":5,
    "processed_text":"great workout today sore throat"
  },
  {
    "label":4,
    "text":"tri short attent span today go product day go see hangov tonight",
    "cleaned_text":"tri short attent span today go product day go see hangov tonight",
    "normalized_text":"tri short attent span today go product day go see hangov tonight",
    "tokens":[
      "tri",
      "short",
      "attent",
      "span",
      "today",
      "go",
      "product",
      "day",
      "go",
      "see",
      "hangov",
      "tonight"
    ],
    "token_count":12,
    "processed_text":"tri short attent span today go product day go see hangov tonight"
  },
  {
    "label":4,
    "text":"wow stand fake kristen stewart got followersthank new follow hope like updat",
    "cleaned_text":"wow stand fake kristen stewart got followersthank new follow hope like updat",
    "normalized_text":"wow stand fake kristen stewart got followersthank new follow hope like updat",
    "tokens":[
      "wow",
      "stand",
      "fake",
      "kristen",
      "stewart",
      "got",
      "followersthank",
      "new",
      "follow",
      "hope",
      "like",
      "updat"
    ],
    "token_count":12,
    "processed_text":"wow stand fake kristen stewart got followersthank new follow hope like updat"
  },
  {
    "label":0,
    "text":"think jinx darnnnnnnn",
    "cleaned_text":"think jinx darnnnnnnn",
    "normalized_text":"think jinx darnnnnnnn",
    "tokens":[
      "think",
      "jinx",
      "darnnnnnnn"
    ],
    "token_count":3,
    "processed_text":"think jinx darnnnnnnn"
  },
  {
    "label":4,
    "text":"ok head home kindergarten author tea ill hear dramat read year old",
    "cleaned_text":"ok head home kindergarten author tea ill hear dramat read year old",
    "normalized_text":"ok head home kindergarten author tea ill hear dramat read year old",
    "tokens":[
      "ok",
      "head",
      "home",
      "kindergarten",
      "author",
      "tea",
      "ill",
      "hear",
      "dramat",
      "read",
      "year",
      "old"
    ],
    "token_count":12,
    "processed_text":"ok head home kindergarten author tea ill hear dramat read year old"
  },
  {
    "label":0,
    "text":"much pain puppi train might go cant cute heal ill would rad",
    "cleaned_text":"much pain puppi train might go cant cute heal ill would rad",
    "normalized_text":"much pain puppi train might go cant cute heal ill would rad",
    "tokens":[
      "much",
      "pain",
      "puppi",
      "train",
      "go",
      "cant",
      "cute",
      "heal",
      "ill",
      "rad"
    ],
    "token_count":10,
    "processed_text":"much pain puppi train go cant cute heal ill rad"
  },
  {
    "label":0,
    "text":"tick horribl dog load frontlin plu still get bitten",
    "cleaned_text":"tick horribl dog load frontlin plu still get bitten",
    "normalized_text":"tick horribl dog load frontlin plu still get bitten",
    "tokens":[
      "tick",
      "horribl",
      "dog",
      "load",
      "frontlin",
      "plu",
      "still",
      "get",
      "bitten"
    ],
    "token_count":9,
    "processed_text":"tick horribl dog load frontlin plu still get bitten"
  },
  {
    "label":0,
    "text":"thank well wish woke still feel lousi mannn suxxx",
    "cleaned_text":"thank well wish woke still feel lousi mannn suxxx",
    "normalized_text":"thank well wish woke still feel lousi mannn suxxx",
    "tokens":[
      "thank",
      "well",
      "wish",
      "woke",
      "still",
      "feel",
      "lousi",
      "mannn",
      "suxxx"
    ],
    "token_count":9,
    "processed_text":"thank well wish woke still feel lousi mannn suxxx"
  },
  {
    "label":0,
    "text":"sound like fun tast sad hear wine went wast though",
    "cleaned_text":"sound like fun tast sad hear wine went wast though",
    "normalized_text":"sound like fun tast sad hear wine went wast though",
    "tokens":[
      "sound",
      "like",
      "fun",
      "tast",
      "sad",
      "hear",
      "wine",
      "went",
      "wast",
      "though"
    ],
    "token_count":10,
    "processed_text":"sound like fun tast sad hear wine went wast though"
  },
  {
    "label":0,
    "text":"said quoti want ailyn back quot appar someon quothackedquot twitter wasnt haha",
    "cleaned_text":"said quoti want ailyn back quot appar someon quothackedquot twitter wasnt haha",
    "normalized_text":"said quoti want ailyn back quot appar someon quothackedquot twitter wasnt haha",
    "tokens":[
      "said",
      "quoti",
      "want",
      "ailyn",
      "back",
      "quot",
      "appar",
      "someon",
      "quothackedquot",
      "twitter",
      "wasnt",
      "haha"
    ],
    "token_count":12,
    "processed_text":"said quoti want ailyn back quot appar someon quothackedquot twitter wasnt haha"
  },
  {
    "label":4,
    "text":"ye know never seem abl take advantag need learn",
    "cleaned_text":"ye know never seem abl take advantag need learn",
    "normalized_text":"ye know never seem abl take advantag need learn",
    "tokens":[
      "ye",
      "know",
      "never",
      "seem",
      "abl",
      "take",
      "advantag",
      "need",
      "learn"
    ],
    "token_count":9,
    "processed_text":"ye know never seem abl take advantag need learn"
  },
  {
    "label":4,
    "text":"tonight heard heartfelt prayer ive ever heard amp yr old",
    "cleaned_text":"tonight heard heartfelt prayer ive ever heard amp yr old",
    "normalized_text":"tonight heard heartfelt prayer ive ever heard amp yr old",
    "tokens":[
      "tonight",
      "heard",
      "heartfelt",
      "prayer",
      "ive",
      "ever",
      "heard",
      "amp",
      "yr",
      "old"
    ],
    "token_count":10,
    "processed_text":"tonight heard heartfelt prayer ive ever heard amp yr old"
  },
  {
    "label":0,
    "text":"beauti weather today perfect round golfif could get timehav abl play singl hole",
    "cleaned_text":"beauti weather today perfect round golfif could get timehav abl play singl hole",
    "normalized_text":"beauti weather today perfect round golfif could get timehav abl play singl hole",
    "tokens":[
      "beauti",
      "weather",
      "today",
      "perfect",
      "round",
      "golfif",
      "get",
      "timehav",
      "abl",
      "play",
      "singl",
      "hole"
    ],
    "token_count":12,
    "processed_text":"beauti weather today perfect round golfif get timehav abl play singl hole"
  },
  {
    "label":4,
    "text":"love movi",
    "cleaned_text":"love movi",
    "normalized_text":"love movi",
    "tokens":[
      "love",
      "movi"
    ],
    "token_count":2,
    "processed_text":"love movi"
  },
  {
    "label":0,
    "text":"good old visa wavier system fill green card plane web page advanc cost progress",
    "cleaned_text":"good old visa wavier system fill green card plane web page advanc cost progress",
    "normalized_text":"good old visa wavier system fill green card plane web page advanc cost progress",
    "tokens":[
      "good",
      "old",
      "visa",
      "wavier",
      "system",
      "fill",
      "green",
      "card",
      "plane",
      "web",
      "page",
      "advanc",
      "cost",
      "progress"
    ],
    "token_count":14,
    "processed_text":"good old visa wavier system fill green card plane web page advanc cost progress"
  },
  {
    "label":0,
    "text":"alway unconfort see puppi anim store window",
    "cleaned_text":"alway unconfort see puppi anim store window",
    "normalized_text":"alway unconfort see puppi anim store window",
    "tokens":[
      "alway",
      "unconfort",
      "see",
      "puppi",
      "anim",
      "store",
      "window"
    ],
    "token_count":7,
    "processed_text":"alway unconfort see puppi anim store window"
  },
  {
    "label":0,
    "text":"fact your go see movi wo moi",
    "cleaned_text":"fact your go see movi wo moi",
    "normalized_text":"fact your go see movi wo moi",
    "tokens":[
      "fact",
      "go",
      "see",
      "movi",
      "wo",
      "moi"
    ],
    "token_count":6,
    "processed_text":"fact go see movi wo moi"
  },
  {
    "label":4,
    "text":"got egg milk bread breakfast time",
    "cleaned_text":"got egg milk bread breakfast time",
    "normalized_text":"got egg milk bread breakfast time",
    "tokens":[
      "got",
      "egg",
      "milk",
      "bread",
      "breakfast",
      "time"
    ],
    "token_count":6,
    "processed_text":"got egg milk bread breakfast time"
  },
  {
    "label":0,
    "text":"nt good memori cri much promis jump big xs matter anyon tell u",
    "cleaned_text":"nt good memori cri much promis jump big xs matter anyon tell u",
    "normalized_text":"nt good memori cri much promis jump big xs matter anyon tell u",
    "tokens":[
      "nt",
      "good",
      "memori",
      "cri",
      "much",
      "promi",
      "jump",
      "big",
      "xs",
      "matter",
      "anyon",
      "tell"
    ],
    "token_count":12,
    "processed_text":"nt good memori cri much promi jump big xs matter anyon tell"
  },
  {
    "label":0,
    "text":"first paragraph talk iphon os alreadi releas download around world",
    "cleaned_text":"first paragraph talk iphon os alreadi releas download around world",
    "normalized_text":"first paragraph talk iphon os alreadi releas download around world",
    "tokens":[
      "first",
      "paragraph",
      "talk",
      "iphon",
      "os",
      "alreadi",
      "relea",
      "download",
      "around",
      "world"
    ],
    "token_count":10,
    "processed_text":"first paragraph talk iphon os alreadi relea download around world"
  },
  {
    "label":0,
    "text":"chemo session start min late instead wait isnt help anyon",
    "cleaned_text":"chemo session start min late instead wait isnt help anyon",
    "normalized_text":"chemo session start min late instead wait isnt help anyon",
    "tokens":[
      "chemo",
      "session",
      "start",
      "min",
      "late",
      "instead",
      "wait",
      "isnt",
      "help",
      "anyon"
    ],
    "token_count":10,
    "processed_text":"chemo session start min late instead wait isnt help anyon"
  },
  {
    "label":0,
    "text":"ms publish ate half concert program go printer week work five hour today",
    "cleaned_text":"ms publish ate half concert program go printer week work five hour today",
    "normalized_text":"ms publish ate half concert program go printer week work five hour today",
    "tokens":[
      "ms",
      "publish",
      "ate",
      "half",
      "concert",
      "program",
      "go",
      "printer",
      "week",
      "work",
      "five",
      "hour",
      "today"
    ],
    "token_count":13,
    "processed_text":"ms publish ate half concert program go printer week work five hour today"
  },
  {
    "label":0,
    "text":"home stretch marathon sunday hope blister heal",
    "cleaned_text":"home stretch marathon sunday hope blister heal",
    "normalized_text":"home stretch marathon sunday hope blister heal",
    "tokens":[
      "home",
      "stretch",
      "marathon",
      "sunday",
      "hope",
      "blister",
      "heal"
    ],
    "token_count":7,
    "processed_text":"home stretch marathon sunday hope blister heal"
  },
  {
    "label":0,
    "text":"omg woke amp felt like gonna pass tha showeramp top hand feel like seriou case arthriti wtf",
    "cleaned_text":"omg woke amp felt like gonna pass tha showeramp top hand feel like seriou case arthriti wtf",
    "normalized_text":"omg woke amp felt like gonna pass tha showeramp top hand feel like seriou case arthriti wtf",
    "tokens":[
      "omg",
      "woke",
      "amp",
      "felt",
      "like",
      "gon",
      "na",
      "pass",
      "tha",
      "showeramp",
      "top",
      "hand",
      "feel",
      "like",
      "seriou",
      "case",
      "arthriti",
      "wtf"
    ],
    "token_count":18,
    "processed_text":"omg woke amp felt like gon na pass tha showeramp top hand feel like seriou case arthriti wtf"
  },
  {
    "label":0,
    "text":"aww didnt",
    "cleaned_text":"aww didnt",
    "normalized_text":"aww didnt",
    "tokens":[
      "aww",
      "didnt"
    ],
    "token_count":2,
    "processed_text":"aww didnt"
  },
  {
    "label":0,
    "text":"wordpress need stop play emoticon real none pop",
    "cleaned_text":"wordpress need stop play emoticon real none pop",
    "normalized_text":"wordpress need stop play emoticon real none pop",
    "tokens":[
      "wordpress",
      "need",
      "stop",
      "play",
      "emoticon",
      "real",
      "none",
      "pop"
    ],
    "token_count":8,
    "processed_text":"wordpress need stop play emoticon real none pop"
  },
  {
    "label":0,
    "text":"ohhhh thunderstormsboo alway make power go",
    "cleaned_text":"ohhhh thunderstormsboo alway make power go",
    "normalized_text":"ohhhh thunderstormsboo alway make power go",
    "tokens":[
      "ohhhh",
      "alway",
      "make",
      "power",
      "go"
    ],
    "token_count":5,
    "processed_text":"ohhhh alway make power go"
  },
  {
    "label":4,
    "text":"everyth carolin loft",
    "cleaned_text":"everyth carolin loft",
    "normalized_text":"everyth carolin loft",
    "tokens":[
      "everyth",
      "carolin",
      "loft"
    ],
    "token_count":3,
    "processed_text":"everyth carolin loft"
  },
  {
    "label":4,
    "text":"got home supermal",
    "cleaned_text":"got home supermal",
    "normalized_text":"got home supermal",
    "tokens":[
      "got",
      "home",
      "superm"
    ],
    "token_count":3,
    "processed_text":"got home superm"
  },
  {
    "label":0,
    "text":"ive got babysit today",
    "cleaned_text":"ive got babysit today",
    "normalized_text":"ive got babysit today",
    "tokens":[
      "ive",
      "got",
      "babysit",
      "today"
    ],
    "token_count":4,
    "processed_text":"ive got babysit today"
  },
  {
    "label":4,
    "text":"lmfao told hahaha",
    "cleaned_text":"lmfao told hahaha",
    "normalized_text":"lmfao told hahaha",
    "tokens":[
      "lmfao",
      "told",
      "hahaha"
    ],
    "token_count":3,
    "processed_text":"lmfao told hahaha"
  },
  {
    "label":4,
    "text":"love famili dinner",
    "cleaned_text":"love famili dinner",
    "normalized_text":"love famili dinner",
    "tokens":[
      "love",
      "famili",
      "dinner"
    ],
    "token_count":3,
    "processed_text":"love famili dinner"
  },
  {
    "label":4,
    "text":"didnt get much twa good r u",
    "cleaned_text":"didnt get much twa good r u",
    "normalized_text":"didnt get much twa good r u",
    "tokens":[
      "didnt",
      "get",
      "much",
      "twa",
      "good"
    ],
    "token_count":5,
    "processed_text":"didnt get much twa good"
  },
  {
    "label":0,
    "text":"listen breakfast tiffani soundtrack miss honey",
    "cleaned_text":"listen breakfast tiffani soundtrack miss honey",
    "normalized_text":"listen breakfast tiffani soundtrack miss honey",
    "tokens":[
      "listen",
      "breakfast",
      "tiffani",
      "soundtrack",
      "miss",
      "honey"
    ],
    "token_count":6,
    "processed_text":"listen breakfast tiffani soundtrack miss honey"
  },
  {
    "label":0,
    "text":"boo got go work im worri holiday traffic psycho lutonvan driver tri kill dog",
    "cleaned_text":"boo got go work im worri holiday traffic psycho lutonvan driver tri kill dog",
    "normalized_text":"boo got go work im worri holiday traffic psycho lutonvan driver tri kill dog",
    "tokens":[
      "boo",
      "got",
      "go",
      "work",
      "im",
      "worri",
      "holiday",
      "traffic",
      "psycho",
      "lutonvan",
      "driver",
      "tri",
      "kill",
      "dog"
    ],
    "token_count":14,
    "processed_text":"boo got go work im worri holiday traffic psycho lutonvan driver tri kill dog"
  },
  {
    "label":4,
    "text":"listen blindmelon",
    "cleaned_text":"listen blindmelon",
    "normalized_text":"listen blindmelon",
    "tokens":[
      "listen",
      "blindmelon"
    ],
    "token_count":2,
    "processed_text":"listen blindmelon"
  },
  {
    "label":4,
    "text":"ooh sound like interest round go twist fellow twitter arm today form team next week",
    "cleaned_text":"ooh sound like interest round go twist fellow twitter arm today form team next week",
    "normalized_text":"ooh sound like interest round go twist fellow twitter arm today form team next week",
    "tokens":[
      "ooh",
      "sound",
      "like",
      "interest",
      "round",
      "go",
      "twist",
      "fellow",
      "twitter",
      "arm",
      "today",
      "form",
      "team",
      "next",
      "week"
    ],
    "token_count":15,
    "processed_text":"ooh sound like interest round go twist fellow twitter arm today form team next week"
  },
  {
    "label":4,
    "text":"pic legsim surpris still themyou nonstop",
    "cleaned_text":"pic legsim surpris still themyou nonstop",
    "normalized_text":"pic legsim surpris still themyou nonstop",
    "tokens":[
      "pic",
      "legsim",
      "surpri",
      "still",
      "themyou",
      "nonstop"
    ],
    "token_count":6,
    "processed_text":"pic legsim surpri still themyou nonstop"
  },
  {
    "label":4,
    "text":"defiinit feel run morn glad leg arent",
    "cleaned_text":"defiinit feel run morn glad leg arent",
    "normalized_text":"defiinit feel run morn glad leg arent",
    "tokens":[
      "defiinit",
      "feel",
      "run",
      "morn",
      "glad",
      "leg",
      "arent"
    ],
    "token_count":7,
    "processed_text":"defiinit feel run morn glad leg arent"
  },
  {
    "label":0,
    "text":"hub poopooer thing cute",
    "cleaned_text":"hub poopooer thing cute",
    "normalized_text":"hub poopooer thing cute",
    "tokens":[
      "hub",
      "poopooer",
      "thing",
      "cute"
    ],
    "token_count":4,
    "processed_text":"hub poopooer thing cute"
  },
  {
    "label":4,
    "text":"excel",
    "cleaned_text":"excel",
    "normalized_text":"excel",
    "tokens":[
      "excel"
    ],
    "token_count":1,
    "processed_text":"excel"
  },
  {
    "label":4,
    "text":"mean littl advic clueless",
    "cleaned_text":"mean littl advic clueless",
    "normalized_text":"mean littl advic clueless",
    "tokens":[
      "mean",
      "littl",
      "advic",
      "clueless"
    ],
    "token_count":4,
    "processed_text":"mean littl advic clueless"
  },
  {
    "label":0,
    "text":"oh shit didnt even realiz",
    "cleaned_text":"oh shit didnt even realiz",
    "normalized_text":"oh shit didnt even realiz",
    "tokens":[
      "oh",
      "shit",
      "didnt",
      "even",
      "realiz"
    ],
    "token_count":5,
    "processed_text":"oh shit didnt even realiz"
  },
  {
    "label":4,
    "text":"grin thank glad enjoy",
    "cleaned_text":"grin thank glad enjoy",
    "normalized_text":"grin thank glad enjoy",
    "tokens":[
      "grin",
      "thank",
      "glad",
      "enjoy"
    ],
    "token_count":4,
    "processed_text":"grin thank glad enjoy"
  },
  {
    "label":4,
    "text":"loooov sunday",
    "cleaned_text":"loooov sunday",
    "normalized_text":"loooov sunday",
    "tokens":[
      "loooov",
      "sunday"
    ],
    "token_count":2,
    "processed_text":"loooov sunday"
  },
  {
    "label":0,
    "text":"got realli bad throat gonna see weve got yoghurt",
    "cleaned_text":"got realli bad throat gonna see weve got yoghurt",
    "normalized_text":"got realli bad throat gonna see weve got yoghurt",
    "tokens":[
      "got",
      "realli",
      "bad",
      "throat",
      "gon",
      "na",
      "see",
      "weve",
      "got",
      "yoghurt"
    ],
    "token_count":10,
    "processed_text":"got realli bad throat gon na see weve got yoghurt"
  },
  {
    "label":0,
    "text":"ugh im crackin sweet messag quoti cant live without pleas babi mean everyth mequot",
    "cleaned_text":"ugh im crackin sweet messag quoti cant live without pleas babi mean everyth mequot",
    "normalized_text":"ugh im crackin sweet messag quoti cant live without pleas babi mean everyth mequot",
    "tokens":[
      "ugh",
      "im",
      "crackin",
      "sweet",
      "messag",
      "quoti",
      "cant",
      "live",
      "without",
      "plea",
      "babi",
      "mean",
      "everyth",
      "mequot"
    ],
    "token_count":14,
    "processed_text":"ugh im crackin sweet messag quoti cant live without plea babi mean everyth mequot"
  },
  {
    "label":4,
    "text":"skye graci pretti name",
    "cleaned_text":"skye graci pretti name",
    "normalized_text":"skye graci pretti name",
    "tokens":[
      "skye",
      "graci",
      "pretti",
      "name"
    ],
    "token_count":4,
    "processed_text":"skye graci pretti name"
  },
  {
    "label":4,
    "text":"lol tht funni dude",
    "cleaned_text":"lol tht funni dude",
    "normalized_text":"lol tht funni dude",
    "tokens":[
      "lol",
      "tht",
      "funni",
      "dude"
    ],
    "token_count":4,
    "processed_text":"lol tht funni dude"
  },
  {
    "label":4,
    "text":"even didnt take long ha ha",
    "cleaned_text":"even didnt take long ha ha",
    "normalized_text":"even didnt take long ha ha",
    "tokens":[
      "even",
      "didnt",
      "take",
      "long",
      "ha",
      "ha"
    ],
    "token_count":6,
    "processed_text":"even didnt take long ha ha"
  },
  {
    "label":0,
    "text":"gah twitter widget blogger ugli guess gotta get flash one",
    "cleaned_text":"gah twitter widget blogger ugli guess gotta get flash one",
    "normalized_text":"gah twitter widget blogger ugli guess gotta get flash one",
    "tokens":[
      "gah",
      "twitter",
      "widget",
      "blogger",
      "ugli",
      "guess",
      "got",
      "ta",
      "get",
      "flash",
      "one"
    ],
    "token_count":11,
    "processed_text":"gah twitter widget blogger ugli guess got ta get flash one"
  },
  {
    "label":0,
    "text":"dude hope guy alright except homecar",
    "cleaned_text":"dude hope guy alright except homecar",
    "normalized_text":"dude hope guy alright except homecar",
    "tokens":[
      "dude",
      "hope",
      "guy",
      "alright",
      "except",
      "homecar"
    ],
    "token_count":6,
    "processed_text":"dude hope guy alright except homecar"
  },
  {
    "label":0,
    "text":"dunno link still work send em bb",
    "cleaned_text":"dunno link still work send em bb",
    "normalized_text":"dunno link still work send em bb",
    "tokens":[
      "dunno",
      "link",
      "still",
      "work",
      "send",
      "em",
      "bb"
    ],
    "token_count":7,
    "processed_text":"dunno link still work send em bb"
  },
  {
    "label":4,
    "text":"yeah dvd could watch commerci free",
    "cleaned_text":"yeah dvd could watch commerci free",
    "normalized_text":"yeah dvd could watch commerci free",
    "tokens":[
      "yeah",
      "dvd",
      "watch",
      "commerci",
      "free"
    ],
    "token_count":5,
    "processed_text":"yeah dvd watch commerci free"
  },
  {
    "label":4,
    "text":"ooh oki ise dat make sens",
    "cleaned_text":"ooh oki ise dat make sens",
    "normalized_text":"ooh oki ise dat make sens",
    "tokens":[
      "ooh",
      "oki",
      "ise",
      "dat",
      "make",
      "sen"
    ],
    "token_count":6,
    "processed_text":"ooh oki ise dat make sen"
  },
  {
    "label":4,
    "text":"listen cover sean kingston much better origin chines vodka lethal yet love appl juic sun",
    "cleaned_text":"listen cover sean kingston much better origin chines vodka lethal yet love appl juic sun",
    "normalized_text":"listen cover sean kingston much better origin chines vodka lethal yet love appl juic sun",
    "tokens":[
      "listen",
      "cover",
      "sean",
      "kingston",
      "much",
      "better",
      "origin",
      "chine",
      "vodka",
      "lethal",
      "yet",
      "love",
      "appl",
      "juic",
      "sun"
    ],
    "token_count":15,
    "processed_text":"listen cover sean kingston much better origin chine vodka lethal yet love appl juic sun"
  },
  {
    "label":4,
    "text":"thank feel justifi hear someon els btw love matern photosso beauti awesom set wthe bed",
    "cleaned_text":"thank feel justifi hear someon els btw love matern photosso beauti awesom set wthe bed",
    "normalized_text":"thank feel justifi hear someon els btw love matern photosso beauti awesom set wthe bed",
    "tokens":[
      "thank",
      "feel",
      "justifi",
      "hear",
      "someon",
      "el",
      "btw",
      "love",
      "matern",
      "photosso",
      "beauti",
      "awesom",
      "set",
      "wthe",
      "bed"
    ],
    "token_count":15,
    "processed_text":"thank feel justifi hear someon el btw love matern photosso beauti awesom set wthe bed"
  },
  {
    "label":0,
    "text":"go bed without jailbreak updat",
    "cleaned_text":"go bed without jailbreak updat",
    "normalized_text":"go bed without jailbreak updat",
    "tokens":[
      "go",
      "bed",
      "without",
      "jailbreak",
      "updat"
    ],
    "token_count":5,
    "processed_text":"go bed without jailbreak updat"
  },
  {
    "label":0,
    "text":"tell one day jealou tri take nap fail miser",
    "cleaned_text":"tell one day jealou tri take nap fail miser",
    "normalized_text":"tell one day jealou tri take nap fail miser",
    "tokens":[
      "tell",
      "one",
      "day",
      "jealou",
      "tri",
      "take",
      "nap",
      "fail",
      "miser"
    ],
    "token_count":9,
    "processed_text":"tell one day jealou tri take nap fail miser"
  },
  {
    "label":4,
    "text":"hmmmwhat philosoph question caus isi enjoy extrem happi life",
    "cleaned_text":"hmmmwhat philosoph question caus isi enjoy extrem happi life",
    "normalized_text":"hmmmwhat philosoph question caus isi enjoy extrem happi life",
    "tokens":[
      "hmmmwhat",
      "philosoph",
      "question",
      "cau",
      "isi",
      "enjoy",
      "extrem",
      "happi",
      "life"
    ],
    "token_count":9,
    "processed_text":"hmmmwhat philosoph question cau isi enjoy extrem happi life"
  },
  {
    "label":0,
    "text":"go sleep work want hr sleep",
    "cleaned_text":"go sleep work want hr sleep",
    "normalized_text":"go sleep work want hr sleep",
    "tokens":[
      "go",
      "sleep",
      "work",
      "want",
      "hr",
      "sleep"
    ],
    "token_count":6,
    "processed_text":"go sleep work want hr sleep"
  },
  {
    "label":0,
    "text":"poor demi go see saturday tomorrow excit",
    "cleaned_text":"poor demi go see saturday tomorrow excit",
    "normalized_text":"poor demi go see saturday tomorrow excit",
    "tokens":[
      "poor",
      "demi",
      "go",
      "see",
      "saturday",
      "tomorrow",
      "excit"
    ],
    "token_count":7,
    "processed_text":"poor demi go see saturday tomorrow excit"
  },
  {
    "label":4,
    "text":"that great guess ill draw someth mother day",
    "cleaned_text":"that great guess ill draw someth mother day",
    "normalized_text":"that great guess ill draw someth mother day",
    "tokens":[
      "great",
      "guess",
      "ill",
      "draw",
      "someth",
      "mother",
      "day"
    ],
    "token_count":7,
    "processed_text":"great guess ill draw someth mother day"
  },
  {
    "label":0,
    "text":"im seriou issu imovi",
    "cleaned_text":"im seriou issu imovi",
    "normalized_text":"im seriou issu imovi",
    "tokens":[
      "im",
      "seriou",
      "issu",
      "imovi"
    ],
    "token_count":4,
    "processed_text":"im seriou issu imovi"
  },
  {
    "label":4,
    "text":"cant dm u tk quot pl state pub news credibl link name blog pleas p",
    "cleaned_text":"cant dm u tk quot pl state pub news credibl link name blog pleas p",
    "normalized_text":"cant dm u tk quot pl state pub news credibl link name blog pleas p",
    "tokens":[
      "cant",
      "dm",
      "tk",
      "quot",
      "pl",
      "state",
      "pub",
      "news",
      "credibl",
      "link",
      "name",
      "blog",
      "plea"
    ],
    "token_count":13,
    "processed_text":"cant dm tk quot pl state pub news credibl link name blog plea"
  },
  {
    "label":4,
    "text":"back lake morn home friday awesom weekend far",
    "cleaned_text":"back lake morn home friday awesom weekend far",
    "normalized_text":"back lake morn home friday awesom weekend far",
    "tokens":[
      "back",
      "lake",
      "morn",
      "home",
      "friday",
      "awesom",
      "weekend",
      "far"
    ],
    "token_count":8,
    "processed_text":"back lake morn home friday awesom weekend far"
  },
  {
    "label":0,
    "text":"im go camp year",
    "cleaned_text":"im go camp year",
    "normalized_text":"im go camp year",
    "tokens":[
      "im",
      "go",
      "camp",
      "year"
    ],
    "token_count":4,
    "processed_text":"im go camp year"
  },
  {
    "label":0,
    "text":"ef tire",
    "cleaned_text":"ef tire",
    "normalized_text":"ef tire",
    "tokens":[
      "ef",
      "tire"
    ],
    "token_count":2,
    "processed_text":"ef tire"
  },
  {
    "label":4,
    "text":"hola random follow",
    "cleaned_text":"hola random follow",
    "normalized_text":"hola random follow",
    "tokens":[
      "hola",
      "random",
      "follow"
    ],
    "token_count":3,
    "processed_text":"hola random follow"
  },
  {
    "label":4,
    "text":"love heeer love nooow love pour hand brow love song",
    "cleaned_text":"love heeer love nooow love pour hand brow love song",
    "normalized_text":"love heeer love nooow love pour hand brow love song",
    "tokens":[
      "love",
      "heeer",
      "love",
      "nooow",
      "love",
      "pour",
      "hand",
      "brow",
      "love",
      "song"
    ],
    "token_count":10,
    "processed_text":"love heeer love nooow love pour hand brow love song"
  },
  {
    "label":4,
    "text":"im berlin cant tell",
    "cleaned_text":"im berlin cant tell",
    "normalized_text":"im berlin cant tell",
    "tokens":[
      "im",
      "berlin",
      "cant",
      "tell"
    ],
    "token_count":4,
    "processed_text":"im berlin cant tell"
  },
  {
    "label":4,
    "text":"yay live improv music theater ridicul funni",
    "cleaned_text":"yay live improv music theater ridicul funni",
    "normalized_text":"yay live improv music theater ridicul funni",
    "tokens":[
      "yay",
      "live",
      "improv",
      "music",
      "theater",
      "ridicul",
      "funni"
    ],
    "token_count":7,
    "processed_text":"yay live improv music theater ridicul funni"
  },
  {
    "label":4,
    "text":"wow wow quotb minequot beauti written lyric touch heart much thank share",
    "cleaned_text":"wow wow quotb minequot beauti written lyric touch heart much thank share",
    "normalized_text":"wow wow quotb minequot beauti written lyric touch heart much thank share",
    "tokens":[
      "wow",
      "wow",
      "quotb",
      "minequot",
      "beauti",
      "written",
      "lyric",
      "touch",
      "heart",
      "much",
      "thank",
      "share"
    ],
    "token_count":12,
    "processed_text":"wow wow quotb minequot beauti written lyric touch heart much thank share"
  },
  {
    "label":0,
    "text":"im appar without cabl internet wed even reason comcast explain",
    "cleaned_text":"im appar without cabl internet wed even reason comcast explain",
    "normalized_text":"im appar without cabl internet wed even reason comcast explain",
    "tokens":[
      "im",
      "appar",
      "without",
      "cabl",
      "internet",
      "wed",
      "even",
      "reason",
      "comcast",
      "explain"
    ],
    "token_count":10,
    "processed_text":"im appar without cabl internet wed even reason comcast explain"
  },
  {
    "label":0,
    "text":"got two blink ticket mail today im watch dexter live room someon come hang",
    "cleaned_text":"got two blink ticket mail today im watch dexter live room someon come hang",
    "normalized_text":"got two blink ticket mail today im watch dexter live room someon come hang",
    "tokens":[
      "got",
      "two",
      "blink",
      "ticket",
      "mail",
      "today",
      "im",
      "watch",
      "dexter",
      "live",
      "room",
      "someon",
      "come",
      "hang"
    ],
    "token_count":14,
    "processed_text":"got two blink ticket mail today im watch dexter live room someon come hang"
  },
  {
    "label":0,
    "text":"nd place bad basketbal til next season cant wait soccer",
    "cleaned_text":"nd place bad basketbal til next season cant wait soccer",
    "normalized_text":"nd place bad basketbal til next season cant wait soccer",
    "tokens":[
      "nd",
      "place",
      "bad",
      "basketb",
      "til",
      "next",
      "season",
      "cant",
      "wait",
      "soccer"
    ],
    "token_count":10,
    "processed_text":"nd place bad basketb til next season cant wait soccer"
  },
  {
    "label":4,
    "text":"cool sit nice ifart pee monkey toilet trainer",
    "cleaned_text":"cool sit nice ifart pee monkey toilet trainer",
    "normalized_text":"cool sit nice ifart pee monkey toilet trainer",
    "tokens":[
      "cool",
      "sit",
      "nice",
      "ifart",
      "pee",
      "monkey",
      "toilet",
      "trainer"
    ],
    "token_count":8,
    "processed_text":"cool sit nice ifart pee monkey toilet trainer"
  },
  {
    "label":0,
    "text":"rite im da mall gettn hair done need hair soooo lifeless serious",
    "cleaned_text":"rite im da mall gettn hair done need hair soooo lifeless serious",
    "normalized_text":"rite im da mall gettn hair done need hair soooo lifeless serious",
    "tokens":[
      "rite",
      "im",
      "da",
      "mall",
      "gettn",
      "hair",
      "done",
      "need",
      "hair",
      "soooo",
      "lifeless",
      "seriou"
    ],
    "token_count":12,
    "processed_text":"rite im da mall gettn hair done need hair soooo lifeless seriou"
  },
  {
    "label":4,
    "text":"ive follow long time",
    "cleaned_text":"ive follow long time",
    "normalized_text":"ive follow long time",
    "tokens":[
      "ive",
      "follow",
      "long",
      "time"
    ],
    "token_count":4,
    "processed_text":"ive follow long time"
  },
  {
    "label":0,
    "text":"miss anyth bbr cant get internet work",
    "cleaned_text":"miss anyth bbr cant get internet work",
    "normalized_text":"miss anyth bbr cant get internet work",
    "tokens":[
      "miss",
      "anyth",
      "bbr",
      "cant",
      "get",
      "internet",
      "work"
    ],
    "token_count":7,
    "processed_text":"miss anyth bbr cant get internet work"
  },
  {
    "label":0,
    "text":"pre paid phone run minut today",
    "cleaned_text":"pre paid phone run minut today",
    "normalized_text":"pre paid phone run minut today",
    "tokens":[
      "pre",
      "paid",
      "phone",
      "run",
      "minut",
      "today"
    ],
    "token_count":6,
    "processed_text":"pre paid phone run minut today"
  },
  {
    "label":0,
    "text":"plz tell bymyself everybodi gooooo",
    "cleaned_text":"plz tell bymyself everybodi gooooo",
    "normalized_text":"plz tell bymyself everybodi gooooo",
    "tokens":[
      "plz",
      "tell",
      "bymyself",
      "everybodi",
      "gooooo"
    ],
    "token_count":5,
    "processed_text":"plz tell bymyself everybodi gooooo"
  },
  {
    "label":0,
    "text":"funni ppl sometim love much caus much chao that u know time let gofam long time friend",
    "cleaned_text":"funni ppl sometim love much caus much chao that u know time let gofam long time friend",
    "normalized_text":"funni ppl sometim love much caus much chao that u know time let gofam long time friend",
    "tokens":[
      "funni",
      "ppl",
      "sometim",
      "love",
      "much",
      "cau",
      "much",
      "chao",
      "know",
      "time",
      "let",
      "gofam",
      "long",
      "time",
      "friend"
    ],
    "token_count":15,
    "processed_text":"funni ppl sometim love much cau much chao know time let gofam long time friend"
  },
  {
    "label":0,
    "text":"sorri babe wanna messag tell meh what wrong",
    "cleaned_text":"sorri babe wanna messag tell meh what wrong",
    "normalized_text":"sorri babe wanna messag tell meh what wrong",
    "tokens":[
      "sorri",
      "babe",
      "wan",
      "na",
      "messag",
      "tell",
      "meh",
      "wrong"
    ],
    "token_count":8,
    "processed_text":"sorri babe wan na messag tell meh wrong"
  },
  {
    "label":0,
    "text":"graduat parti sipper",
    "cleaned_text":"graduat parti sipper",
    "normalized_text":"graduat parti sipper",
    "tokens":[
      "graduat",
      "parti",
      "sipper"
    ],
    "token_count":3,
    "processed_text":"graduat parti sipper"
  },
  {
    "label":0,
    "text":"egad deadpool ever done",
    "cleaned_text":"egad deadpool ever done",
    "normalized_text":"egad deadpool ever done",
    "tokens":[
      "egad",
      "deadpool",
      "ever",
      "done"
    ],
    "token_count":4,
    "processed_text":"egad deadpool ever done"
  },
  {
    "label":4,
    "text":"mi padr casa get settl",
    "cleaned_text":"mi padr casa get settl",
    "normalized_text":"mi padr casa get settl",
    "tokens":[
      "mi",
      "padr",
      "casa",
      "get",
      "settl"
    ],
    "token_count":5,
    "processed_text":"mi padr casa get settl"
  },
  {
    "label":4,
    "text":"final finish benjamin button man loong movi",
    "cleaned_text":"final finish benjamin button man loong movi",
    "normalized_text":"final finish benjamin button man loong movi",
    "tokens":[
      "final",
      "finish",
      "benjamin",
      "button",
      "man",
      "loong",
      "movi"
    ],
    "token_count":7,
    "processed_text":"final finish benjamin button man loong movi"
  },
  {
    "label":4,
    "text":"thank awesom hear guy fun togeth watch clip",
    "cleaned_text":"thank awesom hear guy fun togeth watch clip",
    "normalized_text":"thank awesom hear guy fun togeth watch clip",
    "tokens":[
      "thank",
      "awesom",
      "hear",
      "guy",
      "fun",
      "togeth",
      "watch",
      "clip"
    ],
    "token_count":8,
    "processed_text":"thank awesom hear guy fun togeth watch clip"
  },
  {
    "label":0,
    "text":"arrghi dont like want pepperoniright",
    "cleaned_text":"arrghi dont like want pepperoniright",
    "normalized_text":"arrghi dont like want pepperoniright",
    "tokens":[
      "arrghi",
      "dont",
      "like",
      "want",
      "pepperoniright"
    ],
    "token_count":5,
    "processed_text":"arrghi dont like want pepperoniright"
  },
  {
    "label":0,
    "text":"agre squid",
    "cleaned_text":"agre squid",
    "normalized_text":"agre squid",
    "tokens":[
      "agr",
      "squid"
    ],
    "token_count":2,
    "processed_text":"agr squid"
  },
  {
    "label":4,
    "text":"want dead one",
    "cleaned_text":"want dead one",
    "normalized_text":"want dead one",
    "tokens":[
      "want",
      "dead",
      "one"
    ],
    "token_count":3,
    "processed_text":"want dead one"
  },
  {
    "label":4,
    "text":"need play someth mari poppin everyon know that room get tidi",
    "cleaned_text":"need play someth mari poppin everyon know that room get tidi",
    "normalized_text":"need play someth mari poppin everyon know that room get tidi",
    "tokens":[
      "need",
      "play",
      "someth",
      "mari",
      "poppin",
      "everyon",
      "know",
      "room",
      "get",
      "tidi"
    ],
    "token_count":10,
    "processed_text":"need play someth mari poppin everyon know room get tidi"
  },
  {
    "label":4,
    "text":"make sure u let kno cuz juli ill def extend trip n go",
    "cleaned_text":"make sure u let kno cuz juli ill def extend trip n go",
    "normalized_text":"make sure u let kno cuz juli ill def extend trip n go",
    "tokens":[
      "make",
      "sure",
      "let",
      "kno",
      "cuz",
      "juli",
      "ill",
      "def",
      "extend",
      "trip",
      "go"
    ],
    "token_count":11,
    "processed_text":"make sure let kno cuz juli ill def extend trip go"
  },
  {
    "label":0,
    "text":"urgh also need wash dont want brave rain get washer bad time",
    "cleaned_text":"urgh also need wash dont want brave rain get washer bad time",
    "normalized_text":"urgh also need wash dont want brave rain get washer bad time",
    "tokens":[
      "urgh",
      "also",
      "need",
      "wash",
      "dont",
      "want",
      "brave",
      "rain",
      "get",
      "washer",
      "bad",
      "time"
    ],
    "token_count":12,
    "processed_text":"urgh also need wash dont want brave rain get washer bad time"
  },
  {
    "label":4,
    "text":"gooooood afternoon morn plan great sunday",
    "cleaned_text":"gooooood afternoon morn plan great sunday",
    "normalized_text":"gooooood afternoon morn plan great sunday",
    "tokens":[
      "gooooood",
      "afternoon",
      "morn",
      "plan",
      "great",
      "sunday"
    ],
    "token_count":6,
    "processed_text":"gooooood afternoon morn plan great sunday"
  },
  {
    "label":4,
    "text":"how ur day far congrat young mula record deal",
    "cleaned_text":"how ur day far congrat young mula record deal",
    "normalized_text":"how ur day far congrat young mula record deal",
    "tokens":[
      "ur",
      "day",
      "far",
      "congrat",
      "young",
      "mula",
      "record",
      "deal"
    ],
    "token_count":8,
    "processed_text":"ur day far congrat young mula record deal"
  },
  {
    "label":4,
    "text":"lunch time",
    "cleaned_text":"lunch time",
    "normalized_text":"lunch time",
    "tokens":[
      "lunch",
      "time"
    ],
    "token_count":2,
    "processed_text":"lunch time"
  },
  {
    "label":4,
    "text":"succ",
    "cleaned_text":"succ",
    "normalized_text":"succ",
    "tokens":[
      "succ"
    ],
    "token_count":1,
    "processed_text":"succ"
  },
  {
    "label":0,
    "text":"take long hibbi get readi morn",
    "cleaned_text":"take long hibbi get readi morn",
    "normalized_text":"take long hibbi get readi morn",
    "tokens":[
      "take",
      "long",
      "hibbi",
      "get",
      "readi",
      "morn"
    ],
    "token_count":6,
    "processed_text":"take long hibbi get readi morn"
  },
  {
    "label":4,
    "text":"ill visit head minotaur saturday btw",
    "cleaned_text":"ill visit head minotaur saturday btw",
    "normalized_text":"ill visit head minotaur saturday btw",
    "tokens":[
      "ill",
      "visit",
      "head",
      "minotaur",
      "saturday",
      "btw"
    ],
    "token_count":6,
    "processed_text":"ill visit head minotaur saturday btw"
  },
  {
    "label":4,
    "text":"mornig",
    "cleaned_text":"mornig",
    "normalized_text":"mornig",
    "tokens":[
      "mornig"
    ],
    "token_count":1,
    "processed_text":"mornig"
  },
  {
    "label":4,
    "text":"day good omg feel like ive neglect yall amp twitter today lmao im still tri figur go",
    "cleaned_text":"day good omg feel like ive neglect yall amp twitter today lmao im still tri figur go",
    "normalized_text":"day good omg feel like ive neglect yall amp twitter today lmao im still tri figur go",
    "tokens":[
      "day",
      "good",
      "omg",
      "feel",
      "like",
      "ive",
      "neglect",
      "yall",
      "amp",
      "twitter",
      "today",
      "lmao",
      "im",
      "still",
      "tri",
      "figur",
      "go"
    ],
    "token_count":17,
    "processed_text":"day good omg feel like ive neglect yall amp twitter today lmao im still tri figur go"
  },
  {
    "label":0,
    "text":"miss lot peopl",
    "cleaned_text":"miss lot peopl",
    "normalized_text":"miss lot peopl",
    "tokens":[
      "miss",
      "lot",
      "peopl"
    ],
    "token_count":3,
    "processed_text":"miss lot peopl"
  },
  {
    "label":4,
    "text":"wowowo look good tracey",
    "cleaned_text":"wowowo look good tracey",
    "normalized_text":"wowowo look good tracey",
    "tokens":[
      "wowowo",
      "look",
      "good",
      "tracey"
    ],
    "token_count":4,
    "processed_text":"wowowo look good tracey"
  },
  {
    "label":0,
    "text":"eww dislik",
    "cleaned_text":"eww dislik",
    "normalized_text":"eww dislik",
    "tokens":[
      "eww",
      "dislik"
    ],
    "token_count":2,
    "processed_text":"eww dislik"
  },
  {
    "label":0,
    "text":"home dont know domi arm hurt hate shot",
    "cleaned_text":"home dont know domi arm hurt hate shot",
    "normalized_text":"home dont know domi arm hurt hate shot",
    "tokens":[
      "home",
      "dont",
      "know",
      "domi",
      "arm",
      "hurt",
      "hate",
      "shot"
    ],
    "token_count":8,
    "processed_text":"home dont know domi arm hurt hate shot"
  },
  {
    "label":4,
    "text":"look forward listen fit entertain valu normal podcast realli appreci prep work etc",
    "cleaned_text":"look forward listen fit entertain valu normal podcast realli appreci prep work etc",
    "normalized_text":"look forward listen fit entertain valu normal podcast realli appreci prep work etc",
    "tokens":[
      "look",
      "forward",
      "listen",
      "fit",
      "entertain",
      "valu",
      "normal",
      "podcast",
      "realli",
      "appreci",
      "prep",
      "work",
      "etc"
    ],
    "token_count":13,
    "processed_text":"look forward listen fit entertain valu normal podcast realli appreci prep work etc"
  },
  {
    "label":0,
    "text":"heard hubbi laugh way vega one wretch beat ever",
    "cleaned_text":"heard hubbi laugh way vega one wretch beat ever",
    "normalized_text":"heard hubbi laugh way vega one wretch beat ever",
    "tokens":[
      "heard",
      "hubbi",
      "laugh",
      "way",
      "vega",
      "one",
      "wretch",
      "beat",
      "ever"
    ],
    "token_count":9,
    "processed_text":"heard hubbi laugh way vega one wretch beat ever"
  },
  {
    "label":0,
    "text":"without",
    "cleaned_text":"without",
    "normalized_text":"without",
    "tokens":[
      "without"
    ],
    "token_count":1,
    "processed_text":"without"
  },
  {
    "label":4,
    "text":"nake",
    "cleaned_text":"nake",
    "normalized_text":"nake",
    "tokens":[
      "nake"
    ],
    "token_count":1,
    "processed_text":"nake"
  },
  {
    "label":0,
    "text":"miss peoplegoodby leagueim gonna miss u ppl",
    "cleaned_text":"miss peoplegoodby leagueim gonna miss u ppl",
    "normalized_text":"miss peoplegoodby leagueim gonna miss u ppl",
    "tokens":[
      "miss",
      "peoplegoodbi",
      "leagueim",
      "gon",
      "na",
      "miss",
      "ppl"
    ],
    "token_count":7,
    "processed_text":"miss peoplegoodbi leagueim gon na miss ppl"
  },
  {
    "label":4,
    "text":"pleas come back soon david love",
    "cleaned_text":"pleas come back soon david love",
    "normalized_text":"pleas come back soon david love",
    "tokens":[
      "plea",
      "come",
      "back",
      "soon",
      "david",
      "love"
    ],
    "token_count":6,
    "processed_text":"plea come back soon david love"
  },
  {
    "label":0,
    "text":"harder still miss game entir like",
    "cleaned_text":"harder still miss game entir like",
    "normalized_text":"harder still miss game entir like",
    "tokens":[
      "harder",
      "still",
      "miss",
      "game",
      "entir",
      "like"
    ],
    "token_count":6,
    "processed_text":"harder still miss game entir like"
  },
  {
    "label":4,
    "text":"thank stress imagin thank",
    "cleaned_text":"thank stress imagin thank",
    "normalized_text":"thank stress imagin thank",
    "tokens":[
      "thank",
      "stress",
      "imagin",
      "thank"
    ],
    "token_count":4,
    "processed_text":"thank stress imagin thank"
  },
  {
    "label":0,
    "text":"dont even know honestli hope fix time get back itll suck stuck home month",
    "cleaned_text":"dont even know honestli hope fix time get back itll suck stuck home month",
    "normalized_text":"dont even know honestli hope fix time get back itll suck stuck home month",
    "tokens":[
      "dont",
      "even",
      "know",
      "honestli",
      "hope",
      "fix",
      "time",
      "get",
      "back",
      "itll",
      "suck",
      "stuck",
      "home",
      "month"
    ],
    "token_count":14,
    "processed_text":"dont even know honestli hope fix time get back itll suck stuck home month"
  },
  {
    "label":4,
    "text":"ye realli itun",
    "cleaned_text":"ye realli itun",
    "normalized_text":"ye realli itun",
    "tokens":[
      "ye",
      "realli",
      "itun"
    ],
    "token_count":3,
    "processed_text":"ye realli itun"
  },
  {
    "label":4,
    "text":"home",
    "cleaned_text":"home",
    "normalized_text":"home",
    "tokens":[
      "home"
    ],
    "token_count":1,
    "processed_text":"home"
  },
  {
    "label":4,
    "text":"safe trip back lt",
    "cleaned_text":"safe trip back lt",
    "normalized_text":"safe trip back lt",
    "tokens":[
      "safe",
      "trip",
      "back",
      "lt"
    ],
    "token_count":4,
    "processed_text":"safe trip back lt"
  },
  {
    "label":0,
    "text":"hahha nice",
    "cleaned_text":"hahha nice",
    "normalized_text":"hahha nice",
    "tokens":[
      "hahha",
      "nice"
    ],
    "token_count":2,
    "processed_text":"hahha nice"
  },
  {
    "label":0,
    "text":"watch jon kate plu eight sad kid go",
    "cleaned_text":"watch jon kate plu eight sad kid go",
    "normalized_text":"watch jon kate plu eight sad kid go",
    "tokens":[
      "watch",
      "jon",
      "kate",
      "plu",
      "eight",
      "sad",
      "kid",
      "go"
    ],
    "token_count":8,
    "processed_text":"watch jon kate plu eight sad kid go"
  },
  {
    "label":0,
    "text":"tri say seem like peopl facebook dont agre guess fail cri",
    "cleaned_text":"tri say seem like peopl facebook dont agre guess fail cri",
    "normalized_text":"tri say seem like peopl facebook dont agre guess fail cri",
    "tokens":[
      "tri",
      "say",
      "seem",
      "like",
      "peopl",
      "facebook",
      "dont",
      "agr",
      "guess",
      "fail",
      "cri"
    ],
    "token_count":11,
    "processed_text":"tri say seem like peopl facebook dont agr guess fail cri"
  },
  {
    "label":0,
    "text":"sit wait wish cold",
    "cleaned_text":"sit wait wish cold",
    "normalized_text":"sit wait wish cold",
    "tokens":[
      "sit",
      "wait",
      "wish",
      "cold"
    ],
    "token_count":4,
    "processed_text":"sit wait wish cold"
  },
  {
    "label":0,
    "text":"bet hate bloomin mdx shiver thought write anoth queri",
    "cleaned_text":"bet hate bloomin mdx shiver thought write anoth queri",
    "normalized_text":"bet hate bloomin mdx shiver thought write anoth queri",
    "tokens":[
      "bet",
      "hate",
      "bloomin",
      "mdx",
      "shiver",
      "thought",
      "write",
      "anoth",
      "queri"
    ],
    "token_count":9,
    "processed_text":"bet hate bloomin mdx shiver thought write anoth queri"
  },
  {
    "label":0,
    "text":"twitter good insomniac",
    "cleaned_text":"twitter good insomniac",
    "normalized_text":"twitter good insomniac",
    "tokens":[
      "twitter",
      "good",
      "insomniac"
    ],
    "token_count":3,
    "processed_text":"twitter good insomniac"
  },
  {
    "label":0,
    "text":"actual wish stand booth hand fan card pocono right miss ask team",
    "cleaned_text":"actual wish stand booth hand fan card pocono right miss ask team",
    "normalized_text":"actual wish stand booth hand fan card pocono right miss ask team",
    "tokens":[
      "actual",
      "wish",
      "stand",
      "booth",
      "hand",
      "fan",
      "card",
      "pocono",
      "right",
      "miss",
      "ask",
      "team"
    ],
    "token_count":12,
    "processed_text":"actual wish stand booth hand fan card pocono right miss ask team"
  },
  {
    "label":4,
    "text":"what hashtag",
    "cleaned_text":"what hashtag",
    "normalized_text":"what hashtag",
    "tokens":[
      "hashtag"
    ],
    "token_count":1,
    "processed_text":"hashtag"
  },
  {
    "label":0,
    "text":"wifi star still tmobil",
    "cleaned_text":"wifi star still tmobil",
    "normalized_text":"wifi star still tmobil",
    "tokens":[
      "wifi",
      "star",
      "still",
      "tmobil"
    ],
    "token_count":4,
    "processed_text":"wifi star still tmobil"
  },
  {
    "label":0,
    "text":"twello foodi tweet spici spici spici lunch im much luck food",
    "cleaned_text":"twello foodi tweet spici spici spici lunch im much luck food",
    "normalized_text":"twello foodi tweet spici spici spici lunch im much luck food",
    "tokens":[
      "twello",
      "foodi",
      "tweet",
      "spici",
      "spici",
      "spici",
      "lunch",
      "im",
      "much",
      "luck",
      "food"
    ],
    "token_count":11,
    "processed_text":"twello foodi tweet spici spici spici lunch im much luck food"
  },
  {
    "label":4,
    "text":"quit disturbingbut real band bond momentrecord went well thinkwil let know hear premixnight",
    "cleaned_text":"quit disturbingbut real band bond momentrecord went well thinkwil let know hear premixnight",
    "normalized_text":"quit disturbingbut real band bond momentrecord went well thinkwil let know hear premixnight",
    "tokens":[
      "quit",
      "disturbingbut",
      "real",
      "band",
      "bond",
      "momentrecord",
      "went",
      "well",
      "thinkwil",
      "let",
      "know",
      "hear",
      "premixnight"
    ],
    "token_count":13,
    "processed_text":"quit disturbingbut real band bond momentrecord went well thinkwil let know hear premixnight"
  },
  {
    "label":0,
    "text":"nik loz gone",
    "cleaned_text":"nik loz gone",
    "normalized_text":"nik loz gone",
    "tokens":[
      "nik",
      "loz",
      "gone"
    ],
    "token_count":3,
    "processed_text":"nik loz gone"
  },
  {
    "label":0,
    "text":"yeah dont stair cours",
    "cleaned_text":"yeah dont stair cours",
    "normalized_text":"yeah dont stair cours",
    "tokens":[
      "yeah",
      "dont",
      "stair",
      "cour"
    ],
    "token_count":4,
    "processed_text":"yeah dont stair cour"
  },
  {
    "label":0,
    "text":"twitter suck without u soo guess ill go bed ive wait youuu goodknight jon lt",
    "cleaned_text":"twitter suck without u soo guess ill go bed ive wait youuu goodknight jon lt",
    "normalized_text":"twitter suck without u soo guess ill go bed ive wait youuu goodknight jon lt",
    "tokens":[
      "twitter",
      "suck",
      "without",
      "soo",
      "guess",
      "ill",
      "go",
      "bed",
      "ive",
      "wait",
      "youuu",
      "goodknight",
      "jon",
      "lt"
    ],
    "token_count":14,
    "processed_text":"twitter suck without soo guess ill go bed ive wait youuu goodknight jon lt"
  },
  {
    "label":0,
    "text":"ignor text boooo",
    "cleaned_text":"ignor text boooo",
    "normalized_text":"ignor text boooo",
    "tokens":[
      "ignor",
      "text",
      "boooo"
    ],
    "token_count":3,
    "processed_text":"ignor text boooo"
  },
  {
    "label":4,
    "text":"wait leno last hurrah hello conan love serious yey",
    "cleaned_text":"wait leno last hurrah hello conan love serious yey",
    "normalized_text":"wait leno last hurrah hello conan love serious yey",
    "tokens":[
      "wait",
      "leno",
      "last",
      "hurrah",
      "hello",
      "conan",
      "love",
      "seriou",
      "yey"
    ],
    "token_count":9,
    "processed_text":"wait leno last hurrah hello conan love seriou yey"
  },
  {
    "label":0,
    "text":"chase season nonexist season anybodi go pierr sd day possibl superecel far day hr away",
    "cleaned_text":"chase season nonexist season anybodi go pierr sd day possibl superecel far day hr away",
    "normalized_text":"chase season nonexist season anybodi go pierr sd day possibl superecel far day hr away",
    "tokens":[
      "chase",
      "season",
      "nonexist",
      "season",
      "anybodi",
      "go",
      "pierr",
      "sd",
      "day",
      "possibl",
      "superecel",
      "far",
      "day",
      "hr",
      "away"
    ],
    "token_count":15,
    "processed_text":"chase season nonexist season anybodi go pierr sd day possibl superecel far day hr away"
  },
  {
    "label":0,
    "text":"except want except webbas",
    "cleaned_text":"except want except webbas",
    "normalized_text":"except want except webbas",
    "tokens":[
      "except",
      "want",
      "except",
      "webba"
    ],
    "token_count":4,
    "processed_text":"except want except webba"
  },
  {
    "label":4,
    "text":"hey cali guy sound know ok",
    "cleaned_text":"hey cali guy sound know ok",
    "normalized_text":"hey cali guy sound know ok",
    "tokens":[
      "hey",
      "cali",
      "guy",
      "sound",
      "know",
      "ok"
    ],
    "token_count":6,
    "processed_text":"hey cali guy sound know ok"
  },
  {
    "label":4,
    "text":"love that awesom doubl awesom mr robl cmon senior year great youll love real soon",
    "cleaned_text":"love that awesom doubl awesom mr robl cmon senior year great youll love real soon",
    "normalized_text":"love that awesom doubl awesom mr robl cmon senior year great youll love real soon",
    "tokens":[
      "love",
      "awesom",
      "doubl",
      "awesom",
      "mr",
      "robl",
      "cmon",
      "senior",
      "year",
      "great",
      "youll",
      "love",
      "real",
      "soon"
    ],
    "token_count":14,
    "processed_text":"love awesom doubl awesom mr robl cmon senior year great youll love real soon"
  },
  {
    "label":4,
    "text":"write driver sensor new interest",
    "cleaned_text":"write driver sensor new interest",
    "normalized_text":"write driver sensor new interest",
    "tokens":[
      "write",
      "driver",
      "sensor",
      "new",
      "interest"
    ],
    "token_count":5,
    "processed_text":"write driver sensor new interest"
  },
  {
    "label":4,
    "text":"horribl back pain left earli work relax relax someth im good",
    "cleaned_text":"horribl back pain left earli work relax relax someth im good",
    "normalized_text":"horribl back pain left earli work relax relax someth im good",
    "tokens":[
      "horribl",
      "back",
      "pain",
      "left",
      "earli",
      "work",
      "relax",
      "relax",
      "someth",
      "im",
      "good"
    ],
    "token_count":11,
    "processed_text":"horribl back pain left earli work relax relax someth im good"
  },
  {
    "label":0,
    "text":"got appl store doncast holi shit mean say lost spew",
    "cleaned_text":"got appl store doncast holi shit mean say lost spew",
    "normalized_text":"got appl store doncast holi shit mean say lost spew",
    "tokens":[
      "got",
      "appl",
      "store",
      "doncast",
      "holi",
      "shit",
      "mean",
      "say",
      "lost",
      "spew"
    ],
    "token_count":10,
    "processed_text":"got appl store doncast holi shit mean say lost spew"
  },
  {
    "label":4,
    "text":"hello twitter",
    "cleaned_text":"hello twitter",
    "normalized_text":"hello twitter",
    "tokens":[
      "hello",
      "twitter"
    ],
    "token_count":2,
    "processed_text":"hello twitter"
  },
  {
    "label":0,
    "text":"need skin bold notic scratch mine fck",
    "cleaned_text":"need skin bold notic scratch mine fck",
    "normalized_text":"need skin bold notic scratch mine fck",
    "tokens":[
      "need",
      "skin",
      "bold",
      "notic",
      "scratch",
      "mine",
      "fck"
    ],
    "token_count":7,
    "processed_text":"need skin bold notic scratch mine fck"
  },
  {
    "label":0,
    "text":"rub saki doesnt come till th sept",
    "cleaned_text":"rub saki doesnt come till th sept",
    "normalized_text":"rub saki doesnt come till th sept",
    "tokens":[
      "rub",
      "saki",
      "doesnt",
      "come",
      "till",
      "th",
      "sept"
    ],
    "token_count":7,
    "processed_text":"rub saki doesnt come till th sept"
  },
  {
    "label":4,
    "text":"ps venceram menina eu fiz um twitter ok",
    "cleaned_text":"ps venceram menina eu fiz um twitter ok",
    "normalized_text":"ps venceram menina eu fiz um twitter ok",
    "tokens":[
      "ps",
      "venceram",
      "menina",
      "eu",
      "fiz",
      "um",
      "twitter",
      "ok"
    ],
    "token_count":8,
    "processed_text":"ps venceram menina eu fiz um twitter ok"
  },
  {
    "label":4,
    "text":"wow god bless bet everyon realli proud",
    "cleaned_text":"wow god bless bet everyon realli proud",
    "normalized_text":"wow god bless bet everyon realli proud",
    "tokens":[
      "wow",
      "god",
      "bless",
      "bet",
      "everyon",
      "realli",
      "proud"
    ],
    "token_count":7,
    "processed_text":"wow god bless bet everyon realli proud"
  },
  {
    "label":4,
    "text":"go bed earli got mucho homework beauti stuff tomorow hair color night twit",
    "cleaned_text":"go bed earli got mucho homework beauti stuff tomorow hair color night twit",
    "normalized_text":"go bed earli got mucho homework beauti stuff tomorow hair color night twit",
    "tokens":[
      "go",
      "bed",
      "earli",
      "got",
      "mucho",
      "homework",
      "beauti",
      "stuff",
      "tomorow",
      "hair",
      "color",
      "night",
      "twit"
    ],
    "token_count":13,
    "processed_text":"go bed earli got mucho homework beauti stuff tomorow hair color night twit"
  },
  {
    "label":4,
    "text":"anytim hear leav weekend believ safe fun know caus hey britney",
    "cleaned_text":"anytim hear leav weekend believ safe fun know caus hey britney",
    "normalized_text":"anytim hear leav weekend believ safe fun know caus hey britney",
    "tokens":[
      "anytim",
      "hear",
      "leav",
      "weekend",
      "believ",
      "safe",
      "fun",
      "know",
      "cau",
      "hey",
      "britney"
    ],
    "token_count":11,
    "processed_text":"anytim hear leav weekend believ safe fun know cau hey britney"
  },
  {
    "label":0,
    "text":"feel bit emot day dont know got mnth review afternoon feel bit stress incas emot take",
    "cleaned_text":"feel bit emot day dont know got mnth review afternoon feel bit stress incas emot take",
    "normalized_text":"feel bit emot day dont know got mnth review afternoon feel bit stress incas emot take",
    "tokens":[
      "feel",
      "bit",
      "emot",
      "day",
      "dont",
      "know",
      "got",
      "mnth",
      "review",
      "afternoon",
      "feel",
      "bit",
      "stress",
      "inca",
      "emot",
      "take"
    ],
    "token_count":16,
    "processed_text":"feel bit emot day dont know got mnth review afternoon feel bit stress inca emot take"
  },
  {
    "label":4,
    "text":"wonder weekend big climb next weekend",
    "cleaned_text":"wonder weekend big climb next weekend",
    "normalized_text":"wonder weekend big climb next weekend",
    "tokens":[
      "wonder",
      "weekend",
      "big",
      "climb",
      "next",
      "weekend"
    ],
    "token_count":6,
    "processed_text":"wonder weekend big climb next weekend"
  },
  {
    "label":4,
    "text":"addict everyon crave",
    "cleaned_text":"addict everyon crave",
    "normalized_text":"addict everyon crave",
    "tokens":[
      "addict",
      "everyon",
      "crave"
    ],
    "token_count":3,
    "processed_text":"addict everyon crave"
  },
  {
    "label":4,
    "text":"one quotmyquot research speak im proud",
    "cleaned_text":"one quotmyquot research speak im proud",
    "normalized_text":"one quotmyquot research speak im proud",
    "tokens":[
      "one",
      "quotmyquot",
      "research",
      "speak",
      "im",
      "proud"
    ],
    "token_count":6,
    "processed_text":"one quotmyquot research speak im proud"
  },
  {
    "label":0,
    "text":"last day vacat",
    "cleaned_text":"last day vacat",
    "normalized_text":"last day vacat",
    "tokens":[
      "last",
      "day",
      "vacat"
    ],
    "token_count":3,
    "processed_text":"last day vacat"
  },
  {
    "label":4,
    "text":"use strlen c ahh googl",
    "cleaned_text":"use strlen c ahh googl",
    "normalized_text":"use strlen c ahh googl",
    "tokens":[
      "use",
      "strlen",
      "ahh",
      "googl"
    ],
    "token_count":4,
    "processed_text":"use strlen ahh googl"
  },
  {
    "label":0,
    "text":"feel less solvent right",
    "cleaned_text":"feel less solvent right",
    "normalized_text":"feel less solvent right",
    "tokens":[
      "feel",
      "less",
      "solvent",
      "right"
    ],
    "token_count":4,
    "processed_text":"feel less solvent right"
  },
  {
    "label":0,
    "text":"miss nantucket dearli also miss brooklyn dearli hard torn",
    "cleaned_text":"miss nantucket dearli also miss brooklyn dearli hard torn",
    "normalized_text":"miss nantucket dearli also miss brooklyn dearli hard torn",
    "tokens":[
      "miss",
      "nantucket",
      "dearli",
      "also",
      "miss",
      "brooklyn",
      "dearli",
      "hard",
      "torn"
    ],
    "token_count":9,
    "processed_text":"miss nantucket dearli also miss brooklyn dearli hard torn"
  },
  {
    "label":0,
    "text":"plan yesterday night didnt work like id plan",
    "cleaned_text":"plan yesterday night didnt work like id plan",
    "normalized_text":"plan yesterday night didnt work like id plan",
    "tokens":[
      "plan",
      "yesterday",
      "night",
      "didnt",
      "work",
      "like",
      "id",
      "plan"
    ],
    "token_count":8,
    "processed_text":"plan yesterday night didnt work like id plan"
  },
  {
    "label":0,
    "text":"huge zit pop chinand hurt",
    "cleaned_text":"huge zit pop chinand hurt",
    "normalized_text":"huge zit pop chinand hurt",
    "tokens":[
      "huge",
      "zit",
      "pop",
      "chinand",
      "hurt"
    ],
    "token_count":5,
    "processed_text":"huge zit pop chinand hurt"
  },
  {
    "label":4,
    "text":"laker sure thing win championship dwight frickin beast though might tough",
    "cleaned_text":"laker sure thing win championship dwight frickin beast though might tough",
    "normalized_text":"laker sure thing win championship dwight frickin beast though might tough",
    "tokens":[
      "laker",
      "sure",
      "thing",
      "win",
      "championship",
      "dwight",
      "frickin",
      "beast",
      "though",
      "tough"
    ],
    "token_count":10,
    "processed_text":"laker sure thing win championship dwight frickin beast though tough"
  },
  {
    "label":4,
    "text":"happi anniversari hope great day asid kid",
    "cleaned_text":"happi anniversari hope great day asid kid",
    "normalized_text":"happi anniversari hope great day asid kid",
    "tokens":[
      "happi",
      "anniversari",
      "hope",
      "great",
      "day",
      "asid",
      "kid"
    ],
    "token_count":7,
    "processed_text":"happi anniversari hope great day asid kid"
  },
  {
    "label":0,
    "text":"hmm miss missyy ohh wow",
    "cleaned_text":"hmm miss missyy ohh wow",
    "normalized_text":"hmm miss missyy ohh wow",
    "tokens":[
      "hmm",
      "miss",
      "missyy",
      "ohh",
      "wow"
    ],
    "token_count":5,
    "processed_text":"hmm miss missyy ohh wow"
  },
  {
    "label":4,
    "text":"realli doubt mathematica heart core wolframalpha",
    "cleaned_text":"realli doubt mathematica heart core wolframalpha",
    "normalized_text":"realli doubt mathematica heart core wolframalpha",
    "tokens":[
      "realli",
      "doubt",
      "mathematica",
      "heart",
      "core",
      "wolframalpha"
    ],
    "token_count":6,
    "processed_text":"realli doubt mathematica heart core wolframalpha"
  },
  {
    "label":4,
    "text":"ive neuter sure stop",
    "cleaned_text":"ive neuter sure stop",
    "normalized_text":"ive neuter sure stop",
    "tokens":[
      "ive",
      "neuter",
      "sure",
      "stop"
    ],
    "token_count":4,
    "processed_text":"ive neuter sure stop"
  },
  {
    "label":4,
    "text":"watch new crank didnt like",
    "cleaned_text":"watch new crank didnt like",
    "normalized_text":"watch new crank didnt like",
    "tokens":[
      "watch",
      "new",
      "crank",
      "didnt",
      "like"
    ],
    "token_count":5,
    "processed_text":"watch new crank didnt like"
  },
  {
    "label":4,
    "text":"dear explod oki doki good night",
    "cleaned_text":"dear explod oki doki good night",
    "normalized_text":"dear explod oki doki good night",
    "tokens":[
      "dear",
      "explod",
      "oki",
      "doki",
      "good",
      "night"
    ],
    "token_count":6,
    "processed_text":"dear explod oki doki good night"
  },
  {
    "label":0,
    "text":"hubbi couldnt sleep turn tv woke fall asleep im wide awak",
    "cleaned_text":"hubbi couldnt sleep turn tv woke fall asleep im wide awak",
    "normalized_text":"hubbi couldnt sleep turn tv woke fall asleep im wide awak",
    "tokens":[
      "hubbi",
      "couldnt",
      "sleep",
      "turn",
      "tv",
      "woke",
      "fall",
      "asleep",
      "im",
      "wide",
      "awak"
    ],
    "token_count":11,
    "processed_text":"hubbi couldnt sleep turn tv woke fall asleep im wide awak"
  },
  {
    "label":0,
    "text":"doh look forward possibl talk",
    "cleaned_text":"doh look forward possibl talk",
    "normalized_text":"doh look forward possibl talk",
    "tokens":[
      "doh",
      "look",
      "forward",
      "possibl",
      "talk"
    ],
    "token_count":5,
    "processed_text":"doh look forward possibl talk"
  },
  {
    "label":0,
    "text":"want watch movi empir badli",
    "cleaned_text":"want watch movi empir badli",
    "normalized_text":"want watch movi empir badli",
    "tokens":[
      "want",
      "watch",
      "movi",
      "empir",
      "badli"
    ],
    "token_count":5,
    "processed_text":"want watch movi empir badli"
  },
  {
    "label":4,
    "text":"shnizzl tidi",
    "cleaned_text":"shnizzl tidi",
    "normalized_text":"shnizzl tidi",
    "tokens":[
      "shnizzl",
      "tidi"
    ],
    "token_count":2,
    "processed_text":"shnizzl tidi"
  },
  {
    "label":4,
    "text":"today basic perfectfound got job got suit ate twice min lol ime friend back home",
    "cleaned_text":"today basic perfectfound got job got suit ate twice min lol ime friend back home",
    "normalized_text":"today basic perfectfound got job got suit ate twice min lol ime friend back home",
    "tokens":[
      "today",
      "basic",
      "perfectfound",
      "got",
      "job",
      "got",
      "suit",
      "ate",
      "twice",
      "min",
      "lol",
      "ime",
      "friend",
      "back",
      "home"
    ],
    "token_count":15,
    "processed_text":"today basic perfectfound got job got suit ate twice min lol ime friend back home"
  },
  {
    "label":0,
    "text":"stupid pocket watch decid stop grrr",
    "cleaned_text":"stupid pocket watch decid stop grrr",
    "normalized_text":"stupid pocket watch decid stop grrr",
    "tokens":[
      "stupid",
      "pocket",
      "watch",
      "decid",
      "stop",
      "grrr"
    ],
    "token_count":6,
    "processed_text":"stupid pocket watch decid stop grrr"
  },
  {
    "label":0,
    "text":"thought set dvr record watch tori dean",
    "cleaned_text":"thought set dvr record watch tori dean",
    "normalized_text":"thought set dvr record watch tori dean",
    "tokens":[
      "thought",
      "set",
      "dvr",
      "record",
      "watch",
      "tori",
      "dean"
    ],
    "token_count":7,
    "processed_text":"thought set dvr record watch tori dean"
  },
  {
    "label":4,
    "text":"polit voter say",
    "cleaned_text":"polit voter say",
    "normalized_text":"polit voter say",
    "tokens":[
      "polit",
      "voter",
      "say"
    ],
    "token_count":3,
    "processed_text":"polit voter say"
  },
  {
    "label":0,
    "text":"love song",
    "cleaned_text":"love song",
    "normalized_text":"love song",
    "tokens":[
      "love",
      "song"
    ],
    "token_count":2,
    "processed_text":"love song"
  },
  {
    "label":4,
    "text":"ya slept like hour listen twilight scorewhich relax btw never slept good awhil",
    "cleaned_text":"ya slept like hour listen twilight scorewhich relax btw never slept good awhil",
    "normalized_text":"ya slept like hour listen twilight scorewhich relax btw never slept good awhil",
    "tokens":[
      "ya",
      "slept",
      "like",
      "hour",
      "listen",
      "twilight",
      "scorewhich",
      "relax",
      "btw",
      "never",
      "slept",
      "good",
      "awhil"
    ],
    "token_count":13,
    "processed_text":"ya slept like hour listen twilight scorewhich relax btw never slept good awhil"
  },
  {
    "label":4,
    "text":"uhmm actual thing xd eat someth oo cooki yummi yummi",
    "cleaned_text":"uhmm actual thing xd eat someth oo cooki yummi yummi",
    "normalized_text":"uhmm actual thing xd eat someth oo cooki yummi yummi",
    "tokens":[
      "uhmm",
      "actual",
      "thing",
      "xd",
      "eat",
      "someth",
      "oo",
      "cooki",
      "yummi",
      "yummi"
    ],
    "token_count":10,
    "processed_text":"uhmm actual thing xd eat someth oo cooki yummi yummi"
  },
  {
    "label":0,
    "text":"sad noth",
    "cleaned_text":"sad noth",
    "normalized_text":"sad noth",
    "tokens":[
      "sad",
      "noth"
    ],
    "token_count":2,
    "processed_text":"sad noth"
  },
  {
    "label":0,
    "text":"bang head top bunk hard think im tweet dead",
    "cleaned_text":"bang head top bunk hard think im tweet dead",
    "normalized_text":"bang head top bunk hard think im tweet dead",
    "tokens":[
      "bang",
      "head",
      "top",
      "bunk",
      "hard",
      "think",
      "im",
      "tweet",
      "dead"
    ],
    "token_count":9,
    "processed_text":"bang head top bunk hard think im tweet dead"
  },
  {
    "label":4,
    "text":"appar pluto scorpio amp gener interest gender issu",
    "cleaned_text":"appar pluto scorpio amp gener interest gender issu",
    "normalized_text":"appar pluto scorpio amp gener interest gender issu",
    "tokens":[
      "appar",
      "pluto",
      "scorpio",
      "amp",
      "gener",
      "interest",
      "gender",
      "issu"
    ],
    "token_count":8,
    "processed_text":"appar pluto scorpio amp gener interest gender issu"
  },
  {
    "label":4,
    "text":"vampir weekend rockkkssssssssssss",
    "cleaned_text":"vampir weekend rockkkssssssssssss",
    "normalized_text":"vampir weekend rockkkssssssssssss",
    "tokens":[
      "vampir",
      "weekend"
    ],
    "token_count":2,
    "processed_text":"vampir weekend"
  },
  {
    "label":0,
    "text":"final watch offic definit best one long time poor kevin ill make chili okay buddi",
    "cleaned_text":"final watch offic definit best one long time poor kevin ill make chili okay buddi",
    "normalized_text":"final watch offic definit best one long time poor kevin ill make chili okay buddi",
    "tokens":[
      "final",
      "watch",
      "offic",
      "definit",
      "best",
      "one",
      "long",
      "time",
      "poor",
      "kevin",
      "ill",
      "make",
      "chili",
      "okay",
      "buddi"
    ],
    "token_count":15,
    "processed_text":"final watch offic definit best one long time poor kevin ill make chili okay buddi"
  },
  {
    "label":0,
    "text":"uh oh im worri love busdriv amp look forward new record eeep",
    "cleaned_text":"uh oh im worri love busdriv amp look forward new record eeep",
    "normalized_text":"uh oh im worri love busdriv amp look forward new record eeep",
    "tokens":[
      "uh",
      "oh",
      "im",
      "worri",
      "love",
      "busdriv",
      "amp",
      "look",
      "forward",
      "new",
      "record",
      "eeep"
    ],
    "token_count":12,
    "processed_text":"uh oh im worri love busdriv amp look forward new record eeep"
  },
  {
    "label":0,
    "text":"would nice ficli quotaboutquot page actual told us histori ppl creat",
    "cleaned_text":"would nice ficli quotaboutquot page actual told us histori ppl creat",
    "normalized_text":"would nice ficli quotaboutquot page actual told us histori ppl creat",
    "tokens":[
      "nice",
      "ficli",
      "quotaboutquot",
      "page",
      "actual",
      "told",
      "us",
      "histori",
      "ppl",
      "creat"
    ],
    "token_count":10,
    "processed_text":"nice ficli quotaboutquot page actual told us histori ppl creat"
  },
  {
    "label":0,
    "text":"great apar wait month get nose repierc co cant find nose stud start close alreadi",
    "cleaned_text":"great apar wait month get nose repierc co cant find nose stud start close alreadi",
    "normalized_text":"great apar wait month get nose repierc co cant find nose stud start close alreadi",
    "tokens":[
      "great",
      "apar",
      "wait",
      "month",
      "get",
      "nose",
      "repierc",
      "co",
      "cant",
      "find",
      "nose",
      "stud",
      "start",
      "close",
      "alreadi"
    ],
    "token_count":15,
    "processed_text":"great apar wait month get nose repierc co cant find nose stud start close alreadi"
  },
  {
    "label":4,
    "text":"awesom plan meet goal end year",
    "cleaned_text":"awesom plan meet goal end year",
    "normalized_text":"awesom plan meet goal end year",
    "tokens":[
      "awesom",
      "plan",
      "meet",
      "goal",
      "end",
      "year"
    ],
    "token_count":6,
    "processed_text":"awesom plan meet goal end year"
  },
  {
    "label":4,
    "text":"awesom get finger warm",
    "cleaned_text":"awesom get finger warm",
    "normalized_text":"awesom get finger warm",
    "tokens":[
      "awesom",
      "get",
      "finger",
      "warm"
    ],
    "token_count":4,
    "processed_text":"awesom get finger warm"
  },
  {
    "label":4,
    "text":"heheheh problem least gue jd tau ada fb chat di adium thank",
    "cleaned_text":"heheheh problem least gue jd tau ada fb chat di adium thank",
    "normalized_text":"heheheh problem least gue jd tau ada fb chat di adium thank",
    "tokens":[
      "heheheh",
      "problem",
      "least",
      "gue",
      "jd",
      "tau",
      "ada",
      "fb",
      "chat",
      "di",
      "adium",
      "thank"
    ],
    "token_count":12,
    "processed_text":"heheheh problem least gue jd tau ada fb chat di adium thank"
  },
  {
    "label":0,
    "text":"dnt like rain friday",
    "cleaned_text":"dnt like rain friday",
    "normalized_text":"dnt like rain friday",
    "tokens":[
      "dnt",
      "like",
      "rain",
      "friday"
    ],
    "token_count":4,
    "processed_text":"dnt like rain friday"
  },
  {
    "label":4,
    "text":"perfect place read beauti creatur yumeat bbq",
    "cleaned_text":"perfect place read beauti creatur yumeat bbq",
    "normalized_text":"perfect place read beauti creatur yumeat bbq",
    "tokens":[
      "perfect",
      "place",
      "read",
      "beauti",
      "creatur",
      "yumeat",
      "bbq"
    ],
    "token_count":7,
    "processed_text":"perfect place read beauti creatur yumeat bbq"
  },
  {
    "label":4,
    "text":"love nintendo dsi matter fact im use send twitter",
    "cleaned_text":"love nintendo dsi matter fact im use send twitter",
    "normalized_text":"love nintendo dsi matter fact im use send twitter",
    "tokens":[
      "love",
      "nintendo",
      "dsi",
      "matter",
      "fact",
      "im",
      "use",
      "send",
      "twitter"
    ],
    "token_count":9,
    "processed_text":"love nintendo dsi matter fact im use send twitter"
  },
  {
    "label":0,
    "text":"ill pray",
    "cleaned_text":"ill pray",
    "normalized_text":"ill pray",
    "tokens":[
      "ill",
      "pray"
    ],
    "token_count":2,
    "processed_text":"ill pray"
  },
  {
    "label":4,
    "text":"simpli point domain server info",
    "cleaned_text":"simpli point domain server info",
    "normalized_text":"simpli point domain server info",
    "tokens":[
      "simpli",
      "point",
      "domain",
      "server",
      "info"
    ],
    "token_count":5,
    "processed_text":"simpli point domain server info"
  },
  {
    "label":0,
    "text":"upset saw realli awesom ed hardi shoe didnt size",
    "cleaned_text":"upset saw realli awesom ed hardi shoe didnt size",
    "normalized_text":"upset saw realli awesom ed hardi shoe didnt size",
    "tokens":[
      "upset",
      "saw",
      "realli",
      "awesom",
      "ed",
      "hardi",
      "shoe",
      "didnt",
      "size"
    ],
    "token_count":9,
    "processed_text":"upset saw realli awesom ed hardi shoe didnt size"
  },
  {
    "label":0,
    "text":"aww hun your much luck sleep either work bound tire readi zzzz later xxxx",
    "cleaned_text":"aww hun your much luck sleep either work bound tire readi zzzz later xxxx",
    "normalized_text":"aww hun your much luck sleep either work bound tire readi zzzz later xxxx",
    "tokens":[
      "aww",
      "hun",
      "much",
      "luck",
      "sleep",
      "either",
      "work",
      "bound",
      "tire",
      "readi",
      "zzzz",
      "later",
      "xxxx"
    ],
    "token_count":13,
    "processed_text":"aww hun much luck sleep either work bound tire readi zzzz later xxxx"
  },
  {
    "label":4,
    "text":"listen nickasaur im realli right reason",
    "cleaned_text":"listen nickasaur im realli right reason",
    "normalized_text":"listen nickasaur im realli right reason",
    "tokens":[
      "listen",
      "nickasaur",
      "im",
      "realli",
      "right",
      "reason"
    ],
    "token_count":6,
    "processed_text":"listen nickasaur im realli right reason"
  },
  {
    "label":0,
    "text":"well aint lookin good",
    "cleaned_text":"well aint lookin good",
    "normalized_text":"well aint lookin good",
    "tokens":[
      "well",
      "aint",
      "lookin",
      "good"
    ],
    "token_count":4,
    "processed_text":"well aint lookin good"
  },
  {
    "label":4,
    "text":"yay new follow wassup yall",
    "cleaned_text":"yay new follow wassup yall",
    "normalized_text":"yay new follow wassup yall",
    "tokens":[
      "yay",
      "new",
      "follow",
      "wassup",
      "yall"
    ],
    "token_count":5,
    "processed_text":"yay new follow wassup yall"
  },
  {
    "label":4,
    "text":"danish quotrytmequot",
    "cleaned_text":"danish quotrytmequot",
    "normalized_text":"danish quotrytmequot",
    "tokens":[
      "danish",
      "quotrytmequot"
    ],
    "token_count":2,
    "processed_text":"danish quotrytmequot"
  },
  {
    "label":4,
    "text":"check new haircut twitpic haha",
    "cleaned_text":"check new haircut twitpic haha",
    "normalized_text":"check new haircut twitpic haha",
    "tokens":[
      "check",
      "new",
      "haircut",
      "twitpic",
      "haha"
    ],
    "token_count":5,
    "processed_text":"check new haircut twitpic haha"
  },
  {
    "label":4,
    "text":"ohh well suppos could wors could write someth know noth whatsoev haha",
    "cleaned_text":"ohh well suppos could wors could write someth know noth whatsoev haha",
    "normalized_text":"ohh well suppos could wors could write someth know noth whatsoev haha",
    "tokens":[
      "ohh",
      "well",
      "suppo",
      "wor",
      "write",
      "someth",
      "know",
      "noth",
      "whatsoev",
      "haha"
    ],
    "token_count":10,
    "processed_text":"ohh well suppo wor write someth know noth whatsoev haha"
  },
  {
    "label":4,
    "text":"last night event laugh longgg time tonight get see whole crew",
    "cleaned_text":"last night event laugh longgg time tonight get see whole crew",
    "normalized_text":"last night event laugh longgg time tonight get see whole crew",
    "tokens":[
      "last",
      "night",
      "event",
      "laugh",
      "longgg",
      "time",
      "tonight",
      "get",
      "see",
      "whole",
      "crew"
    ],
    "token_count":11,
    "processed_text":"last night event laugh longgg time tonight get see whole crew"
  },
  {
    "label":0,
    "text":"hahaha wish think there someth wrong phone",
    "cleaned_text":"hahaha wish think there someth wrong phone",
    "normalized_text":"hahaha wish think there someth wrong phone",
    "tokens":[
      "hahaha",
      "wish",
      "think",
      "someth",
      "wrong",
      "phone"
    ],
    "token_count":6,
    "processed_text":"hahaha wish think someth wrong phone"
  },
  {
    "label":4,
    "text":"im mom year old babi girl found im preg number",
    "cleaned_text":"im mom year old babi girl found im preg number",
    "normalized_text":"im mom year old babi girl found im preg number",
    "tokens":[
      "im",
      "mom",
      "year",
      "old",
      "babi",
      "girl",
      "found",
      "im",
      "preg",
      "number"
    ],
    "token_count":10,
    "processed_text":"im mom year old babi girl found im preg number"
  },
  {
    "label":4,
    "text":"ok ill take",
    "cleaned_text":"ok ill take",
    "normalized_text":"ok ill take",
    "tokens":[
      "ok",
      "ill",
      "take"
    ],
    "token_count":3,
    "processed_text":"ok ill take"
  },
  {
    "label":4,
    "text":"listen music",
    "cleaned_text":"listen music",
    "normalized_text":"listen music",
    "tokens":[
      "listen",
      "music"
    ],
    "token_count":2,
    "processed_text":"listen music"
  },
  {
    "label":4,
    "text":"len test",
    "cleaned_text":"len test",
    "normalized_text":"len test",
    "tokens":[
      "len",
      "test"
    ],
    "token_count":2,
    "processed_text":"len test"
  },
  {
    "label":4,
    "text":"listen god aw cbeebi programm lenri henri amaz dudley accent",
    "cleaned_text":"listen god aw cbeebi programm lenri henri amaz dudley accent",
    "normalized_text":"listen god aw cbeebi programm lenri henri amaz dudley accent",
    "tokens":[
      "listen",
      "god",
      "aw",
      "cbeebi",
      "programm",
      "lenri",
      "henri",
      "amaz",
      "dudley",
      "accent"
    ],
    "token_count":10,
    "processed_text":"listen god aw cbeebi programm lenri henri amaz dudley accent"
  },
  {
    "label":0,
    "text":"thought said trophi",
    "cleaned_text":"thought said trophi",
    "normalized_text":"thought said trophi",
    "tokens":[
      "thought",
      "said",
      "trophi"
    ],
    "token_count":3,
    "processed_text":"thought said trophi"
  },
  {
    "label":0,
    "text":"want watch ladi gaga concert august soooo bad hope theyll chang date aug tuesday",
    "cleaned_text":"want watch ladi gaga concert august soooo bad hope theyll chang date aug tuesday",
    "normalized_text":"want watch ladi gaga concert august soooo bad hope theyll chang date aug tuesday",
    "tokens":[
      "want",
      "watch",
      "ladi",
      "gaga",
      "concert",
      "august",
      "soooo",
      "bad",
      "hope",
      "theyll",
      "chang",
      "date",
      "aug",
      "tuesday"
    ],
    "token_count":14,
    "processed_text":"want watch ladi gaga concert august soooo bad hope theyll chang date aug tuesday"
  },
  {
    "label":0,
    "text":"morn look hour shift wor day ill dead zone ill tweetless",
    "cleaned_text":"morn look hour shift wor day ill dead zone ill tweetless",
    "normalized_text":"morn look hour shift wor day ill dead zone ill tweetless",
    "tokens":[
      "morn",
      "look",
      "hour",
      "shift",
      "wor",
      "day",
      "ill",
      "dead",
      "zone",
      "ill",
      "tweetless"
    ],
    "token_count":11,
    "processed_text":"morn look hour shift wor day ill dead zone ill tweetless"
  },
  {
    "label":0,
    "text":"miss bestfriend havent seen agess well seem like agess read",
    "cleaned_text":"miss bestfriend havent seen agess well seem like agess read",
    "normalized_text":"miss bestfriend havent seen agess well seem like agess read",
    "tokens":[
      "miss",
      "bestfriend",
      "havent",
      "seen",
      "agess",
      "well",
      "seem",
      "like",
      "agess",
      "read"
    ],
    "token_count":10,
    "processed_text":"miss bestfriend havent seen agess well seem like agess read"
  },
  {
    "label":0,
    "text":"heheh black pant oo dusti rose pink top match heel hate think dawam cloth",
    "cleaned_text":"heheh black pant oo dusti rose pink top match heel hate think dawam cloth",
    "normalized_text":"heheh black pant oo dusti rose pink top match heel hate think dawam cloth",
    "tokens":[
      "heheh",
      "black",
      "pant",
      "oo",
      "dusti",
      "rose",
      "pink",
      "top",
      "match",
      "heel",
      "hate",
      "think",
      "dawam",
      "cloth"
    ],
    "token_count":14,
    "processed_text":"heheh black pant oo dusti rose pink top match heel hate think dawam cloth"
  },
  {
    "label":0,
    "text":"that twitter valu",
    "cleaned_text":"that twitter valu",
    "normalized_text":"that twitter valu",
    "tokens":[
      "twitter",
      "valu"
    ],
    "token_count":2,
    "processed_text":"twitter valu"
  },
  {
    "label":0,
    "text":"happi father day wish mine wasnt far away right",
    "cleaned_text":"happi father day wish mine wasnt far away right",
    "normalized_text":"happi father day wish mine wasnt far away right",
    "tokens":[
      "happi",
      "father",
      "day",
      "wish",
      "mine",
      "wasnt",
      "far",
      "away",
      "right"
    ],
    "token_count":9,
    "processed_text":"happi father day wish mine wasnt far away right"
  },
  {
    "label":0,
    "text":"argh ice cream anymor mum pick",
    "cleaned_text":"argh ice cream anymor mum pick",
    "normalized_text":"argh ice cream anymor mum pick",
    "tokens":[
      "argh",
      "ice",
      "cream",
      "anymor",
      "mum",
      "pick"
    ],
    "token_count":6,
    "processed_text":"argh ice cream anymor mum pick"
  },
  {
    "label":0,
    "text":"happi way left apart though especi sinc knock lush shelf",
    "cleaned_text":"happi way left apart though especi sinc knock lush shelf",
    "normalized_text":"happi way left apart though especi sinc knock lush shelf",
    "tokens":[
      "happi",
      "way",
      "left",
      "apart",
      "though",
      "especi",
      "sinc",
      "knock",
      "lush",
      "shelf"
    ],
    "token_count":10,
    "processed_text":"happi way left apart though especi sinc knock lush shelf"
  },
  {
    "label":4,
    "text":"ive join ur club day",
    "cleaned_text":"ive join ur club day",
    "normalized_text":"ive join ur club day",
    "tokens":[
      "ive",
      "join",
      "ur",
      "club",
      "day"
    ],
    "token_count":5,
    "processed_text":"ive join ur club day"
  },
  {
    "label":0,
    "text":"ok ive got letter written pictur peopl im sit lone caus kid town hubbi golf",
    "cleaned_text":"ok ive got letter written pictur peopl im sit lone caus kid town hubbi golf",
    "normalized_text":"ok ive got letter written pictur peopl im sit lone caus kid town hubbi golf",
    "tokens":[
      "ok",
      "ive",
      "got",
      "letter",
      "written",
      "pictur",
      "peopl",
      "im",
      "sit",
      "lone",
      "cau",
      "kid",
      "town",
      "hubbi",
      "golf"
    ],
    "token_count":15,
    "processed_text":"ok ive got letter written pictur peopl im sit lone cau kid town hubbi golf"
  },
  {
    "label":0,
    "text":"haha see total true need cri miss much",
    "cleaned_text":"haha see total true need cri miss much",
    "normalized_text":"haha see total true need cri miss much",
    "tokens":[
      "haha",
      "see",
      "total",
      "true",
      "need",
      "cri",
      "miss",
      "much"
    ],
    "token_count":8,
    "processed_text":"haha see total true need cri miss much"
  },
  {
    "label":4,
    "text":"rt god give u second day take sec say thanku u got take amp say thank u mom",
    "cleaned_text":"rt god give u second day take sec say thanku u got take amp say thank u mom",
    "normalized_text":"rt god give u second day take sec say thanku u got take amp say thank u mom",
    "tokens":[
      "rt",
      "god",
      "give",
      "second",
      "day",
      "take",
      "sec",
      "say",
      "thanku",
      "got",
      "take",
      "amp",
      "say",
      "thank",
      "mom"
    ],
    "token_count":15,
    "processed_text":"rt god give second day take sec say thanku got take amp say thank mom"
  },
  {
    "label":0,
    "text":"ahhhh haha thought go die day im even kid miss much",
    "cleaned_text":"ahhhh haha thought go die day im even kid miss much",
    "normalized_text":"ahhhh haha thought go die day im even kid miss much",
    "tokens":[
      "ahhhh",
      "haha",
      "thought",
      "go",
      "die",
      "day",
      "im",
      "even",
      "kid",
      "miss",
      "much"
    ],
    "token_count":11,
    "processed_text":"ahhhh haha thought go die day im even kid miss much"
  },
  {
    "label":0,
    "text":"snitter work properli dont know",
    "cleaned_text":"snitter work properli dont know",
    "normalized_text":"snitter work properli dont know",
    "tokens":[
      "snitter",
      "work",
      "properli",
      "dont",
      "know"
    ],
    "token_count":5,
    "processed_text":"snitter work properli dont know"
  },
  {
    "label":0,
    "text":"awesom freestyl day drew cant rememb",
    "cleaned_text":"awesom freestyl day drew cant rememb",
    "normalized_text":"awesom freestyl day drew cant rememb",
    "tokens":[
      "awesom",
      "freestyl",
      "day",
      "drew",
      "cant",
      "rememb"
    ],
    "token_count":6,
    "processed_text":"awesom freestyl day drew cant rememb"
  },
  {
    "label":4,
    "text":"im almost done clean room clean mean put stuff box put box closet",
    "cleaned_text":"im almost done clean room clean mean put stuff box put box closet",
    "normalized_text":"im almost done clean room clean mean put stuff box put box closet",
    "tokens":[
      "im",
      "almost",
      "done",
      "clean",
      "room",
      "clean",
      "mean",
      "put",
      "stuff",
      "box",
      "put",
      "box",
      "closet"
    ],
    "token_count":13,
    "processed_text":"im almost done clean room clean mean put stuff box put box closet"
  },
  {
    "label":4,
    "text":"respect enough draw next week westhil gala hour stand watch kid footbal least bbq",
    "cleaned_text":"respect enough draw next week westhil gala hour stand watch kid footbal least bbq",
    "normalized_text":"respect enough draw next week westhil gala hour stand watch kid footbal least bbq",
    "tokens":[
      "respect",
      "enough",
      "draw",
      "next",
      "week",
      "westhil",
      "gala",
      "hour",
      "stand",
      "watch",
      "kid",
      "footbal",
      "least",
      "bbq"
    ],
    "token_count":14,
    "processed_text":"respect enough draw next week westhil gala hour stand watch kid footbal least bbq"
  },
  {
    "label":4,
    "text":"spray tan wonder thing excit tomorrow",
    "cleaned_text":"spray tan wonder thing excit tomorrow",
    "normalized_text":"spray tan wonder thing excit tomorrow",
    "tokens":[
      "spray",
      "tan",
      "wonder",
      "thing",
      "excit",
      "tomorrow"
    ],
    "token_count":6,
    "processed_text":"spray tan wonder thing excit tomorrow"
  },
  {
    "label":0,
    "text":"aaahh lastfm due temperatur problem scrobbl",
    "cleaned_text":"aaahh lastfm due temperatur problem scrobbl",
    "normalized_text":"aaahh lastfm due temperatur problem scrobbl",
    "tokens":[
      "aaahh",
      "lastfm",
      "due",
      "temperatur",
      "problem",
      "scrobbl"
    ],
    "token_count":6,
    "processed_text":"aaahh lastfm due temperatur problem scrobbl"
  },
  {
    "label":4,
    "text":"fun videoshoot sophi cant wait see xoxo",
    "cleaned_text":"fun videoshoot sophi cant wait see xoxo",
    "normalized_text":"fun videoshoot sophi cant wait see xoxo",
    "tokens":[
      "fun",
      "videoshoot",
      "sophi",
      "cant",
      "wait",
      "see",
      "xoxo"
    ],
    "token_count":7,
    "processed_text":"fun videoshoot sophi cant wait see xoxo"
  },
  {
    "label":0,
    "text":"miss tweet",
    "cleaned_text":"miss tweet",
    "normalized_text":"miss tweet",
    "tokens":[
      "miss",
      "tweet"
    ],
    "token_count":2,
    "processed_text":"miss tweet"
  },
  {
    "label":4,
    "text":"pronounc mraz weirdli accord akmal",
    "cleaned_text":"pronounc mraz weirdli accord akmal",
    "normalized_text":"pronounc mraz weirdli accord akmal",
    "tokens":[
      "pronounc",
      "mraz",
      "weirdli",
      "accord",
      "akmal"
    ],
    "token_count":5,
    "processed_text":"pronounc mraz weirdli accord akmal"
  },
  {
    "label":4,
    "text":"beauti allway goodnight sleep well",
    "cleaned_text":"beauti allway goodnight sleep well",
    "normalized_text":"beauti allway goodnight sleep well",
    "tokens":[
      "beauti",
      "allway",
      "goodnight",
      "sleep",
      "well"
    ],
    "token_count":5,
    "processed_text":"beauti allway goodnight sleep well"
  },
  {
    "label":0,
    "text":"sorri love left over come sat mi casa ton birthday well ton food",
    "cleaned_text":"sorri love left over come sat mi casa ton birthday well ton food",
    "normalized_text":"sorri love left over come sat mi casa ton birthday well ton food",
    "tokens":[
      "sorri",
      "love",
      "left",
      "come",
      "sat",
      "mi",
      "casa",
      "ton",
      "birthday",
      "well",
      "ton",
      "food"
    ],
    "token_count":12,
    "processed_text":"sorri love left come sat mi casa ton birthday well ton food"
  },
  {
    "label":4,
    "text":"im nyc girl rlli love place wonder",
    "cleaned_text":"im nyc girl rlli love place wonder",
    "normalized_text":"im nyc girl rlli love place wonder",
    "tokens":[
      "im",
      "nyc",
      "girl",
      "rlli",
      "love",
      "place",
      "wonder"
    ],
    "token_count":7,
    "processed_text":"im nyc girl rlli love place wonder"
  },
  {
    "label":4,
    "text":"andi want film grow fascin good look charm",
    "cleaned_text":"andi want film grow fascin good look charm",
    "normalized_text":"andi want film grow fascin good look charm",
    "tokens":[
      "andi",
      "want",
      "film",
      "grow",
      "fascin",
      "good",
      "look",
      "charm"
    ],
    "token_count":8,
    "processed_text":"andi want film grow fascin good look charm"
  },
  {
    "label":0,
    "text":"studi hard focu rpattz danc around nake piti girlish mind",
    "cleaned_text":"studi hard focu rpattz danc around nake piti girlish mind",
    "normalized_text":"studi hard focu rpattz danc around nake piti girlish mind",
    "tokens":[
      "studi",
      "hard",
      "focu",
      "rpattz",
      "danc",
      "around",
      "nake",
      "piti",
      "girlish",
      "mind"
    ],
    "token_count":10,
    "processed_text":"studi hard focu rpattz danc around nake piti girlish mind"
  },
  {
    "label":0,
    "text":"homemad bread cakeporn ok oop ate could photo sorri good",
    "cleaned_text":"homemad bread cakeporn ok oop ate could photo sorri good",
    "normalized_text":"homemad bread cakeporn ok oop ate could photo sorri good",
    "tokens":[
      "homemad",
      "bread",
      "cakeporn",
      "ok",
      "oop",
      "ate",
      "photo",
      "sorri",
      "good"
    ],
    "token_count":9,
    "processed_text":"homemad bread cakeporn ok oop ate photo sorri good"
  },
  {
    "label":0,
    "text":"lol ate place hometown sorri",
    "cleaned_text":"lol ate place hometown sorri",
    "normalized_text":"lol ate place hometown sorri",
    "tokens":[
      "lol",
      "ate",
      "place",
      "hometown",
      "sorri"
    ],
    "token_count":5,
    "processed_text":"lol ate place hometown sorri"
  },
  {
    "label":0,
    "text":"cant sleep hunniiii isnt next",
    "cleaned_text":"cant sleep hunniiii isnt next",
    "normalized_text":"cant sleep hunniiii isnt next",
    "tokens":[
      "cant",
      "sleep",
      "hunniiii",
      "isnt",
      "next"
    ],
    "token_count":5,
    "processed_text":"cant sleep hunniiii isnt next"
  },
  {
    "label":0,
    "text":"make sad alway forget play florida",
    "cleaned_text":"make sad alway forget play florida",
    "normalized_text":"make sad alway forget play florida",
    "tokens":[
      "make",
      "sad",
      "alway",
      "forget",
      "play",
      "florida"
    ],
    "token_count":6,
    "processed_text":"make sad alway forget play florida"
  },
  {
    "label":0,
    "text":"thought day thing fackk youu miss pe",
    "cleaned_text":"thought day thing fackk youu miss pe",
    "normalized_text":"thought day thing fackk youu miss pe",
    "tokens":[
      "thought",
      "day",
      "thing",
      "fackk",
      "youu",
      "miss",
      "pe"
    ],
    "token_count":7,
    "processed_text":"thought day thing fackk youu miss pe"
  },
  {
    "label":4,
    "text":"see point tonight",
    "cleaned_text":"see point tonight",
    "normalized_text":"see point tonight",
    "tokens":[
      "see",
      "point",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"see point tonight"
  },
  {
    "label":0,
    "text":"hug wish could come hang wouldnt leav hous youd compani lt",
    "cleaned_text":"hug wish could come hang wouldnt leav hous youd compani lt",
    "normalized_text":"hug wish could come hang wouldnt leav hous youd compani lt",
    "tokens":[
      "hug",
      "wish",
      "come",
      "hang",
      "wouldnt",
      "leav",
      "hou",
      "youd",
      "compani",
      "lt"
    ],
    "token_count":10,
    "processed_text":"hug wish come hang wouldnt leav hou youd compani lt"
  },
  {
    "label":0,
    "text":"oprah need go johnni come late shallow hip plu flood commerci interest follow",
    "cleaned_text":"oprah need go johnni come late shallow hip plu flood commerci interest follow",
    "normalized_text":"oprah need go johnni come late shallow hip plu flood commerci interest follow",
    "tokens":[
      "oprah",
      "need",
      "go",
      "johnni",
      "come",
      "late",
      "shallow",
      "hip",
      "plu",
      "flood",
      "commerci",
      "interest",
      "follow"
    ],
    "token_count":13,
    "processed_text":"oprah need go johnni come late shallow hip plu flood commerci interest follow"
  },
  {
    "label":0,
    "text":"cri cuz im stupid cuz face kevin remind someon ok boohoohoo let say kindof miss day",
    "cleaned_text":"cri cuz im stupid cuz face kevin remind someon ok boohoohoo let say kindof miss day",
    "normalized_text":"cri cuz im stupid cuz face kevin remind someon ok boohoohoo let say kindof miss day",
    "tokens":[
      "cri",
      "cuz",
      "im",
      "stupid",
      "cuz",
      "face",
      "kevin",
      "remind",
      "someon",
      "ok",
      "boohoohoo",
      "let",
      "say",
      "kindof",
      "miss",
      "day"
    ],
    "token_count":16,
    "processed_text":"cri cuz im stupid cuz face kevin remind someon ok boohoohoo let say kindof miss day"
  },
  {
    "label":0,
    "text":"got home noth heavi head must heat titl read bookstor",
    "cleaned_text":"got home noth heavi head must heat titl read bookstor",
    "normalized_text":"got home noth heavi head must heat titl read bookstor",
    "tokens":[
      "got",
      "home",
      "noth",
      "heavi",
      "head",
      "heat",
      "titl",
      "read",
      "bookstor"
    ],
    "token_count":9,
    "processed_text":"got home noth heavi head heat titl read bookstor"
  },
  {
    "label":4,
    "text":"good hear u arriv safe vbad ur feel well ur sleep make better",
    "cleaned_text":"good hear u arriv safe vbad ur feel well ur sleep make better",
    "normalized_text":"good hear u arriv safe vbad ur feel well ur sleep make better",
    "tokens":[
      "good",
      "hear",
      "arriv",
      "safe",
      "vbad",
      "ur",
      "feel",
      "well",
      "ur",
      "sleep",
      "make",
      "better"
    ],
    "token_count":12,
    "processed_text":"good hear arriv safe vbad ur feel well ur sleep make better"
  },
  {
    "label":4,
    "text":"group tweeti",
    "cleaned_text":"group tweeti",
    "normalized_text":"group tweeti",
    "tokens":[
      "group",
      "tweeti"
    ],
    "token_count":2,
    "processed_text":"group tweeti"
  },
  {
    "label":0,
    "text":"awe that cool",
    "cleaned_text":"awe that cool",
    "normalized_text":"awe that cool",
    "tokens":[
      "awe",
      "cool"
    ],
    "token_count":2,
    "processed_text":"awe cool"
  },
  {
    "label":0,
    "text":"much pastaaaa hope didnt go bad sat trunk car last night haha",
    "cleaned_text":"much pastaaaa hope didnt go bad sat trunk car last night haha",
    "normalized_text":"much pastaaaa hope didnt go bad sat trunk car last night haha",
    "tokens":[
      "much",
      "pastaaaa",
      "hope",
      "didnt",
      "go",
      "bad",
      "sat",
      "trunk",
      "car",
      "last",
      "night",
      "haha"
    ],
    "token_count":12,
    "processed_text":"much pastaaaa hope didnt go bad sat trunk car last night haha"
  },
  {
    "label":4,
    "text":"hmm confus fine beauti niec",
    "cleaned_text":"hmm confus fine beauti niec",
    "normalized_text":"hmm confus fine beauti niec",
    "tokens":[
      "hmm",
      "confu",
      "fine",
      "beauti",
      "niec"
    ],
    "token_count":5,
    "processed_text":"hmm confu fine beauti niec"
  },
  {
    "label":0,
    "text":"think",
    "cleaned_text":"think",
    "normalized_text":"think",
    "tokens":[
      "think"
    ],
    "token_count":1,
    "processed_text":"think"
  },
  {
    "label":4,
    "text":"bad littl gun john best purchas yet bad didnt get quotbroth discountquot tho vid",
    "cleaned_text":"bad littl gun john best purchas yet bad didnt get quotbroth discountquot tho vid",
    "normalized_text":"bad littl gun john best purchas yet bad didnt get quotbroth discountquot tho vid",
    "tokens":[
      "bad",
      "littl",
      "gun",
      "john",
      "best",
      "purcha",
      "yet",
      "bad",
      "didnt",
      "get",
      "quotbroth",
      "discountquot",
      "tho",
      "vid"
    ],
    "token_count":14,
    "processed_text":"bad littl gun john best purcha yet bad didnt get quotbroth discountquot tho vid"
  },
  {
    "label":0,
    "text":"regret say ye hous guest",
    "cleaned_text":"regret say ye hous guest",
    "normalized_text":"regret say ye hous guest",
    "tokens":[
      "regret",
      "say",
      "ye",
      "hou",
      "guest"
    ],
    "token_count":5,
    "processed_text":"regret say ye hou guest"
  },
  {
    "label":0,
    "text":"feel underestim",
    "cleaned_text":"feel underestim",
    "normalized_text":"feel underestim",
    "tokens":[
      "feel",
      "underestim"
    ],
    "token_count":2,
    "processed_text":"feel underestim"
  },
  {
    "label":4,
    "text":"home relax bit pamper",
    "cleaned_text":"home relax bit pamper",
    "normalized_text":"home relax bit pamper",
    "tokens":[
      "home",
      "relax",
      "bit",
      "pamper"
    ],
    "token_count":4,
    "processed_text":"home relax bit pamper"
  },
  {
    "label":0,
    "text":"yay internet pool time today",
    "cleaned_text":"yay internet pool time today",
    "normalized_text":"yay internet pool time today",
    "tokens":[
      "yay",
      "internet",
      "pool",
      "time",
      "today"
    ],
    "token_count":5,
    "processed_text":"yay internet pool time today"
  },
  {
    "label":4,
    "text":"watch movi rent women mom btw realiz follow im go tri get",
    "cleaned_text":"watch movi rent women mom btw realiz follow im go tri get",
    "normalized_text":"watch movi rent women mom btw realiz follow im go tri get",
    "tokens":[
      "watch",
      "movi",
      "rent",
      "women",
      "mom",
      "btw",
      "realiz",
      "follow",
      "im",
      "go",
      "tri",
      "get"
    ],
    "token_count":12,
    "processed_text":"watch movi rent women mom btw realiz follow im go tri get"
  },
  {
    "label":4,
    "text":"love starbuck boy mem dr",
    "cleaned_text":"love starbuck boy mem dr",
    "normalized_text":"love starbuck boy mem dr",
    "tokens":[
      "love",
      "starbuck",
      "boy",
      "mem",
      "dr"
    ],
    "token_count":5,
    "processed_text":"love starbuck boy mem dr"
  },
  {
    "label":0,
    "text":"buzz shit listen mani time",
    "cleaned_text":"buzz shit listen mani time",
    "normalized_text":"buzz shit listen mani time",
    "tokens":[
      "buzz",
      "shit",
      "listen",
      "mani",
      "time"
    ],
    "token_count":5,
    "processed_text":"buzz shit listen mani time"
  },
  {
    "label":4,
    "text":"checkin ju made home llol",
    "cleaned_text":"checkin ju made home llol",
    "normalized_text":"checkin ju made home llol",
    "tokens":[
      "checkin",
      "ju",
      "made",
      "home",
      "llol"
    ],
    "token_count":5,
    "processed_text":"checkin ju made home llol"
  },
  {
    "label":0,
    "text":"sit sun listen gomez sip vailla latt shame work",
    "cleaned_text":"sit sun listen gomez sip vailla latt shame work",
    "normalized_text":"sit sun listen gomez sip vailla latt shame work",
    "tokens":[
      "sit",
      "sun",
      "listen",
      "gomez",
      "sip",
      "vailla",
      "latt",
      "shame",
      "work"
    ],
    "token_count":9,
    "processed_text":"sit sun listen gomez sip vailla latt shame work"
  },
  {
    "label":0,
    "text":"fuck air bed pop big ass hole cant get repairedreplac tonight late go back wendi place argh",
    "cleaned_text":"fuck air bed pop big ass hole cant get repairedreplac tonight late go back wendi place argh",
    "normalized_text":"fuck air bed pop big ass hole cant get repairedreplac tonight late go back wendi place argh",
    "tokens":[
      "fuck",
      "air",
      "bed",
      "pop",
      "big",
      "ass",
      "hole",
      "cant",
      "get",
      "repairedreplac",
      "tonight",
      "late",
      "go",
      "back",
      "wendi",
      "place",
      "argh"
    ],
    "token_count":17,
    "processed_text":"fuck air bed pop big ass hole cant get repairedreplac tonight late go back wendi place argh"
  },
  {
    "label":0,
    "text":"ohwow moon look sssoooo beauti could stare till forev wish someon share",
    "cleaned_text":"ohwow moon look sssoooo beauti could stare till forev wish someon share",
    "normalized_text":"ohwow moon look sssoooo beauti could stare till forev wish someon share",
    "tokens":[
      "ohwow",
      "moon",
      "look",
      "sssoooo",
      "beauti",
      "stare",
      "till",
      "forev",
      "wish",
      "someon",
      "share"
    ],
    "token_count":11,
    "processed_text":"ohwow moon look sssoooo beauti stare till forev wish someon share"
  },
  {
    "label":0,
    "text":"hate gymmmm",
    "cleaned_text":"hate gymmmm",
    "normalized_text":"hate gymmmm",
    "tokens":[
      "hate",
      "gymmmm"
    ],
    "token_count":2,
    "processed_text":"hate gymmmm"
  },
  {
    "label":4,
    "text":"well need fun even confus",
    "cleaned_text":"well need fun even confus",
    "normalized_text":"well need fun even confus",
    "tokens":[
      "well",
      "need",
      "fun",
      "even",
      "confu"
    ],
    "token_count":5,
    "processed_text":"well need fun even confu"
  },
  {
    "label":4,
    "text":"nice meet last night expo teresa get min would rock youd check websit",
    "cleaned_text":"nice meet last night expo teresa get min would rock youd check websit",
    "normalized_text":"nice meet last night expo teresa get min would rock youd check websit",
    "tokens":[
      "nice",
      "meet",
      "last",
      "night",
      "expo",
      "teresa",
      "get",
      "min",
      "rock",
      "youd",
      "check",
      "websit"
    ],
    "token_count":12,
    "processed_text":"nice meet last night expo teresa get min rock youd check websit"
  },
  {
    "label":0,
    "text":"midnight feel hungri",
    "cleaned_text":"midnight feel hungri",
    "normalized_text":"midnight feel hungri",
    "tokens":[
      "midnight",
      "feel",
      "hungri"
    ],
    "token_count":3,
    "processed_text":"midnight feel hungri"
  },
  {
    "label":4,
    "text":"thank that anoth great resourc keep em come",
    "cleaned_text":"thank that anoth great resourc keep em come",
    "normalized_text":"thank that anoth great resourc keep em come",
    "tokens":[
      "thank",
      "anoth",
      "great",
      "resourc",
      "keep",
      "em",
      "come"
    ],
    "token_count":7,
    "processed_text":"thank anoth great resourc keep em come"
  },
  {
    "label":0,
    "text":"goodnight know knowyal must thinkingquothuh steph sleep earlyquot yeah co gotta earli tmr",
    "cleaned_text":"goodnight know knowyal must thinkingquothuh steph sleep earlyquot yeah co gotta earli tmr",
    "normalized_text":"goodnight know knowyal must thinkingquothuh steph sleep earlyquot yeah co gotta earli tmr",
    "tokens":[
      "goodnight",
      "know",
      "knowyal",
      "thinkingquothuh",
      "steph",
      "sleep",
      "earlyquot",
      "yeah",
      "co",
      "got",
      "ta",
      "earli",
      "tmr"
    ],
    "token_count":13,
    "processed_text":"goodnight know knowyal thinkingquothuh steph sleep earlyquot yeah co got ta earli tmr"
  },
  {
    "label":4,
    "text":"first time long time happi",
    "cleaned_text":"first time long time happi",
    "normalized_text":"first time long time happi",
    "tokens":[
      "first",
      "time",
      "long",
      "time",
      "happi"
    ],
    "token_count":5,
    "processed_text":"first time long time happi"
  },
  {
    "label":0,
    "text":"found anoth one found daddi long leg hous heck never see spider one day creepi",
    "cleaned_text":"found anoth one found daddi long leg hous heck never see spider one day creepi",
    "normalized_text":"found anoth one found daddi long leg hous heck never see spider one day creepi",
    "tokens":[
      "found",
      "anoth",
      "one",
      "found",
      "daddi",
      "long",
      "leg",
      "hou",
      "heck",
      "never",
      "see",
      "spider",
      "one",
      "day",
      "creepi"
    ],
    "token_count":15,
    "processed_text":"found anoth one found daddi long leg hou heck never see spider one day creepi"
  },
  {
    "label":0,
    "text":"love kid sick",
    "cleaned_text":"love kid sick",
    "normalized_text":"love kid sick",
    "tokens":[
      "love",
      "kid",
      "sick"
    ],
    "token_count":3,
    "processed_text":"love kid sick"
  },
  {
    "label":0,
    "text":"thank guy support well reopen soon possibl keep inform sorri let regular",
    "cleaned_text":"thank guy support well reopen soon possibl keep inform sorri let regular",
    "normalized_text":"thank guy support well reopen soon possibl keep inform sorri let regular",
    "tokens":[
      "thank",
      "guy",
      "support",
      "well",
      "reopen",
      "soon",
      "possibl",
      "keep",
      "inform",
      "sorri",
      "let",
      "regular"
    ],
    "token_count":12,
    "processed_text":"thank guy support well reopen soon possibl keep inform sorri let regular"
  },
  {
    "label":4,
    "text":"like come back",
    "cleaned_text":"like come back",
    "normalized_text":"like come back",
    "tokens":[
      "like",
      "come",
      "back"
    ],
    "token_count":3,
    "processed_text":"like come back"
  },
  {
    "label":4,
    "text":"thank birthday wish univers decid give beauti weather today",
    "cleaned_text":"thank birthday wish univers decid give beauti weather today",
    "normalized_text":"thank birthday wish univers decid give beauti weather today",
    "tokens":[
      "thank",
      "birthday",
      "wish",
      "univ",
      "decid",
      "give",
      "beauti",
      "weather",
      "today"
    ],
    "token_count":9,
    "processed_text":"thank birthday wish univ decid give beauti weather today"
  },
  {
    "label":4,
    "text":"watch sunris love daili dose news sport weather entertain",
    "cleaned_text":"watch sunris love daili dose news sport weather entertain",
    "normalized_text":"watch sunris love daili dose news sport weather entertain",
    "tokens":[
      "watch",
      "sunri",
      "love",
      "daili",
      "dose",
      "news",
      "sport",
      "weather",
      "entertain"
    ],
    "token_count":9,
    "processed_text":"watch sunri love daili dose news sport weather entertain"
  },
  {
    "label":0,
    "text":"tummi ach",
    "cleaned_text":"tummi ach",
    "normalized_text":"tummi ach",
    "tokens":[
      "tummi",
      "ach"
    ],
    "token_count":2,
    "processed_text":"tummi ach"
  },
  {
    "label":0,
    "text":"shop rain x",
    "cleaned_text":"shop rain x",
    "normalized_text":"shop rain x",
    "tokens":[
      "shop",
      "rain"
    ],
    "token_count":2,
    "processed_text":"shop rain"
  },
  {
    "label":4,
    "text":"happi conan back dont work tomorrow gonna",
    "cleaned_text":"happi conan back dont work tomorrow gonna",
    "normalized_text":"happi conan back dont work tomorrow gonna",
    "tokens":[
      "happi",
      "conan",
      "back",
      "dont",
      "work",
      "tomorrow",
      "gon",
      "na"
    ],
    "token_count":8,
    "processed_text":"happi conan back dont work tomorrow gon na"
  },
  {
    "label":0,
    "text":"cant even watch coz dad watchin telli grrr",
    "cleaned_text":"cant even watch coz dad watchin telli grrr",
    "normalized_text":"cant even watch coz dad watchin telli grrr",
    "tokens":[
      "cant",
      "even",
      "watch",
      "coz",
      "dad",
      "watchin",
      "telli",
      "grrr"
    ],
    "token_count":8,
    "processed_text":"cant even watch coz dad watchin telli grrr"
  },
  {
    "label":0,
    "text":"whit lot neg energi mind fear worst",
    "cleaned_text":"whit lot neg energi mind fear worst",
    "normalized_text":"whit lot neg energi mind fear worst",
    "tokens":[
      "whit",
      "lot",
      "neg",
      "energi",
      "mind",
      "fear",
      "worst"
    ],
    "token_count":7,
    "processed_text":"whit lot neg energi mind fear worst"
  },
  {
    "label":4,
    "text":"know ridicul think wifi human right",
    "cleaned_text":"know ridicul think wifi human right",
    "normalized_text":"know ridicul think wifi human right",
    "tokens":[
      "know",
      "ridicul",
      "think",
      "wifi",
      "human",
      "right"
    ],
    "token_count":6,
    "processed_text":"know ridicul think wifi human right"
  },
  {
    "label":0,
    "text":"cellphon brain chose lockunlock",
    "cleaned_text":"cellphon brain chose lockunlock",
    "normalized_text":"cellphon brain chose lockunlock",
    "tokens":[
      "cellphon",
      "brain",
      "chose",
      "lockunlock"
    ],
    "token_count":4,
    "processed_text":"cellphon brain chose lockunlock"
  },
  {
    "label":4,
    "text":"hi alic",
    "cleaned_text":"hi alic",
    "normalized_text":"hi alic",
    "tokens":[
      "hi",
      "alic"
    ],
    "token_count":2,
    "processed_text":"hi alic"
  },
  {
    "label":0,
    "text":"love bit asid earlier nonononononono",
    "cleaned_text":"love bit asid earlier nonononononono",
    "normalized_text":"love bit asid earlier nonononononono",
    "tokens":[
      "love",
      "bit",
      "asid",
      "earlier",
      "nonononononono"
    ],
    "token_count":5,
    "processed_text":"love bit asid earlier nonononononono"
  },
  {
    "label":0,
    "text":"dog name rocki",
    "cleaned_text":"dog name rocki",
    "normalized_text":"dog name rocki",
    "tokens":[
      "dog",
      "name",
      "rocki"
    ],
    "token_count":3,
    "processed_text":"dog name rocki"
  },
  {
    "label":4,
    "text":"extend vacat exel",
    "cleaned_text":"extend vacat exel",
    "normalized_text":"extend vacat exel",
    "tokens":[
      "extend",
      "vacat",
      "exel"
    ],
    "token_count":3,
    "processed_text":"extend vacat exel"
  },
  {
    "label":0,
    "text":"time open laptop surf net updat itouch",
    "cleaned_text":"time open laptop surf net updat itouch",
    "normalized_text":"time open laptop surf net updat itouch",
    "tokens":[
      "time",
      "open",
      "laptop",
      "surf",
      "net",
      "updat",
      "itouch"
    ],
    "token_count":7,
    "processed_text":"time open laptop surf net updat itouch"
  },
  {
    "label":0,
    "text":"dont want ur heart hurtin",
    "cleaned_text":"dont want ur heart hurtin",
    "normalized_text":"dont want ur heart hurtin",
    "tokens":[
      "dont",
      "want",
      "ur",
      "heart",
      "hurtin"
    ],
    "token_count":5,
    "processed_text":"dont want ur heart hurtin"
  },
  {
    "label":4,
    "text":"anoth adventur sam gonna go watch plane tonight",
    "cleaned_text":"anoth adventur sam gonna go watch plane tonight",
    "normalized_text":"anoth adventur sam gonna go watch plane tonight",
    "tokens":[
      "anoth",
      "adventur",
      "sam",
      "gon",
      "na",
      "go",
      "watch",
      "plane",
      "tonight"
    ],
    "token_count":9,
    "processed_text":"anoth adventur sam gon na go watch plane tonight"
  },
  {
    "label":4,
    "text":"awe thank ukulel pretti easi learn good luck",
    "cleaned_text":"awe thank ukulel pretti easi learn good luck",
    "normalized_text":"awe thank ukulel pretti easi learn good luck",
    "tokens":[
      "awe",
      "thank",
      "ukulel",
      "pretti",
      "easi",
      "learn",
      "good",
      "luck"
    ],
    "token_count":8,
    "processed_text":"awe thank ukulel pretti easi learn good luck"
  },
  {
    "label":4,
    "text":"yay glad im one",
    "cleaned_text":"yay glad im one",
    "normalized_text":"yay glad im one",
    "tokens":[
      "yay",
      "glad",
      "im",
      "one"
    ],
    "token_count":4,
    "processed_text":"yay glad im one"
  },
  {
    "label":0,
    "text":"battl epic guardian need life",
    "cleaned_text":"battl epic guardian need life",
    "normalized_text":"battl epic guardian need life",
    "tokens":[
      "battl",
      "epic",
      "guardian",
      "need",
      "life"
    ],
    "token_count":5,
    "processed_text":"battl epic guardian need life"
  },
  {
    "label":4,
    "text":"hi mate australia jenniff birthday would love could wish happi birthday air behalf",
    "cleaned_text":"hi mate australia jenniff birthday would love could wish happi birthday air behalf",
    "normalized_text":"hi mate australia jenniff birthday would love could wish happi birthday air behalf",
    "tokens":[
      "hi",
      "mate",
      "australia",
      "jenniff",
      "birthday",
      "love",
      "wish",
      "happi",
      "birthday",
      "air",
      "behalf"
    ],
    "token_count":11,
    "processed_text":"hi mate australia jenniff birthday love wish happi birthday air behalf"
  },
  {
    "label":0,
    "text":"feel bore work dont think like anymor ugh",
    "cleaned_text":"feel bore work dont think like anymor ugh",
    "normalized_text":"feel bore work dont think like anymor ugh",
    "tokens":[
      "feel",
      "bore",
      "work",
      "dont",
      "think",
      "like",
      "anymor",
      "ugh"
    ],
    "token_count":8,
    "processed_text":"feel bore work dont think like anymor ugh"
  },
  {
    "label":0,
    "text":"dont know sorri",
    "cleaned_text":"dont know sorri",
    "normalized_text":"dont know sorri",
    "tokens":[
      "dont",
      "know",
      "sorri"
    ],
    "token_count":3,
    "processed_text":"dont know sorri"
  },
  {
    "label":0,
    "text":"feel shitti sleep amp day still feel tire amp nauseou hungri cant eat hate sick",
    "cleaned_text":"feel shitti sleep amp day still feel tire amp nauseou hungri cant eat hate sick",
    "normalized_text":"feel shitti sleep amp day still feel tire amp nauseou hungri cant eat hate sick",
    "tokens":[
      "feel",
      "shitti",
      "sleep",
      "amp",
      "day",
      "still",
      "feel",
      "tire",
      "amp",
      "nauseou",
      "hungri",
      "cant",
      "eat",
      "hate",
      "sick"
    ],
    "token_count":15,
    "processed_text":"feel shitti sleep amp day still feel tire amp nauseou hungri cant eat hate sick"
  },
  {
    "label":4,
    "text":"werent specif kind mark want dear",
    "cleaned_text":"werent specif kind mark want dear",
    "normalized_text":"werent specif kind mark want dear",
    "tokens":[
      "werent",
      "specif",
      "kind",
      "mark",
      "want",
      "dear"
    ],
    "token_count":6,
    "processed_text":"werent specif kind mark want dear"
  },
  {
    "label":0,
    "text":"thx saw post regard conview doesnt work twitterfon",
    "cleaned_text":"thx saw post regard conview doesnt work twitterfon",
    "normalized_text":"thx saw post regard conview doesnt work twitterfon",
    "tokens":[
      "thx",
      "saw",
      "post",
      "regard",
      "conview",
      "doesnt",
      "work",
      "twitterfon"
    ],
    "token_count":8,
    "processed_text":"thx saw post regard conview doesnt work twitterfon"
  },
  {
    "label":4,
    "text":"thank follow xoxo",
    "cleaned_text":"thank follow xoxo",
    "normalized_text":"thank follow xoxo",
    "tokens":[
      "thank",
      "follow",
      "xoxo"
    ],
    "token_count":3,
    "processed_text":"thank follow xoxo"
  },
  {
    "label":0,
    "text":"sure",
    "cleaned_text":"sure",
    "normalized_text":"sure",
    "tokens":[
      "sure"
    ],
    "token_count":1,
    "processed_text":"sure"
  },
  {
    "label":0,
    "text":"hour sleep didnt even rage last night uggh time make",
    "cleaned_text":"hour sleep didnt even rage last night uggh time make",
    "normalized_text":"hour sleep didnt even rage last night uggh time make",
    "tokens":[
      "hour",
      "sleep",
      "didnt",
      "even",
      "rage",
      "last",
      "night",
      "uggh",
      "time",
      "make"
    ],
    "token_count":10,
    "processed_text":"hour sleep didnt even rage last night uggh time make"
  },
  {
    "label":0,
    "text":"wish like kid school sayin quotyay last day school let summer beginquot littl know realiti blow",
    "cleaned_text":"wish like kid school sayin quotyay last day school let summer beginquot littl know realiti blow",
    "normalized_text":"wish like kid school sayin quotyay last day school let summer beginquot littl know realiti blow",
    "tokens":[
      "wish",
      "like",
      "kid",
      "school",
      "sayin",
      "quotyay",
      "last",
      "day",
      "school",
      "let",
      "summer",
      "beginquot",
      "littl",
      "know",
      "realiti",
      "blow"
    ],
    "token_count":16,
    "processed_text":"wish like kid school sayin quotyay last day school let summer beginquot littl know realiti blow"
  },
  {
    "label":0,
    "text":"knee hurt turn old ladi fuck like fuck lmao",
    "cleaned_text":"knee hurt turn old ladi fuck like fuck lmao",
    "normalized_text":"knee hurt turn old ladi fuck like fuck lmao",
    "tokens":[
      "knee",
      "hurt",
      "turn",
      "old",
      "ladi",
      "fuck",
      "like",
      "fuck",
      "lmao"
    ],
    "token_count":9,
    "processed_text":"knee hurt turn old ladi fuck like fuck lmao"
  },
  {
    "label":4,
    "text":"wanna thank lord wake morn beauti day god truli smile",
    "cleaned_text":"wanna thank lord wake morn beauti day god truli smile",
    "normalized_text":"wanna thank lord wake morn beauti day god truli smile",
    "tokens":[
      "wan",
      "na",
      "thank",
      "lord",
      "wake",
      "morn",
      "beauti",
      "day",
      "god",
      "truli",
      "smile"
    ],
    "token_count":11,
    "processed_text":"wan na thank lord wake morn beauti day god truli smile"
  },
  {
    "label":0,
    "text":"miss much",
    "cleaned_text":"miss much",
    "normalized_text":"miss much",
    "tokens":[
      "miss",
      "much"
    ],
    "token_count":2,
    "processed_text":"miss much"
  },
  {
    "label":4,
    "text":"isnt part super crazi aj rafael hype doesnt think he bad",
    "cleaned_text":"isnt part super crazi aj rafael hype doesnt think he bad",
    "normalized_text":"isnt part super crazi aj rafael hype doesnt think he bad",
    "tokens":[
      "isnt",
      "part",
      "super",
      "crazi",
      "aj",
      "rafael",
      "hype",
      "doesnt",
      "think",
      "bad"
    ],
    "token_count":10,
    "processed_text":"isnt part super crazi aj rafael hype doesnt think bad"
  },
  {
    "label":4,
    "text":"get bore quotshaman kingquot khanh got sick mayni day",
    "cleaned_text":"get bore quotshaman kingquot khanh got sick mayni day",
    "normalized_text":"get bore quotshaman kingquot khanh got sick mayni day",
    "tokens":[
      "get",
      "bore",
      "quotshaman",
      "kingquot",
      "khanh",
      "got",
      "sick",
      "mayni",
      "day"
    ],
    "token_count":9,
    "processed_text":"get bore quotshaman kingquot khanh got sick mayni day"
  },
  {
    "label":4,
    "text":"back bournemouth",
    "cleaned_text":"back bournemouth",
    "normalized_text":"back bournemouth",
    "tokens":[
      "back",
      "bournemouth"
    ],
    "token_count":2,
    "processed_text":"back bournemouth"
  },
  {
    "label":0,
    "text":"ignor",
    "cleaned_text":"ignor",
    "normalized_text":"ignor",
    "tokens":[
      "ignor"
    ],
    "token_count":1,
    "processed_text":"ignor"
  },
  {
    "label":0,
    "text":"awesom time hate leav earlier",
    "cleaned_text":"awesom time hate leav earlier",
    "normalized_text":"awesom time hate leav earlier",
    "tokens":[
      "awesom",
      "time",
      "hate",
      "leav",
      "earlier"
    ],
    "token_count":5,
    "processed_text":"awesom time hate leav earlier"
  },
  {
    "label":4,
    "text":"wow thank show awesom even better srew love coldplay gaga ok",
    "cleaned_text":"wow thank show awesom even better srew love coldplay gaga ok",
    "normalized_text":"wow thank show awesom even better srew love coldplay gaga ok",
    "tokens":[
      "wow",
      "thank",
      "show",
      "awesom",
      "even",
      "better",
      "srew",
      "love",
      "coldplay",
      "gaga",
      "ok"
    ],
    "token_count":11,
    "processed_text":"wow thank show awesom even better srew love coldplay gaga ok"
  },
  {
    "label":4,
    "text":"termin enough homag origin without cheesi enough action remind like much awesom",
    "cleaned_text":"termin enough homag origin without cheesi enough action remind like much awesom",
    "normalized_text":"termin enough homag origin without cheesi enough action remind like much awesom",
    "tokens":[
      "termin",
      "enough",
      "homag",
      "origin",
      "without",
      "cheesi",
      "enough",
      "action",
      "remind",
      "like",
      "much",
      "awesom"
    ],
    "token_count":12,
    "processed_text":"termin enough homag origin without cheesi enough action remind like much awesom"
  },
  {
    "label":0,
    "text":"wish could find simon againc get back site",
    "cleaned_text":"wish could find simon againc get back site",
    "normalized_text":"wish could find simon againc get back site",
    "tokens":[
      "wish",
      "find",
      "simon",
      "againc",
      "get",
      "back",
      "site"
    ],
    "token_count":7,
    "processed_text":"wish find simon againc get back site"
  },
  {
    "label":4,
    "text":"done tan way home lol",
    "cleaned_text":"done tan way home lol",
    "normalized_text":"done tan way home lol",
    "tokens":[
      "done",
      "tan",
      "way",
      "home",
      "lol"
    ],
    "token_count":5,
    "processed_text":"done tan way home lol"
  },
  {
    "label":0,
    "text":"son gave heart attack fell bed poor thing",
    "cleaned_text":"son gave heart attack fell bed poor thing",
    "normalized_text":"son gave heart attack fell bed poor thing",
    "tokens":[
      "son",
      "gave",
      "heart",
      "attack",
      "fell",
      "bed",
      "poor",
      "thing"
    ],
    "token_count":8,
    "processed_text":"son gave heart attack fell bed poor thing"
  },
  {
    "label":0,
    "text":"pleas get home",
    "cleaned_text":"pleas get home",
    "normalized_text":"pleas get home",
    "tokens":[
      "plea",
      "get",
      "home"
    ],
    "token_count":3,
    "processed_text":"plea get home"
  },
  {
    "label":4,
    "text":"get shit wreck love amp jammin tune happi fuck",
    "cleaned_text":"get shit wreck love amp jammin tune happi fuck",
    "normalized_text":"get shit wreck love amp jammin tune happi fuck",
    "tokens":[
      "get",
      "shit",
      "wreck",
      "love",
      "amp",
      "jammin",
      "tune",
      "happi",
      "fuck"
    ],
    "token_count":9,
    "processed_text":"get shit wreck love amp jammin tune happi fuck"
  },
  {
    "label":4,
    "text":"realli awww cute",
    "cleaned_text":"realli awww cute",
    "normalized_text":"realli awww cute",
    "tokens":[
      "realli",
      "awww",
      "cute"
    ],
    "token_count":3,
    "processed_text":"realli awww cute"
  },
  {
    "label":0,
    "text":"wish cute guy stay hotel didnt think either scari plain kinda depress p",
    "cleaned_text":"wish cute guy stay hotel didnt think either scari plain kinda depress p",
    "normalized_text":"wish cute guy stay hotel didnt think either scari plain kinda depress p",
    "tokens":[
      "wish",
      "cute",
      "guy",
      "stay",
      "hotel",
      "didnt",
      "think",
      "either",
      "scari",
      "plain",
      "kinda",
      "depress"
    ],
    "token_count":12,
    "processed_text":"wish cute guy stay hotel didnt think either scari plain kinda depress"
  },
  {
    "label":4,
    "text":"back da night club awesom night",
    "cleaned_text":"back da night club awesom night",
    "normalized_text":"back da night club awesom night",
    "tokens":[
      "back",
      "da",
      "night",
      "club",
      "awesom",
      "night"
    ],
    "token_count":6,
    "processed_text":"back da night club awesom night"
  },
  {
    "label":0,
    "text":"realli fuck love",
    "cleaned_text":"realli fuck love",
    "normalized_text":"realli fuck love",
    "tokens":[
      "realli",
      "fuck",
      "love"
    ],
    "token_count":3,
    "processed_text":"realli fuck love"
  },
  {
    "label":4,
    "text":"hope head feel better today",
    "cleaned_text":"hope head feel better today",
    "normalized_text":"hope head feel better today",
    "tokens":[
      "hope",
      "head",
      "feel",
      "better",
      "today"
    ],
    "token_count":5,
    "processed_text":"hope head feel better today"
  },
  {
    "label":0,
    "text":"homemi babi must love lol",
    "cleaned_text":"homemi babi must love lol",
    "normalized_text":"homemi babi must love lol",
    "tokens":[
      "homemi",
      "babi",
      "love",
      "lol"
    ],
    "token_count":4,
    "processed_text":"homemi babi love lol"
  },
  {
    "label":4,
    "text":"boyfriend",
    "cleaned_text":"boyfriend",
    "normalized_text":"boyfriend",
    "tokens":[
      "boyfriend"
    ],
    "token_count":1,
    "processed_text":"boyfriend"
  },
  {
    "label":4,
    "text":"dant protect trust know better screw babi",
    "cleaned_text":"dant protect trust know better screw babi",
    "normalized_text":"dant protect trust know better screw babi",
    "tokens":[
      "dant",
      "protect",
      "trust",
      "know",
      "better",
      "screw",
      "babi"
    ],
    "token_count":7,
    "processed_text":"dant protect trust know better screw babi"
  },
  {
    "label":0,
    "text":"think anyth",
    "cleaned_text":"think anyth",
    "normalized_text":"think anyth",
    "tokens":[
      "think",
      "anyth"
    ],
    "token_count":2,
    "processed_text":"think anyth"
  },
  {
    "label":4,
    "text":"haha brian dobson quoti collaps ahern machinequot mauric ahern disast ff irishelect",
    "cleaned_text":"haha brian dobson quoti collaps ahern machinequot mauric ahern disast ff irishelect",
    "normalized_text":"haha brian dobson quoti collaps ahern machinequot mauric ahern disast ff irishelect",
    "tokens":[
      "haha",
      "brian",
      "dobson",
      "quoti",
      "collap",
      "ahern",
      "machinequot",
      "mauric",
      "ahern",
      "disast",
      "ff",
      "irishelect"
    ],
    "token_count":12,
    "processed_text":"haha brian dobson quoti collap ahern machinequot mauric ahern disast ff irishelect"
  },
  {
    "label":0,
    "text":"booooo that rubbish hope u",
    "cleaned_text":"booooo that rubbish hope u",
    "normalized_text":"booooo that rubbish hope u",
    "tokens":[
      "booooo",
      "rubbish",
      "hope"
    ],
    "token_count":3,
    "processed_text":"booooo rubbish hope"
  },
  {
    "label":0,
    "text":"nice cant figur autorecord featur caus miss show want recor",
    "cleaned_text":"nice cant figur autorecord featur caus miss show want recor",
    "normalized_text":"nice cant figur autorecord featur caus miss show want recor",
    "tokens":[
      "nice",
      "cant",
      "figur",
      "autorecord",
      "featur",
      "cau",
      "miss",
      "show",
      "want",
      "recor"
    ],
    "token_count":10,
    "processed_text":"nice cant figur autorecord featur cau miss show want recor"
  },
  {
    "label":0,
    "text":"soon need money dont need flat londonium anymor",
    "cleaned_text":"soon need money dont need flat londonium anymor",
    "normalized_text":"soon need money dont need flat londonium anymor",
    "tokens":[
      "soon",
      "need",
      "money",
      "dont",
      "need",
      "flat",
      "londonium",
      "anymor"
    ],
    "token_count":8,
    "processed_text":"soon need money dont need flat londonium anymor"
  },
  {
    "label":0,
    "text":"track mother day pink eye booooo",
    "cleaned_text":"track mother day pink eye booooo",
    "normalized_text":"track mother day pink eye booooo",
    "tokens":[
      "track",
      "mother",
      "day",
      "pink",
      "eye",
      "booooo"
    ],
    "token_count":6,
    "processed_text":"track mother day pink eye booooo"
  },
  {
    "label":4,
    "text":"go aunt hope townsquar",
    "cleaned_text":"go aunt hope townsquar",
    "normalized_text":"go aunt hope townsquar",
    "tokens":[
      "go",
      "aunt",
      "hope",
      "townsquar"
    ],
    "token_count":4,
    "processed_text":"go aunt hope townsquar"
  },
  {
    "label":4,
    "text":"well that good seem realli enjoy hang mom",
    "cleaned_text":"well that good seem realli enjoy hang mom",
    "normalized_text":"well that good seem realli enjoy hang mom",
    "tokens":[
      "well",
      "good",
      "seem",
      "realli",
      "enjoy",
      "hang",
      "mom"
    ],
    "token_count":7,
    "processed_text":"well good seem realli enjoy hang mom"
  },
  {
    "label":4,
    "text":"dri mouth sore head mean good time brightonpissup act blast everyon mental need coffe",
    "cleaned_text":"dri mouth sore head mean good time brightonpissup act blast everyon mental need coffe",
    "normalized_text":"dri mouth sore head mean good time brightonpissup act blast everyon mental need coffe",
    "tokens":[
      "dri",
      "mouth",
      "sore",
      "head",
      "mean",
      "good",
      "time",
      "brightonpissup",
      "act",
      "blast",
      "everyon",
      "mental",
      "need",
      "coff"
    ],
    "token_count":14,
    "processed_text":"dri mouth sore head mean good time brightonpissup act blast everyon mental need coff"
  },
  {
    "label":0,
    "text":"followerss",
    "cleaned_text":"followerss",
    "normalized_text":"followerss",
    "tokens":[
      "followerss"
    ],
    "token_count":1,
    "processed_text":"followerss"
  },
  {
    "label":0,
    "text":"dark run",
    "cleaned_text":"dark run",
    "normalized_text":"dark run",
    "tokens":[
      "dark",
      "run"
    ],
    "token_count":2,
    "processed_text":"dark run"
  },
  {
    "label":0,
    "text":"stupid twitter wont let upload pitter",
    "cleaned_text":"stupid twitter wont let upload pitter",
    "normalized_text":"stupid twitter wont let upload pitter",
    "tokens":[
      "stupid",
      "twitter",
      "wont",
      "let",
      "upload",
      "pitter"
    ],
    "token_count":6,
    "processed_text":"stupid twitter wont let upload pitter"
  },
  {
    "label":0,
    "text":"yea dont hahaha",
    "cleaned_text":"yea dont hahaha",
    "normalized_text":"yea dont hahaha",
    "tokens":[
      "yea",
      "dont",
      "hahaha"
    ],
    "token_count":3,
    "processed_text":"yea dont hahaha"
  },
  {
    "label":4,
    "text":"writechat hey ever need cheerleadermotiv boot hindequart im size shoe wpompom",
    "cleaned_text":"writechat hey ever need cheerleadermotiv boot hindequart im size shoe wpompom",
    "normalized_text":"writechat hey ever need cheerleadermotiv boot hindequart im size shoe wpompom",
    "tokens":[
      "writechat",
      "hey",
      "ever",
      "need",
      "boot",
      "hindequart",
      "im",
      "size",
      "shoe",
      "wpompom"
    ],
    "token_count":10,
    "processed_text":"writechat hey ever need boot hindequart im size shoe wpompom"
  },
  {
    "label":0,
    "text":"chocol strawberri good chocol kept fall",
    "cleaned_text":"chocol strawberri good chocol kept fall",
    "normalized_text":"chocol strawberri good chocol kept fall",
    "tokens":[
      "chocol",
      "strawberri",
      "good",
      "chocol",
      "kept",
      "fall"
    ],
    "token_count":6,
    "processed_text":"chocol strawberri good chocol kept fall"
  },
  {
    "label":4,
    "text":"total bake cooki figur im wear dinner",
    "cleaned_text":"total bake cooki figur im wear dinner",
    "normalized_text":"total bake cooki figur im wear dinner",
    "tokens":[
      "total",
      "bake",
      "cooki",
      "figur",
      "im",
      "wear",
      "dinner"
    ],
    "token_count":7,
    "processed_text":"total bake cooki figur im wear dinner"
  },
  {
    "label":0,
    "text":"dude cap lost sue",
    "cleaned_text":"dude cap lost sue",
    "normalized_text":"dude cap lost sue",
    "tokens":[
      "dude",
      "cap",
      "lost",
      "sue"
    ],
    "token_count":4,
    "processed_text":"dude cap lost sue"
  },
  {
    "label":0,
    "text":"that right job inaninyonga na sijazoea tai",
    "cleaned_text":"that right job inaninyonga na sijazoea tai",
    "normalized_text":"that right job inaninyonga na sijazoea tai",
    "tokens":[
      "right",
      "job",
      "inaninyonga",
      "na",
      "sijazoea",
      "tai"
    ],
    "token_count":6,
    "processed_text":"right job inaninyonga na sijazoea tai"
  },
  {
    "label":0,
    "text":"that meannnnn dont",
    "cleaned_text":"that meannnnn dont",
    "normalized_text":"that meannnnn dont",
    "tokens":[
      "meannnnn",
      "dont"
    ],
    "token_count":2,
    "processed_text":"meannnnn dont"
  },
  {
    "label":4,
    "text":"quotmak intellect well age wine yearsan irreplac resourc surviv happinessquot mom",
    "cleaned_text":"quotmak intellect well age wine yearsan irreplac resourc surviv happinessquot mom",
    "normalized_text":"quotmak intellect well age wine yearsan irreplac resourc surviv happinessquot mom",
    "tokens":[
      "quotmak",
      "intellect",
      "well",
      "age",
      "wine",
      "yearsan",
      "irreplac",
      "resourc",
      "surviv",
      "happinessquot",
      "mom"
    ],
    "token_count":11,
    "processed_text":"quotmak intellect well age wine yearsan irreplac resourc surviv happinessquot mom"
  },
  {
    "label":0,
    "text":"yeah ny still cool firstborn well keep quit busi well time",
    "cleaned_text":"yeah ny still cool firstborn well keep quit busi well time",
    "normalized_text":"yeah ny still cool firstborn well keep quit busi well time",
    "tokens":[
      "yeah",
      "ny",
      "still",
      "cool",
      "firstborn",
      "well",
      "keep",
      "quit",
      "busi",
      "well",
      "time"
    ],
    "token_count":11,
    "processed_text":"yeah ny still cool firstborn well keep quit busi well time"
  },
  {
    "label":4,
    "text":"flock bird gaggl gees pride lion herd cattl giggl teenag girl",
    "cleaned_text":"flock bird gaggl gees pride lion herd cattl giggl teenag girl",
    "normalized_text":"flock bird gaggl gees pride lion herd cattl giggl teenag girl",
    "tokens":[
      "flock",
      "bird",
      "gaggl",
      "gee",
      "pride",
      "lion",
      "herd",
      "cattl",
      "giggl",
      "teenag",
      "girl"
    ],
    "token_count":11,
    "processed_text":"flock bird gaggl gee pride lion herd cattl giggl teenag girl"
  },
  {
    "label":0,
    "text":"ruin good suit ball",
    "cleaned_text":"ruin good suit ball",
    "normalized_text":"ruin good suit ball",
    "tokens":[
      "ruin",
      "good",
      "suit",
      "ball"
    ],
    "token_count":4,
    "processed_text":"ruin good suit ball"
  },
  {
    "label":0,
    "text":"wish gaga wud hav concert london x",
    "cleaned_text":"wish gaga wud hav concert london x",
    "normalized_text":"wish gaga wud hav concert london x",
    "tokens":[
      "wish",
      "gaga",
      "wud",
      "hav",
      "concert",
      "london"
    ],
    "token_count":6,
    "processed_text":"wish gaga wud hav concert london"
  },
  {
    "label":0,
    "text":"awww",
    "cleaned_text":"awww",
    "normalized_text":"awww",
    "tokens":[
      "awww"
    ],
    "token_count":1,
    "processed_text":"awww"
  },
  {
    "label":0,
    "text":"pretti much sum tire right liter feel sick want day",
    "cleaned_text":"pretti much sum tire right liter feel sick want day",
    "normalized_text":"pretti much sum tire right liter feel sick want day",
    "tokens":[
      "pretti",
      "much",
      "sum",
      "tire",
      "right",
      "liter",
      "feel",
      "sick",
      "want",
      "day"
    ],
    "token_count":10,
    "processed_text":"pretti much sum tire right liter feel sick want day"
  },
  {
    "label":4,
    "text":"well kick west coast butt night point urgent get wisdom teeth complic",
    "cleaned_text":"well kick west coast butt night point urgent get wisdom teeth complic",
    "normalized_text":"well kick west coast butt night point urgent get wisdom teeth complic",
    "tokens":[
      "well",
      "kick",
      "west",
      "coast",
      "butt",
      "night",
      "point",
      "urgent",
      "get",
      "wisdom",
      "teeth",
      "complic"
    ],
    "token_count":12,
    "processed_text":"well kick west coast butt night point urgent get wisdom teeth complic"
  },
  {
    "label":0,
    "text":"ugh woke headach",
    "cleaned_text":"ugh woke headach",
    "normalized_text":"ugh woke headach",
    "tokens":[
      "ugh",
      "woke",
      "headach"
    ],
    "token_count":3,
    "processed_text":"ugh woke headach"
  },
  {
    "label":4,
    "text":"need eat someth see guy tomorrow haha",
    "cleaned_text":"need eat someth see guy tomorrow haha",
    "normalized_text":"need eat someth see guy tomorrow haha",
    "tokens":[
      "need",
      "eat",
      "someth",
      "see",
      "guy",
      "tomorrow",
      "haha"
    ],
    "token_count":7,
    "processed_text":"need eat someth see guy tomorrow haha"
  },
  {
    "label":0,
    "text":"sorri hon wanna talk know find",
    "cleaned_text":"sorri hon wanna talk know find",
    "normalized_text":"sorri hon wanna talk know find",
    "tokens":[
      "sorri",
      "hon",
      "wan",
      "na",
      "talk",
      "know",
      "find"
    ],
    "token_count":7,
    "processed_text":"sorri hon wan na talk know find"
  },
  {
    "label":0,
    "text":"ah well weather good last go bbq inform",
    "cleaned_text":"ah well weather good last go bbq inform",
    "normalized_text":"ah well weather good last go bbq inform",
    "tokens":[
      "ah",
      "well",
      "weather",
      "good",
      "last",
      "go",
      "bbq",
      "inform"
    ],
    "token_count":8,
    "processed_text":"ah well weather good last go bbq inform"
  },
  {
    "label":0,
    "text":"oh tear drop tear drop ill waitn lol",
    "cleaned_text":"oh tear drop tear drop ill waitn lol",
    "normalized_text":"oh tear drop tear drop ill waitn lol",
    "tokens":[
      "oh",
      "tear",
      "drop",
      "tear",
      "drop",
      "ill",
      "waitn",
      "lol"
    ],
    "token_count":8,
    "processed_text":"oh tear drop tear drop ill waitn lol"
  },
  {
    "label":0,
    "text":"vomit thing",
    "cleaned_text":"vomit thing",
    "normalized_text":"vomit thing",
    "tokens":[
      "vomit",
      "thing"
    ],
    "token_count":2,
    "processed_text":"vomit thing"
  },
  {
    "label":4,
    "text":"groan nice joke",
    "cleaned_text":"groan nice joke",
    "normalized_text":"groan nice joke",
    "tokens":[
      "groan",
      "nice",
      "joke"
    ],
    "token_count":3,
    "processed_text":"groan nice joke"
  },
  {
    "label":4,
    "text":"wish would sometim tweet stuff earli morn bad invent thingi",
    "cleaned_text":"wish would sometim tweet stuff earli morn bad invent thingi",
    "normalized_text":"wish would sometim tweet stuff earli morn bad invent thingi",
    "tokens":[
      "wish",
      "sometim",
      "tweet",
      "stuff",
      "earli",
      "morn",
      "bad",
      "invent",
      "thingi"
    ],
    "token_count":9,
    "processed_text":"wish sometim tweet stuff earli morn bad invent thingi"
  },
  {
    "label":4,
    "text":"love date night girl",
    "cleaned_text":"love date night girl",
    "normalized_text":"love date night girl",
    "tokens":[
      "love",
      "date",
      "night",
      "girl"
    ],
    "token_count":4,
    "processed_text":"love date night girl"
  },
  {
    "label":0,
    "text":"cant sleep text",
    "cleaned_text":"cant sleep text",
    "normalized_text":"cant sleep text",
    "tokens":[
      "cant",
      "sleep",
      "text"
    ],
    "token_count":3,
    "processed_text":"cant sleep text"
  },
  {
    "label":4,
    "text":"wow pcp acronym made onto acronymatticcom",
    "cleaned_text":"wow pcp acronym made onto acronymatticcom",
    "normalized_text":"wow pcp acronym made onto acronymatticcom",
    "tokens":[
      "wow",
      "pcp",
      "acronym",
      "made",
      "onto",
      "acronymatticcom"
    ],
    "token_count":6,
    "processed_text":"wow pcp acronym made onto acronymatticcom"
  },
  {
    "label":4,
    "text":"get fed",
    "cleaned_text":"get fed",
    "normalized_text":"get fed",
    "tokens":[
      "get",
      "fed"
    ],
    "token_count":2,
    "processed_text":"get fed"
  },
  {
    "label":0,
    "text":"wanna go ac tonight ever go im start think",
    "cleaned_text":"wanna go ac tonight ever go im start think",
    "normalized_text":"wanna go ac tonight ever go im start think",
    "tokens":[
      "wan",
      "na",
      "go",
      "ac",
      "tonight",
      "ever",
      "go",
      "im",
      "start",
      "think"
    ],
    "token_count":10,
    "processed_text":"wan na go ac tonight ever go im start think"
  },
  {
    "label":0,
    "text":"alreadi come home",
    "cleaned_text":"alreadi come home",
    "normalized_text":"alreadi come home",
    "tokens":[
      "alreadi",
      "come",
      "home"
    ],
    "token_count":3,
    "processed_text":"alreadi come home"
  },
  {
    "label":4,
    "text":"morn finish coffe sort garden get hot",
    "cleaned_text":"morn finish coffe sort garden get hot",
    "normalized_text":"morn finish coffe sort garden get hot",
    "tokens":[
      "morn",
      "finish",
      "coff",
      "sort",
      "garden",
      "get",
      "hot"
    ],
    "token_count":7,
    "processed_text":"morn finish coff sort garden get hot"
  },
  {
    "label":4,
    "text":"hahaha saw mall cop theater actuallyso saw twice even saw record",
    "cleaned_text":"hahaha saw mall cop theater actuallyso saw twice even saw record",
    "normalized_text":"hahaha saw mall cop theater actuallyso saw twice even saw record",
    "tokens":[
      "hahaha",
      "saw",
      "mall",
      "cop",
      "theater",
      "actuallyso",
      "saw",
      "twice",
      "even",
      "saw",
      "record"
    ],
    "token_count":11,
    "processed_text":"hahaha saw mall cop theater actuallyso saw twice even saw record"
  },
  {
    "label":0,
    "text":"dont play innoc variat rock age next normal grrr",
    "cleaned_text":"dont play innoc variat rock age next normal grrr",
    "normalized_text":"dont play innoc variat rock age next normal grrr",
    "tokens":[
      "dont",
      "play",
      "innoc",
      "variat",
      "rock",
      "age",
      "next",
      "normal",
      "grrr"
    ],
    "token_count":9,
    "processed_text":"dont play innoc variat rock age next normal grrr"
  },
  {
    "label":4,
    "text":"make enough pleeeeas sound sooo delici",
    "cleaned_text":"make enough pleeeeas sound sooo delici",
    "normalized_text":"make enough pleeeeas sound sooo delici",
    "tokens":[
      "make",
      "enough",
      "pleeeea",
      "sound",
      "sooo",
      "delici"
    ],
    "token_count":6,
    "processed_text":"make enough pleeeea sound sooo delici"
  },
  {
    "label":4,
    "text":"awwww bet youll amaz nerv new countri stuff youll fine x",
    "cleaned_text":"awwww bet youll amaz nerv new countri stuff youll fine x",
    "normalized_text":"awwww bet youll amaz nerv new countri stuff youll fine x",
    "tokens":[
      "awwww",
      "bet",
      "youll",
      "amaz",
      "nerv",
      "new",
      "countri",
      "stuff",
      "youll",
      "fine"
    ],
    "token_count":10,
    "processed_text":"awwww bet youll amaz nerv new countri stuff youll fine"
  },
  {
    "label":0,
    "text":"trip memori lane hertogenbosch continu trip belgium look fwd",
    "cleaned_text":"trip memori lane hertogenbosch continu trip belgium look fwd",
    "normalized_text":"trip memori lane hertogenbosch continu trip belgium look fwd",
    "tokens":[
      "trip",
      "memori",
      "lane",
      "hertogenbosch",
      "continu",
      "trip",
      "belgium",
      "look",
      "fwd"
    ],
    "token_count":9,
    "processed_text":"trip memori lane hertogenbosch continu trip belgium look fwd"
  },
  {
    "label":4,
    "text":"babi shower numbah do",
    "cleaned_text":"babi shower numbah do",
    "normalized_text":"babi shower numbah do",
    "tokens":[
      "babi",
      "shower",
      "numbah"
    ],
    "token_count":3,
    "processed_text":"babi shower numbah"
  },
  {
    "label":0,
    "text":"doesnt think today could get much wors miss grandpa",
    "cleaned_text":"doesnt think today could get much wors miss grandpa",
    "normalized_text":"doesnt think today could get much wors miss grandpa",
    "tokens":[
      "doesnt",
      "think",
      "today",
      "get",
      "much",
      "wor",
      "miss",
      "grandpa"
    ],
    "token_count":8,
    "processed_text":"doesnt think today get much wor miss grandpa"
  },
  {
    "label":4,
    "text":"witti time afternoon",
    "cleaned_text":"witti time afternoon",
    "normalized_text":"witti time afternoon",
    "tokens":[
      "witti",
      "time",
      "afternoon"
    ],
    "token_count":3,
    "processed_text":"witti time afternoon"
  },
  {
    "label":4,
    "text":"got done first set senior pic pretti",
    "cleaned_text":"got done first set senior pic pretti",
    "normalized_text":"got done first set senior pic pretti",
    "tokens":[
      "got",
      "done",
      "first",
      "set",
      "senior",
      "pic",
      "pretti"
    ],
    "token_count":7,
    "processed_text":"got done first set senior pic pretti"
  },
  {
    "label":4,
    "text":"man want breakfast hous chocol lover heaven lol",
    "cleaned_text":"man want breakfast hous chocol lover heaven lol",
    "normalized_text":"man want breakfast hous chocol lover heaven lol",
    "tokens":[
      "man",
      "want",
      "breakfast",
      "hou",
      "chocol",
      "lover",
      "heaven",
      "lol"
    ],
    "token_count":8,
    "processed_text":"man want breakfast hou chocol lover heaven lol"
  },
  {
    "label":0,
    "text":"need blackerri bold bitch ahhhh hate sonywalkmanphonegay",
    "cleaned_text":"need blackerri bold bitch ahhhh hate sonywalkmanphonegay",
    "normalized_text":"need blackerri bold bitch ahhhh hate sonywalkmanphonegay",
    "tokens":[
      "need",
      "blackerri",
      "bold",
      "bitch",
      "ahhhh",
      "hate"
    ],
    "token_count":6,
    "processed_text":"need blackerri bold bitch ahhhh hate"
  },
  {
    "label":4,
    "text":"hi iam new twitter",
    "cleaned_text":"hi iam new twitter",
    "normalized_text":"hi iam new twitter",
    "tokens":[
      "hi",
      "iam",
      "new",
      "twitter"
    ],
    "token_count":4,
    "processed_text":"hi iam new twitter"
  },
  {
    "label":0,
    "text":"talk enni get readi work toadsuck way spend saturday",
    "cleaned_text":"talk enni get readi work toadsuck way spend saturday",
    "normalized_text":"talk enni get readi work toadsuck way spend saturday",
    "tokens":[
      "talk",
      "enni",
      "get",
      "readi",
      "work",
      "toadsuck",
      "way",
      "spend",
      "saturday"
    ],
    "token_count":9,
    "processed_text":"talk enni get readi work toadsuck way spend saturday"
  },
  {
    "label":0,
    "text":"done hope sunday gone",
    "cleaned_text":"done hope sunday gone",
    "normalized_text":"done hope sunday gone",
    "tokens":[
      "done",
      "hope",
      "sunday",
      "gone"
    ],
    "token_count":4,
    "processed_text":"done hope sunday gone"
  },
  {
    "label":0,
    "text":"frustrat still cant see conan first show youtub iphon",
    "cleaned_text":"frustrat still cant see conan first show youtub iphon",
    "normalized_text":"frustrat still cant see conan first show youtub iphon",
    "tokens":[
      "frustrat",
      "still",
      "cant",
      "see",
      "conan",
      "first",
      "show",
      "youtub",
      "iphon"
    ],
    "token_count":9,
    "processed_text":"frustrat still cant see conan first show youtub iphon"
  },
  {
    "label":4,
    "text":"agre dumb ass nigger need go",
    "cleaned_text":"agre dumb ass nigger need go",
    "normalized_text":"agre dumb ass nigger need go",
    "tokens":[
      "agr",
      "dumb",
      "ass",
      "nigger",
      "need",
      "go"
    ],
    "token_count":6,
    "processed_text":"agr dumb ass nigger need go"
  },
  {
    "label":4,
    "text":"aww yay im glad got meet jane xxx",
    "cleaned_text":"aww yay im glad got meet jane xxx",
    "normalized_text":"aww yay im glad got meet jane xxx",
    "tokens":[
      "aww",
      "yay",
      "im",
      "glad",
      "got",
      "meet",
      "jane",
      "xxx"
    ],
    "token_count":8,
    "processed_text":"aww yay im glad got meet jane xxx"
  },
  {
    "label":4,
    "text":"could kristen stewart couldnt get weirder lol wait there kiss ah taylor fuckin sexi especi shirtless",
    "cleaned_text":"could kristen stewart couldnt get weirder lol wait there kiss ah taylor fuckin sexi especi shirtless",
    "normalized_text":"could kristen stewart couldnt get weirder lol wait there kiss ah taylor fuckin sexi especi shirtless",
    "tokens":[
      "kristen",
      "stewart",
      "couldnt",
      "get",
      "weirder",
      "lol",
      "wait",
      "kiss",
      "ah",
      "taylor",
      "fuckin",
      "sexi",
      "especi",
      "shirtless"
    ],
    "token_count":14,
    "processed_text":"kristen stewart couldnt get weirder lol wait kiss ah taylor fuckin sexi especi shirtless"
  },
  {
    "label":0,
    "text":"hmmmm want confer call guess ill go home",
    "cleaned_text":"hmmmm want confer call guess ill go home",
    "normalized_text":"hmmmm want confer call guess ill go home",
    "tokens":[
      "hmmmm",
      "want",
      "confer",
      "call",
      "guess",
      "ill",
      "go",
      "home"
    ],
    "token_count":8,
    "processed_text":"hmmmm want confer call guess ill go home"
  },
  {
    "label":4,
    "text":"goin mor chese wnt pizza cook pizza ill burn n set da alarm ill stick hot chees yummi",
    "cleaned_text":"goin mor chese wnt pizza cook pizza ill burn n set da alarm ill stick hot chees yummi",
    "normalized_text":"goin mor chese wnt pizza cook pizza ill burn n set da alarm ill stick hot chees yummi",
    "tokens":[
      "goin",
      "mor",
      "chese",
      "wnt",
      "pizza",
      "cook",
      "pizza",
      "ill",
      "burn",
      "set",
      "da",
      "alarm",
      "ill",
      "stick",
      "hot",
      "chee",
      "yummi"
    ],
    "token_count":17,
    "processed_text":"goin mor chese wnt pizza cook pizza ill burn set da alarm ill stick hot chee yummi"
  },
  {
    "label":0,
    "text":"wonder theyr ask interest im home insid booo",
    "cleaned_text":"wonder theyr ask interest im home insid booo",
    "normalized_text":"wonder theyr ask interest im home insid booo",
    "tokens":[
      "wonder",
      "theyr",
      "ask",
      "interest",
      "im",
      "home",
      "insid",
      "booo"
    ],
    "token_count":8,
    "processed_text":"wonder theyr ask interest im home insid booo"
  },
  {
    "label":0,
    "text":"plan go csydney tonight look like rain",
    "cleaned_text":"plan go csydney tonight look like rain",
    "normalized_text":"plan go csydney tonight look like rain",
    "tokens":[
      "plan",
      "go",
      "csydney",
      "tonight",
      "look",
      "like",
      "rain"
    ],
    "token_count":7,
    "processed_text":"plan go csydney tonight look like rain"
  },
  {
    "label":0,
    "text":"amhzz u get invit miss u",
    "cleaned_text":"amhzz u get invit miss u",
    "normalized_text":"amhzz u get invit miss u",
    "tokens":[
      "amhzz",
      "get",
      "invit",
      "miss"
    ],
    "token_count":4,
    "processed_text":"amhzz get invit miss"
  },
  {
    "label":4,
    "text":"home watch tvwhat beauti day",
    "cleaned_text":"home watch tvwhat beauti day",
    "normalized_text":"home watch tvwhat beauti day",
    "tokens":[
      "home",
      "watch",
      "tvwhat",
      "beauti",
      "day"
    ],
    "token_count":5,
    "processed_text":"home watch tvwhat beauti day"
  },
  {
    "label":0,
    "text":"bath bunch kid actual two toddler load fun",
    "cleaned_text":"bath bunch kid actual two toddler load fun",
    "normalized_text":"bath bunch kid actual two toddler load fun",
    "tokens":[
      "bath",
      "bunch",
      "kid",
      "actual",
      "two",
      "toddler",
      "load",
      "fun"
    ],
    "token_count":8,
    "processed_text":"bath bunch kid actual two toddler load fun"
  },
  {
    "label":0,
    "text":"good day week",
    "cleaned_text":"good day week",
    "normalized_text":"good day week",
    "tokens":[
      "good",
      "day",
      "week"
    ],
    "token_count":3,
    "processed_text":"good day week"
  },
  {
    "label":4,
    "text":"done record millionair bust plan shoot video promo healthi tv",
    "cleaned_text":"done record millionair bust plan shoot video promo healthi tv",
    "normalized_text":"done record millionair bust plan shoot video promo healthi tv",
    "tokens":[
      "done",
      "record",
      "millionair",
      "bust",
      "plan",
      "shoot",
      "video",
      "promo",
      "healthi",
      "tv"
    ],
    "token_count":10,
    "processed_text":"done record millionair bust plan shoot video promo healthi tv"
  },
  {
    "label":4,
    "text":"thank bless day take care",
    "cleaned_text":"thank bless day take care",
    "normalized_text":"thank bless day take care",
    "tokens":[
      "thank",
      "bless",
      "day",
      "take",
      "care"
    ],
    "token_count":5,
    "processed_text":"thank bless day take care"
  },
  {
    "label":4,
    "text":"watch two half men",
    "cleaned_text":"watch two half men",
    "normalized_text":"watch two half men",
    "tokens":[
      "watch",
      "two",
      "half",
      "men"
    ],
    "token_count":4,
    "processed_text":"watch two half men"
  },
  {
    "label":4,
    "text":"agre young cope dont want susan boyl win pretti much winner",
    "cleaned_text":"agre young cope dont want susan boyl win pretti much winner",
    "normalized_text":"agre young cope dont want susan boyl win pretti much winner",
    "tokens":[
      "agr",
      "young",
      "cope",
      "dont",
      "want",
      "susan",
      "boyl",
      "win",
      "pretti",
      "much",
      "winner"
    ],
    "token_count":11,
    "processed_text":"agr young cope dont want susan boyl win pretti much winner"
  },
  {
    "label":4,
    "text":"oh understand well good luck write gurl",
    "cleaned_text":"oh understand well good luck write gurl",
    "normalized_text":"oh understand well good luck write gurl",
    "tokens":[
      "oh",
      "understand",
      "well",
      "good",
      "luck",
      "write",
      "gurl"
    ],
    "token_count":7,
    "processed_text":"oh understand well good luck write gurl"
  },
  {
    "label":4,
    "text":"im write stuff right",
    "cleaned_text":"im write stuff right",
    "normalized_text":"im write stuff right",
    "tokens":[
      "im",
      "write",
      "stuff",
      "right"
    ],
    "token_count":4,
    "processed_text":"im write stuff right"
  },
  {
    "label":4,
    "text":"cd offici come east coast feel day final come",
    "cleaned_text":"cd offici come east coast feel day final come",
    "normalized_text":"cd offici come east coast feel day final come",
    "tokens":[
      "cd",
      "offici",
      "come",
      "east",
      "coast",
      "feel",
      "day",
      "final",
      "come"
    ],
    "token_count":9,
    "processed_text":"cd offici come east coast feel day final come"
  },
  {
    "label":4,
    "text":"wow updat",
    "cleaned_text":"wow updat",
    "normalized_text":"wow updat",
    "tokens":[
      "wow",
      "updat"
    ],
    "token_count":2,
    "processed_text":"wow updat"
  },
  {
    "label":0,
    "text":"bestivehad werefrom new zealand cant rememb name best cant find longer",
    "cleaned_text":"bestivehad werefrom new zealand cant rememb name best cant find longer",
    "normalized_text":"bestivehad werefrom new zealand cant rememb name best cant find longer",
    "tokens":[
      "bestivehad",
      "werefrom",
      "new",
      "zealand",
      "cant",
      "rememb",
      "name",
      "best",
      "cant",
      "find",
      "longer"
    ],
    "token_count":11,
    "processed_text":"bestivehad werefrom new zealand cant rememb name best cant find longer"
  },
  {
    "label":0,
    "text":"shitttt dog get surgeri who scare kristin",
    "cleaned_text":"shitttt dog get surgeri who scare kristin",
    "normalized_text":"shitttt dog get surgeri who scare kristin",
    "tokens":[
      "shitttt",
      "dog",
      "get",
      "surgeri",
      "scare",
      "kristin"
    ],
    "token_count":6,
    "processed_text":"shitttt dog get surgeri scare kristin"
  },
  {
    "label":4,
    "text":"woke good charlott wonder head im listen young hopeless",
    "cleaned_text":"woke good charlott wonder head im listen young hopeless",
    "normalized_text":"woke good charlott wonder head im listen young hopeless",
    "tokens":[
      "woke",
      "good",
      "charlott",
      "wonder",
      "head",
      "im",
      "listen",
      "young",
      "hopeless"
    ],
    "token_count":9,
    "processed_text":"woke good charlott wonder head im listen young hopeless"
  },
  {
    "label":0,
    "text":"laker parad look pretti cool couldnt make def celebr tomorrow night",
    "cleaned_text":"laker parad look pretti cool couldnt make def celebr tomorrow night",
    "normalized_text":"laker parad look pretti cool couldnt make def celebr tomorrow night",
    "tokens":[
      "laker",
      "parad",
      "look",
      "pretti",
      "cool",
      "couldnt",
      "make",
      "def",
      "celebr",
      "tomorrow",
      "night"
    ],
    "token_count":11,
    "processed_text":"laker parad look pretti cool couldnt make def celebr tomorrow night"
  },
  {
    "label":4,
    "text":"sleep well good luck u",
    "cleaned_text":"sleep well good luck u",
    "normalized_text":"sleep well good luck u",
    "tokens":[
      "sleep",
      "well",
      "good",
      "luck"
    ],
    "token_count":4,
    "processed_text":"sleep well good luck"
  },
  {
    "label":4,
    "text":"prob long your steal soul like clown good",
    "cleaned_text":"prob long your steal soul like clown good",
    "normalized_text":"prob long your steal soul like clown good",
    "tokens":[
      "prob",
      "long",
      "steal",
      "soul",
      "like",
      "clown",
      "good"
    ],
    "token_count":7,
    "processed_text":"prob long steal soul like clown good"
  },
  {
    "label":0,
    "text":"im feel tonight episod sytycd",
    "cleaned_text":"im feel tonight episod sytycd",
    "normalized_text":"im feel tonight episod sytycd",
    "tokens":[
      "im",
      "feel",
      "tonight",
      "episod",
      "sytycd"
    ],
    "token_count":5,
    "processed_text":"im feel tonight episod sytycd"
  },
  {
    "label":0,
    "text":"yesssssss know club suck run morn",
    "cleaned_text":"yesssssss know club suck run morn",
    "normalized_text":"yesssssss know club suck run morn",
    "tokens":[
      "yesssssss",
      "know",
      "club",
      "suck",
      "run",
      "morn"
    ],
    "token_count":6,
    "processed_text":"yesssssss know club suck run morn"
  },
  {
    "label":4,
    "text":"listen jason mraz album wait realli good",
    "cleaned_text":"listen jason mraz album wait realli good",
    "normalized_text":"listen jason mraz album wait realli good",
    "tokens":[
      "listen",
      "jason",
      "mraz",
      "album",
      "wait",
      "realli",
      "good"
    ],
    "token_count":7,
    "processed_text":"listen jason mraz album wait realli good"
  },
  {
    "label":4,
    "text":"awww yeah tri im glad yall fun guy parti hard like said aint parti like zane parti",
    "cleaned_text":"awww yeah tri im glad yall fun guy parti hard like said aint parti like zane parti",
    "normalized_text":"awww yeah tri im glad yall fun guy parti hard like said aint parti like zane parti",
    "tokens":[
      "awww",
      "yeah",
      "tri",
      "im",
      "glad",
      "yall",
      "fun",
      "guy",
      "parti",
      "hard",
      "like",
      "said",
      "aint",
      "parti",
      "like",
      "zane",
      "parti"
    ],
    "token_count":17,
    "processed_text":"awww yeah tri im glad yall fun guy parti hard like said aint parti like zane parti"
  },
  {
    "label":0,
    "text":"hate quotsuportersquot leav thing arent go team way vikinglt vlerenga",
    "cleaned_text":"hate quotsuportersquot leav thing arent go team way vikinglt vlerenga",
    "normalized_text":"hate quotsuportersquot leav thing arent go team way vikinglt vlerenga",
    "tokens":[
      "hate",
      "leav",
      "thing",
      "arent",
      "go",
      "team",
      "way",
      "vikinglt",
      "vlerenga"
    ],
    "token_count":9,
    "processed_text":"hate leav thing arent go team way vikinglt vlerenga"
  },
  {
    "label":4,
    "text":"set twitter account",
    "cleaned_text":"set twitter account",
    "normalized_text":"set twitter account",
    "tokens":[
      "set",
      "twitter",
      "account"
    ],
    "token_count":3,
    "processed_text":"set twitter account"
  },
  {
    "label":0,
    "text":"dont tell caus im gonna want long blast touch screen",
    "cleaned_text":"dont tell caus im gonna want long blast touch screen",
    "normalized_text":"dont tell caus im gonna want long blast touch screen",
    "tokens":[
      "dont",
      "tell",
      "cau",
      "im",
      "gon",
      "na",
      "want",
      "long",
      "blast",
      "touch",
      "screen"
    ],
    "token_count":11,
    "processed_text":"dont tell cau im gon na want long blast touch screen"
  },
  {
    "label":0,
    "text":"store buy though",
    "cleaned_text":"store buy though",
    "normalized_text":"store buy though",
    "tokens":[
      "store",
      "buy",
      "though"
    ],
    "token_count":3,
    "processed_text":"store buy though"
  },
  {
    "label":0,
    "text":"respond twice find go poke boy see care scamper",
    "cleaned_text":"respond twice find go poke boy see care scamper",
    "normalized_text":"respond twice find go poke boy see care scamper",
    "tokens":[
      "respond",
      "twice",
      "find",
      "go",
      "poke",
      "boy",
      "see",
      "care",
      "scamper"
    ],
    "token_count":9,
    "processed_text":"respond twice find go poke boy see care scamper"
  },
  {
    "label":0,
    "text":"geocachingcom quotserv busyquot",
    "cleaned_text":"geocachingcom quotserv busyquot",
    "normalized_text":"geocachingcom quotserv busyquot",
    "tokens":[
      "geocachingcom",
      "quotserv",
      "busyquot"
    ],
    "token_count":3,
    "processed_text":"geocachingcom quotserv busyquot"
  },
  {
    "label":4,
    "text":"lol haha ok think moment tune later ps hope u feelin better pp cav hahaha",
    "cleaned_text":"lol haha ok think moment tune later ps hope u feelin better pp cav hahaha",
    "normalized_text":"lol haha ok think moment tune later ps hope u feelin better pp cav hahaha",
    "tokens":[
      "lol",
      "haha",
      "ok",
      "think",
      "moment",
      "tune",
      "later",
      "ps",
      "hope",
      "feelin",
      "better",
      "pp",
      "cav",
      "hahaha"
    ],
    "token_count":14,
    "processed_text":"lol haha ok think moment tune later ps hope feelin better pp cav hahaha"
  },
  {
    "label":0,
    "text":"oh realli that good",
    "cleaned_text":"oh realli that good",
    "normalized_text":"oh realli that good",
    "tokens":[
      "oh",
      "realli",
      "good"
    ],
    "token_count":3,
    "processed_text":"oh realli good"
  },
  {
    "label":4,
    "text":"god night everyon",
    "cleaned_text":"god night everyon",
    "normalized_text":"god night everyon",
    "tokens":[
      "god",
      "night",
      "everyon"
    ],
    "token_count":3,
    "processed_text":"god night everyon"
  },
  {
    "label":0,
    "text":"cold tire sick",
    "cleaned_text":"cold tire sick",
    "normalized_text":"cold tire sick",
    "tokens":[
      "cold",
      "tire",
      "sick"
    ],
    "token_count":3,
    "processed_text":"cold tire sick"
  },
  {
    "label":4,
    "text":"experienc bi issu rim know",
    "cleaned_text":"experienc bi issu rim know",
    "normalized_text":"experienc bi issu rim know",
    "tokens":[
      "experienc",
      "bi",
      "issu",
      "rim",
      "know"
    ],
    "token_count":5,
    "processed_text":"experienc bi issu rim know"
  },
  {
    "label":0,
    "text":"took big spill morn miss skin thigh ankl elbow ill also need new short rear derailleur",
    "cleaned_text":"took big spill morn miss skin thigh ankl elbow ill also need new short rear derailleur",
    "normalized_text":"took big spill morn miss skin thigh ankl elbow ill also need new short rear derailleur",
    "tokens":[
      "took",
      "big",
      "spill",
      "morn",
      "miss",
      "skin",
      "thigh",
      "ankl",
      "elbow",
      "ill",
      "also",
      "need",
      "new",
      "short",
      "rear",
      "derailleur"
    ],
    "token_count":16,
    "processed_text":"took big spill morn miss skin thigh ankl elbow ill also need new short rear derailleur"
  },
  {
    "label":4,
    "text":"blipfm acct wanna add fave tend like post",
    "cleaned_text":"blipfm acct wanna add fave tend like post",
    "normalized_text":"blipfm acct wanna add fave tend like post",
    "tokens":[
      "blipfm",
      "acct",
      "wan",
      "na",
      "add",
      "fave",
      "tend",
      "like",
      "post"
    ],
    "token_count":9,
    "processed_text":"blipfm acct wan na add fave tend like post"
  },
  {
    "label":0,
    "text":"ugh hate app wont open like restor ipod amp also cant sleep",
    "cleaned_text":"ugh hate app wont open like restor ipod amp also cant sleep",
    "normalized_text":"ugh hate app wont open like restor ipod amp also cant sleep",
    "tokens":[
      "ugh",
      "hate",
      "app",
      "wont",
      "open",
      "like",
      "restor",
      "ipod",
      "amp",
      "also",
      "cant",
      "sleep"
    ],
    "token_count":12,
    "processed_text":"ugh hate app wont open like restor ipod amp also cant sleep"
  },
  {
    "label":4,
    "text":"your welcom doin",
    "cleaned_text":"your welcom doin",
    "normalized_text":"your welcom doin",
    "tokens":[
      "welcom",
      "doin"
    ],
    "token_count":2,
    "processed_text":"welcom doin"
  },
  {
    "label":0,
    "text":"watch seventi show",
    "cleaned_text":"watch seventi show",
    "normalized_text":"watch seventi show",
    "tokens":[
      "watch",
      "seventi",
      "show"
    ],
    "token_count":3,
    "processed_text":"watch seventi show"
  },
  {
    "label":0,
    "text":"grryou need sex everi hour hour",
    "cleaned_text":"grryou need sex everi hour hour",
    "normalized_text":"grryou need sex everi hour hour",
    "tokens":[
      "grryou",
      "need",
      "sex",
      "everi",
      "hour",
      "hour"
    ],
    "token_count":6,
    "processed_text":"grryou need sex everi hour hour"
  },
  {
    "label":4,
    "text":"demi pleaasseee come manchest uk",
    "cleaned_text":"demi pleaasseee come manchest uk",
    "normalized_text":"demi pleaasseee come manchest uk",
    "tokens":[
      "demi",
      "pleaassee",
      "come",
      "manchest",
      "uk"
    ],
    "token_count":5,
    "processed_text":"demi pleaassee come manchest uk"
  },
  {
    "label":0,
    "text":"cant even explain cute grey tabbi cat saw today broke heart cant adopt",
    "cleaned_text":"cant even explain cute grey tabbi cat saw today broke heart cant adopt",
    "normalized_text":"cant even explain cute grey tabbi cat saw today broke heart cant adopt",
    "tokens":[
      "cant",
      "even",
      "explain",
      "cute",
      "grey",
      "tabbi",
      "cat",
      "saw",
      "today",
      "broke",
      "heart",
      "cant",
      "adopt"
    ],
    "token_count":13,
    "processed_text":"cant even explain cute grey tabbi cat saw today broke heart cant adopt"
  },
  {
    "label":0,
    "text":"bubbl everywher wrong soap dishwash",
    "cleaned_text":"bubbl everywher wrong soap dishwash",
    "normalized_text":"bubbl everywher wrong soap dishwash",
    "tokens":[
      "bubbl",
      "everywh",
      "wrong",
      "soap",
      "dishwash"
    ],
    "token_count":5,
    "processed_text":"bubbl everywh wrong soap dishwash"
  },
  {
    "label":0,
    "text":"gave almighti paper cut cranberri carton",
    "cleaned_text":"gave almighti paper cut cranberri carton",
    "normalized_text":"gave almighti paper cut cranberri carton",
    "tokens":[
      "gave",
      "almighti",
      "paper",
      "cut",
      "cranberri",
      "carton"
    ],
    "token_count":6,
    "processed_text":"gave almighti paper cut cranberri carton"
  },
  {
    "label":0,
    "text":"im offic cant click link fact im twit sli alreadi bad enough god im rebel",
    "cleaned_text":"im offic cant click link fact im twit sli alreadi bad enough god im rebel",
    "normalized_text":"im offic cant click link fact im twit sli alreadi bad enough god im rebel",
    "tokens":[
      "im",
      "offic",
      "cant",
      "click",
      "link",
      "fact",
      "im",
      "twit",
      "sli",
      "alreadi",
      "bad",
      "enough",
      "god",
      "im",
      "rebel"
    ],
    "token_count":15,
    "processed_text":"im offic cant click link fact im twit sli alreadi bad enough god im rebel"
  },
  {
    "label":4,
    "text":"home need workout also es see tomorrow hun hope everyon well good night",
    "cleaned_text":"home need workout also es see tomorrow hun hope everyon well good night",
    "normalized_text":"home need workout also es see tomorrow hun hope everyon well good night",
    "tokens":[
      "home",
      "need",
      "workout",
      "also",
      "es",
      "see",
      "tomorrow",
      "hun",
      "hope",
      "everyon",
      "well",
      "good",
      "night"
    ],
    "token_count":13,
    "processed_text":"home need workout also es see tomorrow hun hope everyon well good night"
  },
  {
    "label":0,
    "text":"didnt find anyth good cloth shop",
    "cleaned_text":"didnt find anyth good cloth shop",
    "normalized_text":"didnt find anyth good cloth shop",
    "tokens":[
      "didnt",
      "find",
      "anyth",
      "good",
      "cloth",
      "shop"
    ],
    "token_count":6,
    "processed_text":"didnt find anyth good cloth shop"
  },
  {
    "label":0,
    "text":"im sorri mom think call denial that dont call friend anymor either like die cancer",
    "cleaned_text":"im sorri mom think call denial that dont call friend anymor either like die cancer",
    "normalized_text":"im sorri mom think call denial that dont call friend anymor either like die cancer",
    "tokens":[
      "im",
      "sorri",
      "mom",
      "think",
      "call",
      "denial",
      "dont",
      "call",
      "friend",
      "anymor",
      "either",
      "like",
      "die",
      "cancer"
    ],
    "token_count":14,
    "processed_text":"im sorri mom think call denial dont call friend anymor either like die cancer"
  },
  {
    "label":4,
    "text":"im rest readi go sunday funday bitch",
    "cleaned_text":"im rest readi go sunday funday bitch",
    "normalized_text":"im rest readi go sunday funday bitch",
    "tokens":[
      "im",
      "rest",
      "readi",
      "go",
      "sunday",
      "funday",
      "bitch"
    ],
    "token_count":7,
    "processed_text":"im rest readi go sunday funday bitch"
  },
  {
    "label":4,
    "text":"got better late never",
    "cleaned_text":"got better late never",
    "normalized_text":"got better late never",
    "tokens":[
      "got",
      "better",
      "late",
      "never"
    ],
    "token_count":4,
    "processed_text":"got better late never"
  },
  {
    "label":4,
    "text":"download",
    "cleaned_text":"download",
    "normalized_text":"download",
    "tokens":[
      "download"
    ],
    "token_count":1,
    "processed_text":"download"
  },
  {
    "label":0,
    "text":"even tube strike blame week",
    "cleaned_text":"even tube strike blame week",
    "normalized_text":"even tube strike blame week",
    "tokens":[
      "even",
      "tube",
      "strike",
      "blame",
      "week"
    ],
    "token_count":5,
    "processed_text":"even tube strike blame week"
  },
  {
    "label":0,
    "text":"heylooo okay monday bad how",
    "cleaned_text":"heylooo okay monday bad how",
    "normalized_text":"heylooo okay monday bad how",
    "tokens":[
      "heylooo",
      "okay",
      "monday",
      "bad"
    ],
    "token_count":4,
    "processed_text":"heylooo okay monday bad"
  },
  {
    "label":4,
    "text":"ahh longview green day remind tri swear along loudli could wo told lol",
    "cleaned_text":"ahh longview green day remind tri swear along loudli could wo told lol",
    "normalized_text":"ahh longview green day remind tri swear along loudli could wo told lol",
    "tokens":[
      "ahh",
      "longview",
      "green",
      "day",
      "remind",
      "tri",
      "swear",
      "along",
      "loudli",
      "wo",
      "told",
      "lol"
    ],
    "token_count":12,
    "processed_text":"ahh longview green day remind tri swear along loudli wo told lol"
  },
  {
    "label":0,
    "text":"dooooooont say much amaz pic",
    "cleaned_text":"dooooooont say much amaz pic",
    "normalized_text":"dooooooont say much amaz pic",
    "tokens":[
      "dooooooont",
      "say",
      "much",
      "amaz",
      "pic"
    ],
    "token_count":5,
    "processed_text":"dooooooont say much amaz pic"
  },
  {
    "label":0,
    "text":"town tomorrow window shop co im broke",
    "cleaned_text":"town tomorrow window shop co im broke",
    "normalized_text":"town tomorrow window shop co im broke",
    "tokens":[
      "town",
      "tomorrow",
      "window",
      "shop",
      "co",
      "im",
      "broke"
    ],
    "token_count":7,
    "processed_text":"town tomorrow window shop co im broke"
  },
  {
    "label":4,
    "text":"sssh tri keep quiet doesnt like beaten",
    "cleaned_text":"sssh tri keep quiet doesnt like beaten",
    "normalized_text":"sssh tri keep quiet doesnt like beaten",
    "tokens":[
      "sssh",
      "tri",
      "keep",
      "quiet",
      "doesnt",
      "like",
      "beaten"
    ],
    "token_count":7,
    "processed_text":"sssh tri keep quiet doesnt like beaten"
  },
  {
    "label":0,
    "text":"miss certain person certain past time hope well wish go atl weekend",
    "cleaned_text":"miss certain person certain past time hope well wish go atl weekend",
    "normalized_text":"miss certain person certain past time hope well wish go atl weekend",
    "tokens":[
      "miss",
      "certain",
      "person",
      "certain",
      "past",
      "time",
      "hope",
      "well",
      "wish",
      "go",
      "atl",
      "weekend"
    ],
    "token_count":12,
    "processed_text":"miss certain person certain past time hope well wish go atl weekend"
  },
  {
    "label":4,
    "text":"cookout minu min",
    "cleaned_text":"cookout minu min",
    "normalized_text":"cookout minu min",
    "tokens":[
      "cookout",
      "minu",
      "min"
    ],
    "token_count":3,
    "processed_text":"cookout minu min"
  },
  {
    "label":0,
    "text":"blehh hot bad stinki ga",
    "cleaned_text":"blehh hot bad stinki ga",
    "normalized_text":"blehh hot bad stinki ga",
    "tokens":[
      "blehh",
      "hot",
      "bad",
      "stinki",
      "ga"
    ],
    "token_count":5,
    "processed_text":"blehh hot bad stinki ga"
  },
  {
    "label":0,
    "text":"raini day hope clear get work need soem rain though",
    "cleaned_text":"raini day hope clear get work need soem rain though",
    "normalized_text":"raini day hope clear get work need soem rain though",
    "tokens":[
      "raini",
      "day",
      "hope",
      "clear",
      "get",
      "work",
      "need",
      "soem",
      "rain",
      "though"
    ],
    "token_count":10,
    "processed_text":"raini day hope clear get work need soem rain though"
  },
  {
    "label":0,
    "text":"sorri call realli late",
    "cleaned_text":"sorri call realli late",
    "normalized_text":"sorri call realli late",
    "tokens":[
      "sorri",
      "call",
      "realli",
      "late"
    ],
    "token_count":4,
    "processed_text":"sorri call realli late"
  },
  {
    "label":0,
    "text":"daaaaaa monday offici start load energi drink",
    "cleaned_text":"daaaaaa monday offici start load energi drink",
    "normalized_text":"daaaaaa monday offici start load energi drink",
    "tokens":[
      "daaaaaa",
      "monday",
      "offici",
      "start",
      "load",
      "energi",
      "drink"
    ],
    "token_count":7,
    "processed_text":"daaaaaa monday offici start load energi drink"
  },
  {
    "label":0,
    "text":"woke realli weird stomach pain lt roberta",
    "cleaned_text":"woke realli weird stomach pain lt roberta",
    "normalized_text":"woke realli weird stomach pain lt roberta",
    "tokens":[
      "woke",
      "realli",
      "weird",
      "stomach",
      "pain",
      "lt",
      "roberta"
    ],
    "token_count":7,
    "processed_text":"woke realli weird stomach pain lt roberta"
  },
  {
    "label":4,
    "text":"dont take time lol come eric n matt come dinner",
    "cleaned_text":"dont take time lol come eric n matt come dinner",
    "normalized_text":"dont take time lol come eric n matt come dinner",
    "tokens":[
      "dont",
      "take",
      "time",
      "lol",
      "come",
      "eric",
      "matt",
      "come",
      "dinner"
    ],
    "token_count":9,
    "processed_text":"dont take time lol come eric matt come dinner"
  },
  {
    "label":4,
    "text":"peopl peopl peopl",
    "cleaned_text":"peopl peopl peopl",
    "normalized_text":"peopl peopl peopl",
    "tokens":[
      "peopl",
      "peopl",
      "peopl"
    ],
    "token_count":3,
    "processed_text":"peopl peopl peopl"
  },
  {
    "label":4,
    "text":"back work nice mellow day",
    "cleaned_text":"back work nice mellow day",
    "normalized_text":"back work nice mellow day",
    "tokens":[
      "back",
      "work",
      "nice",
      "mellow",
      "day"
    ],
    "token_count":5,
    "processed_text":"back work nice mellow day"
  },
  {
    "label":0,
    "text":"dont lot thing wanna right",
    "cleaned_text":"dont lot thing wanna right",
    "normalized_text":"dont lot thing wanna right",
    "tokens":[
      "dont",
      "lot",
      "thing",
      "wan",
      "na",
      "right"
    ],
    "token_count":6,
    "processed_text":"dont lot thing wan na right"
  },
  {
    "label":0,
    "text":"omgosh feel hella late dc metro crash ive thing multipl time",
    "cleaned_text":"omgosh feel hella late dc metro crash ive thing multipl time",
    "normalized_text":"omgosh feel hella late dc metro crash ive thing multipl time",
    "tokens":[
      "omgosh",
      "feel",
      "hella",
      "late",
      "dc",
      "metro",
      "crash",
      "ive",
      "thing",
      "multipl",
      "time"
    ],
    "token_count":11,
    "processed_text":"omgosh feel hella late dc metro crash ive thing multipl time"
  },
  {
    "label":4,
    "text":"hey stinki parti",
    "cleaned_text":"hey stinki parti",
    "normalized_text":"hey stinki parti",
    "tokens":[
      "hey",
      "stinki",
      "parti"
    ],
    "token_count":3,
    "processed_text":"hey stinki parti"
  },
  {
    "label":4,
    "text":"awsom stuff lisa vol work like build charact knowledg",
    "cleaned_text":"awsom stuff lisa vol work like build charact knowledg",
    "normalized_text":"awsom stuff lisa vol work like build charact knowledg",
    "tokens":[
      "awsom",
      "stuff",
      "lisa",
      "vol",
      "work",
      "like",
      "build",
      "charact",
      "knowledg"
    ],
    "token_count":9,
    "processed_text":"awsom stuff lisa vol work like build charact knowledg"
  },
  {
    "label":4,
    "text":"yeah super",
    "cleaned_text":"yeah super",
    "normalized_text":"yeah super",
    "tokens":[
      "yeah",
      "super"
    ],
    "token_count":2,
    "processed_text":"yeah super"
  },
  {
    "label":0,
    "text":"billi elliot win best music next normal rob",
    "cleaned_text":"billi elliot win best music next normal rob",
    "normalized_text":"billi elliot win best music next normal rob",
    "tokens":[
      "billi",
      "elliot",
      "win",
      "best",
      "music",
      "next",
      "normal",
      "rob"
    ],
    "token_count":8,
    "processed_text":"billi elliot win best music next normal rob"
  },
  {
    "label":4,
    "text":"ye im parttim monster dont know",
    "cleaned_text":"ye im parttim monster dont know",
    "normalized_text":"ye im parttim monster dont know",
    "tokens":[
      "ye",
      "im",
      "parttim",
      "monster",
      "dont",
      "know"
    ],
    "token_count":6,
    "processed_text":"ye im parttim monster dont know"
  },
  {
    "label":4,
    "text":"im come see new hous like invit",
    "cleaned_text":"im come see new hous like invit",
    "normalized_text":"im come see new hous like invit",
    "tokens":[
      "im",
      "come",
      "see",
      "new",
      "hou",
      "like",
      "invit"
    ],
    "token_count":7,
    "processed_text":"im come see new hou like invit"
  },
  {
    "label":4,
    "text":"crack terrif banana",
    "cleaned_text":"crack terrif banana",
    "normalized_text":"crack terrif banana",
    "tokens":[
      "crack",
      "terrif",
      "banana"
    ],
    "token_count":3,
    "processed_text":"crack terrif banana"
  },
  {
    "label":0,
    "text":"know ya mean",
    "cleaned_text":"know ya mean",
    "normalized_text":"know ya mean",
    "tokens":[
      "know",
      "ya",
      "mean"
    ],
    "token_count":3,
    "processed_text":"know ya mean"
  },
  {
    "label":0,
    "text":"work scari first ever present",
    "cleaned_text":"work scari first ever present",
    "normalized_text":"work scari first ever present",
    "tokens":[
      "work",
      "scari",
      "first",
      "ever",
      "present"
    ],
    "token_count":5,
    "processed_text":"work scari first ever present"
  },
  {
    "label":4,
    "text":"need twit pic",
    "cleaned_text":"need twit pic",
    "normalized_text":"need twit pic",
    "tokens":[
      "need",
      "twit",
      "pic"
    ],
    "token_count":3,
    "processed_text":"need twit pic"
  },
  {
    "label":0,
    "text":"usb said could use didnt say im teknic",
    "cleaned_text":"usb said could use didnt say im teknic",
    "normalized_text":"usb said could use didnt say im teknic",
    "tokens":[
      "usb",
      "said",
      "use",
      "didnt",
      "say",
      "im",
      "teknic"
    ],
    "token_count":7,
    "processed_text":"usb said use didnt say im teknic"
  },
  {
    "label":4,
    "text":"chat ppl",
    "cleaned_text":"chat ppl",
    "normalized_text":"chat ppl",
    "tokens":[
      "chat",
      "ppl"
    ],
    "token_count":2,
    "processed_text":"chat ppl"
  },
  {
    "label":4,
    "text":"look hotel ticket calliiii",
    "cleaned_text":"look hotel ticket calliiii",
    "normalized_text":"look hotel ticket calliiii",
    "tokens":[
      "look",
      "hotel",
      "ticket",
      "calliiii"
    ],
    "token_count":4,
    "processed_text":"look hotel ticket calliiii"
  },
  {
    "label":0,
    "text":"car troubl one get mot one littl thing wrong one shell anoth test motfail",
    "cleaned_text":"car troubl one get mot one littl thing wrong one shell anoth test motfail",
    "normalized_text":"car troubl one get mot one littl thing wrong one shell anoth test motfail",
    "tokens":[
      "car",
      "troubl",
      "one",
      "get",
      "mot",
      "one",
      "littl",
      "thing",
      "wrong",
      "one",
      "shell",
      "anoth",
      "test",
      "motfail"
    ],
    "token_count":14,
    "processed_text":"car troubl one get mot one littl thing wrong one shell anoth test motfail"
  },
  {
    "label":4,
    "text":"chase crawford disturbingli hot",
    "cleaned_text":"chase crawford disturbingli hot",
    "normalized_text":"chase crawford disturbingli hot",
    "tokens":[
      "chase",
      "crawford",
      "disturbingli",
      "hot"
    ],
    "token_count":4,
    "processed_text":"chase crawford disturbingli hot"
  },
  {
    "label":0,
    "text":"love listen old school fall boy rememb school year carefre miss school day dont want",
    "cleaned_text":"love listen old school fall boy rememb school year carefre miss school day dont want",
    "normalized_text":"love listen old school fall boy rememb school year carefre miss school day dont want",
    "tokens":[
      "love",
      "listen",
      "old",
      "school",
      "fall",
      "boy",
      "rememb",
      "school",
      "year",
      "carefr",
      "miss",
      "school",
      "day",
      "dont",
      "want"
    ],
    "token_count":15,
    "processed_text":"love listen old school fall boy rememb school year carefr miss school day dont want"
  },
  {
    "label":0,
    "text":"guess local printer lousi",
    "cleaned_text":"guess local printer lousi",
    "normalized_text":"guess local printer lousi",
    "tokens":[
      "guess",
      "local",
      "printer",
      "lousi"
    ],
    "token_count":4,
    "processed_text":"guess local printer lousi"
  },
  {
    "label":0,
    "text":"gingerampspic die",
    "cleaned_text":"gingerampspic die",
    "normalized_text":"gingerampspic die",
    "tokens":[
      "gingerampsp",
      "die"
    ],
    "token_count":2,
    "processed_text":"gingerampsp die"
  },
  {
    "label":0,
    "text":"never learn whistl",
    "cleaned_text":"never learn whistl",
    "normalized_text":"never learn whistl",
    "tokens":[
      "never",
      "learn",
      "whistl"
    ],
    "token_count":3,
    "processed_text":"never learn whistl"
  },
  {
    "label":4,
    "text":"got explan letter done exactli way want im happi",
    "cleaned_text":"got explan letter done exactli way want im happi",
    "normalized_text":"got explan letter done exactli way want im happi",
    "tokens":[
      "got",
      "explan",
      "letter",
      "done",
      "exactli",
      "way",
      "want",
      "im",
      "happi"
    ],
    "token_count":9,
    "processed_text":"got explan letter done exactli way want im happi"
  },
  {
    "label":4,
    "text":"realiz got new follow hey new follow hope enjoy tweet",
    "cleaned_text":"realiz got new follow hey new follow hope enjoy tweet",
    "normalized_text":"realiz got new follow hey new follow hope enjoy tweet",
    "tokens":[
      "realiz",
      "got",
      "new",
      "follow",
      "hey",
      "new",
      "follow",
      "hope",
      "enjoy",
      "tweet"
    ],
    "token_count":10,
    "processed_text":"realiz got new follow hey new follow hope enjoy tweet"
  },
  {
    "label":0,
    "text":"legit bum nose stud need new one tomorrow exam plu gonna cost like eurgghhh",
    "cleaned_text":"legit bum nose stud need new one tomorrow exam plu gonna cost like eurgghhh",
    "normalized_text":"legit bum nose stud need new one tomorrow exam plu gonna cost like eurgghhh",
    "tokens":[
      "legit",
      "bum",
      "nose",
      "stud",
      "need",
      "new",
      "one",
      "tomorrow",
      "exam",
      "plu",
      "gon",
      "na",
      "cost",
      "like",
      "eurgghhh"
    ],
    "token_count":15,
    "processed_text":"legit bum nose stud need new one tomorrow exam plu gon na cost like eurgghhh"
  },
  {
    "label":4,
    "text":"sinc away come tucson",
    "cleaned_text":"sinc away come tucson",
    "normalized_text":"sinc away come tucson",
    "tokens":[
      "sinc",
      "away",
      "come",
      "tucson"
    ],
    "token_count":4,
    "processed_text":"sinc away come tucson"
  },
  {
    "label":0,
    "text":"still happi tryna gain earli niht sleep still failin happi bunni",
    "cleaned_text":"still happi tryna gain earli niht sleep still failin happi bunni",
    "normalized_text":"still happi tryna gain earli niht sleep still failin happi bunni",
    "tokens":[
      "still",
      "happi",
      "tryna",
      "gain",
      "earli",
      "niht",
      "sleep",
      "still",
      "failin",
      "happi",
      "bunni"
    ],
    "token_count":11,
    "processed_text":"still happi tryna gain earli niht sleep still failin happi bunni"
  },
  {
    "label":0,
    "text":"miss youwher",
    "cleaned_text":"miss youwher",
    "normalized_text":"miss youwher",
    "tokens":[
      "miss",
      "youwher"
    ],
    "token_count":2,
    "processed_text":"miss youwher"
  },
  {
    "label":4,
    "text":"hiya kel awesom weekend got new toy love hope super nice weekend",
    "cleaned_text":"hiya kel awesom weekend got new toy love hope super nice weekend",
    "normalized_text":"hiya kel awesom weekend got new toy love hope super nice weekend",
    "tokens":[
      "hiya",
      "kel",
      "awesom",
      "weekend",
      "got",
      "new",
      "toy",
      "love",
      "hope",
      "super",
      "nice",
      "weekend"
    ],
    "token_count":12,
    "processed_text":"hiya kel awesom weekend got new toy love hope super nice weekend"
  },
  {
    "label":0,
    "text":"awww poor littl thing",
    "cleaned_text":"awww poor littl thing",
    "normalized_text":"awww poor littl thing",
    "tokens":[
      "awww",
      "poor",
      "littl",
      "thing"
    ],
    "token_count":4,
    "processed_text":"awww poor littl thing"
  },
  {
    "label":0,
    "text":"special passport got stolen bag",
    "cleaned_text":"special passport got stolen bag",
    "normalized_text":"special passport got stolen bag",
    "tokens":[
      "special",
      "passport",
      "got",
      "stolen",
      "bag"
    ],
    "token_count":5,
    "processed_text":"special passport got stolen bag"
  },
  {
    "label":0,
    "text":"make us look strang yatrivia",
    "cleaned_text":"make us look strang yatrivia",
    "normalized_text":"make us look strang yatrivia",
    "tokens":[
      "make",
      "us",
      "look",
      "strang",
      "yatrivia"
    ],
    "token_count":5,
    "processed_text":"make us look strang yatrivia"
  },
  {
    "label":4,
    "text":"love guy awesom understand leav hate ppl put guy thru stay real",
    "cleaned_text":"love guy awesom understand leav hate ppl put guy thru stay real",
    "normalized_text":"love guy awesom understand leav hate ppl put guy thru stay real",
    "tokens":[
      "love",
      "guy",
      "awesom",
      "understand",
      "leav",
      "hate",
      "ppl",
      "put",
      "guy",
      "thru",
      "stay",
      "real"
    ],
    "token_count":12,
    "processed_text":"love guy awesom understand leav hate ppl put guy thru stay real"
  },
  {
    "label":4,
    "text":"ps rest earli day tomorrow shop hope great night",
    "cleaned_text":"ps rest earli day tomorrow shop hope great night",
    "normalized_text":"ps rest earli day tomorrow shop hope great night",
    "tokens":[
      "ps",
      "rest",
      "earli",
      "day",
      "tomorrow",
      "shop",
      "hope",
      "great",
      "night"
    ],
    "token_count":9,
    "processed_text":"ps rest earli day tomorrow shop hope great night"
  },
  {
    "label":4,
    "text":"bore citiwatch text",
    "cleaned_text":"bore citiwatch text",
    "normalized_text":"bore citiwatch text",
    "tokens":[
      "bore",
      "citiwatch",
      "text"
    ],
    "token_count":3,
    "processed_text":"bore citiwatch text"
  },
  {
    "label":4,
    "text":"tuesday hot outsid work soon",
    "cleaned_text":"tuesday hot outsid work soon",
    "normalized_text":"tuesday hot outsid work soon",
    "tokens":[
      "tuesday",
      "hot",
      "outsid",
      "work",
      "soon"
    ],
    "token_count":5,
    "processed_text":"tuesday hot outsid work soon"
  },
  {
    "label":4,
    "text":"bed time",
    "cleaned_text":"bed time",
    "normalized_text":"bed time",
    "tokens":[
      "bed",
      "time"
    ],
    "token_count":2,
    "processed_text":"bed time"
  },
  {
    "label":0,
    "text":"friend jaci work realli hard amp gop lawmak put money safeti highway",
    "cleaned_text":"friend jaci work realli hard amp gop lawmak put money safeti highway",
    "normalized_text":"friend jaci work realli hard amp gop lawmak put money safeti highway",
    "tokens":[
      "friend",
      "jaci",
      "work",
      "realli",
      "hard",
      "amp",
      "gop",
      "lawmak",
      "put",
      "money",
      "safeti",
      "highway"
    ],
    "token_count":12,
    "processed_text":"friend jaci work realli hard amp gop lawmak put money safeti highway"
  },
  {
    "label":4,
    "text":"thaaaaaaaaaank",
    "cleaned_text":"thaaaaaaaaaank",
    "normalized_text":"thaaaaaaaaaank",
    "tokens":[
      "thaaaaaaaaaank"
    ],
    "token_count":1,
    "processed_text":"thaaaaaaaaaank"
  },
  {
    "label":4,
    "text":"ever show tonight well see smx convent center well around pm",
    "cleaned_text":"ever show tonight well see smx convent center well around pm",
    "normalized_text":"ever show tonight well see smx convent center well around pm",
    "tokens":[
      "ever",
      "show",
      "tonight",
      "well",
      "see",
      "smx",
      "convent",
      "center",
      "well",
      "around",
      "pm"
    ],
    "token_count":11,
    "processed_text":"ever show tonight well see smx convent center well around pm"
  },
  {
    "label":0,
    "text":"watch ambul take next door neighbor away say prayer lost wife earlier year amp im worri",
    "cleaned_text":"watch ambul take next door neighbor away say prayer lost wife earlier year amp im worri",
    "normalized_text":"watch ambul take next door neighbor away say prayer lost wife earlier year amp im worri",
    "tokens":[
      "watch",
      "ambul",
      "take",
      "next",
      "door",
      "neighbor",
      "away",
      "say",
      "prayer",
      "lost",
      "wife",
      "earlier",
      "year",
      "amp",
      "im",
      "worri"
    ],
    "token_count":16,
    "processed_text":"watch ambul take next door neighbor away say prayer lost wife earlier year amp im worri"
  },
  {
    "label":0,
    "text":"go doctor lucki mesort",
    "cleaned_text":"go doctor lucki mesort",
    "normalized_text":"go doctor lucki mesort",
    "tokens":[
      "go",
      "doctor",
      "lucki",
      "mesort"
    ],
    "token_count":4,
    "processed_text":"go doctor lucki mesort"
  },
  {
    "label":0,
    "text":"im exhaust ankl kill pain killer left",
    "cleaned_text":"im exhaust ankl kill pain killer left",
    "normalized_text":"im exhaust ankl kill pain killer left",
    "tokens":[
      "im",
      "exhaust",
      "ankl",
      "kill",
      "pain",
      "killer",
      "left"
    ],
    "token_count":7,
    "processed_text":"im exhaust ankl kill pain killer left"
  },
  {
    "label":0,
    "text":"want go knott",
    "cleaned_text":"want go knott",
    "normalized_text":"want go knott",
    "tokens":[
      "want",
      "go",
      "knott"
    ],
    "token_count":3,
    "processed_text":"want go knott"
  },
  {
    "label":0,
    "text":"way guy told like one could get theyr one keep break lucki xd",
    "cleaned_text":"way guy told like one could get theyr one keep break lucki xd",
    "normalized_text":"way guy told like one could get theyr one keep break lucki xd",
    "tokens":[
      "way",
      "guy",
      "told",
      "like",
      "one",
      "get",
      "theyr",
      "one",
      "keep",
      "break",
      "lucki",
      "xd"
    ],
    "token_count":12,
    "processed_text":"way guy told like one get theyr one keep break lucki xd"
  },
  {
    "label":4,
    "text":"ha ha",
    "cleaned_text":"ha ha",
    "normalized_text":"ha ha",
    "tokens":[
      "ha",
      "ha"
    ],
    "token_count":2,
    "processed_text":"ha ha"
  },
  {
    "label":4,
    "text":"roland min activ he lazi sunbum like muma",
    "cleaned_text":"roland min activ he lazi sunbum like muma",
    "normalized_text":"roland min activ he lazi sunbum like muma",
    "tokens":[
      "roland",
      "min",
      "activ",
      "lazi",
      "sunbum",
      "like",
      "muma"
    ],
    "token_count":7,
    "processed_text":"roland min activ lazi sunbum like muma"
  },
  {
    "label":4,
    "text":"total appreci direct messag your ok book",
    "cleaned_text":"total appreci direct messag your ok book",
    "normalized_text":"total appreci direct messag your ok book",
    "tokens":[
      "total",
      "appreci",
      "direct",
      "messag",
      "ok",
      "book"
    ],
    "token_count":6,
    "processed_text":"total appreci direct messag ok book"
  },
  {
    "label":0,
    "text":"nope noth",
    "cleaned_text":"nope noth",
    "normalized_text":"nope noth",
    "tokens":[
      "nope",
      "noth"
    ],
    "token_count":2,
    "processed_text":"nope noth"
  },
  {
    "label":0,
    "text":"want go forgot inbox full mail peopl platanoverd forgot",
    "cleaned_text":"want go forgot inbox full mail peopl platanoverd forgot",
    "normalized_text":"want go forgot inbox full mail peopl platanoverd forgot",
    "tokens":[
      "want",
      "go",
      "forgot",
      "inbox",
      "full",
      "mail",
      "peopl",
      "platanoverd",
      "forgot"
    ],
    "token_count":9,
    "processed_text":"want go forgot inbox full mail peopl platanoverd forgot"
  },
  {
    "label":4,
    "text":"bff that ur bff",
    "cleaned_text":"bff that ur bff",
    "normalized_text":"bff that ur bff",
    "tokens":[
      "bff",
      "ur",
      "bff"
    ],
    "token_count":3,
    "processed_text":"bff ur bff"
  },
  {
    "label":4,
    "text":"good dog fish face pitbul nice big huge chile",
    "cleaned_text":"good dog fish face pitbul nice big huge chile",
    "normalized_text":"good dog fish face pitbul nice big huge chile",
    "tokens":[
      "good",
      "dog",
      "fish",
      "face",
      "pitbul",
      "nice",
      "big",
      "huge",
      "chile"
    ],
    "token_count":9,
    "processed_text":"good dog fish face pitbul nice big huge chile"
  },
  {
    "label":4,
    "text":"hee recip involv queu buy soup univers centr check though thank",
    "cleaned_text":"hee recip involv queu buy soup univers centr check though thank",
    "normalized_text":"hee recip involv queu buy soup univers centr check though thank",
    "tokens":[
      "hee",
      "recip",
      "involv",
      "queu",
      "buy",
      "soup",
      "univ",
      "centr",
      "check",
      "though",
      "thank"
    ],
    "token_count":11,
    "processed_text":"hee recip involv queu buy soup univ centr check though thank"
  },
  {
    "label":0,
    "text":"done someth stupid",
    "cleaned_text":"done someth stupid",
    "normalized_text":"done someth stupid",
    "tokens":[
      "done",
      "someth",
      "stupid"
    ],
    "token_count":3,
    "processed_text":"done someth stupid"
  },
  {
    "label":4,
    "text":"twitter better myspac",
    "cleaned_text":"twitter better myspac",
    "normalized_text":"twitter better myspac",
    "tokens":[
      "twitter",
      "better",
      "myspac"
    ],
    "token_count":3,
    "processed_text":"twitter better myspac"
  },
  {
    "label":0,
    "text":"want bake good who bday",
    "cleaned_text":"want bake good who bday",
    "normalized_text":"want bake good who bday",
    "tokens":[
      "want",
      "bake",
      "good",
      "bday"
    ],
    "token_count":4,
    "processed_text":"want bake good bday"
  },
  {
    "label":4,
    "text":"brother ador fit well though add bit drama famili duh she girl",
    "cleaned_text":"brother ador fit well though add bit drama famili duh she girl",
    "normalized_text":"brother ador fit well though add bit drama famili duh she girl",
    "tokens":[
      "brother",
      "ador",
      "fit",
      "well",
      "though",
      "add",
      "bit",
      "drama",
      "famili",
      "duh",
      "girl"
    ],
    "token_count":11,
    "processed_text":"brother ador fit well though add bit drama famili duh girl"
  },
  {
    "label":4,
    "text":"final fix email send phone would much easier send stuff phone via email",
    "cleaned_text":"final fix email send phone would much easier send stuff phone via email",
    "normalized_text":"final fix email send phone would much easier send stuff phone via email",
    "tokens":[
      "final",
      "fix",
      "email",
      "send",
      "phone",
      "much",
      "easier",
      "send",
      "stuff",
      "phone",
      "via",
      "email"
    ],
    "token_count":12,
    "processed_text":"final fix email send phone much easier send stuff phone via email"
  },
  {
    "label":4,
    "text":"sweet dream",
    "cleaned_text":"sweet dream",
    "normalized_text":"sweet dream",
    "tokens":[
      "sweet",
      "dream"
    ],
    "token_count":2,
    "processed_text":"sweet dream"
  },
  {
    "label":4,
    "text":"yeah afternoon start",
    "cleaned_text":"yeah afternoon start",
    "normalized_text":"yeah afternoon start",
    "tokens":[
      "yeah",
      "afternoon",
      "start"
    ],
    "token_count":3,
    "processed_text":"yeah afternoon start"
  },
  {
    "label":0,
    "text":"love amp im go miss youu soo much alll",
    "cleaned_text":"love amp im go miss youu soo much alll",
    "normalized_text":"love amp im go miss youu soo much alll",
    "tokens":[
      "love",
      "amp",
      "im",
      "go",
      "miss",
      "youu",
      "soo",
      "much",
      "alll"
    ],
    "token_count":9,
    "processed_text":"love amp im go miss youu soo much alll"
  },
  {
    "label":0,
    "text":"instal netbook fat cd drive avail",
    "cleaned_text":"instal netbook fat cd drive avail",
    "normalized_text":"instal netbook fat cd drive avail",
    "tokens":[
      "instal",
      "netbook",
      "fat",
      "cd",
      "drive",
      "avail"
    ],
    "token_count":6,
    "processed_text":"instal netbook fat cd drive avail"
  },
  {
    "label":4,
    "text":"think reach follow end day would absolutli fantast pleas help achiev",
    "cleaned_text":"think reach follow end day would absolutli fantast pleas help achiev",
    "normalized_text":"think reach follow end day would absolutli fantast pleas help achiev",
    "tokens":[
      "think",
      "reach",
      "follow",
      "end",
      "day",
      "absolutli",
      "fantast",
      "plea",
      "help",
      "achiev"
    ],
    "token_count":10,
    "processed_text":"think reach follow end day absolutli fantast plea help achiev"
  },
  {
    "label":0,
    "text":"shah alam far dude",
    "cleaned_text":"shah alam far dude",
    "normalized_text":"shah alam far dude",
    "tokens":[
      "shah",
      "alam",
      "far",
      "dude"
    ],
    "token_count":4,
    "processed_text":"shah alam far dude"
  },
  {
    "label":4,
    "text":"fun",
    "cleaned_text":"fun",
    "normalized_text":"fun",
    "tokens":[
      "fun"
    ],
    "token_count":1,
    "processed_text":"fun"
  },
  {
    "label":4,
    "text":"toronto anyon vitamix sale crave green drink left champion state",
    "cleaned_text":"toronto anyon vitamix sale crave green drink left champion state",
    "normalized_text":"toronto anyon vitamix sale crave green drink left champion state",
    "tokens":[
      "toronto",
      "anyon",
      "vitamix",
      "sale",
      "crave",
      "green",
      "drink",
      "left",
      "champion",
      "state"
    ],
    "token_count":10,
    "processed_text":"toronto anyon vitamix sale crave green drink left champion state"
  },
  {
    "label":4,
    "text":"need send email contact check emailunlimit",
    "cleaned_text":"need send email contact check emailunlimit",
    "normalized_text":"need send email contact check emailunlimit",
    "tokens":[
      "need",
      "send",
      "email",
      "contact",
      "check",
      "emailunlimit"
    ],
    "token_count":6,
    "processed_text":"need send email contact check emailunlimit"
  },
  {
    "label":4,
    "text":"get excit babi shower today",
    "cleaned_text":"get excit babi shower today",
    "normalized_text":"get excit babi shower today",
    "tokens":[
      "get",
      "excit",
      "babi",
      "shower",
      "today"
    ],
    "token_count":5,
    "processed_text":"get excit babi shower today"
  },
  {
    "label":0,
    "text":"turn ac today open window",
    "cleaned_text":"turn ac today open window",
    "normalized_text":"turn ac today open window",
    "tokens":[
      "turn",
      "ac",
      "today",
      "open",
      "window"
    ],
    "token_count":5,
    "processed_text":"turn ac today open window"
  },
  {
    "label":4,
    "text":"love new revamp site look awesom",
    "cleaned_text":"love new revamp site look awesom",
    "normalized_text":"love new revamp site look awesom",
    "tokens":[
      "love",
      "new",
      "revamp",
      "site",
      "look",
      "awesom"
    ],
    "token_count":6,
    "processed_text":"love new revamp site look awesom"
  },
  {
    "label":0,
    "text":"need follow want get",
    "cleaned_text":"need follow want get",
    "normalized_text":"need follow want get",
    "tokens":[
      "need",
      "follow",
      "want",
      "get"
    ],
    "token_count":4,
    "processed_text":"need follow want get"
  },
  {
    "label":4,
    "text":"yeah right would fun thank mayb next time ill tape gtd",
    "cleaned_text":"yeah right would fun thank mayb next time ill tape gtd",
    "normalized_text":"yeah right would fun thank mayb next time ill tape gtd",
    "tokens":[
      "yeah",
      "right",
      "fun",
      "thank",
      "mayb",
      "next",
      "time",
      "ill",
      "tape",
      "gtd"
    ],
    "token_count":10,
    "processed_text":"yeah right fun thank mayb next time ill tape gtd"
  },
  {
    "label":4,
    "text":"start work day tasti ice white mocha",
    "cleaned_text":"start work day tasti ice white mocha",
    "normalized_text":"start work day tasti ice white mocha",
    "tokens":[
      "start",
      "work",
      "day",
      "tasti",
      "ice",
      "white",
      "mocha"
    ],
    "token_count":7,
    "processed_text":"start work day tasti ice white mocha"
  },
  {
    "label":4,
    "text":"wow ubuntu featur realli cool got",
    "cleaned_text":"wow ubuntu featur realli cool got",
    "normalized_text":"wow ubuntu featur realli cool got",
    "tokens":[
      "wow",
      "ubuntu",
      "featur",
      "realli",
      "cool",
      "got"
    ],
    "token_count":6,
    "processed_text":"wow ubuntu featur realli cool got"
  },
  {
    "label":4,
    "text":"last day babysittingwhat crazi busi week",
    "cleaned_text":"last day babysittingwhat crazi busi week",
    "normalized_text":"last day babysittingwhat crazi busi week",
    "tokens":[
      "last",
      "day",
      "babysittingwhat",
      "crazi",
      "busi",
      "week"
    ],
    "token_count":6,
    "processed_text":"last day babysittingwhat crazi busi week"
  },
  {
    "label":4,
    "text":"soldier beach wow fun",
    "cleaned_text":"soldier beach wow fun",
    "normalized_text":"soldier beach wow fun",
    "tokens":[
      "soldier",
      "beach",
      "wow",
      "fun"
    ],
    "token_count":4,
    "processed_text":"soldier beach wow fun"
  },
  {
    "label":0,
    "text":"oldest niec wont go much plan traffic children across state line",
    "cleaned_text":"oldest niec wont go much plan traffic children across state line",
    "normalized_text":"oldest niec wont go much plan traffic children across state line",
    "tokens":[
      "oldest",
      "niec",
      "wont",
      "go",
      "much",
      "plan",
      "traffic",
      "children",
      "across",
      "state",
      "line"
    ],
    "token_count":11,
    "processed_text":"oldest niec wont go much plan traffic children across state line"
  },
  {
    "label":4,
    "text":"thank kimmiekream",
    "cleaned_text":"thank kimmiekream",
    "normalized_text":"thank kimmiekream",
    "tokens":[
      "thank",
      "kimmiekream"
    ],
    "token_count":2,
    "processed_text":"thank kimmiekream"
  },
  {
    "label":4,
    "text":"good luck interview",
    "cleaned_text":"good luck interview",
    "normalized_text":"good luck interview",
    "tokens":[
      "good",
      "luck",
      "interview"
    ],
    "token_count":3,
    "processed_text":"good luck interview"
  },
  {
    "label":0,
    "text":"realli life hellooo talk mee please ugh wish sleep possibl",
    "cleaned_text":"realli life hellooo talk mee please ugh wish sleep possibl",
    "normalized_text":"realli life hellooo talk mee please ugh wish sleep possibl",
    "tokens":[
      "realli",
      "life",
      "hellooo",
      "talk",
      "mee",
      "pleas",
      "ugh",
      "wish",
      "sleep",
      "possibl"
    ],
    "token_count":10,
    "processed_text":"realli life hellooo talk mee pleas ugh wish sleep possibl"
  },
  {
    "label":4,
    "text":"happi mother day mom mom mom mom woot woot mom rule",
    "cleaned_text":"happi mother day mom mom mom mom woot woot mom rule",
    "normalized_text":"happi mother day mom mom mom mom woot woot mom rule",
    "tokens":[
      "happi",
      "mother",
      "day",
      "mom",
      "mom",
      "mom",
      "mom",
      "woot",
      "woot",
      "mom",
      "rule"
    ],
    "token_count":11,
    "processed_text":"happi mother day mom mom mom mom woot woot mom rule"
  },
  {
    "label":4,
    "text":"undoubtedli left right impair health educ busi",
    "cleaned_text":"undoubtedli left right impair health educ busi",
    "normalized_text":"undoubtedli left right impair health educ busi",
    "tokens":[
      "undoubtedli",
      "left",
      "right",
      "impair",
      "health",
      "educ",
      "busi"
    ],
    "token_count":7,
    "processed_text":"undoubtedli left right impair health educ busi"
  },
  {
    "label":4,
    "text":"love first friday racin awesometast",
    "cleaned_text":"love first friday racin awesometast",
    "normalized_text":"love first friday racin awesometast",
    "tokens":[
      "love",
      "first",
      "friday",
      "racin",
      "awesometast"
    ],
    "token_count":5,
    "processed_text":"love first friday racin awesometast"
  },
  {
    "label":0,
    "text":"heck yeah leann perri tonight though",
    "cleaned_text":"heck yeah leann perri tonight though",
    "normalized_text":"heck yeah leann perri tonight though",
    "tokens":[
      "heck",
      "yeah",
      "leann",
      "perri",
      "tonight",
      "though"
    ],
    "token_count":6,
    "processed_text":"heck yeah leann perri tonight though"
  },
  {
    "label":4,
    "text":"ad u thank deari",
    "cleaned_text":"ad u thank deari",
    "normalized_text":"ad u thank deari",
    "tokens":[
      "ad",
      "thank",
      "deari"
    ],
    "token_count":3,
    "processed_text":"ad thank deari"
  },
  {
    "label":0,
    "text":"argh wish printer would fcking work hate brother dcpc pile crap",
    "cleaned_text":"argh wish printer would fcking work hate brother dcpc pile crap",
    "normalized_text":"argh wish printer would fcking work hate brother dcpc pile crap",
    "tokens":[
      "argh",
      "wish",
      "printer",
      "fcking",
      "work",
      "hate",
      "brother",
      "dcpc",
      "pile",
      "crap"
    ],
    "token_count":10,
    "processed_text":"argh wish printer fcking work hate brother dcpc pile crap"
  },
  {
    "label":0,
    "text":"nice ive never live north west part like scotland everyth green lol",
    "cleaned_text":"nice ive never live north west part like scotland everyth green lol",
    "normalized_text":"nice ive never live north west part like scotland everyth green lol",
    "tokens":[
      "nice",
      "ive",
      "never",
      "live",
      "north",
      "west",
      "part",
      "like",
      "scotland",
      "everyth",
      "green",
      "lol"
    ],
    "token_count":12,
    "processed_text":"nice ive never live north west part like scotland everyth green lol"
  },
  {
    "label":4,
    "text":"excel point",
    "cleaned_text":"excel point",
    "normalized_text":"excel point",
    "tokens":[
      "excel",
      "point"
    ],
    "token_count":2,
    "processed_text":"excel point"
  },
  {
    "label":0,
    "text":"girl sit front class passionfruit lipgloss smelt like cat pee",
    "cleaned_text":"girl sit front class passionfruit lipgloss smelt like cat pee",
    "normalized_text":"girl sit front class passionfruit lipgloss smelt like cat pee",
    "tokens":[
      "girl",
      "sit",
      "front",
      "class",
      "passionfruit",
      "lipgloss",
      "smelt",
      "like",
      "cat",
      "pee"
    ],
    "token_count":10,
    "processed_text":"girl sit front class passionfruit lipgloss smelt like cat pee"
  },
  {
    "label":0,
    "text":"hurt lot wheel",
    "cleaned_text":"hurt lot wheel",
    "normalized_text":"hurt lot wheel",
    "tokens":[
      "hurt",
      "lot",
      "wheel"
    ],
    "token_count":3,
    "processed_text":"hurt lot wheel"
  },
  {
    "label":4,
    "text":"love day boat hilli birthday bbq",
    "cleaned_text":"love day boat hilli birthday bbq",
    "normalized_text":"love day boat hilli birthday bbq",
    "tokens":[
      "love",
      "day",
      "boat",
      "hilli",
      "birthday",
      "bbq"
    ],
    "token_count":6,
    "processed_text":"love day boat hilli birthday bbq"
  },
  {
    "label":4,
    "text":"good morn haha im well hope",
    "cleaned_text":"good morn haha im well hope",
    "normalized_text":"good morn haha im well hope",
    "tokens":[
      "good",
      "morn",
      "haha",
      "im",
      "well",
      "hope"
    ],
    "token_count":6,
    "processed_text":"good morn haha im well hope"
  },
  {
    "label":0,
    "text":"cold june sheesh",
    "cleaned_text":"cold june sheesh",
    "normalized_text":"cold june sheesh",
    "tokens":[
      "cold",
      "june",
      "sheesh"
    ],
    "token_count":3,
    "processed_text":"cold june sheesh"
  },
  {
    "label":4,
    "text":"gonna watch cheaper dozen awwe cute young taylor lautner alyson stoner lt",
    "cleaned_text":"gonna watch cheaper dozen awwe cute young taylor lautner alyson stoner lt",
    "normalized_text":"gonna watch cheaper dozen awwe cute young taylor lautner alyson stoner lt",
    "tokens":[
      "gon",
      "na",
      "watch",
      "cheaper",
      "dozen",
      "aww",
      "cute",
      "young",
      "taylor",
      "lautner",
      "alyson",
      "stoner",
      "lt"
    ],
    "token_count":13,
    "processed_text":"gon na watch cheaper dozen aww cute young taylor lautner alyson stoner lt"
  },
  {
    "label":0,
    "text":"oh suck big fat one",
    "cleaned_text":"oh suck big fat one",
    "normalized_text":"oh suck big fat one",
    "tokens":[
      "oh",
      "suck",
      "big",
      "fat",
      "one"
    ],
    "token_count":5,
    "processed_text":"oh suck big fat one"
  },
  {
    "label":4,
    "text":"hmm decent height tend like short girl",
    "cleaned_text":"hmm decent height tend like short girl",
    "normalized_text":"hmm decent height tend like short girl",
    "tokens":[
      "hmm",
      "decent",
      "height",
      "tend",
      "like",
      "short",
      "girl"
    ],
    "token_count":7,
    "processed_text":"hmm decent height tend like short girl"
  },
  {
    "label":0,
    "text":"want start school assign cant",
    "cleaned_text":"want start school assign cant",
    "normalized_text":"want start school assign cant",
    "tokens":[
      "want",
      "start",
      "school",
      "assign",
      "cant"
    ],
    "token_count":5,
    "processed_text":"want start school assign cant"
  },
  {
    "label":0,
    "text":"grandma hospit",
    "cleaned_text":"grandma hospit",
    "normalized_text":"grandma hospit",
    "tokens":[
      "grandma",
      "hospit"
    ],
    "token_count":2,
    "processed_text":"grandma hospit"
  },
  {
    "label":0,
    "text":"awwww come sit lap let hug",
    "cleaned_text":"awwww come sit lap let hug",
    "normalized_text":"awwww come sit lap let hug",
    "tokens":[
      "awwww",
      "come",
      "sit",
      "lap",
      "let",
      "hug"
    ],
    "token_count":6,
    "processed_text":"awwww come sit lap let hug"
  },
  {
    "label":0,
    "text":"uuurg pm dont want studi wanna sleep test tomorrow didnt estudi anyth im still twitter",
    "cleaned_text":"uuurg pm dont want studi wanna sleep test tomorrow didnt estudi anyth im still twitter",
    "normalized_text":"uuurg pm dont want studi wanna sleep test tomorrow didnt estudi anyth im still twitter",
    "tokens":[
      "uuurg",
      "pm",
      "dont",
      "want",
      "studi",
      "wan",
      "na",
      "sleep",
      "test",
      "tomorrow",
      "didnt",
      "estudi",
      "anyth",
      "im",
      "still",
      "twitter"
    ],
    "token_count":16,
    "processed_text":"uuurg pm dont want studi wan na sleep test tomorrow didnt estudi anyth im still twitter"
  },
  {
    "label":4,
    "text":"ah also went wed parti two weekend ago haha friend mine fun",
    "cleaned_text":"ah also went wed parti two weekend ago haha friend mine fun",
    "normalized_text":"ah also went wed parti two weekend ago haha friend mine fun",
    "tokens":[
      "ah",
      "also",
      "went",
      "wed",
      "parti",
      "two",
      "weekend",
      "ago",
      "haha",
      "friend",
      "mine",
      "fun"
    ],
    "token_count":12,
    "processed_text":"ah also went wed parti two weekend ago haha friend mine fun"
  },
  {
    "label":4,
    "text":"awesom like look forward launch",
    "cleaned_text":"awesom like look forward launch",
    "normalized_text":"awesom like look forward launch",
    "tokens":[
      "awesom",
      "like",
      "look",
      "forward",
      "launch"
    ],
    "token_count":5,
    "processed_text":"awesom like look forward launch"
  },
  {
    "label":4,
    "text":"r u jealouslolal said met amaz guyappl still accept",
    "cleaned_text":"r u jealouslolal said met amaz guyappl still accept",
    "normalized_text":"r u jealouslolal said met amaz guyappl still accept",
    "tokens":[
      "jealouslol",
      "said",
      "met",
      "amaz",
      "guyappl",
      "still",
      "accept"
    ],
    "token_count":7,
    "processed_text":"jealouslol said met amaz guyappl still accept"
  },
  {
    "label":4,
    "text":"wooooooooooooooooooooooooooooooooo speak w u tomorrow btw import buisi matter",
    "cleaned_text":"wooooooooooooooooooooooooooooooooo speak w u tomorrow btw import buisi matter",
    "normalized_text":"wooooooooooooooooooooooooooooooooo speak w u tomorrow btw import buisi matter",
    "tokens":[
      "speak",
      "tomorrow",
      "btw",
      "import",
      "buisi",
      "matter"
    ],
    "token_count":6,
    "processed_text":"speak tomorrow btw import buisi matter"
  },
  {
    "label":0,
    "text":"dont talk way",
    "cleaned_text":"dont talk way",
    "normalized_text":"dont talk way",
    "tokens":[
      "dont",
      "talk",
      "way"
    ],
    "token_count":3,
    "processed_text":"dont talk way"
  },
  {
    "label":0,
    "text":"got back amp ejust hope ok march saturday",
    "cleaned_text":"got back amp ejust hope ok march saturday",
    "normalized_text":"got back amp ejust hope ok march saturday",
    "tokens":[
      "got",
      "back",
      "amp",
      "ejust",
      "hope",
      "ok",
      "march",
      "saturday"
    ],
    "token_count":8,
    "processed_text":"got back amp ejust hope ok march saturday"
  },
  {
    "label":0,
    "text":"crap im earli tire got exam today sleepi",
    "cleaned_text":"crap im earli tire got exam today sleepi",
    "normalized_text":"crap im earli tire got exam today sleepi",
    "tokens":[
      "crap",
      "im",
      "earli",
      "tire",
      "got",
      "exam",
      "today",
      "sleepi"
    ],
    "token_count":8,
    "processed_text":"crap im earli tire got exam today sleepi"
  },
  {
    "label":0,
    "text":"mom doesnt let call mom public",
    "cleaned_text":"mom doesnt let call mom public",
    "normalized_text":"mom doesnt let call mom public",
    "tokens":[
      "mom",
      "doesnt",
      "let",
      "call",
      "mom",
      "public"
    ],
    "token_count":6,
    "processed_text":"mom doesnt let call mom public"
  },
  {
    "label":4,
    "text":"go bed night",
    "cleaned_text":"go bed night",
    "normalized_text":"go bed night",
    "tokens":[
      "go",
      "bed",
      "night"
    ],
    "token_count":3,
    "processed_text":"go bed night"
  },
  {
    "label":4,
    "text":"im grate show think think lot chin chin",
    "cleaned_text":"im grate show think think lot chin chin",
    "normalized_text":"im grate show think think lot chin chin",
    "tokens":[
      "im",
      "grate",
      "show",
      "think",
      "think",
      "lot",
      "chin",
      "chin"
    ],
    "token_count":8,
    "processed_text":"im grate show think think lot chin chin"
  },
  {
    "label":0,
    "text":"today last day quottolerancequot spymast folk sorri",
    "cleaned_text":"today last day quottolerancequot spymast folk sorri",
    "normalized_text":"today last day quottolerancequot spymast folk sorri",
    "tokens":[
      "today",
      "last",
      "day",
      "spymast",
      "folk",
      "sorri"
    ],
    "token_count":6,
    "processed_text":"today last day spymast folk sorri"
  },
  {
    "label":0,
    "text":"busi wson hs grad w cold comin back tho thank",
    "cleaned_text":"busi wson hs grad w cold comin back tho thank",
    "normalized_text":"busi wson hs grad w cold comin back tho thank",
    "tokens":[
      "busi",
      "wson",
      "hs",
      "grad",
      "cold",
      "comin",
      "back",
      "tho",
      "thank"
    ],
    "token_count":9,
    "processed_text":"busi wson hs grad cold comin back tho thank"
  },
  {
    "label":0,
    "text":"iphon rock love search screen im still wait appl releas quotidiskquot app",
    "cleaned_text":"iphon rock love search screen im still wait appl releas quotidiskquot app",
    "normalized_text":"iphon rock love search screen im still wait appl releas quotidiskquot app",
    "tokens":[
      "iphon",
      "rock",
      "love",
      "search",
      "screen",
      "im",
      "still",
      "wait",
      "appl",
      "relea",
      "quotidiskquot",
      "app"
    ],
    "token_count":12,
    "processed_text":"iphon rock love search screen im still wait appl relea quotidiskquot app"
  },
  {
    "label":0,
    "text":"im abl watch jimmi fallon appear im travel montenegro wont show onlin",
    "cleaned_text":"im abl watch jimmi fallon appear im travel montenegro wont show onlin",
    "normalized_text":"im abl watch jimmi fallon appear im travel montenegro wont show onlin",
    "tokens":[
      "im",
      "abl",
      "watch",
      "jimmi",
      "fallon",
      "appear",
      "im",
      "travel",
      "montenegro",
      "wont",
      "show",
      "onlin"
    ],
    "token_count":12,
    "processed_text":"im abl watch jimmi fallon appear im travel montenegro wont show onlin"
  },
  {
    "label":4,
    "text":"juzta close loop got shirt pink one thk like",
    "cleaned_text":"juzta close loop got shirt pink one thk like",
    "normalized_text":"juzta close loop got shirt pink one thk like",
    "tokens":[
      "juzta",
      "close",
      "loop",
      "got",
      "shirt",
      "pink",
      "one",
      "thk",
      "like"
    ],
    "token_count":9,
    "processed_text":"juzta close loop got shirt pink one thk like"
  },
  {
    "label":4,
    "text":"one earlier",
    "cleaned_text":"one earlier",
    "normalized_text":"one earlier",
    "tokens":[
      "one",
      "earlier"
    ],
    "token_count":2,
    "processed_text":"one earlier"
  },
  {
    "label":0,
    "text":"day week rain today",
    "cleaned_text":"day week rain today",
    "normalized_text":"day week rain today",
    "tokens":[
      "day",
      "week",
      "rain",
      "today"
    ],
    "token_count":4,
    "processed_text":"day week rain today"
  },
  {
    "label":4,
    "text":"bumper line soni cool guy",
    "cleaned_text":"bumper line soni cool guy",
    "normalized_text":"bumper line soni cool guy",
    "tokens":[
      "bumper",
      "line",
      "soni",
      "cool",
      "guy"
    ],
    "token_count":5,
    "processed_text":"bumper line soni cool guy"
  },
  {
    "label":0,
    "text":"complet season lost good abl find season offici site abccom",
    "cleaned_text":"complet season lost good abl find season offici site abccom",
    "normalized_text":"complet season lost good abl find season offici site abccom",
    "tokens":[
      "complet",
      "season",
      "lost",
      "good",
      "abl",
      "find",
      "season",
      "offici",
      "site",
      "abccom"
    ],
    "token_count":10,
    "processed_text":"complet season lost good abl find season offici site abccom"
  },
  {
    "label":0,
    "text":"squarespac intern winner yet jump wave quothello european custom enter tooquot",
    "cleaned_text":"squarespac intern winner yet jump wave quothello european custom enter tooquot",
    "normalized_text":"squarespac intern winner yet jump wave quothello european custom enter tooquot",
    "tokens":[
      "squarespac",
      "intern",
      "winner",
      "yet",
      "jump",
      "wave",
      "quothello",
      "european",
      "custom",
      "enter",
      "tooquot"
    ],
    "token_count":11,
    "processed_text":"squarespac intern winner yet jump wave quothello european custom enter tooquot"
  },
  {
    "label":4,
    "text":"oh dear followfriday alreadi ill mine bit later",
    "cleaned_text":"oh dear followfriday alreadi ill mine bit later",
    "normalized_text":"oh dear followfriday alreadi ill mine bit later",
    "tokens":[
      "oh",
      "dear",
      "followfriday",
      "alreadi",
      "ill",
      "mine",
      "bit",
      "later"
    ],
    "token_count":8,
    "processed_text":"oh dear followfriday alreadi ill mine bit later"
  },
  {
    "label":0,
    "text":"work go along day squarespac trackl",
    "cleaned_text":"work go along day squarespac trackl",
    "normalized_text":"work go along day squarespac trackl",
    "tokens":[
      "work",
      "go",
      "along",
      "day",
      "squarespac",
      "trackl"
    ],
    "token_count":6,
    "processed_text":"work go along day squarespac trackl"
  },
  {
    "label":0,
    "text":"lame raing skate day postpon",
    "cleaned_text":"lame raing skate day postpon",
    "normalized_text":"lame raing skate day postpon",
    "tokens":[
      "lame",
      "ra",
      "skate",
      "day",
      "postpon"
    ],
    "token_count":5,
    "processed_text":"lame ra skate day postpon"
  },
  {
    "label":0,
    "text":"wooot day bore aunt grandmoth birthday today yesterday uncl geeezh",
    "cleaned_text":"wooot day bore aunt grandmoth birthday today yesterday uncl geeezh",
    "normalized_text":"wooot day bore aunt grandmoth birthday today yesterday uncl geeezh",
    "tokens":[
      "wooot",
      "day",
      "bore",
      "aunt",
      "grandmoth",
      "birthday",
      "today",
      "yesterday",
      "uncl",
      "geeezh"
    ],
    "token_count":10,
    "processed_text":"wooot day bore aunt grandmoth birthday today yesterday uncl geeezh"
  },
  {
    "label":4,
    "text":"boo well nearli wkend",
    "cleaned_text":"boo well nearli wkend",
    "normalized_text":"boo well nearli wkend",
    "tokens":[
      "boo",
      "well",
      "nearli",
      "wkend"
    ],
    "token_count":4,
    "processed_text":"boo well nearli wkend"
  },
  {
    "label":0,
    "text":"oh im sorri that terribl news",
    "cleaned_text":"oh im sorri that terribl news",
    "normalized_text":"oh im sorri that terribl news",
    "tokens":[
      "oh",
      "im",
      "sorri",
      "terribl",
      "news"
    ],
    "token_count":5,
    "processed_text":"oh im sorri terribl news"
  },
  {
    "label":0,
    "text":"ooh littl stinker u good remedi",
    "cleaned_text":"ooh littl stinker u good remedi",
    "normalized_text":"ooh littl stinker u good remedi",
    "tokens":[
      "ooh",
      "littl",
      "stinker",
      "good",
      "remedi"
    ],
    "token_count":5,
    "processed_text":"ooh littl stinker good remedi"
  },
  {
    "label":4,
    "text":"night museum total ador cant believ havent seen movi look forward next one",
    "cleaned_text":"night museum total ador cant believ havent seen movi look forward next one",
    "normalized_text":"night museum total ador cant believ havent seen movi look forward next one",
    "tokens":[
      "night",
      "museum",
      "total",
      "ador",
      "cant",
      "believ",
      "havent",
      "seen",
      "movi",
      "look",
      "forward",
      "next",
      "one"
    ],
    "token_count":13,
    "processed_text":"night museum total ador cant believ havent seen movi look forward next one"
  },
  {
    "label":0,
    "text":"im burnt annd work mcdonald close im goin hate lt",
    "cleaned_text":"im burnt annd work mcdonald close im goin hate lt",
    "normalized_text":"im burnt annd work mcdonald close im goin hate lt",
    "tokens":[
      "im",
      "burnt",
      "annd",
      "work",
      "mcdonald",
      "close",
      "im",
      "goin",
      "hate",
      "lt"
    ],
    "token_count":10,
    "processed_text":"im burnt annd work mcdonald close im goin hate lt"
  },
  {
    "label":0,
    "text":"im sorri your cranki pmsi what fmeveryth",
    "cleaned_text":"im sorri your cranki pmsi what fmeveryth",
    "normalized_text":"im sorri your cranki pmsi what fmeveryth",
    "tokens":[
      "im",
      "sorri",
      "cranki",
      "pmsi",
      "fmeveryth"
    ],
    "token_count":5,
    "processed_text":"im sorri cranki pmsi fmeveryth"
  },
  {
    "label":0,
    "text":"good grief sig must shit load frequent flyer mile fun mike row suck",
    "cleaned_text":"good grief sig must shit load frequent flyer mile fun mike row suck",
    "normalized_text":"good grief sig must shit load frequent flyer mile fun mike row suck",
    "tokens":[
      "good",
      "grief",
      "sig",
      "shit",
      "load",
      "frequent",
      "flyer",
      "mile",
      "fun",
      "mike",
      "row",
      "suck"
    ],
    "token_count":12,
    "processed_text":"good grief sig shit load frequent flyer mile fun mike row suck"
  },
  {
    "label":0,
    "text":"twitterberri doesnt work connect oh wat wrong youbabi",
    "cleaned_text":"twitterberri doesnt work connect oh wat wrong youbabi",
    "normalized_text":"twitterberri doesnt work connect oh wat wrong youbabi",
    "tokens":[
      "twitterberri",
      "doesnt",
      "work",
      "connect",
      "oh",
      "wat",
      "wrong",
      "youbabi"
    ],
    "token_count":8,
    "processed_text":"twitterberri doesnt work connect oh wat wrong youbabi"
  },
  {
    "label":0,
    "text":"hey guy say monday th rest aaaaaaaaaahhhhhhh",
    "cleaned_text":"hey guy say monday th rest aaaaaaaaaahhhhhhh",
    "normalized_text":"hey guy say monday th rest aaaaaaaaaahhhhhhh",
    "tokens":[
      "hey",
      "guy",
      "say",
      "monday",
      "th",
      "rest"
    ],
    "token_count":6,
    "processed_text":"hey guy say monday th rest"
  },
  {
    "label":4,
    "text":"board st flight st loui tri nervou",
    "cleaned_text":"board st flight st loui tri nervou",
    "normalized_text":"board st flight st loui tri nervou",
    "tokens":[
      "board",
      "st",
      "flight",
      "st",
      "loui",
      "tri",
      "nervou"
    ],
    "token_count":7,
    "processed_text":"board st flight st loui tri nervou"
  },
  {
    "label":0,
    "text":"languag english love want spanish",
    "cleaned_text":"languag english love want spanish",
    "normalized_text":"languag english love want spanish",
    "tokens":[
      "languag",
      "english",
      "love",
      "want",
      "spanish"
    ],
    "token_count":5,
    "processed_text":"languag english love want spanish"
  },
  {
    "label":4,
    "text":"drivinggg publix",
    "cleaned_text":"drivinggg publix",
    "normalized_text":"drivinggg publix",
    "tokens":[
      "drivinggg",
      "publix"
    ],
    "token_count":2,
    "processed_text":"drivinggg publix"
  },
  {
    "label":4,
    "text":"yay maria anoth talent show",
    "cleaned_text":"yay maria anoth talent show",
    "normalized_text":"yay maria anoth talent show",
    "tokens":[
      "yay",
      "maria",
      "anoth",
      "talent",
      "show"
    ],
    "token_count":5,
    "processed_text":"yay maria anoth talent show"
  },
  {
    "label":0,
    "text":"chang wasnt util option",
    "cleaned_text":"chang wasnt util option",
    "normalized_text":"chang wasnt util option",
    "tokens":[
      "chang",
      "wasnt",
      "util",
      "option"
    ],
    "token_count":4,
    "processed_text":"chang wasnt util option"
  },
  {
    "label":4,
    "text":"mayb youll get lot new euphoris crazi meezerfurriend happypawlucki catnip",
    "cleaned_text":"mayb youll get lot new euphoris crazi meezerfurriend happypawlucki catnip",
    "normalized_text":"mayb youll get lot new euphoris crazi meezerfurriend happypawlucki catnip",
    "tokens":[
      "mayb",
      "youll",
      "get",
      "lot",
      "new",
      "euphori",
      "crazi",
      "meezerfurriend",
      "happypawlucki",
      "catnip"
    ],
    "token_count":10,
    "processed_text":"mayb youll get lot new euphori crazi meezerfurriend happypawlucki catnip"
  },
  {
    "label":0,
    "text":"nope freebi",
    "cleaned_text":"nope freebi",
    "normalized_text":"nope freebi",
    "tokens":[
      "nope",
      "freebi"
    ],
    "token_count":2,
    "processed_text":"nope freebi"
  },
  {
    "label":4,
    "text":"offici wife leav honeymoon",
    "cleaned_text":"offici wife leav honeymoon",
    "normalized_text":"offici wife leav honeymoon",
    "tokens":[
      "offici",
      "wife",
      "leav",
      "honeymoon"
    ],
    "token_count":4,
    "processed_text":"offici wife leav honeymoon"
  },
  {
    "label":4,
    "text":"good morn peac",
    "cleaned_text":"good morn peac",
    "normalized_text":"good morn peac",
    "tokens":[
      "good",
      "morn",
      "peac"
    ],
    "token_count":3,
    "processed_text":"good morn peac"
  },
  {
    "label":0,
    "text":"today took shit blah",
    "cleaned_text":"today took shit blah",
    "normalized_text":"today took shit blah",
    "tokens":[
      "today",
      "took",
      "shit",
      "blah"
    ],
    "token_count":4,
    "processed_text":"today took shit blah"
  },
  {
    "label":0,
    "text":"thank got hurt wednesday go vet today",
    "cleaned_text":"thank got hurt wednesday go vet today",
    "normalized_text":"thank got hurt wednesday go vet today",
    "tokens":[
      "thank",
      "got",
      "hurt",
      "wednesday",
      "go",
      "vet",
      "today"
    ],
    "token_count":7,
    "processed_text":"thank got hurt wednesday go vet today"
  },
  {
    "label":4,
    "text":"pleas form went wrong reason never got answer fix later",
    "cleaned_text":"pleas form went wrong reason never got answer fix later",
    "normalized_text":"pleas form went wrong reason never got answer fix later",
    "tokens":[
      "plea",
      "form",
      "went",
      "wrong",
      "reason",
      "never",
      "got",
      "answer",
      "fix",
      "later"
    ],
    "token_count":10,
    "processed_text":"plea form went wrong reason never got answer fix later"
  },
  {
    "label":0,
    "text":"rlli want sch holiday extend everyon need pleas moe",
    "cleaned_text":"rlli want sch holiday extend everyon need pleas moe",
    "normalized_text":"rlli want sch holiday extend everyon need pleas moe",
    "tokens":[
      "rlli",
      "want",
      "sch",
      "holiday",
      "extend",
      "everyon",
      "need",
      "plea",
      "moe"
    ],
    "token_count":9,
    "processed_text":"rlli want sch holiday extend everyon need plea moe"
  },
  {
    "label":0,
    "text":"dont dough love movi though",
    "cleaned_text":"dont dough love movi though",
    "normalized_text":"dont dough love movi though",
    "tokens":[
      "dont",
      "dough",
      "love",
      "movi",
      "though"
    ],
    "token_count":5,
    "processed_text":"dont dough love movi though"
  },
  {
    "label":4,
    "text":"goood morninn world hous smell like pancak",
    "cleaned_text":"goood morninn world hous smell like pancak",
    "normalized_text":"goood morninn world hous smell like pancak",
    "tokens":[
      "goood",
      "morninn",
      "world",
      "hou",
      "smell",
      "like",
      "pancak"
    ],
    "token_count":7,
    "processed_text":"goood morninn world hou smell like pancak"
  },
  {
    "label":0,
    "text":"go canada stupid rain",
    "cleaned_text":"go canada stupid rain",
    "normalized_text":"go canada stupid rain",
    "tokens":[
      "go",
      "canada",
      "stupid",
      "rain"
    ],
    "token_count":4,
    "processed_text":"go canada stupid rain"
  },
  {
    "label":4,
    "text":"steak doesnt expens butcher nearbi mayb even groceri store meat dept",
    "cleaned_text":"steak doesnt expens butcher nearbi mayb even groceri store meat dept",
    "normalized_text":"steak doesnt expens butcher nearbi mayb even groceri store meat dept",
    "tokens":[
      "steak",
      "doesnt",
      "expen",
      "butcher",
      "nearbi",
      "mayb",
      "even",
      "groceri",
      "store",
      "meat",
      "dept"
    ],
    "token_count":11,
    "processed_text":"steak doesnt expen butcher nearbi mayb even groceri store meat dept"
  },
  {
    "label":4,
    "text":"english went well essay final left go",
    "cleaned_text":"english went well essay final left go",
    "normalized_text":"english went well essay final left go",
    "tokens":[
      "english",
      "went",
      "well",
      "essay",
      "final",
      "left",
      "go"
    ],
    "token_count":7,
    "processed_text":"english went well essay final left go"
  },
  {
    "label":0,
    "text":"graduat went great sadli field trip end year",
    "cleaned_text":"graduat went great sadli field trip end year",
    "normalized_text":"graduat went great sadli field trip end year",
    "tokens":[
      "graduat",
      "went",
      "great",
      "sadli",
      "field",
      "trip",
      "end",
      "year"
    ],
    "token_count":8,
    "processed_text":"graduat went great sadli field trip end year"
  },
  {
    "label":0,
    "text":"that stink got sunburn shin stomach sunscreen",
    "cleaned_text":"that stink got sunburn shin stomach sunscreen",
    "normalized_text":"that stink got sunburn shin stomach sunscreen",
    "tokens":[
      "stink",
      "got",
      "sunburn",
      "shin",
      "stomach",
      "sunscreen"
    ],
    "token_count":6,
    "processed_text":"stink got sunburn shin stomach sunscreen"
  },
  {
    "label":4,
    "text":"last night count crow gig awesomegreat support wonder hold steadi cours count crow great",
    "cleaned_text":"last night count crow gig awesomegreat support wonder hold steadi cours count crow great",
    "normalized_text":"last night count crow gig awesomegreat support wonder hold steadi cours count crow great",
    "tokens":[
      "last",
      "night",
      "count",
      "crow",
      "gig",
      "awesomegreat",
      "support",
      "wonder",
      "hold",
      "steadi",
      "cour",
      "count",
      "crow",
      "great"
    ],
    "token_count":14,
    "processed_text":"last night count crow gig awesomegreat support wonder hold steadi cour count crow great"
  },
  {
    "label":0,
    "text":"hmmm never got miss xox",
    "cleaned_text":"hmmm never got miss xox",
    "normalized_text":"hmmm never got miss xox",
    "tokens":[
      "hmmm",
      "never",
      "got",
      "miss",
      "xox"
    ],
    "token_count":5,
    "processed_text":"hmmm never got miss xox"
  },
  {
    "label":4,
    "text":"haha yeah want us suffer eat delici pancak",
    "cleaned_text":"haha yeah want us suffer eat delici pancak",
    "normalized_text":"haha yeah want us suffer eat delici pancak",
    "tokens":[
      "haha",
      "yeah",
      "want",
      "us",
      "suffer",
      "eat",
      "delici",
      "pancak"
    ],
    "token_count":8,
    "processed_text":"haha yeah want us suffer eat delici pancak"
  },
  {
    "label":4,
    "text":"tell use frame fail fail problem solv",
    "cleaned_text":"tell use frame fail fail problem solv",
    "normalized_text":"tell use frame fail fail problem solv",
    "tokens":[
      "tell",
      "use",
      "frame",
      "fail",
      "fail",
      "problem",
      "solv"
    ],
    "token_count":7,
    "processed_text":"tell use frame fail fail problem solv"
  },
  {
    "label":4,
    "text":"ate shit",
    "cleaned_text":"ate shit",
    "normalized_text":"ate shit",
    "tokens":[
      "ate",
      "shit"
    ],
    "token_count":2,
    "processed_text":"ate shit"
  },
  {
    "label":4,
    "text":"way home",
    "cleaned_text":"way home",
    "normalized_text":"way home",
    "tokens":[
      "way",
      "home"
    ],
    "token_count":2,
    "processed_text":"way home"
  },
  {
    "label":0,
    "text":"that yo nigga",
    "cleaned_text":"that yo nigga",
    "normalized_text":"that yo nigga",
    "tokens":[
      "yo",
      "nigga"
    ],
    "token_count":2,
    "processed_text":"yo nigga"
  },
  {
    "label":0,
    "text":"day tomorrow involv buy new iphon caus im appl fanboy drive mum father day sunday",
    "cleaned_text":"day tomorrow involv buy new iphon caus im appl fanboy drive mum father day sunday",
    "normalized_text":"day tomorrow involv buy new iphon caus im appl fanboy drive mum father day sunday",
    "tokens":[
      "day",
      "tomorrow",
      "involv",
      "buy",
      "new",
      "iphon",
      "cau",
      "im",
      "appl",
      "fanboy",
      "drive",
      "mum",
      "father",
      "day",
      "sunday"
    ],
    "token_count":15,
    "processed_text":"day tomorrow involv buy new iphon cau im appl fanboy drive mum father day sunday"
  },
  {
    "label":0,
    "text":"would mail one today",
    "cleaned_text":"would mail one today",
    "normalized_text":"would mail one today",
    "tokens":[
      "mail",
      "one",
      "today"
    ],
    "token_count":3,
    "processed_text":"mail one today"
  },
  {
    "label":0,
    "text":"wish live closer steve came home put dent trip plan think much amp kayla want",
    "cleaned_text":"wish live closer steve came home put dent trip plan think much amp kayla want",
    "normalized_text":"wish live closer steve came home put dent trip plan think much amp kayla want",
    "tokens":[
      "wish",
      "live",
      "closer",
      "steve",
      "came",
      "home",
      "put",
      "dent",
      "trip",
      "plan",
      "think",
      "much",
      "amp",
      "kayla",
      "want"
    ],
    "token_count":15,
    "processed_text":"wish live closer steve came home put dent trip plan think much amp kayla want"
  },
  {
    "label":4,
    "text":"facebook",
    "cleaned_text":"facebook",
    "normalized_text":"facebook",
    "tokens":[
      "facebook"
    ],
    "token_count":1,
    "processed_text":"facebook"
  },
  {
    "label":0,
    "text":"wait pizza hungri food hous hate storm",
    "cleaned_text":"wait pizza hungri food hous hate storm",
    "normalized_text":"wait pizza hungri food hous hate storm",
    "tokens":[
      "wait",
      "pizza",
      "hungri",
      "food",
      "hou",
      "hate",
      "storm"
    ],
    "token_count":7,
    "processed_text":"wait pizza hungri food hou hate storm"
  },
  {
    "label":4,
    "text":"congrat excit",
    "cleaned_text":"congrat excit",
    "normalized_text":"congrat excit",
    "tokens":[
      "congrat",
      "excit"
    ],
    "token_count":2,
    "processed_text":"congrat excit"
  },
  {
    "label":4,
    "text":"drank sugar milk coffe coffe brain",
    "cleaned_text":"drank sugar milk coffe coffe brain",
    "normalized_text":"drank sugar milk coffe coffe brain",
    "tokens":[
      "drank",
      "sugar",
      "milk",
      "coff",
      "coff",
      "brain"
    ],
    "token_count":6,
    "processed_text":"drank sugar milk coff coff brain"
  },
  {
    "label":4,
    "text":"thank im look taxonomi geograph zone amp socioeconom class quotmiddl classquot quotruralquot etc",
    "cleaned_text":"thank im look taxonomi geograph zone amp socioeconom class quotmiddl classquot quotruralquot etc",
    "normalized_text":"thank im look taxonomi geograph zone amp socioeconom class quotmiddl classquot quotruralquot etc",
    "tokens":[
      "thank",
      "im",
      "look",
      "taxonomi",
      "geograph",
      "zone",
      "amp",
      "socioeconom",
      "class",
      "quotmiddl",
      "classquot",
      "quotruralquot",
      "etc"
    ],
    "token_count":13,
    "processed_text":"thank im look taxonomi geograph zone amp socioeconom class quotmiddl classquot quotruralquot etc"
  },
  {
    "label":0,
    "text":"final chang timet five day one",
    "cleaned_text":"final chang timet five day one",
    "normalized_text":"final chang timet five day one",
    "tokens":[
      "final",
      "chang",
      "timet",
      "five",
      "day",
      "one"
    ],
    "token_count":6,
    "processed_text":"final chang timet five day one"
  },
  {
    "label":4,
    "text":"watch star trek boyssss chri pine yum",
    "cleaned_text":"watch star trek boyssss chri pine yum",
    "normalized_text":"watch star trek boyssss chri pine yum",
    "tokens":[
      "watch",
      "star",
      "trek",
      "boyssss",
      "chri",
      "pine",
      "yum"
    ],
    "token_count":7,
    "processed_text":"watch star trek boyssss chri pine yum"
  },
  {
    "label":0,
    "text":"woke earli im ill",
    "cleaned_text":"woke earli im ill",
    "normalized_text":"woke earli im ill",
    "tokens":[
      "woke",
      "earli",
      "im",
      "ill"
    ],
    "token_count":4,
    "processed_text":"woke earli im ill"
  },
  {
    "label":0,
    "text":"believ best friend call stupid cut real deep",
    "cleaned_text":"believ best friend call stupid cut real deep",
    "normalized_text":"believ best friend call stupid cut real deep",
    "tokens":[
      "believ",
      "best",
      "friend",
      "call",
      "stupid",
      "cut",
      "real",
      "deep"
    ],
    "token_count":8,
    "processed_text":"believ best friend call stupid cut real deep"
  },
  {
    "label":4,
    "text":"rick kogan benefit auction octob cool guy",
    "cleaned_text":"rick kogan benefit auction octob cool guy",
    "normalized_text":"rick kogan benefit auction octob cool guy",
    "tokens":[
      "rick",
      "kogan",
      "benefit",
      "auction",
      "octob",
      "cool",
      "guy"
    ],
    "token_count":7,
    "processed_text":"rick kogan benefit auction octob cool guy"
  },
  {
    "label":4,
    "text":"lmfao yay good hehe u late last night lmfao",
    "cleaned_text":"lmfao yay good hehe u late last night lmfao",
    "normalized_text":"lmfao yay good hehe u late last night lmfao",
    "tokens":[
      "lmfao",
      "yay",
      "good",
      "hehe",
      "late",
      "last",
      "night",
      "lmfao"
    ],
    "token_count":8,
    "processed_text":"lmfao yay good hehe late last night lmfao"
  },
  {
    "label":0,
    "text":"long night ahead",
    "cleaned_text":"long night ahead",
    "normalized_text":"long night ahead",
    "tokens":[
      "long",
      "night",
      "ahead"
    ],
    "token_count":3,
    "processed_text":"long night ahead"
  },
  {
    "label":4,
    "text":"well sunshin gone oh well least duck happi quack quack",
    "cleaned_text":"well sunshin gone oh well least duck happi quack quack",
    "normalized_text":"well sunshin gone oh well least duck happi quack quack",
    "tokens":[
      "well",
      "sunshin",
      "gone",
      "oh",
      "well",
      "least",
      "duck",
      "happi",
      "quack",
      "quack"
    ],
    "token_count":10,
    "processed_text":"well sunshin gone oh well least duck happi quack quack"
  },
  {
    "label":0,
    "text":"world leah coughenour haha",
    "cleaned_text":"world leah coughenour haha",
    "normalized_text":"world leah coughenour haha",
    "tokens":[
      "world",
      "leah",
      "coughenour",
      "haha"
    ],
    "token_count":4,
    "processed_text":"world leah coughenour haha"
  },
  {
    "label":4,
    "text":"like simpl voic great left comment would love hear finish version someday",
    "cleaned_text":"like simpl voic great left comment would love hear finish version someday",
    "normalized_text":"like simpl voic great left comment would love hear finish version someday",
    "tokens":[
      "like",
      "simpl",
      "voic",
      "great",
      "left",
      "comment",
      "love",
      "hear",
      "finish",
      "version",
      "someday"
    ],
    "token_count":11,
    "processed_text":"like simpl voic great left comment love hear finish version someday"
  },
  {
    "label":4,
    "text":"austin favorit comeback quotyour facequot learn",
    "cleaned_text":"austin favorit comeback quotyour facequot learn",
    "normalized_text":"austin favorit comeback quotyour facequot learn",
    "tokens":[
      "austin",
      "favorit",
      "comeback",
      "quotyour",
      "facequot",
      "learn"
    ],
    "token_count":6,
    "processed_text":"austin favorit comeback quotyour facequot learn"
  },
  {
    "label":0,
    "text":"still play past episod second season new sam amp max",
    "cleaned_text":"still play past episod second season new sam amp max",
    "normalized_text":"still play past episod second season new sam amp max",
    "tokens":[
      "still",
      "play",
      "past",
      "episod",
      "second",
      "season",
      "new",
      "sam",
      "amp",
      "max"
    ],
    "token_count":10,
    "processed_text":"still play past episod second season new sam amp max"
  },
  {
    "label":4,
    "text":"morn yall gym yoga",
    "cleaned_text":"morn yall gym yoga",
    "normalized_text":"morn yall gym yoga",
    "tokens":[
      "morn",
      "yall",
      "gym",
      "yoga"
    ],
    "token_count":4,
    "processed_text":"morn yall gym yoga"
  },
  {
    "label":4,
    "text":"good night amp get betta",
    "cleaned_text":"good night amp get betta",
    "normalized_text":"good night amp get betta",
    "tokens":[
      "good",
      "night",
      "amp",
      "get",
      "betta"
    ],
    "token_count":5,
    "processed_text":"good night amp get betta"
  },
  {
    "label":4,
    "text":"dont worri keep thing simpl",
    "cleaned_text":"dont worri keep thing simpl",
    "normalized_text":"dont worri keep thing simpl",
    "tokens":[
      "dont",
      "worri",
      "keep",
      "thing",
      "simpl"
    ],
    "token_count":5,
    "processed_text":"dont worri keep thing simpl"
  },
  {
    "label":0,
    "text":"go say someth peopl would think there pun anyway lower half ach gym",
    "cleaned_text":"go say someth peopl would think there pun anyway lower half ach gym",
    "normalized_text":"go say someth peopl would think there pun anyway lower half ach gym",
    "tokens":[
      "go",
      "say",
      "someth",
      "peopl",
      "think",
      "pun",
      "anyway",
      "lower",
      "half",
      "ach",
      "gym"
    ],
    "token_count":11,
    "processed_text":"go say someth peopl think pun anyway lower half ach gym"
  },
  {
    "label":4,
    "text":"day two offici begun crawpalooza pic may allow",
    "cleaned_text":"day two offici begun crawpalooza pic may allow",
    "normalized_text":"day two offici begun crawpalooza pic may allow",
    "tokens":[
      "day",
      "two",
      "offici",
      "begun",
      "crawpalooza",
      "pic",
      "may",
      "allow"
    ],
    "token_count":8,
    "processed_text":"day two offici begun crawpalooza pic may allow"
  },
  {
    "label":4,
    "text":"hey baba worri went lunch anyway n gonna go drving lesson oh god wish luck xxx",
    "cleaned_text":"hey baba worri went lunch anyway n gonna go drving lesson oh god wish luck xxx",
    "normalized_text":"hey baba worri went lunch anyway n gonna go drving lesson oh god wish luck xxx",
    "tokens":[
      "hey",
      "baba",
      "worri",
      "went",
      "lunch",
      "anyway",
      "gon",
      "na",
      "go",
      "drving",
      "lesson",
      "oh",
      "god",
      "wish",
      "luck",
      "xxx"
    ],
    "token_count":16,
    "processed_text":"hey baba worri went lunch anyway gon na go drving lesson oh god wish luck xxx"
  },
  {
    "label":4,
    "text":"im bore ann mean dude sit next obnoxi homi that ann say heheheh lt",
    "cleaned_text":"im bore ann mean dude sit next obnoxi homi that ann say heheheh lt",
    "normalized_text":"im bore ann mean dude sit next obnoxi homi that ann say heheheh lt",
    "tokens":[
      "im",
      "bore",
      "ann",
      "mean",
      "dude",
      "sit",
      "next",
      "obnoxi",
      "homi",
      "ann",
      "say",
      "heheheh",
      "lt"
    ],
    "token_count":13,
    "processed_text":"im bore ann mean dude sit next obnoxi homi ann say heheheh lt"
  },
  {
    "label":4,
    "text":"serious much laugh wwwurbandictionarycom tri guy",
    "cleaned_text":"serious much laugh wwwurbandictionarycom tri guy",
    "normalized_text":"serious much laugh wwwurbandictionarycom tri guy",
    "tokens":[
      "seriou",
      "much",
      "laugh",
      "tri",
      "guy"
    ],
    "token_count":5,
    "processed_text":"seriou much laugh tri guy"
  },
  {
    "label":4,
    "text":"thank much come phoenix thank publish send your awesom author",
    "cleaned_text":"thank much come phoenix thank publish send your awesom author",
    "normalized_text":"thank much come phoenix thank publish send your awesom author",
    "tokens":[
      "thank",
      "much",
      "come",
      "phoenix",
      "thank",
      "publish",
      "send",
      "awesom",
      "author"
    ],
    "token_count":9,
    "processed_text":"thank much come phoenix thank publish send awesom author"
  },
  {
    "label":4,
    "text":"final home eve yahoo",
    "cleaned_text":"final home eve yahoo",
    "normalized_text":"final home eve yahoo",
    "tokens":[
      "final",
      "home",
      "eve",
      "yahoo"
    ],
    "token_count":4,
    "processed_text":"final home eve yahoo"
  },
  {
    "label":4,
    "text":"worri thing doesnt spell check im sure one mind littl tweet mistak heheh",
    "cleaned_text":"worri thing doesnt spell check im sure one mind littl tweet mistak heheh",
    "normalized_text":"worri thing doesnt spell check im sure one mind littl tweet mistak heheh",
    "tokens":[
      "worri",
      "thing",
      "doesnt",
      "spell",
      "check",
      "im",
      "sure",
      "one",
      "mind",
      "littl",
      "tweet",
      "mistak",
      "heheh"
    ],
    "token_count":13,
    "processed_text":"worri thing doesnt spell check im sure one mind littl tweet mistak heheh"
  },
  {
    "label":0,
    "text":"haha life isnt treat well kapoi na kaayu wake morninghahahaha",
    "cleaned_text":"haha life isnt treat well kapoi na kaayu wake morninghahahaha",
    "normalized_text":"haha life isnt treat well kapoi na kaayu wake morninghahahaha",
    "tokens":[
      "haha",
      "life",
      "isnt",
      "treat",
      "well",
      "kapoi",
      "na",
      "kaayu",
      "wake",
      "morninghahahaha"
    ],
    "token_count":10,
    "processed_text":"haha life isnt treat well kapoi na kaayu wake morninghahahaha"
  },
  {
    "label":4,
    "text":"wow realli sleep like babi ive drank leftov pizza chicken sweetcorn swich breakfast",
    "cleaned_text":"wow realli sleep like babi ive drank leftov pizza chicken sweetcorn swich breakfast",
    "normalized_text":"wow realli sleep like babi ive drank leftov pizza chicken sweetcorn swich breakfast",
    "tokens":[
      "wow",
      "realli",
      "sleep",
      "like",
      "babi",
      "ive",
      "drank",
      "leftov",
      "pizza",
      "chicken",
      "sweetcorn",
      "swich",
      "breakfast"
    ],
    "token_count":13,
    "processed_text":"wow realli sleep like babi ive drank leftov pizza chicken sweetcorn swich breakfast"
  },
  {
    "label":4,
    "text":"im glad thought good one",
    "cleaned_text":"im glad thought good one",
    "normalized_text":"im glad thought good one",
    "tokens":[
      "im",
      "glad",
      "thought",
      "good",
      "one"
    ],
    "token_count":5,
    "processed_text":"im glad thought good one"
  },
  {
    "label":4,
    "text":"ugh realli like pic make happi june june th post pictur",
    "cleaned_text":"ugh realli like pic make happi june june th post pictur",
    "normalized_text":"ugh realli like pic make happi june june th post pictur",
    "tokens":[
      "ugh",
      "realli",
      "like",
      "pic",
      "make",
      "happi",
      "june",
      "june",
      "th",
      "post",
      "pictur"
    ],
    "token_count":11,
    "processed_text":"ugh realli like pic make happi june june th post pictur"
  },
  {
    "label":0,
    "text":"anoth beauti day outsid im stuck insid still write essay went outsid sunshin minut gt motiv back",
    "cleaned_text":"anoth beauti day outsid im stuck insid still write essay went outsid sunshin minut gt motiv back",
    "normalized_text":"anoth beauti day outsid im stuck insid still write essay went outsid sunshin minut gt motiv back",
    "tokens":[
      "anoth",
      "beauti",
      "day",
      "outsid",
      "im",
      "stuck",
      "insid",
      "still",
      "write",
      "essay",
      "went",
      "outsid",
      "sunshin",
      "minut",
      "gt",
      "motiv",
      "back"
    ],
    "token_count":17,
    "processed_text":"anoth beauti day outsid im stuck insid still write essay went outsid sunshin minut gt motiv back"
  },
  {
    "label":4,
    "text":"amp jame",
    "cleaned_text":"amp jame",
    "normalized_text":"amp jame",
    "tokens":[
      "amp",
      "jame"
    ],
    "token_count":2,
    "processed_text":"amp jame"
  },
  {
    "label":4,
    "text":"got get scare fuck good",
    "cleaned_text":"got get scare fuck good",
    "normalized_text":"got get scare fuck good",
    "tokens":[
      "got",
      "get",
      "scare",
      "fuck",
      "good"
    ],
    "token_count":5,
    "processed_text":"got get scare fuck good"
  },
  {
    "label":0,
    "text":"tent spin",
    "cleaned_text":"tent spin",
    "normalized_text":"tent spin",
    "tokens":[
      "tent",
      "spin"
    ],
    "token_count":2,
    "processed_text":"tent spin"
  },
  {
    "label":4,
    "text":"friday night vega ill start first friday",
    "cleaned_text":"friday night vega ill start first friday",
    "normalized_text":"friday night vega ill start first friday",
    "tokens":[
      "friday",
      "night",
      "vega",
      "ill",
      "start",
      "first",
      "friday"
    ],
    "token_count":7,
    "processed_text":"friday night vega ill start first friday"
  },
  {
    "label":0,
    "text":"someth save us bnp councillor nice one england",
    "cleaned_text":"someth save us bnp councillor nice one england",
    "normalized_text":"someth save us bnp councillor nice one england",
    "tokens":[
      "someth",
      "save",
      "us",
      "bnp",
      "councillor",
      "nice",
      "one",
      "england"
    ],
    "token_count":8,
    "processed_text":"someth save us bnp councillor nice one england"
  },
  {
    "label":4,
    "text":"hilari",
    "cleaned_text":"hilari",
    "normalized_text":"hilari",
    "tokens":[
      "hilari"
    ],
    "token_count":1,
    "processed_text":"hilari"
  },
  {
    "label":0,
    "text":"hate degre fever",
    "cleaned_text":"hate degre fever",
    "normalized_text":"hate degre fever",
    "tokens":[
      "hate",
      "degr",
      "fever"
    ],
    "token_count":3,
    "processed_text":"hate degr fever"
  },
  {
    "label":4,
    "text":"well noth flash differ chocol chang good meet u yesterday gr atmospher amp discuss",
    "cleaned_text":"well noth flash differ chocol chang good meet u yesterday gr atmospher amp discuss",
    "normalized_text":"well noth flash differ chocol chang good meet u yesterday gr atmospher amp discuss",
    "tokens":[
      "well",
      "noth",
      "flash",
      "differ",
      "chocol",
      "chang",
      "good",
      "meet",
      "yesterday",
      "gr",
      "atmosph",
      "amp",
      "discuss"
    ],
    "token_count":13,
    "processed_text":"well noth flash differ chocol chang good meet yesterday gr atmosph amp discuss"
  },
  {
    "label":4,
    "text":"blind pianist master tune hear amaz stori",
    "cleaned_text":"blind pianist master tune hear amaz stori",
    "normalized_text":"blind pianist master tune hear amaz stori",
    "tokens":[
      "blind",
      "pianist",
      "master",
      "tune",
      "hear",
      "amaz",
      "stori"
    ],
    "token_count":7,
    "processed_text":"blind pianist master tune hear amaz stori"
  },
  {
    "label":4,
    "text":"yup pretti good motown night lineup wasnt around incept grew nice end eve",
    "cleaned_text":"yup pretti good motown night lineup wasnt around incept grew nice end eve",
    "normalized_text":"yup pretti good motown night lineup wasnt around incept grew nice end eve",
    "tokens":[
      "yup",
      "pretti",
      "good",
      "motown",
      "night",
      "lineup",
      "wasnt",
      "around",
      "incept",
      "grew",
      "nice",
      "end",
      "eve"
    ],
    "token_count":13,
    "processed_text":"yup pretti good motown night lineup wasnt around incept grew nice end eve"
  },
  {
    "label":4,
    "text":"hope thought never see make realli sad want give danni one hug",
    "cleaned_text":"hope thought never see make realli sad want give danni one hug",
    "normalized_text":"hope thought never see make realli sad want give danni one hug",
    "tokens":[
      "hope",
      "thought",
      "never",
      "see",
      "make",
      "realli",
      "sad",
      "want",
      "give",
      "danni",
      "one",
      "hug"
    ],
    "token_count":12,
    "processed_text":"hope thought never see make realli sad want give danni one hug"
  },
  {
    "label":0,
    "text":"kid joey watch stuart littl",
    "cleaned_text":"kid joey watch stuart littl",
    "normalized_text":"kid joey watch stuart littl",
    "tokens":[
      "kid",
      "joey",
      "watch",
      "stuart",
      "littl"
    ],
    "token_count":5,
    "processed_text":"kid joey watch stuart littl"
  },
  {
    "label":4,
    "text":"hey thank follow",
    "cleaned_text":"hey thank follow",
    "normalized_text":"hey thank follow",
    "tokens":[
      "hey",
      "thank",
      "follow"
    ],
    "token_count":3,
    "processed_text":"hey thank follow"
  },
  {
    "label":4,
    "text":"twitter co got fb pword chang til exam look forward wednesday dad come back us amp thursday",
    "cleaned_text":"twitter co got fb pword chang til exam look forward wednesday dad come back us amp thursday",
    "normalized_text":"twitter co got fb pword chang til exam look forward wednesday dad come back us amp thursday",
    "tokens":[
      "twitter",
      "co",
      "got",
      "fb",
      "pword",
      "chang",
      "til",
      "exam",
      "look",
      "forward",
      "wednesday",
      "dad",
      "come",
      "back",
      "us",
      "amp",
      "thursday"
    ],
    "token_count":17,
    "processed_text":"twitter co got fb pword chang til exam look forward wednesday dad come back us amp thursday"
  },
  {
    "label":4,
    "text":"im defiantli vote miley choic award lol",
    "cleaned_text":"im defiantli vote miley choic award lol",
    "normalized_text":"im defiantli vote miley choic award lol",
    "tokens":[
      "im",
      "defiantli",
      "vote",
      "miley",
      "choic",
      "award",
      "lol"
    ],
    "token_count":7,
    "processed_text":"im defiantli vote miley choic award lol"
  },
  {
    "label":0,
    "text":"glad jenn amp jason fun boston go bike ride lori sonic bed work earrrli besttt friend",
    "cleaned_text":"glad jenn amp jason fun boston go bike ride lori sonic bed work earrrli besttt friend",
    "normalized_text":"glad jenn amp jason fun boston go bike ride lori sonic bed work earrrli besttt friend",
    "tokens":[
      "glad",
      "jenn",
      "amp",
      "jason",
      "fun",
      "boston",
      "go",
      "bike",
      "ride",
      "lori",
      "sonic",
      "bed",
      "work",
      "earrrli",
      "besttt",
      "friend"
    ],
    "token_count":16,
    "processed_text":"glad jenn amp jason fun boston go bike ride lori sonic bed work earrrli besttt friend"
  },
  {
    "label":4,
    "text":"wow sp twitter im enchantz spacemunkey anyway one user",
    "cleaned_text":"wow sp twitter im enchantz spacemunkey anyway one user",
    "normalized_text":"wow sp twitter im enchantz spacemunkey anyway one user",
    "tokens":[
      "wow",
      "sp",
      "twitter",
      "im",
      "enchantz",
      "spacemunkey",
      "anyway",
      "one",
      "user"
    ],
    "token_count":9,
    "processed_text":"wow sp twitter im enchantz spacemunkey anyway one user"
  },
  {
    "label":0,
    "text":"tire studin stupid exam",
    "cleaned_text":"tire studin stupid exam",
    "normalized_text":"tire studin stupid exam",
    "tokens":[
      "tire",
      "studin",
      "stupid",
      "exam"
    ],
    "token_count":4,
    "processed_text":"tire studin stupid exam"
  },
  {
    "label":4,
    "text":"lmfao ate capn crunch wow ima nerd peanut butter caus that kind like",
    "cleaned_text":"lmfao ate capn crunch wow ima nerd peanut butter caus that kind like",
    "normalized_text":"lmfao ate capn crunch wow ima nerd peanut butter caus that kind like",
    "tokens":[
      "lmfao",
      "ate",
      "capn",
      "crunch",
      "wow",
      "ima",
      "nerd",
      "peanut",
      "butter",
      "cau",
      "kind",
      "like"
    ],
    "token_count":12,
    "processed_text":"lmfao ate capn crunch wow ima nerd peanut butter cau kind like"
  },
  {
    "label":4,
    "text":"morn mike long time tweet",
    "cleaned_text":"morn mike long time tweet",
    "normalized_text":"morn mike long time tweet",
    "tokens":[
      "morn",
      "mike",
      "long",
      "time",
      "tweet"
    ],
    "token_count":5,
    "processed_text":"morn mike long time tweet"
  },
  {
    "label":4,
    "text":"say poem readi write quotnoth els mattersquot well known song tell us",
    "cleaned_text":"say poem readi write quotnoth els mattersquot well known song tell us",
    "normalized_text":"say poem readi write quotnoth els mattersquot well known song tell us",
    "tokens":[
      "say",
      "poem",
      "readi",
      "write",
      "quotnoth",
      "el",
      "mattersquot",
      "well",
      "known",
      "song",
      "tell",
      "us"
    ],
    "token_count":12,
    "processed_text":"say poem readi write quotnoth el mattersquot well known song tell us"
  },
  {
    "label":4,
    "text":"told hit got back",
    "cleaned_text":"told hit got back",
    "normalized_text":"told hit got back",
    "tokens":[
      "told",
      "hit",
      "got",
      "back"
    ],
    "token_count":4,
    "processed_text":"told hit got back"
  },
  {
    "label":4,
    "text":"yep yep yep hi",
    "cleaned_text":"yep yep yep hi",
    "normalized_text":"yep yep yep hi",
    "tokens":[
      "yep",
      "yep",
      "yep",
      "hi"
    ],
    "token_count":4,
    "processed_text":"yep yep yep hi"
  },
  {
    "label":4,
    "text":"meet byer",
    "cleaned_text":"meet byer",
    "normalized_text":"meet byer",
    "tokens":[
      "meet",
      "byer"
    ],
    "token_count":2,
    "processed_text":"meet byer"
  },
  {
    "label":4,
    "text":"good night",
    "cleaned_text":"good night",
    "normalized_text":"good night",
    "tokens":[
      "good",
      "night"
    ],
    "token_count":2,
    "processed_text":"good night"
  },
  {
    "label":4,
    "text":"well know easier said done never get right keep practic ha ha",
    "cleaned_text":"well know easier said done never get right keep practic ha ha",
    "normalized_text":"well know easier said done never get right keep practic ha ha",
    "tokens":[
      "well",
      "know",
      "easier",
      "said",
      "done",
      "never",
      "get",
      "right",
      "keep",
      "practic",
      "ha",
      "ha"
    ],
    "token_count":12,
    "processed_text":"well know easier said done never get right keep practic ha ha"
  },
  {
    "label":0,
    "text":"miss friend graduat gah feel bad",
    "cleaned_text":"miss friend graduat gah feel bad",
    "normalized_text":"miss friend graduat gah feel bad",
    "tokens":[
      "miss",
      "friend",
      "graduat",
      "gah",
      "feel",
      "bad"
    ],
    "token_count":6,
    "processed_text":"miss friend graduat gah feel bad"
  },
  {
    "label":4,
    "text":"cant wait meet u summer your cool kid could hang",
    "cleaned_text":"cant wait meet u summer your cool kid could hang",
    "normalized_text":"cant wait meet u summer your cool kid could hang",
    "tokens":[
      "cant",
      "wait",
      "meet",
      "summer",
      "cool",
      "kid",
      "hang"
    ],
    "token_count":7,
    "processed_text":"cant wait meet summer cool kid hang"
  },
  {
    "label":4,
    "text":"oh thank alway want lorrain win until lie cv put typo lmao face comic",
    "cleaned_text":"oh thank alway want lorrain win until lie cv put typo lmao face comic",
    "normalized_text":"oh thank alway want lorrain win until lie cv put typo lmao face comic",
    "tokens":[
      "oh",
      "thank",
      "alway",
      "want",
      "lorrain",
      "win",
      "lie",
      "cv",
      "put",
      "typo",
      "lmao",
      "face",
      "comic"
    ],
    "token_count":13,
    "processed_text":"oh thank alway want lorrain win lie cv put typo lmao face comic"
  },
  {
    "label":4,
    "text":"mayb could bungku bawa balik rumahhaha",
    "cleaned_text":"mayb could bungku bawa balik rumahhaha",
    "normalized_text":"mayb could bungku bawa balik rumahhaha",
    "tokens":[
      "mayb",
      "bungku",
      "bawa",
      "balik",
      "rumahhaha"
    ],
    "token_count":5,
    "processed_text":"mayb bungku bawa balik rumahhaha"
  },
  {
    "label":4,
    "text":"pretti happi hehe",
    "cleaned_text":"pretti happi hehe",
    "normalized_text":"pretti happi hehe",
    "tokens":[
      "pretti",
      "happi",
      "hehe"
    ],
    "token_count":3,
    "processed_text":"pretti happi hehe"
  },
  {
    "label":0,
    "text":"drink vitamin water even though horribl oh well lol",
    "cleaned_text":"drink vitamin water even though horribl oh well lol",
    "normalized_text":"drink vitamin water even though horribl oh well lol",
    "tokens":[
      "drink",
      "vitamin",
      "water",
      "even",
      "though",
      "horribl",
      "oh",
      "well",
      "lol"
    ],
    "token_count":9,
    "processed_text":"drink vitamin water even though horribl oh well lol"
  },
  {
    "label":4,
    "text":"ive lot think late ive come realiz im place im peac lt",
    "cleaned_text":"ive lot think late ive come realiz im place im peac lt",
    "normalized_text":"ive lot think late ive come realiz im place im peac lt",
    "tokens":[
      "ive",
      "lot",
      "think",
      "late",
      "ive",
      "come",
      "realiz",
      "im",
      "place",
      "im",
      "peac",
      "lt"
    ],
    "token_count":12,
    "processed_text":"ive lot think late ive come realiz im place im peac lt"
  },
  {
    "label":0,
    "text":"good morn twitter hate woman sometim everyon enjoy day",
    "cleaned_text":"good morn twitter hate woman sometim everyon enjoy day",
    "normalized_text":"good morn twitter hate woman sometim everyon enjoy day",
    "tokens":[
      "good",
      "morn",
      "twitter",
      "hate",
      "woman",
      "sometim",
      "everyon",
      "enjoy",
      "day"
    ],
    "token_count":9,
    "processed_text":"good morn twitter hate woman sometim everyon enjoy day"
  },
  {
    "label":0,
    "text":"im wait home chelsea take aj hospit caus slit eye open corner tabl",
    "cleaned_text":"im wait home chelsea take aj hospit caus slit eye open corner tabl",
    "normalized_text":"im wait home chelsea take aj hospit caus slit eye open corner tabl",
    "tokens":[
      "im",
      "wait",
      "home",
      "chelsea",
      "take",
      "aj",
      "hospit",
      "cau",
      "slit",
      "eye",
      "open",
      "corner",
      "tabl"
    ],
    "token_count":13,
    "processed_text":"im wait home chelsea take aj hospit cau slit eye open corner tabl"
  },
  {
    "label":4,
    "text":"clip lost play star trek alamo draft hous pretti accur",
    "cleaned_text":"clip lost play star trek alamo draft hous pretti accur",
    "normalized_text":"clip lost play star trek alamo draft hous pretti accur",
    "tokens":[
      "clip",
      "lost",
      "play",
      "star",
      "trek",
      "alamo",
      "draft",
      "hou",
      "pretti",
      "accur"
    ],
    "token_count":10,
    "processed_text":"clip lost play star trek alamo draft hou pretti accur"
  },
  {
    "label":0,
    "text":"yeah im actin lk th new doesnt let know ive new tweet",
    "cleaned_text":"yeah im actin lk th new doesnt let know ive new tweet",
    "normalized_text":"yeah im actin lk th new doesnt let know ive new tweet",
    "tokens":[
      "yeah",
      "im",
      "actin",
      "lk",
      "th",
      "new",
      "doesnt",
      "let",
      "know",
      "ive",
      "new",
      "tweet"
    ],
    "token_count":12,
    "processed_text":"yeah im actin lk th new doesnt let know ive new tweet"
  },
  {
    "label":0,
    "text":"im caramel latt keep eye open",
    "cleaned_text":"im caramel latt keep eye open",
    "normalized_text":"im caramel latt keep eye open",
    "tokens":[
      "im",
      "caramel",
      "latt",
      "keep",
      "eye",
      "open"
    ],
    "token_count":6,
    "processed_text":"im caramel latt keep eye open"
  },
  {
    "label":4,
    "text":"he star hous plu huge british actor he done lot lot much list tri googl filmographi",
    "cleaned_text":"he star hous plu huge british actor he done lot lot much list tri googl filmographi",
    "normalized_text":"he star hous plu huge british actor he done lot lot much list tri googl filmographi",
    "tokens":[
      "star",
      "hou",
      "plu",
      "huge",
      "british",
      "actor",
      "done",
      "lot",
      "lot",
      "much",
      "list",
      "tri",
      "googl",
      "filmographi"
    ],
    "token_count":14,
    "processed_text":"star hou plu huge british actor done lot lot much list tri googl filmographi"
  },
  {
    "label":0,
    "text":"know think went see live somethinghead tom watson etc boo",
    "cleaned_text":"know think went see live somethinghead tom watson etc boo",
    "normalized_text":"know think went see live somethinghead tom watson etc boo",
    "tokens":[
      "know",
      "think",
      "went",
      "see",
      "live",
      "somethinghead",
      "tom",
      "watson",
      "etc",
      "boo"
    ],
    "token_count":10,
    "processed_text":"know think went see live somethinghead tom watson etc boo"
  },
  {
    "label":4,
    "text":"aliv tonight epicentr breakfast club play go funif doesnt rain",
    "cleaned_text":"aliv tonight epicentr breakfast club play go funif doesnt rain",
    "normalized_text":"aliv tonight epicentr breakfast club play go funif doesnt rain",
    "tokens":[
      "aliv",
      "tonight",
      "epicentr",
      "breakfast",
      "club",
      "play",
      "go",
      "funif",
      "doesnt",
      "rain"
    ],
    "token_count":10,
    "processed_text":"aliv tonight epicentr breakfast club play go funif doesnt rain"
  },
  {
    "label":4,
    "text":"interest enuff yall tha tast interest ya probabl yal ever consid suit",
    "cleaned_text":"interest enuff yall tha tast interest ya probabl yal ever consid suit",
    "normalized_text":"interest enuff yall tha tast interest ya probabl yal ever consid suit",
    "tokens":[
      "interest",
      "enuff",
      "yall",
      "tha",
      "tast",
      "interest",
      "ya",
      "probabl",
      "yal",
      "ever",
      "consid",
      "suit"
    ],
    "token_count":12,
    "processed_text":"interest enuff yall tha tast interest ya probabl yal ever consid suit"
  },
  {
    "label":4,
    "text":"festa junina fail pub go",
    "cleaned_text":"festa junina fail pub go",
    "normalized_text":"festa junina fail pub go",
    "tokens":[
      "festa",
      "junina",
      "fail",
      "pub",
      "go"
    ],
    "token_count":5,
    "processed_text":"festa junina fail pub go"
  },
  {
    "label":4,
    "text":"got j im lazi webmonkey knock onlin date affilli site see pay",
    "cleaned_text":"got j im lazi webmonkey knock onlin date affilli site see pay",
    "normalized_text":"got j im lazi webmonkey knock onlin date affilli site see pay",
    "tokens":[
      "got",
      "im",
      "lazi",
      "webmonkey",
      "knock",
      "onlin",
      "date",
      "affilli",
      "site",
      "see",
      "pay"
    ],
    "token_count":11,
    "processed_text":"got im lazi webmonkey knock onlin date affilli site see pay"
  },
  {
    "label":4,
    "text":"mmm morn coffe",
    "cleaned_text":"mmm morn coffe",
    "normalized_text":"mmm morn coffe",
    "tokens":[
      "mmm",
      "morn",
      "coff"
    ],
    "token_count":3,
    "processed_text":"mmm morn coff"
  },
  {
    "label":0,
    "text":"clash titan remak nnnnnnnnoooooooo",
    "cleaned_text":"clash titan remak nnnnnnnnoooooooo",
    "normalized_text":"clash titan remak nnnnnnnnoooooooo",
    "tokens":[
      "clash",
      "titan",
      "remak"
    ],
    "token_count":3,
    "processed_text":"clash titan remak"
  },
  {
    "label":0,
    "text":"tstorm rest week morgantown",
    "cleaned_text":"tstorm rest week morgantown",
    "normalized_text":"tstorm rest week morgantown",
    "tokens":[
      "tstorm",
      "rest",
      "week",
      "morgantown"
    ],
    "token_count":4,
    "processed_text":"tstorm rest week morgantown"
  },
  {
    "label":4,
    "text":"rememb vist utah haha crazi right",
    "cleaned_text":"rememb vist utah haha crazi right",
    "normalized_text":"rememb vist utah haha crazi right",
    "tokens":[
      "rememb",
      "vist",
      "utah",
      "haha",
      "crazi",
      "right"
    ],
    "token_count":6,
    "processed_text":"rememb vist utah haha crazi right"
  },
  {
    "label":0,
    "text":"feel betray couldnt stop cri longthank ginger help",
    "cleaned_text":"feel betray couldnt stop cri longthank ginger help",
    "normalized_text":"feel betray couldnt stop cri longthank ginger help",
    "tokens":[
      "feel",
      "betray",
      "couldnt",
      "stop",
      "cri",
      "longthank",
      "ginger",
      "help"
    ],
    "token_count":8,
    "processed_text":"feel betray couldnt stop cri longthank ginger help"
  },
  {
    "label":0,
    "text":"ahh sunburn happen there sun",
    "cleaned_text":"ahh sunburn happen there sun",
    "normalized_text":"ahh sunburn happen there sun",
    "tokens":[
      "ahh",
      "sunburn",
      "happen",
      "sun"
    ],
    "token_count":4,
    "processed_text":"ahh sunburn happen sun"
  },
  {
    "label":0,
    "text":"ouch appli ec summer graduat hug",
    "cleaned_text":"ouch appli ec summer graduat hug",
    "normalized_text":"ouch appli ec summer graduat hug",
    "tokens":[
      "ouch",
      "appli",
      "ec",
      "summer",
      "graduat",
      "hug"
    ],
    "token_count":6,
    "processed_text":"ouch appli ec summer graduat hug"
  },
  {
    "label":0,
    "text":"get call work today appar found transmiss grom gsr stolen year ago oh miss car",
    "cleaned_text":"get call work today appar found transmiss grom gsr stolen year ago oh miss car",
    "normalized_text":"get call work today appar found transmiss grom gsr stolen year ago oh miss car",
    "tokens":[
      "get",
      "call",
      "work",
      "today",
      "appar",
      "found",
      "transmiss",
      "grom",
      "gsr",
      "stolen",
      "year",
      "ago",
      "oh",
      "miss",
      "car"
    ],
    "token_count":15,
    "processed_text":"get call work today appar found transmiss grom gsr stolen year ago oh miss car"
  },
  {
    "label":0,
    "text":"ive alway thought u crazi amaz gay man hear word come someon respect much realli hurt",
    "cleaned_text":"ive alway thought u crazi amaz gay man hear word come someon respect much realli hurt",
    "normalized_text":"ive alway thought u crazi amaz gay man hear word come someon respect much realli hurt",
    "tokens":[
      "ive",
      "alway",
      "thought",
      "crazi",
      "amaz",
      "gay",
      "man",
      "hear",
      "word",
      "come",
      "someon",
      "respect",
      "much",
      "realli",
      "hurt"
    ],
    "token_count":15,
    "processed_text":"ive alway thought crazi amaz gay man hear word come someon respect much realli hurt"
  },
  {
    "label":4,
    "text":"use name twit appar im climb everest year well uhmmm thanx",
    "cleaned_text":"use name twit appar im climb everest year well uhmmm thanx",
    "normalized_text":"use name twit appar im climb everest year well uhmmm thanx",
    "tokens":[
      "use",
      "name",
      "twit",
      "appar",
      "im",
      "climb",
      "everest",
      "year",
      "well",
      "uhmmm",
      "thanx"
    ],
    "token_count":11,
    "processed_text":"use name twit appar im climb everest year well uhmmm thanx"
  },
  {
    "label":4,
    "text":"yay product",
    "cleaned_text":"yay product",
    "normalized_text":"yay product",
    "tokens":[
      "yay",
      "product"
    ],
    "token_count":2,
    "processed_text":"yay product"
  },
  {
    "label":4,
    "text":"anyon els love serious wast time wolframalpha",
    "cleaned_text":"anyon els love serious wast time wolframalpha",
    "normalized_text":"anyon els love serious wast time wolframalpha",
    "tokens":[
      "anyon",
      "el",
      "love",
      "seriou",
      "wast",
      "time",
      "wolframalpha"
    ],
    "token_count":7,
    "processed_text":"anyon el love seriou wast time wolframalpha"
  },
  {
    "label":0,
    "text":"dont dislik one",
    "cleaned_text":"dont dislik one",
    "normalized_text":"dont dislik one",
    "tokens":[
      "dont",
      "dislik",
      "one"
    ],
    "token_count":3,
    "processed_text":"dont dislik one"
  },
  {
    "label":4,
    "text":"cocktail wine",
    "cleaned_text":"cocktail wine",
    "normalized_text":"cocktail wine",
    "tokens":[
      "cocktail",
      "wine"
    ],
    "token_count":2,
    "processed_text":"cocktail wine"
  },
  {
    "label":0,
    "text":"im wide awak bore x",
    "cleaned_text":"im wide awak bore x",
    "normalized_text":"im wide awak bore x",
    "tokens":[
      "im",
      "wide",
      "awak",
      "bore"
    ],
    "token_count":4,
    "processed_text":"im wide awak bore"
  },
  {
    "label":0,
    "text":"im probabl gonna fail biolog pretest",
    "cleaned_text":"im probabl gonna fail biolog pretest",
    "normalized_text":"im probabl gonna fail biolog pretest",
    "tokens":[
      "im",
      "probabl",
      "gon",
      "na",
      "fail",
      "biolog",
      "pretest"
    ],
    "token_count":7,
    "processed_text":"im probabl gon na fail biolog pretest"
  },
  {
    "label":4,
    "text":"final horror back itun new album may still love death chapel ridicul",
    "cleaned_text":"final horror back itun new album may still love death chapel ridicul",
    "normalized_text":"final horror back itun new album may still love death chapel ridicul",
    "tokens":[
      "final",
      "horror",
      "back",
      "itun",
      "new",
      "album",
      "may",
      "still",
      "love",
      "death",
      "chapel",
      "ridicul"
    ],
    "token_count":12,
    "processed_text":"final horror back itun new album may still love death chapel ridicul"
  },
  {
    "label":4,
    "text":"see camera obscura tonight",
    "cleaned_text":"see camera obscura tonight",
    "normalized_text":"see camera obscura tonight",
    "tokens":[
      "see",
      "camera",
      "obscura",
      "tonight"
    ],
    "token_count":4,
    "processed_text":"see camera obscura tonight"
  },
  {
    "label":4,
    "text":"ohhh okay thanx anyway",
    "cleaned_text":"ohhh okay thanx anyway",
    "normalized_text":"ohhh okay thanx anyway",
    "tokens":[
      "ohhh",
      "okay",
      "thanx",
      "anyway"
    ],
    "token_count":4,
    "processed_text":"ohhh okay thanx anyway"
  },
  {
    "label":0,
    "text":"made round trip eureka spring wed wed great car ride pure hell twelv hour car",
    "cleaned_text":"made round trip eureka spring wed wed great car ride pure hell twelv hour car",
    "normalized_text":"made round trip eureka spring wed wed great car ride pure hell twelv hour car",
    "tokens":[
      "made",
      "round",
      "trip",
      "eureka",
      "spring",
      "wed",
      "wed",
      "great",
      "car",
      "ride",
      "pure",
      "hell",
      "twelv",
      "hour",
      "car"
    ],
    "token_count":15,
    "processed_text":"made round trip eureka spring wed wed great car ride pure hell twelv hour car"
  },
  {
    "label":0,
    "text":"happi wednesday everyon read reax hr back homeit bad",
    "cleaned_text":"happi wednesday everyon read reax hr back homeit bad",
    "normalized_text":"happi wednesday everyon read reax hr back homeit bad",
    "tokens":[
      "happi",
      "wednesday",
      "everyon",
      "read",
      "reax",
      "hr",
      "back",
      "homeit",
      "bad"
    ],
    "token_count":9,
    "processed_text":"happi wednesday everyon read reax hr back homeit bad"
  },
  {
    "label":0,
    "text":"lump floor grrrrr cant even express frustrat stifl hate want much",
    "cleaned_text":"lump floor grrrrr cant even express frustrat stifl hate want much",
    "normalized_text":"lump floor grrrrr cant even express frustrat stifl hate want much",
    "tokens":[
      "lump",
      "floor",
      "grrrrr",
      "cant",
      "even",
      "express",
      "frustrat",
      "stifl",
      "hate",
      "want",
      "much"
    ],
    "token_count":11,
    "processed_text":"lump floor grrrrr cant even express frustrat stifl hate want much"
  },
  {
    "label":4,
    "text":"sleepov super fun",
    "cleaned_text":"sleepov super fun",
    "normalized_text":"sleepov super fun",
    "tokens":[
      "sleepov",
      "super",
      "fun"
    ],
    "token_count":3,
    "processed_text":"sleepov super fun"
  },
  {
    "label":4,
    "text":"gonna buy friday th tuesday know haha",
    "cleaned_text":"gonna buy friday th tuesday know haha",
    "normalized_text":"gonna buy friday th tuesday know haha",
    "tokens":[
      "gon",
      "na",
      "buy",
      "friday",
      "th",
      "tuesday",
      "know",
      "haha"
    ],
    "token_count":8,
    "processed_text":"gon na buy friday th tuesday know haha"
  },
  {
    "label":4,
    "text":"hey heyi thank sir btw loveee mean love mix",
    "cleaned_text":"hey heyi thank sir btw loveee mean love mix",
    "normalized_text":"hey heyi thank sir btw loveee mean love mix",
    "tokens":[
      "hey",
      "heyi",
      "thank",
      "sir",
      "btw",
      "lovee",
      "mean",
      "love",
      "mix"
    ],
    "token_count":9,
    "processed_text":"hey heyi thank sir btw lovee mean love mix"
  },
  {
    "label":4,
    "text":"today good day",
    "cleaned_text":"today good day",
    "normalized_text":"today good day",
    "tokens":[
      "today",
      "good",
      "day"
    ],
    "token_count":3,
    "processed_text":"today good day"
  },
  {
    "label":4,
    "text":"home school go run bit later need fine someth wear weekend",
    "cleaned_text":"home school go run bit later need fine someth wear weekend",
    "normalized_text":"home school go run bit later need fine someth wear weekend",
    "tokens":[
      "home",
      "school",
      "go",
      "run",
      "bit",
      "later",
      "need",
      "fine",
      "someth",
      "wear",
      "weekend"
    ],
    "token_count":11,
    "processed_text":"home school go run bit later need fine someth wear weekend"
  },
  {
    "label":4,
    "text":"gr info amp especi like last dont give",
    "cleaned_text":"gr info amp especi like last dont give",
    "normalized_text":"gr info amp especi like last dont give",
    "tokens":[
      "gr",
      "info",
      "amp",
      "especi",
      "like",
      "last",
      "dont",
      "give"
    ],
    "token_count":8,
    "processed_text":"gr info amp especi like last dont give"
  },
  {
    "label":4,
    "text":"btw three",
    "cleaned_text":"btw three",
    "normalized_text":"btw three",
    "tokens":[
      "btw",
      "three"
    ],
    "token_count":2,
    "processed_text":"btw three"
  },
  {
    "label":4,
    "text":"ooo sound amaz might tri soon",
    "cleaned_text":"ooo sound amaz might tri soon",
    "normalized_text":"ooo sound amaz might tri soon",
    "tokens":[
      "ooo",
      "sound",
      "amaz",
      "tri",
      "soon"
    ],
    "token_count":5,
    "processed_text":"ooo sound amaz tri soon"
  },
  {
    "label":0,
    "text":"oh heard tremi iss offici realli hate tremi",
    "cleaned_text":"oh heard tremi iss offici realli hate tremi",
    "normalized_text":"oh heard tremi iss offici realli hate tremi",
    "tokens":[
      "oh",
      "heard",
      "tremi",
      "iss",
      "offici",
      "realli",
      "hate",
      "tremi"
    ],
    "token_count":8,
    "processed_text":"oh heard tremi iss offici realli hate tremi"
  },
  {
    "label":4,
    "text":"okayher come sunha minut yet whata think bout texa weather lol",
    "cleaned_text":"okayher come sunha minut yet whata think bout texa weather lol",
    "normalized_text":"okayher come sunha minut yet whata think bout texa weather lol",
    "tokens":[
      "okayh",
      "come",
      "sunha",
      "minut",
      "yet",
      "whata",
      "think",
      "bout",
      "texa",
      "weather",
      "lol"
    ],
    "token_count":11,
    "processed_text":"okayh come sunha minut yet whata think bout texa weather lol"
  },
  {
    "label":0,
    "text":"quotsometim around midnightquot airborn toxic event liter cannot escap fml",
    "cleaned_text":"quotsometim around midnightquot airborn toxic event liter cannot escap fml",
    "normalized_text":"quotsometim around midnightquot airborn toxic event liter cannot escap fml",
    "tokens":[
      "quotsometim",
      "around",
      "midnightquot",
      "airborn",
      "toxic",
      "event",
      "liter",
      "escap",
      "fml"
    ],
    "token_count":9,
    "processed_text":"quotsometim around midnightquot airborn toxic event liter escap fml"
  },
  {
    "label":0,
    "text":"ye exactli csu mayo clinic anim basic realli tough pup get",
    "cleaned_text":"ye exactli csu mayo clinic anim basic realli tough pup get",
    "normalized_text":"ye exactli csu mayo clinic anim basic realli tough pup get",
    "tokens":[
      "ye",
      "exactli",
      "csu",
      "mayo",
      "clinic",
      "anim",
      "basic",
      "realli",
      "tough",
      "pup",
      "get"
    ],
    "token_count":11,
    "processed_text":"ye exactli csu mayo clinic anim basic realli tough pup get"
  },
  {
    "label":4,
    "text":"cant believ orlando magic final time la drink girli",
    "cleaned_text":"cant believ orlando magic final time la drink girli",
    "normalized_text":"cant believ orlando magic final time la drink girli",
    "tokens":[
      "cant",
      "believ",
      "orlando",
      "magic",
      "final",
      "time",
      "la",
      "drink",
      "girli"
    ],
    "token_count":9,
    "processed_text":"cant believ orlando magic final time la drink girli"
  },
  {
    "label":0,
    "text":"sun go chilli park",
    "cleaned_text":"sun go chilli park",
    "normalized_text":"sun go chilli park",
    "tokens":[
      "sun",
      "go",
      "chilli",
      "park"
    ],
    "token_count":4,
    "processed_text":"sun go chilli park"
  },
  {
    "label":4,
    "text":"im priceless fucc yall dime movement thank one steadi shit everi nite chrissi feelin good",
    "cleaned_text":"im priceless fucc yall dime movement thank one steadi shit everi nite chrissi feelin good",
    "normalized_text":"im priceless fucc yall dime movement thank one steadi shit everi nite chrissi feelin good",
    "tokens":[
      "im",
      "priceless",
      "fucc",
      "yall",
      "dime",
      "movement",
      "thank",
      "one",
      "steadi",
      "shit",
      "everi",
      "nite",
      "chrissi",
      "feelin",
      "good"
    ],
    "token_count":15,
    "processed_text":"im priceless fucc yall dime movement thank one steadi shit everi nite chrissi feelin good"
  },
  {
    "label":0,
    "text":"think yellow feel blue know financi class today porra",
    "cleaned_text":"think yellow feel blue know financi class today porra",
    "normalized_text":"think yellow feel blue know financi class today porra",
    "tokens":[
      "think",
      "yellow",
      "feel",
      "blue",
      "know",
      "financ",
      "class",
      "today",
      "porra"
    ],
    "token_count":9,
    "processed_text":"think yellow feel blue know financ class today porra"
  },
  {
    "label":0,
    "text":"know stupid swollen im abt go lol",
    "cleaned_text":"know stupid swollen im abt go lol",
    "normalized_text":"know stupid swollen im abt go lol",
    "tokens":[
      "know",
      "stupid",
      "swollen",
      "im",
      "abt",
      "go",
      "lol"
    ],
    "token_count":7,
    "processed_text":"know stupid swollen im abt go lol"
  },
  {
    "label":4,
    "text":"pretti today go tan",
    "cleaned_text":"pretti today go tan",
    "normalized_text":"pretti today go tan",
    "tokens":[
      "pretti",
      "today",
      "go",
      "tan"
    ],
    "token_count":4,
    "processed_text":"pretti today go tan"
  },
  {
    "label":0,
    "text":"wait airplan pick take wwdc late",
    "cleaned_text":"wait airplan pick take wwdc late",
    "normalized_text":"wait airplan pick take wwdc late",
    "tokens":[
      "wait",
      "airplan",
      "pick",
      "take",
      "wwdc",
      "late"
    ],
    "token_count":6,
    "processed_text":"wait airplan pick take wwdc late"
  },
  {
    "label":4,
    "text":"takin easi mum delici sandwich yum",
    "cleaned_text":"takin easi mum delici sandwich yum",
    "normalized_text":"takin easi mum delici sandwich yum",
    "tokens":[
      "takin",
      "easi",
      "mum",
      "delici",
      "sandwich",
      "yum"
    ],
    "token_count":6,
    "processed_text":"takin easi mum delici sandwich yum"
  },
  {
    "label":4,
    "text":"leav tomorrow nc ill back friday textcal",
    "cleaned_text":"leav tomorrow nc ill back friday textcal",
    "normalized_text":"leav tomorrow nc ill back friday textcal",
    "tokens":[
      "leav",
      "tomorrow",
      "nc",
      "ill",
      "back",
      "friday",
      "textcal"
    ],
    "token_count":7,
    "processed_text":"leav tomorrow nc ill back friday textcal"
  },
  {
    "label":0,
    "text":"feel like pasta haha sooooper dooooper borednoth ground ugggh rocklobst song stuck headbahahaha",
    "cleaned_text":"feel like pasta haha sooooper dooooper borednoth ground ugggh rocklobst song stuck headbahahaha",
    "normalized_text":"feel like pasta haha sooooper dooooper borednoth ground ugggh rocklobst song stuck headbahahaha",
    "tokens":[
      "feel",
      "like",
      "pasta",
      "haha",
      "sooooper",
      "dooooper",
      "borednoth",
      "ground",
      "ugggh",
      "rocklobst",
      "song",
      "stuck",
      "headbahahaha"
    ],
    "token_count":13,
    "processed_text":"feel like pasta haha sooooper dooooper borednoth ground ugggh rocklobst song stuck headbahahaha"
  },
  {
    "label":0,
    "text":"long somewhat terribl day",
    "cleaned_text":"long somewhat terribl day",
    "normalized_text":"long somewhat terribl day",
    "tokens":[
      "long",
      "somewhat",
      "terribl",
      "day"
    ],
    "token_count":4,
    "processed_text":"long somewhat terribl day"
  },
  {
    "label":0,
    "text":"cant believ forgot set dvr record boston marathon grrrr",
    "cleaned_text":"cant believ forgot set dvr record boston marathon grrrr",
    "normalized_text":"cant believ forgot set dvr record boston marathon grrrr",
    "tokens":[
      "cant",
      "believ",
      "forgot",
      "set",
      "dvr",
      "record",
      "boston",
      "marathon",
      "grrrr"
    ],
    "token_count":9,
    "processed_text":"cant believ forgot set dvr record boston marathon grrrr"
  },
  {
    "label":4,
    "text":"love new hairdo",
    "cleaned_text":"love new hairdo",
    "normalized_text":"love new hairdo",
    "tokens":[
      "love",
      "new",
      "hairdo"
    ],
    "token_count":3,
    "processed_text":"love new hairdo"
  },
  {
    "label":4,
    "text":"fuck yall heard chivalri aint dead lol",
    "cleaned_text":"fuck yall heard chivalri aint dead lol",
    "normalized_text":"fuck yall heard chivalri aint dead lol",
    "tokens":[
      "fuck",
      "yall",
      "heard",
      "chivalri",
      "aint",
      "dead",
      "lol"
    ],
    "token_count":7,
    "processed_text":"fuck yall heard chivalri aint dead lol"
  },
  {
    "label":0,
    "text":"nener nener nener lesson daylawn mower alway friend",
    "cleaned_text":"nener nener nener lesson daylawn mower alway friend",
    "normalized_text":"nener nener nener lesson daylawn mower alway friend",
    "tokens":[
      "nener",
      "nener",
      "nener",
      "lesson",
      "daylawn",
      "mower",
      "alway",
      "friend"
    ],
    "token_count":8,
    "processed_text":"nener nener nener lesson daylawn mower alway friend"
  },
  {
    "label":4,
    "text":"busi busi guy im happi acquaint think your pretti great guy",
    "cleaned_text":"busi busi guy im happi acquaint think your pretti great guy",
    "normalized_text":"busi busi guy im happi acquaint think your pretti great guy",
    "tokens":[
      "busi",
      "busi",
      "guy",
      "im",
      "happi",
      "acquaint",
      "think",
      "pretti",
      "great",
      "guy"
    ],
    "token_count":10,
    "processed_text":"busi busi guy im happi acquaint think pretti great guy"
  },
  {
    "label":0,
    "text":"hope soidk cant help worri babe",
    "cleaned_text":"hope soidk cant help worri babe",
    "normalized_text":"hope soidk cant help worri babe",
    "tokens":[
      "hope",
      "soidk",
      "cant",
      "help",
      "worri",
      "babe"
    ],
    "token_count":6,
    "processed_text":"hope soidk cant help worri babe"
  },
  {
    "label":4,
    "text":"hill twitter cute ahahah",
    "cleaned_text":"hill twitter cute ahahah",
    "normalized_text":"hill twitter cute ahahah",
    "tokens":[
      "hill",
      "twitter",
      "cute",
      "ahahah"
    ],
    "token_count":4,
    "processed_text":"hill twitter cute ahahah"
  },
  {
    "label":4,
    "text":"day dub sun ju shinein studi cant wait leavin",
    "cleaned_text":"day dub sun ju shinein studi cant wait leavin",
    "normalized_text":"day dub sun ju shinein studi cant wait leavin",
    "tokens":[
      "day",
      "dub",
      "sun",
      "ju",
      "shinein",
      "studi",
      "cant",
      "wait",
      "leavin"
    ],
    "token_count":9,
    "processed_text":"day dub sun ju shinein studi cant wait leavin"
  },
  {
    "label":4,
    "text":"think way journalist nowaday look forward chat away",
    "cleaned_text":"think way journalist nowaday look forward chat away",
    "normalized_text":"think way journalist nowaday look forward chat away",
    "tokens":[
      "think",
      "way",
      "journalist",
      "nowaday",
      "look",
      "forward",
      "chat",
      "away"
    ],
    "token_count":8,
    "processed_text":"think way journalist nowaday look forward chat away"
  },
  {
    "label":4,
    "text":"sunshin jonni amp jess come afternoon",
    "cleaned_text":"sunshin jonni amp jess come afternoon",
    "normalized_text":"sunshin jonni amp jess come afternoon",
    "tokens":[
      "sunshin",
      "jonni",
      "amp",
      "jess",
      "come",
      "afternoon"
    ],
    "token_count":6,
    "processed_text":"sunshin jonni amp jess come afternoon"
  },
  {
    "label":4,
    "text":"gonna wonder day gorgeou girlfriend",
    "cleaned_text":"gonna wonder day gorgeou girlfriend",
    "normalized_text":"gonna wonder day gorgeou girlfriend",
    "tokens":[
      "gon",
      "na",
      "wonder",
      "day",
      "gorgeou",
      "girlfriend"
    ],
    "token_count":6,
    "processed_text":"gon na wonder day gorgeou girlfriend"
  },
  {
    "label":0,
    "text":"dog attack rottwiel owner rais dog aggress simpl train grr",
    "cleaned_text":"dog attack rottwiel owner rais dog aggress simpl train grr",
    "normalized_text":"dog attack rottwiel owner rais dog aggress simpl train grr",
    "tokens":[
      "dog",
      "attack",
      "rottwiel",
      "owner",
      "rai",
      "dog",
      "aggress",
      "simpl",
      "train",
      "grr"
    ],
    "token_count":10,
    "processed_text":"dog attack rottwiel owner rai dog aggress simpl train grr"
  },
  {
    "label":0,
    "text":"work stare rain make sad",
    "cleaned_text":"work stare rain make sad",
    "normalized_text":"work stare rain make sad",
    "tokens":[
      "work",
      "stare",
      "rain",
      "make",
      "sad"
    ],
    "token_count":5,
    "processed_text":"work stare rain make sad"
  },
  {
    "label":4,
    "text":"sure first person spoke preview amp introduc also ceospac",
    "cleaned_text":"sure first person spoke preview amp introduc also ceospac",
    "normalized_text":"sure first person spoke preview amp introduc also ceospac",
    "tokens":[
      "sure",
      "first",
      "person",
      "spoke",
      "preview",
      "amp",
      "introduc",
      "also",
      "ceospac"
    ],
    "token_count":9,
    "processed_text":"sure first person spoke preview amp introduc also ceospac"
  },
  {
    "label":0,
    "text":"that good poor littl girl poor mommi",
    "cleaned_text":"that good poor littl girl poor mommi",
    "normalized_text":"that good poor littl girl poor mommi",
    "tokens":[
      "good",
      "poor",
      "littl",
      "girl",
      "poor",
      "mommi"
    ],
    "token_count":6,
    "processed_text":"good poor littl girl poor mommi"
  },
  {
    "label":4,
    "text":"happi bday good day x",
    "cleaned_text":"happi bday good day x",
    "normalized_text":"happi bday good day x",
    "tokens":[
      "happi",
      "bday",
      "good",
      "day"
    ],
    "token_count":4,
    "processed_text":"happi bday good day"
  },
  {
    "label":4,
    "text":"morn first day rest life",
    "cleaned_text":"morn first day rest life",
    "normalized_text":"morn first day rest life",
    "tokens":[
      "morn",
      "first",
      "day",
      "rest",
      "life"
    ],
    "token_count":5,
    "processed_text":"morn first day rest life"
  },
  {
    "label":4,
    "text":"bob statu updat protect",
    "cleaned_text":"bob statu updat protect",
    "normalized_text":"bob statu updat protect",
    "tokens":[
      "bob",
      "statu",
      "updat",
      "protect"
    ],
    "token_count":4,
    "processed_text":"bob statu updat protect"
  },
  {
    "label":4,
    "text":"tannyyyy",
    "cleaned_text":"tannyyyy",
    "normalized_text":"tannyyyy",
    "tokens":[
      "tannyyyy"
    ],
    "token_count":1,
    "processed_text":"tannyyyy"
  },
  {
    "label":0,
    "text":"incred tick sinc im one miss school stupid pool could get fill happi",
    "cleaned_text":"incred tick sinc im one miss school stupid pool could get fill happi",
    "normalized_text":"incred tick sinc im one miss school stupid pool could get fill happi",
    "tokens":[
      "incr",
      "tick",
      "sinc",
      "im",
      "one",
      "miss",
      "school",
      "stupid",
      "pool",
      "get",
      "fill",
      "happi"
    ],
    "token_count":12,
    "processed_text":"incr tick sinc im one miss school stupid pool get fill happi"
  },
  {
    "label":0,
    "text":"yay one day",
    "cleaned_text":"yay one day",
    "normalized_text":"yay one day",
    "tokens":[
      "yay",
      "one",
      "day"
    ],
    "token_count":3,
    "processed_text":"yay one day"
  },
  {
    "label":4,
    "text":"wish could la nice weather germani rain ton",
    "cleaned_text":"wish could la nice weather germani rain ton",
    "normalized_text":"wish could la nice weather germani rain ton",
    "tokens":[
      "wish",
      "la",
      "nice",
      "weather",
      "germani",
      "rain",
      "ton"
    ],
    "token_count":7,
    "processed_text":"wish la nice weather germani rain ton"
  },
  {
    "label":4,
    "text":"gooodnight",
    "cleaned_text":"gooodnight",
    "normalized_text":"gooodnight",
    "tokens":[
      "gooodnight"
    ],
    "token_count":1,
    "processed_text":"gooodnight"
  },
  {
    "label":4,
    "text":"watch sex citi day",
    "cleaned_text":"watch sex citi day",
    "normalized_text":"watch sex citi day",
    "tokens":[
      "watch",
      "sex",
      "citi",
      "day"
    ],
    "token_count":4,
    "processed_text":"watch sex citi day"
  },
  {
    "label":4,
    "text":"heh im tri need commun feedback keep us cours feel quit respons amp public",
    "cleaned_text":"heh im tri need commun feedback keep us cours feel quit respons amp public",
    "normalized_text":"heh im tri need commun feedback keep us cours feel quit respons amp public",
    "tokens":[
      "heh",
      "im",
      "tri",
      "need",
      "commun",
      "feedback",
      "keep",
      "us",
      "cour",
      "feel",
      "quit",
      "respon",
      "amp",
      "public"
    ],
    "token_count":14,
    "processed_text":"heh im tri need commun feedback keep us cour feel quit respon amp public"
  },
  {
    "label":4,
    "text":"watch lifetim movi",
    "cleaned_text":"watch lifetim movi",
    "normalized_text":"watch lifetim movi",
    "tokens":[
      "watch",
      "lifetim",
      "movi"
    ],
    "token_count":3,
    "processed_text":"watch lifetim movi"
  },
  {
    "label":0,
    "text":"night mayb get decent sleep tonight",
    "cleaned_text":"night mayb get decent sleep tonight",
    "normalized_text":"night mayb get decent sleep tonight",
    "tokens":[
      "night",
      "mayb",
      "get",
      "decent",
      "sleep",
      "tonight"
    ],
    "token_count":6,
    "processed_text":"night mayb get decent sleep tonight"
  },
  {
    "label":0,
    "text":"dont feel like work",
    "cleaned_text":"dont feel like work",
    "normalized_text":"dont feel like work",
    "tokens":[
      "dont",
      "feel",
      "like",
      "work"
    ],
    "token_count":4,
    "processed_text":"dont feel like work"
  },
  {
    "label":0,
    "text":"ate best ice cream ever sort make feel better school tomorrow also im un natur tire pm booo ahhhh",
    "cleaned_text":"ate best ice cream ever sort make feel better school tomorrow also im un natur tire pm booo ahhhh",
    "normalized_text":"ate best ice cream ever sort make feel better school tomorrow also im un natur tire pm booo ahhhh",
    "tokens":[
      "ate",
      "best",
      "ice",
      "cream",
      "ever",
      "sort",
      "make",
      "feel",
      "better",
      "school",
      "tomorrow",
      "also",
      "im",
      "un",
      "natur",
      "tire",
      "pm",
      "booo",
      "ahhhh"
    ],
    "token_count":19,
    "processed_text":"ate best ice cream ever sort make feel better school tomorrow also im un natur tire pm booo ahhhh"
  },
  {
    "label":0,
    "text":"yeah cold still made dish tssssk",
    "cleaned_text":"yeah cold still made dish tssssk",
    "normalized_text":"yeah cold still made dish tssssk",
    "tokens":[
      "yeah",
      "cold",
      "still",
      "made",
      "dish",
      "tssssk"
    ],
    "token_count":6,
    "processed_text":"yeah cold still made dish tssssk"
  },
  {
    "label":0,
    "text":"sundayi sch monday morn dinner sunday",
    "cleaned_text":"sundayi sch monday morn dinner sunday",
    "normalized_text":"sundayi sch monday morn dinner sunday",
    "tokens":[
      "sundayi",
      "sch",
      "monday",
      "morn",
      "dinner",
      "sunday"
    ],
    "token_count":6,
    "processed_text":"sundayi sch monday morn dinner sunday"
  },
  {
    "label":0,
    "text":"im answer mine ala u dont number lol",
    "cleaned_text":"im answer mine ala u dont number lol",
    "normalized_text":"im answer mine ala u dont number lol",
    "tokens":[
      "im",
      "answer",
      "mine",
      "ala",
      "dont",
      "number",
      "lol"
    ],
    "token_count":7,
    "processed_text":"im answer mine ala dont number lol"
  },
  {
    "label":0,
    "text":"melt traffic tri bring mom",
    "cleaned_text":"melt traffic tri bring mom",
    "normalized_text":"melt traffic tri bring mom",
    "tokens":[
      "melt",
      "traffic",
      "tri",
      "bring",
      "mom"
    ],
    "token_count":5,
    "processed_text":"melt traffic tri bring mom"
  },
  {
    "label":4,
    "text":"whassup mijo",
    "cleaned_text":"whassup mijo",
    "normalized_text":"whassup mijo",
    "tokens":[
      "whassup",
      "mijo"
    ],
    "token_count":2,
    "processed_text":"whassup mijo"
  },
  {
    "label":0,
    "text":"hey stacey kri allan american idol sit car wait dad rain",
    "cleaned_text":"hey stacey kri allan american idol sit car wait dad rain",
    "normalized_text":"hey stacey kri allan american idol sit car wait dad rain",
    "tokens":[
      "hey",
      "stacey",
      "kri",
      "allan",
      "american",
      "idol",
      "sit",
      "car",
      "wait",
      "dad",
      "rain"
    ],
    "token_count":11,
    "processed_text":"hey stacey kri allan american idol sit car wait dad rain"
  },
  {
    "label":4,
    "text":"anoth ladi drive mph intersect honk horn everyon slow drive kill peopl",
    "cleaned_text":"anoth ladi drive mph intersect honk horn everyon slow drive kill peopl",
    "normalized_text":"anoth ladi drive mph intersect honk horn everyon slow drive kill peopl",
    "tokens":[
      "anoth",
      "ladi",
      "drive",
      "mph",
      "intersect",
      "honk",
      "horn",
      "everyon",
      "slow",
      "drive",
      "kill",
      "peopl"
    ],
    "token_count":12,
    "processed_text":"anoth ladi drive mph intersect honk horn everyon slow drive kill peopl"
  },
  {
    "label":4,
    "text":"dylan boy amp girl club aidan clean hous watchin cartoon fun sun",
    "cleaned_text":"dylan boy amp girl club aidan clean hous watchin cartoon fun sun",
    "normalized_text":"dylan boy amp girl club aidan clean hous watchin cartoon fun sun",
    "tokens":[
      "dylan",
      "boy",
      "amp",
      "girl",
      "club",
      "aidan",
      "clean",
      "hou",
      "watchin",
      "cartoon",
      "fun",
      "sun"
    ],
    "token_count":12,
    "processed_text":"dylan boy amp girl club aidan clean hou watchin cartoon fun sun"
  },
  {
    "label":4,
    "text":"sleep spend daynight britt janel possibl band practic sunday hang britt byyyyy",
    "cleaned_text":"sleep spend daynight britt janel possibl band practic sunday hang britt byyyyy",
    "normalized_text":"sleep spend daynight britt janel possibl band practic sunday hang britt byyyyy",
    "tokens":[
      "sleep",
      "spend",
      "daynight",
      "britt",
      "janel",
      "possibl",
      "band",
      "practic",
      "sunday",
      "hang",
      "britt",
      "byyyyi"
    ],
    "token_count":12,
    "processed_text":"sleep spend daynight britt janel possibl band practic sunday hang britt byyyyi"
  },
  {
    "label":4,
    "text":"normal end bedroom offic victim time glad",
    "cleaned_text":"normal end bedroom offic victim time glad",
    "normalized_text":"normal end bedroom offic victim time glad",
    "tokens":[
      "normal",
      "end",
      "bedroom",
      "offic",
      "victim",
      "time",
      "glad"
    ],
    "token_count":7,
    "processed_text":"normal end bedroom offic victim time glad"
  },
  {
    "label":0,
    "text":"hi ive got bad grade math test",
    "cleaned_text":"hi ive got bad grade math test",
    "normalized_text":"hi ive got bad grade math test",
    "tokens":[
      "hi",
      "ive",
      "got",
      "bad",
      "grade",
      "math",
      "test"
    ],
    "token_count":7,
    "processed_text":"hi ive got bad grade math test"
  },
  {
    "label":0,
    "text":"feel weird im still hungri",
    "cleaned_text":"feel weird im still hungri",
    "normalized_text":"feel weird im still hungri",
    "tokens":[
      "feel",
      "weird",
      "im",
      "still",
      "hungri"
    ],
    "token_count":5,
    "processed_text":"feel weird im still hungri"
  },
  {
    "label":4,
    "text":"look good lol",
    "cleaned_text":"look good lol",
    "normalized_text":"look good lol",
    "tokens":[
      "look",
      "good",
      "lol"
    ],
    "token_count":3,
    "processed_text":"look good lol"
  },
  {
    "label":0,
    "text":"keep dad prayer he realli sick",
    "cleaned_text":"keep dad prayer he realli sick",
    "normalized_text":"keep dad prayer he realli sick",
    "tokens":[
      "keep",
      "dad",
      "prayer",
      "realli",
      "sick"
    ],
    "token_count":5,
    "processed_text":"keep dad prayer realli sick"
  },
  {
    "label":4,
    "text":"somehow dont think boy quail appreci tag quotlittl bob thingiequot",
    "cleaned_text":"somehow dont think boy quail appreci tag quotlittl bob thingiequot",
    "normalized_text":"somehow dont think boy quail appreci tag quotlittl bob thingiequot",
    "tokens":[
      "somehow",
      "dont",
      "think",
      "boy",
      "quail",
      "appreci",
      "tag",
      "quotlittl",
      "bob",
      "thingiequot"
    ],
    "token_count":10,
    "processed_text":"somehow dont think boy quail appreci tag quotlittl bob thingiequot"
  },
  {
    "label":0,
    "text":"good morn weather great ankl isnt better today work cu later",
    "cleaned_text":"good morn weather great ankl isnt better today work cu later",
    "normalized_text":"good morn weather great ankl isnt better today work cu later",
    "tokens":[
      "good",
      "morn",
      "weather",
      "great",
      "ankl",
      "isnt",
      "better",
      "today",
      "work",
      "cu",
      "later"
    ],
    "token_count":11,
    "processed_text":"good morn weather great ankl isnt better today work cu later"
  },
  {
    "label":0,
    "text":"work decid stay home downward spiral begin",
    "cleaned_text":"work decid stay home downward spiral begin",
    "normalized_text":"work decid stay home downward spiral begin",
    "tokens":[
      "work",
      "decid",
      "stay",
      "home",
      "downward",
      "spiral",
      "begin"
    ],
    "token_count":7,
    "processed_text":"work decid stay home downward spiral begin"
  },
  {
    "label":0,
    "text":"site longer cant open arrgghhhh",
    "cleaned_text":"site longer cant open arrgghhhh",
    "normalized_text":"site longer cant open arrgghhhh",
    "tokens":[
      "site",
      "longer",
      "cant",
      "open",
      "arrgghhhh"
    ],
    "token_count":5,
    "processed_text":"site longer cant open arrgghhhh"
  },
  {
    "label":0,
    "text":"much pack last day confer tomorrowtoday last event sad",
    "cleaned_text":"much pack last day confer tomorrowtoday last event sad",
    "normalized_text":"much pack last day confer tomorrowtoday last event sad",
    "tokens":[
      "much",
      "pack",
      "last",
      "day",
      "confer",
      "tomorrowtoday",
      "last",
      "event",
      "sad"
    ],
    "token_count":9,
    "processed_text":"much pack last day confer tomorrowtoday last event sad"
  },
  {
    "label":4,
    "text":"that stupid secur alway bad mood tri make everyon els feel pain",
    "cleaned_text":"that stupid secur alway bad mood tri make everyon els feel pain",
    "normalized_text":"that stupid secur alway bad mood tri make everyon els feel pain",
    "tokens":[
      "stupid",
      "secur",
      "alway",
      "bad",
      "mood",
      "tri",
      "make",
      "everyon",
      "el",
      "feel",
      "pain"
    ],
    "token_count":11,
    "processed_text":"stupid secur alway bad mood tri make everyon el feel pain"
  },
  {
    "label":4,
    "text":"toooooooooooooday lost omggg im excit",
    "cleaned_text":"toooooooooooooday lost omggg im excit",
    "normalized_text":"toooooooooooooday lost omggg im excit",
    "tokens":[
      "lost",
      "omggg",
      "im",
      "excit"
    ],
    "token_count":4,
    "processed_text":"lost omggg im excit"
  },
  {
    "label":0,
    "text":"sorri miss first friend jen packag mix orlando took short",
    "cleaned_text":"sorri miss first friend jen packag mix orlando took short",
    "normalized_text":"sorri miss first friend jen packag mix orlando took short",
    "tokens":[
      "sorri",
      "miss",
      "first",
      "friend",
      "jen",
      "packag",
      "mix",
      "orlando",
      "took",
      "short"
    ],
    "token_count":10,
    "processed_text":"sorri miss first friend jen packag mix orlando took short"
  },
  {
    "label":4,
    "text":"thank crystal followfriday follow friendli",
    "cleaned_text":"thank crystal followfriday follow friendli",
    "normalized_text":"thank crystal followfriday follow friendli",
    "tokens":[
      "thank",
      "crystal",
      "followfriday",
      "follow",
      "friendli"
    ],
    "token_count":5,
    "processed_text":"thank crystal followfriday follow friendli"
  },
  {
    "label":0,
    "text":"support ki ac bad im go miss",
    "cleaned_text":"support ki ac bad im go miss",
    "normalized_text":"support ki ac bad im go miss",
    "tokens":[
      "support",
      "ki",
      "ac",
      "bad",
      "im",
      "go",
      "miss"
    ],
    "token_count":7,
    "processed_text":"support ki ac bad im go miss"
  },
  {
    "label":4,
    "text":"go watch pink panther part duex later twitterit",
    "cleaned_text":"go watch pink panther part duex later twitterit",
    "normalized_text":"go watch pink panther part duex later twitterit",
    "tokens":[
      "go",
      "watch",
      "pink",
      "panther",
      "part",
      "duex",
      "later",
      "twitterit"
    ],
    "token_count":8,
    "processed_text":"go watch pink panther part duex later twitterit"
  },
  {
    "label":0,
    "text":"damn love hood smh",
    "cleaned_text":"damn love hood smh",
    "normalized_text":"damn love hood smh",
    "tokens":[
      "damn",
      "love",
      "hood",
      "smh"
    ],
    "token_count":4,
    "processed_text":"damn love hood smh"
  },
  {
    "label":4,
    "text":"gregoryyyyyyyyyyyyyyyyyyyyyyi final",
    "cleaned_text":"gregoryyyyyyyyyyyyyyyyyyyyyyi final",
    "normalized_text":"gregoryyyyyyyyyyyyyyyyyyyyyyi final",
    "tokens":[
      "final"
    ],
    "token_count":1,
    "processed_text":"final"
  },
  {
    "label":0,
    "text":"aww realiz think sexi chocol job honeymoon anoth one bite dust",
    "cleaned_text":"aww realiz think sexi chocol job honeymoon anoth one bite dust",
    "normalized_text":"aww realiz think sexi chocol job honeymoon anoth one bite dust",
    "tokens":[
      "aww",
      "realiz",
      "think",
      "sexi",
      "chocol",
      "job",
      "honeymoon",
      "anoth",
      "one",
      "bite",
      "dust"
    ],
    "token_count":11,
    "processed_text":"aww realiz think sexi chocol job honeymoon anoth one bite dust"
  },
  {
    "label":4,
    "text":"keep eye handsom hermitwrit sunset color pant",
    "cleaned_text":"keep eye handsom hermitwrit sunset color pant",
    "normalized_text":"keep eye handsom hermitwrit sunset color pant",
    "tokens":[
      "keep",
      "eye",
      "handsom",
      "hermitwrit",
      "sunset",
      "color",
      "pant"
    ],
    "token_count":7,
    "processed_text":"keep eye handsom hermitwrit sunset color pant"
  },
  {
    "label":4,
    "text":"actual quotrun fast sugarcanequot idea",
    "cleaned_text":"actual quotrun fast sugarcanequot idea",
    "normalized_text":"actual quotrun fast sugarcanequot idea",
    "tokens":[
      "actual",
      "quotrun",
      "fast",
      "sugarcanequot",
      "idea"
    ],
    "token_count":5,
    "processed_text":"actual quotrun fast sugarcanequot idea"
  },
  {
    "label":4,
    "text":"oh know susan boyl play sport googl wave googl sponsor stadium wave",
    "cleaned_text":"oh know susan boyl play sport googl wave googl sponsor stadium wave",
    "normalized_text":"oh know susan boyl play sport googl wave googl sponsor stadium wave",
    "tokens":[
      "oh",
      "know",
      "susan",
      "boyl",
      "play",
      "sport",
      "googl",
      "wave",
      "googl",
      "sponsor",
      "stadium",
      "wave"
    ],
    "token_count":12,
    "processed_text":"oh know susan boyl play sport googl wave googl sponsor stadium wave"
  },
  {
    "label":4,
    "text":"time shot ill sleep land",
    "cleaned_text":"time shot ill sleep land",
    "normalized_text":"time shot ill sleep land",
    "tokens":[
      "time",
      "shot",
      "ill",
      "sleep",
      "land"
    ],
    "token_count":5,
    "processed_text":"time shot ill sleep land"
  },
  {
    "label":4,
    "text":"gonna go check cccoe thread see what cali crafter today",
    "cleaned_text":"gonna go check cccoe thread see what cali crafter today",
    "normalized_text":"gonna go check cccoe thread see what cali crafter today",
    "tokens":[
      "gon",
      "na",
      "go",
      "check",
      "cccoe",
      "thread",
      "see",
      "cali",
      "crafter",
      "today"
    ],
    "token_count":10,
    "processed_text":"gon na go check cccoe thread see cali crafter today"
  },
  {
    "label":0,
    "text":"feel bit less sickish back summer school soo idk",
    "cleaned_text":"feel bit less sickish back summer school soo idk",
    "normalized_text":"feel bit less sickish back summer school soo idk",
    "tokens":[
      "feel",
      "bit",
      "less",
      "sickish",
      "back",
      "summer",
      "school",
      "soo",
      "idk"
    ],
    "token_count":9,
    "processed_text":"feel bit less sickish back summer school soo idk"
  },
  {
    "label":4,
    "text":"ever need technic servic oper assist",
    "cleaned_text":"ever need technic servic oper assist",
    "normalized_text":"ever need technic servic oper assist",
    "tokens":[
      "ever",
      "need",
      "technic",
      "servic",
      "oper",
      "assist"
    ],
    "token_count":6,
    "processed_text":"ever need technic servic oper assist"
  },
  {
    "label":0,
    "text":"got back breakfast cassi im gonna miss much",
    "cleaned_text":"got back breakfast cassi im gonna miss much",
    "normalized_text":"got back breakfast cassi im gonna miss much",
    "tokens":[
      "got",
      "back",
      "breakfast",
      "cassi",
      "im",
      "gon",
      "na",
      "miss",
      "much"
    ],
    "token_count":9,
    "processed_text":"got back breakfast cassi im gon na miss much"
  },
  {
    "label":0,
    "text":"good morn tweeter last even funni listen jess cd lol id listen bs tour time",
    "cleaned_text":"good morn tweeter last even funni listen jess cd lol id listen bs tour time",
    "normalized_text":"good morn tweeter last even funni listen jess cd lol id listen bs tour time",
    "tokens":[
      "good",
      "morn",
      "tweeter",
      "last",
      "even",
      "funni",
      "listen",
      "jess",
      "cd",
      "lol",
      "id",
      "listen",
      "bs",
      "tour",
      "time"
    ],
    "token_count":15,
    "processed_text":"good morn tweeter last even funni listen jess cd lol id listen bs tour time"
  },
  {
    "label":0,
    "text":"say dam hell",
    "cleaned_text":"say dam hell",
    "normalized_text":"say dam hell",
    "tokens":[
      "say",
      "dam",
      "hell"
    ],
    "token_count":3,
    "processed_text":"say dam hell"
  },
  {
    "label":4,
    "text":"marii dont know hell time washington your innoc",
    "cleaned_text":"marii dont know hell time washington your innoc",
    "normalized_text":"marii dont know hell time washington your innoc",
    "tokens":[
      "marii",
      "dont",
      "know",
      "hell",
      "time",
      "washington",
      "innoc"
    ],
    "token_count":7,
    "processed_text":"marii dont know hell time washington innoc"
  },
  {
    "label":4,
    "text":"hand plastic spoon",
    "cleaned_text":"hand plastic spoon",
    "normalized_text":"hand plastic spoon",
    "tokens":[
      "hand",
      "plastic",
      "spoon"
    ],
    "token_count":3,
    "processed_text":"hand plastic spoon"
  },
  {
    "label":0,
    "text":"whenev face choic pee make train time alway choos train",
    "cleaned_text":"whenev face choic pee make train time alway choos train",
    "normalized_text":"whenev face choic pee make train time alway choos train",
    "tokens":[
      "whenev",
      "face",
      "choic",
      "pee",
      "make",
      "train",
      "time",
      "alway",
      "choo",
      "train"
    ],
    "token_count":10,
    "processed_text":"whenev face choic pee make train time alway choo train"
  },
  {
    "label":0,
    "text":"ohhh front row almost center quotnewquot fantasm sweet churro werent",
    "cleaned_text":"ohhh front row almost center quotnewquot fantasm sweet churro werent",
    "normalized_text":"ohhh front row almost center quotnewquot fantasm sweet churro werent",
    "tokens":[
      "ohhh",
      "front",
      "row",
      "almost",
      "center",
      "quotnewquot",
      "fantasm",
      "sweet",
      "churro",
      "werent"
    ],
    "token_count":10,
    "processed_text":"ohhh front row almost center quotnewquot fantasm sweet churro werent"
  },
  {
    "label":0,
    "text":"said dont kick feet co that coffe went everywher fault",
    "cleaned_text":"said dont kick feet co that coffe went everywher fault",
    "normalized_text":"said dont kick feet co that coffe went everywher fault",
    "tokens":[
      "said",
      "dont",
      "kick",
      "feet",
      "co",
      "coff",
      "went",
      "everywh",
      "fault"
    ],
    "token_count":9,
    "processed_text":"said dont kick feet co coff went everywh fault"
  },
  {
    "label":0,
    "text":"tweet iphon mummi feel ill",
    "cleaned_text":"tweet iphon mummi feel ill",
    "normalized_text":"tweet iphon mummi feel ill",
    "tokens":[
      "tweet",
      "iphon",
      "mummi",
      "feel",
      "ill"
    ],
    "token_count":5,
    "processed_text":"tweet iphon mummi feel ill"
  },
  {
    "label":4,
    "text":"tee hee",
    "cleaned_text":"tee hee",
    "normalized_text":"tee hee",
    "tokens":[
      "tee",
      "hee"
    ],
    "token_count":2,
    "processed_text":"tee hee"
  },
  {
    "label":4,
    "text":"actual smstweet",
    "cleaned_text":"actual smstweet",
    "normalized_text":"actual smstweet",
    "tokens":[
      "actual",
      "smstweet"
    ],
    "token_count":2,
    "processed_text":"actual smstweet"
  },
  {
    "label":0,
    "text":"hope didnt miss comput crash burn im sure well skype would work anyway sigh",
    "cleaned_text":"hope didnt miss comput crash burn im sure well skype would work anyway sigh",
    "normalized_text":"hope didnt miss comput crash burn im sure well skype would work anyway sigh",
    "tokens":[
      "hope",
      "didnt",
      "miss",
      "comput",
      "crash",
      "burn",
      "im",
      "sure",
      "well",
      "skype",
      "work",
      "anyway",
      "sigh"
    ],
    "token_count":13,
    "processed_text":"hope didnt miss comput crash burn im sure well skype work anyway sigh"
  },
  {
    "label":4,
    "text":"thanx listenin amp appreci",
    "cleaned_text":"thanx listenin amp appreci",
    "normalized_text":"thanx listenin amp appreci",
    "tokens":[
      "thanx",
      "listenin",
      "amp",
      "appreci"
    ],
    "token_count":4,
    "processed_text":"thanx listenin amp appreci"
  },
  {
    "label":0,
    "text":"dont cool twitter friend",
    "cleaned_text":"dont cool twitter friend",
    "normalized_text":"dont cool twitter friend",
    "tokens":[
      "dont",
      "cool",
      "twitter",
      "friend"
    ],
    "token_count":4,
    "processed_text":"dont cool twitter friend"
  },
  {
    "label":4,
    "text":"nice time mtvma",
    "cleaned_text":"nice time mtvma",
    "normalized_text":"nice time mtvma",
    "tokens":[
      "nice",
      "time",
      "mtvma"
    ],
    "token_count":3,
    "processed_text":"nice time mtvma"
  },
  {
    "label":4,
    "text":"thing id twit u made im writer doll",
    "cleaned_text":"thing id twit u made im writer doll",
    "normalized_text":"thing id twit u made im writer doll",
    "tokens":[
      "thing",
      "id",
      "twit",
      "made",
      "im",
      "writer",
      "doll"
    ],
    "token_count":7,
    "processed_text":"thing id twit made im writer doll"
  },
  {
    "label":0,
    "text":"witt earphon ipod broke",
    "cleaned_text":"witt earphon ipod broke",
    "normalized_text":"witt earphon ipod broke",
    "tokens":[
      "witt",
      "earphon",
      "ipod",
      "broke"
    ],
    "token_count":4,
    "processed_text":"witt earphon ipod broke"
  },
  {
    "label":0,
    "text":"noth wear need haircut",
    "cleaned_text":"noth wear need haircut",
    "normalized_text":"noth wear need haircut",
    "tokens":[
      "noth",
      "wear",
      "need",
      "haircut"
    ],
    "token_count":4,
    "processed_text":"noth wear need haircut"
  },
  {
    "label":0,
    "text":"youtub mainten back shortli",
    "cleaned_text":"youtub mainten back shortli",
    "normalized_text":"youtub mainten back shortli",
    "tokens":[
      "youtub",
      "mainten",
      "back",
      "shortli"
    ],
    "token_count":4,
    "processed_text":"youtub mainten back shortli"
  },
  {
    "label":0,
    "text":"latest video isnt upload youtub rd tri that get tri upload high def",
    "cleaned_text":"latest video isnt upload youtub rd tri that get tri upload high def",
    "normalized_text":"latest video isnt upload youtub rd tri that get tri upload high def",
    "tokens":[
      "latest",
      "video",
      "isnt",
      "upload",
      "youtub",
      "rd",
      "tri",
      "get",
      "tri",
      "upload",
      "high",
      "def"
    ],
    "token_count":12,
    "processed_text":"latest video isnt upload youtub rd tri get tri upload high def"
  },
  {
    "label":0,
    "text":"ear piec phone broken",
    "cleaned_text":"ear piec phone broken",
    "normalized_text":"ear piec phone broken",
    "tokens":[
      "ear",
      "piec",
      "phone",
      "broken"
    ],
    "token_count":4,
    "processed_text":"ear piec phone broken"
  },
  {
    "label":4,
    "text":"thank maternalhealth support",
    "cleaned_text":"thank maternalhealth support",
    "normalized_text":"thank maternalhealth support",
    "tokens":[
      "thank",
      "maternalhealth",
      "support"
    ],
    "token_count":3,
    "processed_text":"thank maternalhealth support"
  },
  {
    "label":4,
    "text":"im awak refresh get dress head coffe classtoday gonna good day",
    "cleaned_text":"im awak refresh get dress head coffe classtoday gonna good day",
    "normalized_text":"im awak refresh get dress head coffe classtoday gonna good day",
    "tokens":[
      "im",
      "awak",
      "refresh",
      "get",
      "dress",
      "head",
      "coff",
      "classtoday",
      "gon",
      "na",
      "good",
      "day"
    ],
    "token_count":12,
    "processed_text":"im awak refresh get dress head coff classtoday gon na good day"
  },
  {
    "label":0,
    "text":"cant hahaha shit damnit",
    "cleaned_text":"cant hahaha shit damnit",
    "normalized_text":"cant hahaha shit damnit",
    "tokens":[
      "cant",
      "hahaha",
      "shit",
      "damnit"
    ],
    "token_count":4,
    "processed_text":"cant hahaha shit damnit"
  },
  {
    "label":0,
    "text":"stress",
    "cleaned_text":"stress",
    "normalized_text":"stress",
    "tokens":[
      "stress"
    ],
    "token_count":1,
    "processed_text":"stress"
  },
  {
    "label":0,
    "text":"bore im patient edg connect phone tmobil get g pr",
    "cleaned_text":"bore im patient edg connect phone tmobil get g pr",
    "normalized_text":"bore im patient edg connect phone tmobil get g pr",
    "tokens":[
      "bore",
      "im",
      "patient",
      "edg",
      "connect",
      "phone",
      "tmobil",
      "get",
      "pr"
    ],
    "token_count":9,
    "processed_text":"bore im patient edg connect phone tmobil get pr"
  },
  {
    "label":4,
    "text":"hmmmmmmremind rickster back day",
    "cleaned_text":"hmmmmmmremind rickster back day",
    "normalized_text":"hmmmmmmremind rickster back day",
    "tokens":[
      "hmmmmmmremind",
      "rickster",
      "back",
      "day"
    ],
    "token_count":4,
    "processed_text":"hmmmmmmremind rickster back day"
  },
  {
    "label":0,
    "text":"think postiv",
    "cleaned_text":"think postiv",
    "normalized_text":"think postiv",
    "tokens":[
      "think",
      "postiv"
    ],
    "token_count":2,
    "processed_text":"think postiv"
  },
  {
    "label":4,
    "text":"ok confess like logi even possess one live happi life toilet",
    "cleaned_text":"ok confess like logi even possess one live happi life toilet",
    "normalized_text":"ok confess like logi even possess one live happi life toilet",
    "tokens":[
      "ok",
      "confess",
      "like",
      "logi",
      "even",
      "possess",
      "one",
      "live",
      "happi",
      "life",
      "toilet"
    ],
    "token_count":11,
    "processed_text":"ok confess like logi even possess one live happi life toilet"
  },
  {
    "label":4,
    "text":"good morn tj hope good day",
    "cleaned_text":"good morn tj hope good day",
    "normalized_text":"good morn tj hope good day",
    "tokens":[
      "good",
      "morn",
      "tj",
      "hope",
      "good",
      "day"
    ],
    "token_count":6,
    "processed_text":"good morn tj hope good day"
  },
  {
    "label":0,
    "text":"appar merg mutat version codebas nontrivi problem",
    "cleaned_text":"appar merg mutat version codebas nontrivi problem",
    "normalized_text":"appar merg mutat version codebas nontrivi problem",
    "tokens":[
      "appar",
      "merg",
      "mutat",
      "version",
      "codeba",
      "nontrivi",
      "problem"
    ],
    "token_count":7,
    "processed_text":"appar merg mutat version codeba nontrivi problem"
  },
  {
    "label":4,
    "text":"hey riell look who love",
    "cleaned_text":"hey riell look who love",
    "normalized_text":"hey riell look who love",
    "tokens":[
      "hey",
      "riell",
      "look",
      "love"
    ],
    "token_count":4,
    "processed_text":"hey riell look love"
  },
  {
    "label":0,
    "text":"head hurt",
    "cleaned_text":"head hurt",
    "normalized_text":"head hurt",
    "tokens":[
      "head",
      "hurt"
    ],
    "token_count":2,
    "processed_text":"head hurt"
  },
  {
    "label":4,
    "text":"clubanddancecom fr elektrofan",
    "cleaned_text":"clubanddancecom fr elektrofan",
    "normalized_text":"clubanddancecom fr elektrofan",
    "tokens":[
      "clubanddancecom",
      "fr",
      "elektrofan"
    ],
    "token_count":3,
    "processed_text":"clubanddancecom fr elektrofan"
  },
  {
    "label":4,
    "text":"saw new moon trailer againnnn",
    "cleaned_text":"saw new moon trailer againnnn",
    "normalized_text":"saw new moon trailer againnnn",
    "tokens":[
      "saw",
      "new",
      "moon",
      "trailer",
      "againnnn"
    ],
    "token_count":5,
    "processed_text":"saw new moon trailer againnnn"
  },
  {
    "label":0,
    "text":"ugh studi chem test blow",
    "cleaned_text":"ugh studi chem test blow",
    "normalized_text":"ugh studi chem test blow",
    "tokens":[
      "ugh",
      "studi",
      "chem",
      "test",
      "blow"
    ],
    "token_count":5,
    "processed_text":"ugh studi chem test blow"
  },
  {
    "label":4,
    "text":"aw ionica well miss take care see ya",
    "cleaned_text":"aw ionica well miss take care see ya",
    "normalized_text":"aw ionica well miss take care see ya",
    "tokens":[
      "aw",
      "ionica",
      "well",
      "miss",
      "take",
      "care",
      "see",
      "ya"
    ],
    "token_count":8,
    "processed_text":"aw ionica well miss take care see ya"
  },
  {
    "label":0,
    "text":"sooooo enjoy even daughter wish son enjoy even",
    "cleaned_text":"sooooo enjoy even daughter wish son enjoy even",
    "normalized_text":"sooooo enjoy even daughter wish son enjoy even",
    "tokens":[
      "sooooo",
      "enjoy",
      "even",
      "daughter",
      "wish",
      "son",
      "enjoy",
      "even"
    ],
    "token_count":8,
    "processed_text":"sooooo enjoy even daughter wish son enjoy even"
  },
  {
    "label":4,
    "text":"like link make great entri discov new photophil great job",
    "cleaned_text":"like link make great entri discov new photophil great job",
    "normalized_text":"like link make great entri discov new photophil great job",
    "tokens":[
      "like",
      "link",
      "make",
      "great",
      "entri",
      "discov",
      "new",
      "photophil",
      "great",
      "job"
    ],
    "token_count":10,
    "processed_text":"like link make great entri discov new photophil great job"
  },
  {
    "label":0,
    "text":"excit touch pro excit go work tomorrow",
    "cleaned_text":"excit touch pro excit go work tomorrow",
    "normalized_text":"excit touch pro excit go work tomorrow",
    "tokens":[
      "excit",
      "touch",
      "pro",
      "excit",
      "go",
      "work",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"excit touch pro excit go work tomorrow"
  },
  {
    "label":4,
    "text":"portugues exam easi let see biolog one",
    "cleaned_text":"portugues exam easi let see biolog one",
    "normalized_text":"portugues exam easi let see biolog one",
    "tokens":[
      "portugu",
      "exam",
      "easi",
      "let",
      "see",
      "biolog",
      "one"
    ],
    "token_count":7,
    "processed_text":"portugu exam easi let see biolog one"
  },
  {
    "label":0,
    "text":"serious parent non stop minniapoli newport week weekend san diego miss",
    "cleaned_text":"serious parent non stop minniapoli newport week weekend san diego miss",
    "normalized_text":"serious parent non stop minniapoli newport week weekend san diego miss",
    "tokens":[
      "seriou",
      "parent",
      "non",
      "stop",
      "minniapoli",
      "newport",
      "week",
      "weekend",
      "san",
      "diego",
      "miss"
    ],
    "token_count":11,
    "processed_text":"seriou parent non stop minniapoli newport week weekend san diego miss"
  },
  {
    "label":4,
    "text":"know joke spiritu agre quotth truth lispquot",
    "cleaned_text":"know joke spiritu agre quotth truth lispquot",
    "normalized_text":"know joke spiritu agre quotth truth lispquot",
    "tokens":[
      "know",
      "joke",
      "spiritu",
      "agr",
      "quotth",
      "truth",
      "lispquot"
    ],
    "token_count":7,
    "processed_text":"know joke spiritu agr quotth truth lispquot"
  },
  {
    "label":0,
    "text":"dont think lunch agre",
    "cleaned_text":"dont think lunch agre",
    "normalized_text":"dont think lunch agre",
    "tokens":[
      "dont",
      "think",
      "lunch",
      "agr"
    ],
    "token_count":4,
    "processed_text":"dont think lunch agr"
  },
  {
    "label":0,
    "text":"yeah suck went sleep woke",
    "cleaned_text":"yeah suck went sleep woke",
    "normalized_text":"yeah suck went sleep woke",
    "tokens":[
      "yeah",
      "suck",
      "went",
      "sleep",
      "woke"
    ],
    "token_count":5,
    "processed_text":"yeah suck went sleep woke"
  },
  {
    "label":0,
    "text":"poor drew",
    "cleaned_text":"poor drew",
    "normalized_text":"poor drew",
    "tokens":[
      "poor",
      "drew"
    ],
    "token_count":2,
    "processed_text":"poor drew"
  },
  {
    "label":0,
    "text":"cant actual play render show",
    "cleaned_text":"cant actual play render show",
    "normalized_text":"cant actual play render show",
    "tokens":[
      "cant",
      "actual",
      "play",
      "render",
      "show"
    ],
    "token_count":5,
    "processed_text":"cant actual play render show"
  },
  {
    "label":4,
    "text":"gonna sleep alreadi gudnyt tweopl",
    "cleaned_text":"gonna sleep alreadi gudnyt tweopl",
    "normalized_text":"gonna sleep alreadi gudnyt tweopl",
    "tokens":[
      "gon",
      "na",
      "sleep",
      "alreadi",
      "gudnyt",
      "tweopl"
    ],
    "token_count":6,
    "processed_text":"gon na sleep alreadi gudnyt tweopl"
  },
  {
    "label":4,
    "text":"mobil back realli miss onlin sista",
    "cleaned_text":"mobil back realli miss onlin sista",
    "normalized_text":"mobil back realli miss onlin sista",
    "tokens":[
      "mobil",
      "back",
      "realli",
      "miss",
      "onlin",
      "sista"
    ],
    "token_count":6,
    "processed_text":"mobil back realli miss onlin sista"
  },
  {
    "label":4,
    "text":"log ladi back pm masstweet jk watch come",
    "cleaned_text":"log ladi back pm masstweet jk watch come",
    "normalized_text":"log ladi back pm masstweet jk watch come",
    "tokens":[
      "log",
      "ladi",
      "back",
      "pm",
      "masstweet",
      "jk",
      "watch",
      "come"
    ],
    "token_count":8,
    "processed_text":"log ladi back pm masstweet jk watch come"
  },
  {
    "label":4,
    "text":"itsssssth st month",
    "cleaned_text":"itsssssth st month",
    "normalized_text":"itsssssth st month",
    "tokens":[
      "itsssssth",
      "st",
      "month"
    ],
    "token_count":3,
    "processed_text":"itsssssth st month"
  },
  {
    "label":4,
    "text":"one repres accur",
    "cleaned_text":"one repres accur",
    "normalized_text":"one repres accur",
    "tokens":[
      "one",
      "repr",
      "accur"
    ],
    "token_count":3,
    "processed_text":"one repr accur"
  },
  {
    "label":4,
    "text":"oh fabul thank much",
    "cleaned_text":"oh fabul thank much",
    "normalized_text":"oh fabul thank much",
    "tokens":[
      "oh",
      "fabul",
      "thank",
      "much"
    ],
    "token_count":4,
    "processed_text":"oh fabul thank much"
  },
  {
    "label":4,
    "text":"go shop today yay",
    "cleaned_text":"go shop today yay",
    "normalized_text":"go shop today yay",
    "tokens":[
      "go",
      "shop",
      "today",
      "yay"
    ],
    "token_count":4,
    "processed_text":"go shop today yay"
  },
  {
    "label":4,
    "text":"photo",
    "cleaned_text":"photo",
    "normalized_text":"photo",
    "tokens":[
      "photo"
    ],
    "token_count":1,
    "processed_text":"photo"
  },
  {
    "label":4,
    "text":"found twitter",
    "cleaned_text":"found twitter",
    "normalized_text":"found twitter",
    "tokens":[
      "found",
      "twitter"
    ],
    "token_count":2,
    "processed_text":"found twitter"
  },
  {
    "label":0,
    "text":"wish could next coupl day gotta save vac trip north later sit sob desk",
    "cleaned_text":"wish could next coupl day gotta save vac trip north later sit sob desk",
    "normalized_text":"wish could next coupl day gotta save vac trip north later sit sob desk",
    "tokens":[
      "wish",
      "next",
      "coupl",
      "day",
      "got",
      "ta",
      "save",
      "vac",
      "trip",
      "north",
      "later",
      "sit",
      "sob",
      "desk"
    ],
    "token_count":14,
    "processed_text":"wish next coupl day got ta save vac trip north later sit sob desk"
  },
  {
    "label":4,
    "text":"maryyi havta hang week ok hope fun gym class haha",
    "cleaned_text":"maryyi havta hang week ok hope fun gym class haha",
    "normalized_text":"maryyi havta hang week ok hope fun gym class haha",
    "tokens":[
      "maryyi",
      "havta",
      "hang",
      "week",
      "ok",
      "hope",
      "fun",
      "gym",
      "class",
      "haha"
    ],
    "token_count":10,
    "processed_text":"maryyi havta hang week ok hope fun gym class haha"
  },
  {
    "label":4,
    "text":"thanxth stand followfriday ander",
    "cleaned_text":"thanxth stand followfriday ander",
    "normalized_text":"thanxth stand followfriday ander",
    "tokens":[
      "thanxth",
      "stand",
      "followfriday",
      "ander"
    ],
    "token_count":4,
    "processed_text":"thanxth stand followfriday ander"
  },
  {
    "label":0,
    "text":"total miss",
    "cleaned_text":"total miss",
    "normalized_text":"total miss",
    "tokens":[
      "total",
      "miss"
    ],
    "token_count":2,
    "processed_text":"total miss"
  },
  {
    "label":0,
    "text":"dont want know kill know therefor need know hate think much",
    "cleaned_text":"dont want know kill know therefor need know hate think much",
    "normalized_text":"dont want know kill know therefor need know hate think much",
    "tokens":[
      "dont",
      "want",
      "know",
      "kill",
      "know",
      "therefor",
      "need",
      "know",
      "hate",
      "think",
      "much"
    ],
    "token_count":11,
    "processed_text":"dont want know kill know therefor need know hate think much"
  },
  {
    "label":0,
    "text":"summer school start tomorrow hope still find time vacat",
    "cleaned_text":"summer school start tomorrow hope still find time vacat",
    "normalized_text":"summer school start tomorrow hope still find time vacat",
    "tokens":[
      "summer",
      "school",
      "start",
      "tomorrow",
      "hope",
      "still",
      "find",
      "time",
      "vacat"
    ],
    "token_count":9,
    "processed_text":"summer school start tomorrow hope still find time vacat"
  },
  {
    "label":0,
    "text":"arghhhhhhwanna voic back",
    "cleaned_text":"arghhhhhhwanna voic back",
    "normalized_text":"arghhhhhhwanna voic back",
    "tokens":[
      "arghhhhhhwanna",
      "voic",
      "back"
    ],
    "token_count":3,
    "processed_text":"arghhhhhhwanna voic back"
  },
  {
    "label":4,
    "text":"lol alrighti might work",
    "cleaned_text":"lol alrighti might work",
    "normalized_text":"lol alrighti might work",
    "tokens":[
      "lol",
      "alrighti",
      "work"
    ],
    "token_count":3,
    "processed_text":"lol alrighti work"
  },
  {
    "label":0,
    "text":"love messag histori declin",
    "cleaned_text":"love messag histori declin",
    "normalized_text":"love messag histori declin",
    "tokens":[
      "love",
      "messag",
      "histori",
      "declin"
    ],
    "token_count":4,
    "processed_text":"love messag histori declin"
  },
  {
    "label":0,
    "text":"oh realis feel unwel much yesterday town garden",
    "cleaned_text":"oh realis feel unwel much yesterday town garden",
    "normalized_text":"oh realis feel unwel much yesterday town garden",
    "tokens":[
      "oh",
      "reali",
      "feel",
      "unwel",
      "much",
      "yesterday",
      "town",
      "garden"
    ],
    "token_count":8,
    "processed_text":"oh reali feel unwel much yesterday town garden"
  },
  {
    "label":4,
    "text":"pbt take year life",
    "cleaned_text":"pbt take year life",
    "normalized_text":"pbt take year life",
    "tokens":[
      "pbt",
      "take",
      "year",
      "life"
    ],
    "token_count":4,
    "processed_text":"pbt take year life"
  },
  {
    "label":0,
    "text":"im leav thursday talk jess tonight decid want visit toward end juli miss much",
    "cleaned_text":"im leav thursday talk jess tonight decid want visit toward end juli miss much",
    "normalized_text":"im leav thursday talk jess tonight decid want visit toward end juli miss much",
    "tokens":[
      "im",
      "leav",
      "thursday",
      "talk",
      "jess",
      "tonight",
      "decid",
      "want",
      "visit",
      "toward",
      "end",
      "juli",
      "miss",
      "much"
    ],
    "token_count":14,
    "processed_text":"im leav thursday talk jess tonight decid want visit toward end juli miss much"
  },
  {
    "label":0,
    "text":"im weird bit phobia imag binocular microscop imax movi scare kid like tear",
    "cleaned_text":"im weird bit phobia imag binocular microscop imax movi scare kid like tear",
    "normalized_text":"im weird bit phobia imag binocular microscop imax movi scare kid like tear",
    "tokens":[
      "im",
      "weird",
      "bit",
      "phobia",
      "imag",
      "binocular",
      "microscop",
      "imax",
      "movi",
      "scare",
      "kid",
      "like",
      "tear"
    ],
    "token_count":13,
    "processed_text":"im weird bit phobia imag binocular microscop imax movi scare kid like tear"
  },
  {
    "label":4,
    "text":"role model amaz lair would someth would",
    "cleaned_text":"role model amaz lair would someth would",
    "normalized_text":"role model amaz lair would someth would",
    "tokens":[
      "role",
      "model",
      "amaz",
      "lair",
      "someth"
    ],
    "token_count":5,
    "processed_text":"role model amaz lair someth"
  },
  {
    "label":0,
    "text":"work tw realli dont want get earlyyyy",
    "cleaned_text":"work tw realli dont want get earlyyyy",
    "normalized_text":"work tw realli dont want get earlyyyy",
    "tokens":[
      "work",
      "tw",
      "realli",
      "dont",
      "want",
      "get",
      "earlyyyy"
    ],
    "token_count":7,
    "processed_text":"work tw realli dont want get earlyyyy"
  },
  {
    "label":4,
    "text":"long green field your good",
    "cleaned_text":"long green field your good",
    "normalized_text":"long green field your good",
    "tokens":[
      "long",
      "green",
      "field",
      "good"
    ],
    "token_count":4,
    "processed_text":"long green field good"
  },
  {
    "label":4,
    "text":"cheer white silenc tamar bought hunt eleph even",
    "cleaned_text":"cheer white silenc tamar bought hunt eleph even",
    "normalized_text":"cheer white silenc tamar bought hunt eleph even",
    "tokens":[
      "cheer",
      "white",
      "silenc",
      "tamar",
      "bought",
      "hunt",
      "eleph",
      "even"
    ],
    "token_count":8,
    "processed_text":"cheer white silenc tamar bought hunt eleph even"
  },
  {
    "label":0,
    "text":"mad rt keep tri remov peopl im still get updat",
    "cleaned_text":"mad rt keep tri remov peopl im still get updat",
    "normalized_text":"mad rt keep tri remov peopl im still get updat",
    "tokens":[
      "mad",
      "rt",
      "keep",
      "tri",
      "remov",
      "peopl",
      "im",
      "still",
      "get",
      "updat"
    ],
    "token_count":10,
    "processed_text":"mad rt keep tri remov peopl im still get updat"
  },
  {
    "label":4,
    "text":"home long day night hold littl wii fittin littl paint definit read",
    "cleaned_text":"home long day night hold littl wii fittin littl paint definit read",
    "normalized_text":"home long day night hold littl wii fittin littl paint definit read",
    "tokens":[
      "home",
      "long",
      "day",
      "night",
      "hold",
      "littl",
      "wii",
      "fittin",
      "littl",
      "paint",
      "definit",
      "read"
    ],
    "token_count":12,
    "processed_text":"home long day night hold littl wii fittin littl paint definit read"
  },
  {
    "label":0,
    "text":"june horribl everywher even shade cool u still feel heat",
    "cleaned_text":"june horribl everywher even shade cool u still feel heat",
    "normalized_text":"june horribl everywher even shade cool u still feel heat",
    "tokens":[
      "june",
      "horribl",
      "everywh",
      "even",
      "shade",
      "cool",
      "still",
      "feel",
      "heat"
    ],
    "token_count":9,
    "processed_text":"june horribl everywh even shade cool still feel heat"
  },
  {
    "label":4,
    "text":"areh best shower ever feel refresh warm",
    "cleaned_text":"areh best shower ever feel refresh warm",
    "normalized_text":"areh best shower ever feel refresh warm",
    "tokens":[
      "areh",
      "best",
      "shower",
      "ever",
      "feel",
      "refresh",
      "warm"
    ],
    "token_count":7,
    "processed_text":"areh best shower ever feel refresh warm"
  },
  {
    "label":4,
    "text":"yea ive seen time met time love bit xx",
    "cleaned_text":"yea ive seen time met time love bit xx",
    "normalized_text":"yea ive seen time met time love bit xx",
    "tokens":[
      "yea",
      "ive",
      "seen",
      "time",
      "met",
      "time",
      "love",
      "bit",
      "xx"
    ],
    "token_count":9,
    "processed_text":"yea ive seen time met time love bit xx"
  },
  {
    "label":0,
    "text":"contempl cuttin tha hair gettin tire day",
    "cleaned_text":"contempl cuttin tha hair gettin tire day",
    "normalized_text":"contempl cuttin tha hair gettin tire day",
    "tokens":[
      "contempl",
      "cuttin",
      "tha",
      "hair",
      "gettin",
      "tire",
      "day"
    ],
    "token_count":7,
    "processed_text":"contempl cuttin tha hair gettin tire day"
  },
  {
    "label":0,
    "text":"work hottest day year",
    "cleaned_text":"work hottest day year",
    "normalized_text":"work hottest day year",
    "tokens":[
      "work",
      "hottest",
      "day",
      "year"
    ],
    "token_count":4,
    "processed_text":"work hottest day year"
  },
  {
    "label":4,
    "text":"catch twitter stuff",
    "cleaned_text":"catch twitter stuff",
    "normalized_text":"catch twitter stuff",
    "tokens":[
      "catch",
      "twitter",
      "stuff"
    ],
    "token_count":3,
    "processed_text":"catch twitter stuff"
  },
  {
    "label":0,
    "text":"sdkljfsldfjeicsieufcsvikscnoasiew thirteen updat chang hope prime number die",
    "cleaned_text":"sdkljfsldfjeicsieufcsvikscnoasiew thirteen updat chang hope prime number die",
    "normalized_text":"sdkljfsldfjeicsieufcsvikscnoasiew thirteen updat chang hope prime number die",
    "tokens":[
      "thirteen",
      "updat",
      "chang",
      "hope",
      "prime",
      "number",
      "die"
    ],
    "token_count":7,
    "processed_text":"thirteen updat chang hope prime number die"
  },
  {
    "label":0,
    "text":"wow lot interest stuff twitter facebook gotta work dull day yaxley warm",
    "cleaned_text":"wow lot interest stuff twitter facebook gotta work dull day yaxley warm",
    "normalized_text":"wow lot interest stuff twitter facebook gotta work dull day yaxley warm",
    "tokens":[
      "wow",
      "lot",
      "interest",
      "stuff",
      "twitter",
      "facebook",
      "got",
      "ta",
      "work",
      "dull",
      "day",
      "yaxley",
      "warm"
    ],
    "token_count":13,
    "processed_text":"wow lot interest stuff twitter facebook got ta work dull day yaxley warm"
  },
  {
    "label":4,
    "text":"realli dont know",
    "cleaned_text":"realli dont know",
    "normalized_text":"realli dont know",
    "tokens":[
      "realli",
      "dont",
      "know"
    ],
    "token_count":3,
    "processed_text":"realli dont know"
  },
  {
    "label":4,
    "text":"lol he got mexican wear slouchi beani",
    "cleaned_text":"lol he got mexican wear slouchi beani",
    "normalized_text":"lol he got mexican wear slouchi beani",
    "tokens":[
      "lol",
      "got",
      "mexican",
      "wear",
      "slouchi",
      "beani"
    ],
    "token_count":6,
    "processed_text":"lol got mexican wear slouchi beani"
  },
  {
    "label":4,
    "text":"id rather see year someon els seven day week",
    "cleaned_text":"id rather see year someon els seven day week",
    "normalized_text":"id rather see year someon els seven day week",
    "tokens":[
      "id",
      "rather",
      "see",
      "year",
      "someon",
      "el",
      "seven",
      "day",
      "week"
    ],
    "token_count":9,
    "processed_text":"id rather see year someon el seven day week"
  },
  {
    "label":4,
    "text":"sign tabl flea market bklyn advis thank much",
    "cleaned_text":"sign tabl flea market bklyn advis thank much",
    "normalized_text":"sign tabl flea market bklyn advis thank much",
    "tokens":[
      "sign",
      "tabl",
      "flea",
      "market",
      "bklyn",
      "advi",
      "thank",
      "much"
    ],
    "token_count":8,
    "processed_text":"sign tabl flea market bklyn advi thank much"
  },
  {
    "label":4,
    "text":"watch season final offic made happi ahh love haha",
    "cleaned_text":"watch season final offic made happi ahh love haha",
    "normalized_text":"watch season final offic made happi ahh love haha",
    "tokens":[
      "watch",
      "season",
      "final",
      "offic",
      "made",
      "happi",
      "ahh",
      "love",
      "haha"
    ],
    "token_count":9,
    "processed_text":"watch season final offic made happi ahh love haha"
  },
  {
    "label":0,
    "text":"youv got exam result alreadi havent even finish exam",
    "cleaned_text":"youv got exam result alreadi havent even finish exam",
    "normalized_text":"youv got exam result alreadi havent even finish exam",
    "tokens":[
      "youv",
      "got",
      "exam",
      "result",
      "alreadi",
      "havent",
      "even",
      "finish",
      "exam"
    ],
    "token_count":9,
    "processed_text":"youv got exam result alreadi havent even finish exam"
  },
  {
    "label":0,
    "text":"dont feel like get readi work even go",
    "cleaned_text":"dont feel like get readi work even go",
    "normalized_text":"dont feel like get readi work even go",
    "tokens":[
      "dont",
      "feel",
      "like",
      "get",
      "readi",
      "work",
      "even",
      "go"
    ],
    "token_count":8,
    "processed_text":"dont feel like get readi work even go"
  },
  {
    "label":4,
    "text":"need work hubbi dont smoke enjoy cigar",
    "cleaned_text":"need work hubbi dont smoke enjoy cigar",
    "normalized_text":"need work hubbi dont smoke enjoy cigar",
    "tokens":[
      "need",
      "work",
      "hubbi",
      "dont",
      "smoke",
      "enjoy",
      "cigar"
    ],
    "token_count":7,
    "processed_text":"need work hubbi dont smoke enjoy cigar"
  },
  {
    "label":0,
    "text":"person know want drink st bday",
    "cleaned_text":"person know want drink st bday",
    "normalized_text":"person know want drink st bday",
    "tokens":[
      "person",
      "know",
      "want",
      "drink",
      "st",
      "bday"
    ],
    "token_count":6,
    "processed_text":"person know want drink st bday"
  },
  {
    "label":4,
    "text":"megan",
    "cleaned_text":"megan",
    "normalized_text":"megan",
    "tokens":[
      "megan"
    ],
    "token_count":1,
    "processed_text":"megan"
  },
  {
    "label":4,
    "text":"intent organ clean apart today support",
    "cleaned_text":"intent organ clean apart today support",
    "normalized_text":"intent organ clean apart today support",
    "tokens":[
      "intent",
      "organ",
      "clean",
      "apart",
      "today",
      "support"
    ],
    "token_count":6,
    "processed_text":"intent organ clean apart today support"
  },
  {
    "label":0,
    "text":"cours iphon tether trick doesnt work st gen phone hope copi netshar still work",
    "cleaned_text":"cours iphon tether trick doesnt work st gen phone hope copi netshar still work",
    "normalized_text":"cours iphon tether trick doesnt work st gen phone hope copi netshar still work",
    "tokens":[
      "cour",
      "iphon",
      "tether",
      "trick",
      "doesnt",
      "work",
      "st",
      "gen",
      "phone",
      "hope",
      "copi",
      "netshar",
      "still",
      "work"
    ],
    "token_count":14,
    "processed_text":"cour iphon tether trick doesnt work st gen phone hope copi netshar still work"
  },
  {
    "label":0,
    "text":"wish could go kol concert tonight charleston sc tonight",
    "cleaned_text":"wish could go kol concert tonight charleston sc tonight",
    "normalized_text":"wish could go kol concert tonight charleston sc tonight",
    "tokens":[
      "wish",
      "go",
      "kol",
      "concert",
      "tonight",
      "charleston",
      "sc",
      "tonight"
    ],
    "token_count":8,
    "processed_text":"wish go kol concert tonight charleston sc tonight"
  },
  {
    "label":4,
    "text":"go kilomet bike trip tomorrow morn excit ye scare death absolut",
    "cleaned_text":"go kilomet bike trip tomorrow morn excit ye scare death absolut",
    "normalized_text":"go kilomet bike trip tomorrow morn excit ye scare death absolut",
    "tokens":[
      "go",
      "kilomet",
      "bike",
      "trip",
      "tomorrow",
      "morn",
      "excit",
      "ye",
      "scare",
      "death",
      "absolut"
    ],
    "token_count":11,
    "processed_text":"go kilomet bike trip tomorrow morn excit ye scare death absolut"
  },
  {
    "label":4,
    "text":"aww miss earthquak hope everyon okay",
    "cleaned_text":"aww miss earthquak hope everyon okay",
    "normalized_text":"aww miss earthquak hope everyon okay",
    "tokens":[
      "aww",
      "miss",
      "earthquak",
      "hope",
      "everyon",
      "okay"
    ],
    "token_count":6,
    "processed_text":"aww miss earthquak hope everyon okay"
  },
  {
    "label":0,
    "text":"base true stori anyway guess wiki would found",
    "cleaned_text":"base true stori anyway guess wiki would found",
    "normalized_text":"base true stori anyway guess wiki would found",
    "tokens":[
      "base",
      "true",
      "stori",
      "anyway",
      "guess",
      "wiki",
      "found"
    ],
    "token_count":7,
    "processed_text":"base true stori anyway guess wiki found"
  },
  {
    "label":0,
    "text":"happen mmg ecah sedeylah everyon want leav alon got noth except internet life",
    "cleaned_text":"happen mmg ecah sedeylah everyon want leav alon got noth except internet life",
    "normalized_text":"happen mmg ecah sedeylah everyon want leav alon got noth except internet life",
    "tokens":[
      "happen",
      "mmg",
      "ecah",
      "sedeylah",
      "everyon",
      "want",
      "leav",
      "alon",
      "got",
      "noth",
      "except",
      "internet",
      "life"
    ],
    "token_count":13,
    "processed_text":"happen mmg ecah sedeylah everyon want leav alon got noth except internet life"
  },
  {
    "label":0,
    "text":"wanna cri",
    "cleaned_text":"wanna cri",
    "normalized_text":"wanna cri",
    "tokens":[
      "wan",
      "na",
      "cri"
    ],
    "token_count":3,
    "processed_text":"wan na cri"
  },
  {
    "label":0,
    "text":"home carniv dont feel good spinni ride death",
    "cleaned_text":"home carniv dont feel good spinni ride death",
    "normalized_text":"home carniv dont feel good spinni ride death",
    "tokens":[
      "home",
      "carniv",
      "dont",
      "feel",
      "good",
      "spinni",
      "ride",
      "death"
    ],
    "token_count":8,
    "processed_text":"home carniv dont feel good spinni ride death"
  },
  {
    "label":0,
    "text":"today sad day ok idk feel sad noth miss u nati",
    "cleaned_text":"today sad day ok idk feel sad noth miss u nati",
    "normalized_text":"today sad day ok idk feel sad noth miss u nati",
    "tokens":[
      "today",
      "sad",
      "day",
      "ok",
      "idk",
      "feel",
      "sad",
      "noth",
      "miss",
      "nati"
    ],
    "token_count":10,
    "processed_text":"today sad day ok idk feel sad noth miss nati"
  },
  {
    "label":4,
    "text":"yep ok thank",
    "cleaned_text":"yep ok thank",
    "normalized_text":"yep ok thank",
    "tokens":[
      "yep",
      "ok",
      "thank"
    ],
    "token_count":3,
    "processed_text":"yep ok thank"
  },
  {
    "label":4,
    "text":"lol wonder hed trade someth worri tho weve morn",
    "cleaned_text":"lol wonder hed trade someth worri tho weve morn",
    "normalized_text":"lol wonder hed trade someth worri tho weve morn",
    "tokens":[
      "lol",
      "wonder",
      "hed",
      "trade",
      "someth",
      "worri",
      "tho",
      "weve",
      "morn"
    ],
    "token_count":9,
    "processed_text":"lol wonder hed trade someth worri tho weve morn"
  },
  {
    "label":0,
    "text":"picnic abu ova hadd fun today day",
    "cleaned_text":"picnic abu ova hadd fun today day",
    "normalized_text":"picnic abu ova hadd fun today day",
    "tokens":[
      "picnic",
      "abu",
      "ova",
      "hadd",
      "fun",
      "today",
      "day"
    ],
    "token_count":7,
    "processed_text":"picnic abu ova hadd fun today day"
  },
  {
    "label":4,
    "text":"back l v e ha",
    "cleaned_text":"back l v e ha",
    "normalized_text":"back l v e ha",
    "tokens":[
      "back",
      "ha"
    ],
    "token_count":2,
    "processed_text":"back ha"
  },
  {
    "label":0,
    "text":"hot weather mean less time outsid",
    "cleaned_text":"hot weather mean less time outsid",
    "normalized_text":"hot weather mean less time outsid",
    "tokens":[
      "hot",
      "weather",
      "mean",
      "less",
      "time",
      "outsid"
    ],
    "token_count":6,
    "processed_text":"hot weather mean less time outsid"
  },
  {
    "label":0,
    "text":"awak get readi airport earli morn thunderstorm outsid",
    "cleaned_text":"awak get readi airport earli morn thunderstorm outsid",
    "normalized_text":"awak get readi airport earli morn thunderstorm outsid",
    "tokens":[
      "awak",
      "get",
      "readi",
      "airport",
      "earli",
      "morn",
      "thunderstorm",
      "outsid"
    ],
    "token_count":8,
    "processed_text":"awak get readi airport earli morn thunderstorm outsid"
  },
  {
    "label":0,
    "text":"alright hon im around wanna talk sometim",
    "cleaned_text":"alright hon im around wanna talk sometim",
    "normalized_text":"alright hon im around wanna talk sometim",
    "tokens":[
      "alright",
      "hon",
      "im",
      "around",
      "wan",
      "na",
      "talk",
      "sometim"
    ],
    "token_count":8,
    "processed_text":"alright hon im around wan na talk sometim"
  },
  {
    "label":0,
    "text":"didnt get pictur got pictur babelnot good one kuyt co camera die thank",
    "cleaned_text":"didnt get pictur got pictur babelnot good one kuyt co camera die thank",
    "normalized_text":"didnt get pictur got pictur babelnot good one kuyt co camera die thank",
    "tokens":[
      "didnt",
      "get",
      "pictur",
      "got",
      "pictur",
      "babelnot",
      "good",
      "one",
      "kuyt",
      "co",
      "camera",
      "die",
      "thank"
    ],
    "token_count":13,
    "processed_text":"didnt get pictur got pictur babelnot good one kuyt co camera die thank"
  },
  {
    "label":4,
    "text":"oooh beauti like u",
    "cleaned_text":"oooh beauti like u",
    "normalized_text":"oooh beauti like u",
    "tokens":[
      "oooh",
      "beauti",
      "like"
    ],
    "token_count":3,
    "processed_text":"oooh beauti like"
  },
  {
    "label":4,
    "text":"oh touch hand hahahah funni ass guy damn thing",
    "cleaned_text":"oh touch hand hahahah funni ass guy damn thing",
    "normalized_text":"oh touch hand hahahah funni ass guy damn thing",
    "tokens":[
      "oh",
      "touch",
      "hand",
      "hahahah",
      "funni",
      "ass",
      "guy",
      "damn",
      "thing"
    ],
    "token_count":9,
    "processed_text":"oh touch hand hahahah funni ass guy damn thing"
  },
  {
    "label":4,
    "text":"lol definit understand",
    "cleaned_text":"lol definit understand",
    "normalized_text":"lol definit understand",
    "tokens":[
      "lol",
      "definit",
      "understand"
    ],
    "token_count":3,
    "processed_text":"lol definit understand"
  },
  {
    "label":0,
    "text":"new teca driver licens design ugli",
    "cleaned_text":"new teca driver licens design ugli",
    "normalized_text":"new teca driver licens design ugli",
    "tokens":[
      "new",
      "teca",
      "driver",
      "licen",
      "design",
      "ugli"
    ],
    "token_count":6,
    "processed_text":"new teca driver licen design ugli"
  },
  {
    "label":0,
    "text":"kfog kaboom im sad miss",
    "cleaned_text":"kfog kaboom im sad miss",
    "normalized_text":"kfog kaboom im sad miss",
    "tokens":[
      "kfog",
      "kaboom",
      "im",
      "sad",
      "miss"
    ],
    "token_count":5,
    "processed_text":"kfog kaboom im sad miss"
  },
  {
    "label":4,
    "text":"hey mitch ur cd awesom worth wait keep u rock",
    "cleaned_text":"hey mitch ur cd awesom worth wait keep u rock",
    "normalized_text":"hey mitch ur cd awesom worth wait keep u rock",
    "tokens":[
      "hey",
      "mitch",
      "ur",
      "cd",
      "awesom",
      "worth",
      "wait",
      "keep",
      "rock"
    ],
    "token_count":9,
    "processed_text":"hey mitch ur cd awesom worth wait keep rock"
  },
  {
    "label":0,
    "text":"less half way done paper",
    "cleaned_text":"less half way done paper",
    "normalized_text":"less half way done paper",
    "tokens":[
      "less",
      "half",
      "way",
      "done",
      "paper"
    ],
    "token_count":5,
    "processed_text":"less half way done paper"
  },
  {
    "label":0,
    "text":"cri dog fleet alreadi pass away best dog could die sad",
    "cleaned_text":"cri dog fleet alreadi pass away best dog could die sad",
    "normalized_text":"cri dog fleet alreadi pass away best dog could die sad",
    "tokens":[
      "cri",
      "dog",
      "fleet",
      "alreadi",
      "pass",
      "away",
      "best",
      "dog",
      "die",
      "sad"
    ],
    "token_count":10,
    "processed_text":"cri dog fleet alreadi pass away best dog die sad"
  },
  {
    "label":0,
    "text":"im one dude im scare sleep crazi dream bahah",
    "cleaned_text":"im one dude im scare sleep crazi dream bahah",
    "normalized_text":"im one dude im scare sleep crazi dream bahah",
    "tokens":[
      "im",
      "one",
      "dude",
      "im",
      "scare",
      "sleep",
      "crazi",
      "dream",
      "bahah"
    ],
    "token_count":9,
    "processed_text":"im one dude im scare sleep crazi dream bahah"
  },
  {
    "label":0,
    "text":"updat free fall iphon home button scarili stiff",
    "cleaned_text":"updat free fall iphon home button scarili stiff",
    "normalized_text":"updat free fall iphon home button scarili stiff",
    "tokens":[
      "updat",
      "free",
      "fall",
      "iphon",
      "home",
      "button",
      "scarili",
      "stiff"
    ],
    "token_count":8,
    "processed_text":"updat free fall iphon home button scarili stiff"
  },
  {
    "label":4,
    "text":"decid wolf futur star trek logo game would much cooler chewi star war",
    "cleaned_text":"decid wolf futur star trek logo game would much cooler chewi star war",
    "normalized_text":"decid wolf futur star trek logo game would much cooler chewi star war",
    "tokens":[
      "decid",
      "wolf",
      "futur",
      "star",
      "trek",
      "logo",
      "game",
      "much",
      "cooler",
      "chewi",
      "star",
      "war"
    ],
    "token_count":12,
    "processed_text":"decid wolf futur star trek logo game much cooler chewi star war"
  },
  {
    "label":4,
    "text":"good morn ahh feel good sleep go run beauti errand today",
    "cleaned_text":"good morn ahh feel good sleep go run beauti errand today",
    "normalized_text":"good morn ahh feel good sleep go run beauti errand today",
    "tokens":[
      "good",
      "morn",
      "ahh",
      "feel",
      "good",
      "sleep",
      "go",
      "run",
      "beauti",
      "errand",
      "today"
    ],
    "token_count":11,
    "processed_text":"good morn ahh feel good sleep go run beauti errand today"
  },
  {
    "label":4,
    "text":"btw happi hoppusday everyon",
    "cleaned_text":"btw happi hoppusday everyon",
    "normalized_text":"btw happi hoppusday everyon",
    "tokens":[
      "btw",
      "happi",
      "hoppusday",
      "everyon"
    ],
    "token_count":4,
    "processed_text":"btw happi hoppusday everyon"
  },
  {
    "label":0,
    "text":"listen dont know music well",
    "cleaned_text":"listen dont know music well",
    "normalized_text":"listen dont know music well",
    "tokens":[
      "listen",
      "dont",
      "know",
      "music",
      "well"
    ],
    "token_count":5,
    "processed_text":"listen dont know music well"
  },
  {
    "label":0,
    "text":"rob us cash money felt like insid job got went bag took dough dough ho",
    "cleaned_text":"rob us cash money felt like insid job got went bag took dough dough ho",
    "normalized_text":"rob us cash money felt like insid job got went bag took dough dough ho",
    "tokens":[
      "rob",
      "us",
      "cash",
      "money",
      "felt",
      "like",
      "insid",
      "job",
      "got",
      "went",
      "bag",
      "took",
      "dough",
      "dough",
      "ho"
    ],
    "token_count":15,
    "processed_text":"rob us cash money felt like insid job got went bag took dough dough ho"
  },
  {
    "label":4,
    "text":"refund cours forgt cancel within week evil",
    "cleaned_text":"refund cours forgt cancel within week evil",
    "normalized_text":"refund cours forgt cancel within week evil",
    "tokens":[
      "refund",
      "cour",
      "forgt",
      "cancel",
      "within",
      "week",
      "evil"
    ],
    "token_count":7,
    "processed_text":"refund cour forgt cancel within week evil"
  },
  {
    "label":4,
    "text":"yer print consent form ili",
    "cleaned_text":"yer print consent form ili",
    "normalized_text":"yer print consent form ili",
    "tokens":[
      "yer",
      "print",
      "consent",
      "form",
      "ili"
    ],
    "token_count":5,
    "processed_text":"yer print consent form ili"
  },
  {
    "label":4,
    "text":"work see later twitter",
    "cleaned_text":"work see later twitter",
    "normalized_text":"work see later twitter",
    "tokens":[
      "work",
      "see",
      "later",
      "twitter"
    ],
    "token_count":4,
    "processed_text":"work see later twitter"
  },
  {
    "label":0,
    "text":"heyyy what dont tweet anymor",
    "cleaned_text":"heyyy what dont tweet anymor",
    "normalized_text":"heyyy what dont tweet anymor",
    "tokens":[
      "heyyy",
      "dont",
      "tweet",
      "anymor"
    ],
    "token_count":4,
    "processed_text":"heyyy dont tweet anymor"
  },
  {
    "label":0,
    "text":"school work",
    "cleaned_text":"school work",
    "normalized_text":"school work",
    "tokens":[
      "school",
      "work"
    ],
    "token_count":2,
    "processed_text":"school work"
  },
  {
    "label":4,
    "text":"know right lol",
    "cleaned_text":"know right lol",
    "normalized_text":"know right lol",
    "tokens":[
      "know",
      "right",
      "lol"
    ],
    "token_count":3,
    "processed_text":"know right lol"
  },
  {
    "label":0,
    "text":"well got sent front room diseas oh pleasee dont treat like",
    "cleaned_text":"well got sent front room diseas oh pleasee dont treat like",
    "normalized_text":"well got sent front room diseas oh pleasee dont treat like",
    "tokens":[
      "well",
      "got",
      "sent",
      "front",
      "room",
      "disea",
      "oh",
      "please",
      "dont",
      "treat",
      "like"
    ],
    "token_count":11,
    "processed_text":"well got sent front room disea oh please dont treat like"
  },
  {
    "label":4,
    "text":"cab okay cool thank never even heard",
    "cleaned_text":"cab okay cool thank never even heard",
    "normalized_text":"cab okay cool thank never even heard",
    "tokens":[
      "cab",
      "okay",
      "cool",
      "thank",
      "never",
      "even",
      "heard"
    ],
    "token_count":7,
    "processed_text":"cab okay cool thank never even heard"
  },
  {
    "label":0,
    "text":"time go bed arg morrow gotta go eng cours amp ml class cool go school sta",
    "cleaned_text":"time go bed arg morrow gotta go eng cours amp ml class cool go school sta",
    "normalized_text":"time go bed arg morrow gotta go eng cours amp ml class cool go school sta",
    "tokens":[
      "time",
      "go",
      "bed",
      "arg",
      "morrow",
      "got",
      "ta",
      "go",
      "eng",
      "cour",
      "amp",
      "ml",
      "class",
      "cool",
      "go",
      "school",
      "sta"
    ],
    "token_count":17,
    "processed_text":"time go bed arg morrow got ta go eng cour amp ml class cool go school sta"
  },
  {
    "label":0,
    "text":"problem man im member taz although bit sad af got theyr good",
    "cleaned_text":"problem man im member taz although bit sad af got theyr good",
    "normalized_text":"problem man im member taz although bit sad af got theyr good",
    "tokens":[
      "problem",
      "man",
      "im",
      "member",
      "taz",
      "although",
      "bit",
      "sad",
      "af",
      "got",
      "theyr",
      "good"
    ],
    "token_count":12,
    "processed_text":"problem man im member taz although bit sad af got theyr good"
  },
  {
    "label":4,
    "text":"cool clip",
    "cleaned_text":"cool clip",
    "normalized_text":"cool clip",
    "tokens":[
      "cool",
      "clip"
    ],
    "token_count":2,
    "processed_text":"cool clip"
  },
  {
    "label":0,
    "text":"video ragingcrazyladi peril date dinosaur",
    "cleaned_text":"video ragingcrazyladi peril date dinosaur",
    "normalized_text":"video ragingcrazyladi peril date dinosaur",
    "tokens":[
      "video",
      "ragingcrazyladi",
      "peril",
      "date",
      "dinosaur"
    ],
    "token_count":5,
    "processed_text":"video ragingcrazyladi peril date dinosaur"
  },
  {
    "label":0,
    "text":"doesnt look like im gonna meet gotta head back guelph shortli",
    "cleaned_text":"doesnt look like im gonna meet gotta head back guelph shortli",
    "normalized_text":"doesnt look like im gonna meet gotta head back guelph shortli",
    "tokens":[
      "doesnt",
      "look",
      "like",
      "im",
      "gon",
      "na",
      "meet",
      "got",
      "ta",
      "head",
      "back",
      "guelph",
      "shortli"
    ],
    "token_count":13,
    "processed_text":"doesnt look like im gon na meet got ta head back guelph shortli"
  },
  {
    "label":4,
    "text":"phew much ohh follow thank anyway im fun tweet tweet buddi",
    "cleaned_text":"phew much ohh follow thank anyway im fun tweet tweet buddi",
    "normalized_text":"phew much ohh follow thank anyway im fun tweet tweet buddi",
    "tokens":[
      "phew",
      "much",
      "ohh",
      "follow",
      "thank",
      "anyway",
      "im",
      "fun",
      "tweet",
      "tweet",
      "buddi"
    ],
    "token_count":11,
    "processed_text":"phew much ohh follow thank anyway im fun tweet tweet buddi"
  },
  {
    "label":0,
    "text":"shiiit fi ida known would come back chat didnt sleep wink till bastard headach",
    "cleaned_text":"shiiit fi ida known would come back chat didnt sleep wink till bastard headach",
    "normalized_text":"shiiit fi ida known would come back chat didnt sleep wink till bastard headach",
    "tokens":[
      "shiiit",
      "fi",
      "ida",
      "known",
      "come",
      "back",
      "chat",
      "didnt",
      "sleep",
      "wink",
      "till",
      "bastard",
      "headach"
    ],
    "token_count":13,
    "processed_text":"shiiit fi ida known come back chat didnt sleep wink till bastard headach"
  },
  {
    "label":0,
    "text":"aint theif honest",
    "cleaned_text":"aint theif honest",
    "normalized_text":"aint theif honest",
    "tokens":[
      "aint",
      "theif",
      "honest"
    ],
    "token_count":3,
    "processed_text":"aint theif honest"
  },
  {
    "label":0,
    "text":"wowi spelt cigarett wrong christ ive alway thought gs spell skill usual decent",
    "cleaned_text":"wowi spelt cigarett wrong christ ive alway thought gs spell skill usual decent",
    "normalized_text":"wowi spelt cigarett wrong christ ive alway thought gs spell skill usual decent",
    "tokens":[
      "wowi",
      "spelt",
      "cigarett",
      "wrong",
      "christ",
      "ive",
      "alway",
      "thought",
      "gs",
      "spell",
      "skill",
      "usual",
      "decent"
    ],
    "token_count":13,
    "processed_text":"wowi spelt cigarett wrong christ ive alway thought gs spell skill usual decent"
  },
  {
    "label":4,
    "text":"birthday happi birthday ya",
    "cleaned_text":"birthday happi birthday ya",
    "normalized_text":"birthday happi birthday ya",
    "tokens":[
      "birthday",
      "happi",
      "birthday",
      "ya"
    ],
    "token_count":4,
    "processed_text":"birthday happi birthday ya"
  },
  {
    "label":0,
    "text":"realli sad last night nin show here hope tonight better last night good show got song",
    "cleaned_text":"realli sad last night nin show here hope tonight better last night good show got song",
    "normalized_text":"realli sad last night nin show here hope tonight better last night good show got song",
    "tokens":[
      "realli",
      "sad",
      "last",
      "night",
      "nin",
      "show",
      "hope",
      "tonight",
      "better",
      "last",
      "night",
      "good",
      "show",
      "got",
      "song"
    ],
    "token_count":15,
    "processed_text":"realli sad last night nin show hope tonight better last night good show got song"
  },
  {
    "label":4,
    "text":"new star trek movi pretti good pleasantli surpris entertain",
    "cleaned_text":"new star trek movi pretti good pleasantli surpris entertain",
    "normalized_text":"new star trek movi pretti good pleasantli surpris entertain",
    "tokens":[
      "new",
      "star",
      "trek",
      "movi",
      "pretti",
      "good",
      "pleasantli",
      "surpri",
      "entertain"
    ],
    "token_count":9,
    "processed_text":"new star trek movi pretti good pleasantli surpri entertain"
  },
  {
    "label":4,
    "text":"go bed girl work p tomorrow think breakfast johnni side im crave biscuit gravi",
    "cleaned_text":"go bed girl work p tomorrow think breakfast johnni side im crave biscuit gravi",
    "normalized_text":"go bed girl work p tomorrow think breakfast johnni side im crave biscuit gravi",
    "tokens":[
      "go",
      "bed",
      "girl",
      "work",
      "tomorrow",
      "think",
      "breakfast",
      "johnni",
      "side",
      "im",
      "crave",
      "biscuit",
      "gravi"
    ],
    "token_count":13,
    "processed_text":"go bed girl work tomorrow think breakfast johnni side im crave biscuit gravi"
  },
  {
    "label":4,
    "text":"twitter hot right im guest speak come along im",
    "cleaned_text":"twitter hot right im guest speak come along im",
    "normalized_text":"twitter hot right im guest speak come along im",
    "tokens":[
      "twitter",
      "hot",
      "right",
      "im",
      "guest",
      "speak",
      "come",
      "along",
      "im"
    ],
    "token_count":9,
    "processed_text":"twitter hot right im guest speak come along im"
  },
  {
    "label":4,
    "text":"wow oh yeh logitech rais anu scroll capabilitiesweapon bedroom",
    "cleaned_text":"wow oh yeh logitech rais anu scroll capabilitiesweapon bedroom",
    "normalized_text":"wow oh yeh logitech rais anu scroll capabilitiesweapon bedroom",
    "tokens":[
      "wow",
      "oh",
      "yeh",
      "logitech",
      "rai",
      "anu",
      "scroll",
      "bedroom"
    ],
    "token_count":8,
    "processed_text":"wow oh yeh logitech rai anu scroll bedroom"
  },
  {
    "label":4,
    "text":"tomeatbrigadeiro cook want haha love",
    "cleaned_text":"tomeatbrigadeiro cook want haha love",
    "normalized_text":"tomeatbrigadeiro cook want haha love",
    "tokens":[
      "cook",
      "want",
      "haha",
      "love"
    ],
    "token_count":4,
    "processed_text":"cook want haha love"
  },
  {
    "label":0,
    "text":"last time call deliveri ask upsiiz get glass didnt give",
    "cleaned_text":"last time call deliveri ask upsiiz get glass didnt give",
    "normalized_text":"last time call deliveri ask upsiiz get glass didnt give",
    "tokens":[
      "last",
      "time",
      "call",
      "deliveri",
      "ask",
      "upsiiz",
      "get",
      "glass",
      "didnt",
      "give"
    ],
    "token_count":10,
    "processed_text":"last time call deliveri ask upsiiz get glass didnt give"
  },
  {
    "label":0,
    "text":"im tireee im exit",
    "cleaned_text":"im tireee im exit",
    "normalized_text":"im tireee im exit",
    "tokens":[
      "im",
      "tiree",
      "im",
      "exit"
    ],
    "token_count":4,
    "processed_text":"im tiree im exit"
  },
  {
    "label":4,
    "text":"knit watch across univers",
    "cleaned_text":"knit watch across univers",
    "normalized_text":"knit watch across univers",
    "tokens":[
      "knit",
      "watch",
      "across",
      "univ"
    ],
    "token_count":4,
    "processed_text":"knit watch across univ"
  },
  {
    "label":0,
    "text":"morn yall expect good day listen quotrunquot leona lewi",
    "cleaned_text":"morn yall expect good day listen quotrunquot leona lewi",
    "normalized_text":"morn yall expect good day listen quotrunquot leona lewi",
    "tokens":[
      "morn",
      "yall",
      "expect",
      "good",
      "day",
      "listen",
      "quotrunquot",
      "leona",
      "lewi"
    ],
    "token_count":9,
    "processed_text":"morn yall expect good day listen quotrunquot leona lewi"
  },
  {
    "label":4,
    "text":"cn proudli say dint hit last nite slap cu grab arm amp tri pull outta bf car that",
    "cleaned_text":"cn proudli say dint hit last nite slap cu grab arm amp tri pull outta bf car that",
    "normalized_text":"cn proudli say dint hit last nite slap cu grab arm amp tri pull outta bf car that",
    "tokens":[
      "cn",
      "proudli",
      "say",
      "dint",
      "hit",
      "last",
      "nite",
      "slap",
      "cu",
      "grab",
      "arm",
      "amp",
      "tri",
      "pull",
      "outta",
      "bf",
      "car"
    ],
    "token_count":17,
    "processed_text":"cn proudli say dint hit last nite slap cu grab arm amp tri pull outta bf car"
  },
  {
    "label":4,
    "text":"tri play pov guitar want piano chord though",
    "cleaned_text":"tri play pov guitar want piano chord though",
    "normalized_text":"tri play pov guitar want piano chord though",
    "tokens":[
      "tri",
      "play",
      "pov",
      "guitar",
      "want",
      "piano",
      "chord",
      "though"
    ],
    "token_count":8,
    "processed_text":"tri play pov guitar want piano chord though"
  },
  {
    "label":0,
    "text":"practic broke thumb cousin parti",
    "cleaned_text":"practic broke thumb cousin parti",
    "normalized_text":"practic broke thumb cousin parti",
    "tokens":[
      "practic",
      "broke",
      "thumb",
      "cousin",
      "parti"
    ],
    "token_count":5,
    "processed_text":"practic broke thumb cousin parti"
  },
  {
    "label":0,
    "text":"oknighti night peepsschool tomorrowand reason got slap face fat case homesick blue",
    "cleaned_text":"oknighti night peepsschool tomorrowand reason got slap face fat case homesick blue",
    "normalized_text":"oknighti night peepsschool tomorrowand reason got slap face fat case homesick blue",
    "tokens":[
      "oknighti",
      "night",
      "peepsschool",
      "tomorrowand",
      "reason",
      "got",
      "slap",
      "face",
      "fat",
      "case",
      "homesick",
      "blue"
    ],
    "token_count":12,
    "processed_text":"oknighti night peepsschool tomorrowand reason got slap face fat case homesick blue"
  },
  {
    "label":0,
    "text":"oh realli storm version yet sadsadi cant wait till develop liabl paid product",
    "cleaned_text":"oh realli storm version yet sadsadi cant wait till develop liabl paid product",
    "normalized_text":"oh realli storm version yet sadsadi cant wait till develop liabl paid product",
    "tokens":[
      "oh",
      "realli",
      "storm",
      "version",
      "yet",
      "sadsadi",
      "cant",
      "wait",
      "till",
      "develop",
      "liabl",
      "paid",
      "product"
    ],
    "token_count":13,
    "processed_text":"oh realli storm version yet sadsadi cant wait till develop liabl paid product"
  },
  {
    "label":0,
    "text":"hour histori exam tomorrow dont feel readi",
    "cleaned_text":"hour histori exam tomorrow dont feel readi",
    "normalized_text":"hour histori exam tomorrow dont feel readi",
    "tokens":[
      "hour",
      "histori",
      "exam",
      "tomorrow",
      "dont",
      "feel",
      "readi"
    ],
    "token_count":7,
    "processed_text":"hour histori exam tomorrow dont feel readi"
  },
  {
    "label":4,
    "text":"way back big ranger game tonight irv tx",
    "cleaned_text":"way back big ranger game tonight irv tx",
    "normalized_text":"way back big ranger game tonight irv tx",
    "tokens":[
      "way",
      "back",
      "big",
      "ranger",
      "game",
      "tonight",
      "irv",
      "tx"
    ],
    "token_count":8,
    "processed_text":"way back big ranger game tonight irv tx"
  },
  {
    "label":0,
    "text":"troubl record first track mixtap keep stutter mix word",
    "cleaned_text":"troubl record first track mixtap keep stutter mix word",
    "normalized_text":"troubl record first track mixtap keep stutter mix word",
    "tokens":[
      "troubl",
      "record",
      "first",
      "track",
      "mixtap",
      "keep",
      "stutter",
      "mix",
      "word"
    ],
    "token_count":9,
    "processed_text":"troubl record first track mixtap keep stutter mix word"
  },
  {
    "label":4,
    "text":"that favorit bullet song",
    "cleaned_text":"that favorit bullet song",
    "normalized_text":"that favorit bullet song",
    "tokens":[
      "favorit",
      "bullet",
      "song"
    ],
    "token_count":3,
    "processed_text":"favorit bullet song"
  },
  {
    "label":4,
    "text":"watch mean girl haha love movi",
    "cleaned_text":"watch mean girl haha love movi",
    "normalized_text":"watch mean girl haha love movi",
    "tokens":[
      "watch",
      "mean",
      "girl",
      "haha",
      "love",
      "movi"
    ],
    "token_count":6,
    "processed_text":"watch mean girl haha love movi"
  },
  {
    "label":4,
    "text":"hi good luck revis",
    "cleaned_text":"hi good luck revis",
    "normalized_text":"hi good luck revis",
    "tokens":[
      "hi",
      "good",
      "luck",
      "revi"
    ],
    "token_count":4,
    "processed_text":"hi good luck revi"
  },
  {
    "label":0,
    "text":"wed fabul yesterday morn marlin went fish didnt catch anyth",
    "cleaned_text":"wed fabul yesterday morn marlin went fish didnt catch anyth",
    "normalized_text":"wed fabul yesterday morn marlin went fish didnt catch anyth",
    "tokens":[
      "wed",
      "fabul",
      "yesterday",
      "morn",
      "marlin",
      "went",
      "fish",
      "didnt",
      "catch",
      "anyth"
    ],
    "token_count":10,
    "processed_text":"wed fabul yesterday morn marlin went fish didnt catch anyth"
  },
  {
    "label":4,
    "text":"go siz draw work project mention hope inspir last",
    "cleaned_text":"go siz draw work project mention hope inspir last",
    "normalized_text":"go siz draw work project mention hope inspir last",
    "tokens":[
      "go",
      "siz",
      "draw",
      "work",
      "project",
      "mention",
      "hope",
      "inspir",
      "last"
    ],
    "token_count":9,
    "processed_text":"go siz draw work project mention hope inspir last"
  },
  {
    "label":4,
    "text":"hello know name",
    "cleaned_text":"hello know name",
    "normalized_text":"hello know name",
    "tokens":[
      "hello",
      "know",
      "name"
    ],
    "token_count":3,
    "processed_text":"hello know name"
  },
  {
    "label":4,
    "text":"oh man suck say xx",
    "cleaned_text":"oh man suck say xx",
    "normalized_text":"oh man suck say xx",
    "tokens":[
      "oh",
      "man",
      "suck",
      "say",
      "xx"
    ],
    "token_count":5,
    "processed_text":"oh man suck say xx"
  },
  {
    "label":0,
    "text":"didnt get pen licens excus move around chang school qld vic back",
    "cleaned_text":"didnt get pen licens excus move around chang school qld vic back",
    "normalized_text":"didnt get pen licens excus move around chang school qld vic back",
    "tokens":[
      "didnt",
      "get",
      "pen",
      "licen",
      "excu",
      "move",
      "around",
      "chang",
      "school",
      "qld",
      "vic",
      "back"
    ],
    "token_count":12,
    "processed_text":"didnt get pen licen excu move around chang school qld vic back"
  },
  {
    "label":0,
    "text":"yup anoth nice day look like rain way",
    "cleaned_text":"yup anoth nice day look like rain way",
    "normalized_text":"yup anoth nice day look like rain way",
    "tokens":[
      "yup",
      "anoth",
      "nice",
      "day",
      "look",
      "like",
      "rain",
      "way"
    ],
    "token_count":8,
    "processed_text":"yup anoth nice day look like rain way"
  },
  {
    "label":0,
    "text":"nod crazi week neeeeed new week monday give best humid kill",
    "cleaned_text":"nod crazi week neeeeed new week monday give best humid kill",
    "normalized_text":"nod crazi week neeeeed new week monday give best humid kill",
    "tokens":[
      "nod",
      "crazi",
      "week",
      "neeeeed",
      "new",
      "week",
      "monday",
      "give",
      "best",
      "humid",
      "kill"
    ],
    "token_count":11,
    "processed_text":"nod crazi week neeeeed new week monday give best humid kill"
  },
  {
    "label":0,
    "text":"never respond happi hope fun tour",
    "cleaned_text":"never respond happi hope fun tour",
    "normalized_text":"never respond happi hope fun tour",
    "tokens":[
      "never",
      "respond",
      "happi",
      "hope",
      "fun",
      "tour"
    ],
    "token_count":6,
    "processed_text":"never respond happi hope fun tour"
  },
  {
    "label":4,
    "text":"doubt friend",
    "cleaned_text":"doubt friend",
    "normalized_text":"doubt friend",
    "tokens":[
      "doubt",
      "friend"
    ],
    "token_count":2,
    "processed_text":"doubt friend"
  },
  {
    "label":0,
    "text":"eat cold dinner",
    "cleaned_text":"eat cold dinner",
    "normalized_text":"eat cold dinner",
    "tokens":[
      "eat",
      "cold",
      "dinner"
    ],
    "token_count":3,
    "processed_text":"eat cold dinner"
  },
  {
    "label":4,
    "text":"whoa dont harsh mayb lol jk hope well babe besid tweet",
    "cleaned_text":"whoa dont harsh mayb lol jk hope well babe besid tweet",
    "normalized_text":"whoa dont harsh mayb lol jk hope well babe besid tweet",
    "tokens":[
      "whoa",
      "dont",
      "harsh",
      "mayb",
      "lol",
      "jk",
      "hope",
      "well",
      "babe",
      "besid",
      "tweet"
    ],
    "token_count":11,
    "processed_text":"whoa dont harsh mayb lol jk hope well babe besid tweet"
  },
  {
    "label":0,
    "text":"im gud exam tomorrow fair anyway wrote song today lol xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
    "cleaned_text":"im gud exam tomorrow fair anyway wrote song today lol xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
    "normalized_text":"im gud exam tomorrow fair anyway wrote song today lol xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
    "tokens":[
      "im",
      "gud",
      "exam",
      "tomorrow",
      "fair",
      "anyway",
      "wrote",
      "song",
      "today",
      "lol"
    ],
    "token_count":10,
    "processed_text":"im gud exam tomorrow fair anyway wrote song today lol"
  },
  {
    "label":0,
    "text":"pull muscl side yesterday realli hurt",
    "cleaned_text":"pull muscl side yesterday realli hurt",
    "normalized_text":"pull muscl side yesterday realli hurt",
    "tokens":[
      "pull",
      "muscl",
      "side",
      "yesterday",
      "realli",
      "hurt"
    ],
    "token_count":6,
    "processed_text":"pull muscl side yesterday realli hurt"
  },
  {
    "label":4,
    "text":"tell keivn mohaj say hi date like yr ago still hot",
    "cleaned_text":"tell keivn mohaj say hi date like yr ago still hot",
    "normalized_text":"tell keivn mohaj say hi date like yr ago still hot",
    "tokens":[
      "tell",
      "keivn",
      "mohaj",
      "say",
      "hi",
      "date",
      "like",
      "yr",
      "ago",
      "still",
      "hot"
    ],
    "token_count":11,
    "processed_text":"tell keivn mohaj say hi date like yr ago still hot"
  },
  {
    "label":0,
    "text":"listen new black eye pea album giant ball autotun meaningless lyric happen",
    "cleaned_text":"listen new black eye pea album giant ball autotun meaningless lyric happen",
    "normalized_text":"listen new black eye pea album giant ball autotun meaningless lyric happen",
    "tokens":[
      "listen",
      "new",
      "black",
      "eye",
      "pea",
      "album",
      "giant",
      "ball",
      "autotun",
      "meaningless",
      "lyric",
      "happen"
    ],
    "token_count":12,
    "processed_text":"listen new black eye pea album giant ball autotun meaningless lyric happen"
  },
  {
    "label":4,
    "text":"eat breakfast",
    "cleaned_text":"eat breakfast",
    "normalized_text":"eat breakfast",
    "tokens":[
      "eat",
      "breakfast"
    ],
    "token_count":2,
    "processed_text":"eat breakfast"
  },
  {
    "label":0,
    "text":"hm rewind life",
    "cleaned_text":"hm rewind life",
    "normalized_text":"hm rewind life",
    "tokens":[
      "hm",
      "rewind",
      "life"
    ],
    "token_count":3,
    "processed_text":"hm rewind life"
  },
  {
    "label":4,
    "text":"quit lot pple photograph open ceremoni",
    "cleaned_text":"quit lot pple photograph open ceremoni",
    "normalized_text":"quit lot pple photograph open ceremoni",
    "tokens":[
      "quit",
      "lot",
      "pple",
      "photograph",
      "open",
      "ceremoni"
    ],
    "token_count":6,
    "processed_text":"quit lot pple photograph open ceremoni"
  },
  {
    "label":0,
    "text":"watch video merlin panel london expo gut couldnt though",
    "cleaned_text":"watch video merlin panel london expo gut couldnt though",
    "normalized_text":"watch video merlin panel london expo gut couldnt though",
    "tokens":[
      "watch",
      "video",
      "merlin",
      "panel",
      "london",
      "expo",
      "gut",
      "couldnt",
      "though"
    ],
    "token_count":9,
    "processed_text":"watch video merlin panel london expo gut couldnt though"
  },
  {
    "label":4,
    "text":"fab time",
    "cleaned_text":"fab time",
    "normalized_text":"fab time",
    "tokens":[
      "fab",
      "time"
    ],
    "token_count":2,
    "processed_text":"fab time"
  },
  {
    "label":0,
    "text":"lol boiler itll melt lol",
    "cleaned_text":"lol boiler itll melt lol",
    "normalized_text":"lol boiler itll melt lol",
    "tokens":[
      "lol",
      "boiler",
      "itll",
      "melt",
      "lol"
    ],
    "token_count":5,
    "processed_text":"lol boiler itll melt lol"
  },
  {
    "label":0,
    "text":"sarah bday today work day",
    "cleaned_text":"sarah bday today work day",
    "normalized_text":"sarah bday today work day",
    "tokens":[
      "sarah",
      "bday",
      "today",
      "work",
      "day"
    ],
    "token_count":5,
    "processed_text":"sarah bday today work day"
  },
  {
    "label":0,
    "text":"probabl ive watch year figur would pull someth like",
    "cleaned_text":"probabl ive watch year figur would pull someth like",
    "normalized_text":"probabl ive watch year figur would pull someth like",
    "tokens":[
      "probabl",
      "ive",
      "watch",
      "year",
      "figur",
      "pull",
      "someth",
      "like"
    ],
    "token_count":8,
    "processed_text":"probabl ive watch year figur pull someth like"
  },
  {
    "label":4,
    "text":"work new editori",
    "cleaned_text":"work new editori",
    "normalized_text":"work new editori",
    "tokens":[
      "work",
      "new",
      "editori"
    ],
    "token_count":3,
    "processed_text":"work new editori"
  },
  {
    "label":4,
    "text":"go great rnr look forward hear",
    "cleaned_text":"go great rnr look forward hear",
    "normalized_text":"go great rnr look forward hear",
    "tokens":[
      "go",
      "great",
      "rnr",
      "look",
      "forward",
      "hear"
    ],
    "token_count":6,
    "processed_text":"go great rnr look forward hear"
  },
  {
    "label":0,
    "text":"basic got screw year life thought nut anxieti issu though",
    "cleaned_text":"basic got screw year life thought nut anxieti issu though",
    "normalized_text":"basic got screw year life thought nut anxieti issu though",
    "tokens":[
      "basic",
      "got",
      "screw",
      "year",
      "life",
      "thought",
      "nut",
      "anxieti",
      "issu",
      "though"
    ],
    "token_count":10,
    "processed_text":"basic got screw year life thought nut anxieti issu though"
  },
  {
    "label":4,
    "text":"darn im pari could cool meet irl your your ever stockholm howl",
    "cleaned_text":"darn im pari could cool meet irl your your ever stockholm howl",
    "normalized_text":"darn im pari could cool meet irl your your ever stockholm howl",
    "tokens":[
      "darn",
      "im",
      "pari",
      "cool",
      "meet",
      "irl",
      "ever",
      "stockholm",
      "howl"
    ],
    "token_count":9,
    "processed_text":"darn im pari cool meet irl ever stockholm howl"
  },
  {
    "label":4,
    "text":"ooh yay download",
    "cleaned_text":"ooh yay download",
    "normalized_text":"ooh yay download",
    "tokens":[
      "ooh",
      "yay",
      "download"
    ],
    "token_count":3,
    "processed_text":"ooh yay download"
  },
  {
    "label":0,
    "text":"tri phone",
    "cleaned_text":"tri phone",
    "normalized_text":"tri phone",
    "tokens":[
      "tri",
      "phone"
    ],
    "token_count":2,
    "processed_text":"tri phone"
  },
  {
    "label":4,
    "text":"french final done readi summer",
    "cleaned_text":"french final done readi summer",
    "normalized_text":"french final done readi summer",
    "tokens":[
      "french",
      "final",
      "done",
      "readi",
      "summer"
    ],
    "token_count":5,
    "processed_text":"french final done readi summer"
  },
  {
    "label":4,
    "text":"fi idiot stop read comment site might hit roug spoiler fool eurovis lijepa glazba",
    "cleaned_text":"fi idiot stop read comment site might hit roug spoiler fool eurovis lijepa glazba",
    "normalized_text":"fi idiot stop read comment site might hit roug spoiler fool eurovis lijepa glazba",
    "tokens":[
      "fi",
      "idiot",
      "stop",
      "read",
      "comment",
      "site",
      "hit",
      "roug",
      "spoiler",
      "fool",
      "eurovi",
      "lijepa",
      "glazba"
    ],
    "token_count":13,
    "processed_text":"fi idiot stop read comment site hit roug spoiler fool eurovi lijepa glazba"
  },
  {
    "label":4,
    "text":"got back walk brad thank slurpe",
    "cleaned_text":"got back walk brad thank slurpe",
    "normalized_text":"got back walk brad thank slurpe",
    "tokens":[
      "got",
      "back",
      "walk",
      "brad",
      "thank",
      "slurp"
    ],
    "token_count":6,
    "processed_text":"got back walk brad thank slurp"
  },
  {
    "label":4,
    "text":"challeng accept goe noth",
    "cleaned_text":"challeng accept goe noth",
    "normalized_text":"challeng accept goe noth",
    "tokens":[
      "challeng",
      "accept",
      "goe",
      "noth"
    ],
    "token_count":4,
    "processed_text":"challeng accept goe noth"
  },
  {
    "label":0,
    "text":"im site bed bore watch extrem makeov home additionit upset think im go cri xxxx",
    "cleaned_text":"im site bed bore watch extrem makeov home additionit upset think im go cri xxxx",
    "normalized_text":"im site bed bore watch extrem makeov home additionit upset think im go cri xxxx",
    "tokens":[
      "im",
      "site",
      "bed",
      "bore",
      "watch",
      "extrem",
      "makeov",
      "home",
      "additionit",
      "upset",
      "think",
      "im",
      "go",
      "cri",
      "xxxx"
    ],
    "token_count":15,
    "processed_text":"im site bed bore watch extrem makeov home additionit upset think im go cri xxxx"
  },
  {
    "label":4,
    "text":"ahahah got one toobut rare wear",
    "cleaned_text":"ahahah got one toobut rare wear",
    "normalized_text":"ahahah got one toobut rare wear",
    "tokens":[
      "ahahah",
      "got",
      "one",
      "toobut",
      "rare",
      "wear"
    ],
    "token_count":6,
    "processed_text":"ahahah got one toobut rare wear"
  },
  {
    "label":4,
    "text":"aah nothin beat blink morn neighbour love quoti wana fuck dog assquot full blast aah life",
    "cleaned_text":"aah nothin beat blink morn neighbour love quoti wana fuck dog assquot full blast aah life",
    "normalized_text":"aah nothin beat blink morn neighbour love quoti wana fuck dog assquot full blast aah life",
    "tokens":[
      "aah",
      "nothin",
      "beat",
      "blink",
      "morn",
      "neighbour",
      "love",
      "quoti",
      "wana",
      "fuck",
      "dog",
      "assquot",
      "full",
      "blast",
      "aah",
      "life"
    ],
    "token_count":16,
    "processed_text":"aah nothin beat blink morn neighbour love quoti wana fuck dog assquot full blast aah life"
  },
  {
    "label":4,
    "text":"quotonli learn power sincer amp selfless contribut experi life deepest joy true fulfillmentquot toni robbin",
    "cleaned_text":"quotonli learn power sincer amp selfless contribut experi life deepest joy true fulfillmentquot toni robbin",
    "normalized_text":"quotonli learn power sincer amp selfless contribut experi life deepest joy true fulfillmentquot toni robbin",
    "tokens":[
      "quotonli",
      "learn",
      "power",
      "sincer",
      "amp",
      "selfless",
      "contribut",
      "experi",
      "life",
      "deepest",
      "joy",
      "true",
      "fulfillmentquot",
      "toni",
      "robbin"
    ],
    "token_count":15,
    "processed_text":"quotonli learn power sincer amp selfless contribut experi life deepest joy true fulfillmentquot toni robbin"
  },
  {
    "label":4,
    "text":"im gonna ladi gaga halloween",
    "cleaned_text":"im gonna ladi gaga halloween",
    "normalized_text":"im gonna ladi gaga halloween",
    "tokens":[
      "im",
      "gon",
      "na",
      "ladi",
      "gaga",
      "halloween"
    ],
    "token_count":6,
    "processed_text":"im gon na ladi gaga halloween"
  },
  {
    "label":4,
    "text":"love mommi",
    "cleaned_text":"love mommi",
    "normalized_text":"love mommi",
    "tokens":[
      "love",
      "mommi"
    ],
    "token_count":2,
    "processed_text":"love mommi"
  },
  {
    "label":0,
    "text":"haaaat us whyyyyi whhhhi",
    "cleaned_text":"haaaat us whyyyyi whhhhi",
    "normalized_text":"haaaat us whyyyyi whhhhi",
    "tokens":[
      "haaaat",
      "us",
      "whyyyyi",
      "whhhhi"
    ],
    "token_count":4,
    "processed_text":"haaaat us whyyyyi whhhhi"
  },
  {
    "label":4,
    "text":"quiet amaz buyread book iphonebut batteri cant remov technologyfail",
    "cleaned_text":"quiet amaz buyread book iphonebut batteri cant remov technologyfail",
    "normalized_text":"quiet amaz buyread book iphonebut batteri cant remov technologyfail",
    "tokens":[
      "quiet",
      "amaz",
      "buyread",
      "book",
      "iphonebut",
      "batteri",
      "cant",
      "remov",
      "technologyfail"
    ],
    "token_count":9,
    "processed_text":"quiet amaz buyread book iphonebut batteri cant remov technologyfail"
  },
  {
    "label":0,
    "text":"got burnt like week bit ago back leg recov disappointedangryscar x",
    "cleaned_text":"got burnt like week bit ago back leg recov disappointedangryscar x",
    "normalized_text":"got burnt like week bit ago back leg recov disappointedangryscar x",
    "tokens":[
      "got",
      "burnt",
      "like",
      "week",
      "bit",
      "ago",
      "back",
      "leg",
      "recov"
    ],
    "token_count":9,
    "processed_text":"got burnt like week bit ago back leg recov"
  },
  {
    "label":0,
    "text":"cours work better work suppos",
    "cleaned_text":"cours work better work suppos",
    "normalized_text":"cours work better work suppos",
    "tokens":[
      "cour",
      "work",
      "better",
      "work",
      "suppo"
    ],
    "token_count":5,
    "processed_text":"cour work better work suppo"
  },
  {
    "label":0,
    "text":"wish sadli",
    "cleaned_text":"wish sadli",
    "normalized_text":"wish sadli",
    "tokens":[
      "wish",
      "sadli"
    ],
    "token_count":2,
    "processed_text":"wish sadli"
  },
  {
    "label":0,
    "text":"impress sick busi week sick panadol bed",
    "cleaned_text":"impress sick busi week sick panadol bed",
    "normalized_text":"impress sick busi week sick panadol bed",
    "tokens":[
      "impress",
      "sick",
      "busi",
      "week",
      "sick",
      "panadol",
      "bed"
    ],
    "token_count":7,
    "processed_text":"impress sick busi week sick panadol bed"
  },
  {
    "label":4,
    "text":"number pleas",
    "cleaned_text":"number pleas",
    "normalized_text":"number pleas",
    "tokens":[
      "number",
      "plea"
    ],
    "token_count":2,
    "processed_text":"number plea"
  },
  {
    "label":4,
    "text":"nope burger half bag dorito",
    "cleaned_text":"nope burger half bag dorito",
    "normalized_text":"nope burger half bag dorito",
    "tokens":[
      "nope",
      "burger",
      "half",
      "bag",
      "dorito"
    ],
    "token_count":5,
    "processed_text":"nope burger half bag dorito"
  },
  {
    "label":4,
    "text":"thank follow ill catch tomorrow goodnight tweetheart",
    "cleaned_text":"thank follow ill catch tomorrow goodnight tweetheart",
    "normalized_text":"thank follow ill catch tomorrow goodnight tweetheart",
    "tokens":[
      "thank",
      "follow",
      "ill",
      "catch",
      "tomorrow",
      "goodnight",
      "tweetheart"
    ],
    "token_count":7,
    "processed_text":"thank follow ill catch tomorrow goodnight tweetheart"
  },
  {
    "label":0,
    "text":"get internet fit tri hard forget person love deepli long peopl good",
    "cleaned_text":"get internet fit tri hard forget person love deepli long peopl good",
    "normalized_text":"get internet fit tri hard forget person love deepli long peopl good",
    "tokens":[
      "get",
      "internet",
      "fit",
      "tri",
      "hard",
      "forget",
      "person",
      "love",
      "deepli",
      "long",
      "peopl",
      "good"
    ],
    "token_count":12,
    "processed_text":"get internet fit tri hard forget person love deepli long peopl good"
  },
  {
    "label":0,
    "text":"sun come room bright mosi def wont go back sleep dont next minut tear",
    "cleaned_text":"sun come room bright mosi def wont go back sleep dont next minut tear",
    "normalized_text":"sun come room bright mosi def wont go back sleep dont next minut tear",
    "tokens":[
      "sun",
      "come",
      "room",
      "bright",
      "mosi",
      "def",
      "wont",
      "go",
      "back",
      "sleep",
      "dont",
      "next",
      "minut",
      "tear"
    ],
    "token_count":14,
    "processed_text":"sun come room bright mosi def wont go back sleep dont next minut tear"
  },
  {
    "label":0,
    "text":"hug yet manag intern text yet time bed lt",
    "cleaned_text":"hug yet manag intern text yet time bed lt",
    "normalized_text":"hug yet manag intern text yet time bed lt",
    "tokens":[
      "hug",
      "yet",
      "manag",
      "intern",
      "text",
      "yet",
      "time",
      "bed",
      "lt"
    ],
    "token_count":9,
    "processed_text":"hug yet manag intern text yet time bed lt"
  },
  {
    "label":4,
    "text":"still komprengan today ah let wait",
    "cleaned_text":"still komprengan today ah let wait",
    "normalized_text":"still komprengan today ah let wait",
    "tokens":[
      "still",
      "komprengan",
      "today",
      "ah",
      "let",
      "wait"
    ],
    "token_count":6,
    "processed_text":"still komprengan today ah let wait"
  },
  {
    "label":0,
    "text":"realiz pay bill least favourit pastim appar done damn",
    "cleaned_text":"realiz pay bill least favourit pastim appar done damn",
    "normalized_text":"realiz pay bill least favourit pastim appar done damn",
    "tokens":[
      "realiz",
      "pay",
      "bill",
      "least",
      "favourit",
      "pastim",
      "appar",
      "done",
      "damn"
    ],
    "token_count":9,
    "processed_text":"realiz pay bill least favourit pastim appar done damn"
  },
  {
    "label":4,
    "text":"yeah dad call taltal taken aol back ad k last name coincid tal talk",
    "cleaned_text":"yeah dad call taltal taken aol back ad k last name coincid tal talk",
    "normalized_text":"yeah dad call taltal taken aol back ad k last name coincid tal talk",
    "tokens":[
      "yeah",
      "dad",
      "call",
      "taltal",
      "taken",
      "aol",
      "back",
      "ad",
      "last",
      "name",
      "coincid",
      "tal",
      "talk"
    ],
    "token_count":13,
    "processed_text":"yeah dad call taltal taken aol back ad last name coincid tal talk"
  },
  {
    "label":4,
    "text":"stop fuss youv done best",
    "cleaned_text":"stop fuss youv done best",
    "normalized_text":"stop fuss youv done best",
    "tokens":[
      "stop",
      "fuss",
      "youv",
      "done",
      "best"
    ],
    "token_count":5,
    "processed_text":"stop fuss youv done best"
  },
  {
    "label":0,
    "text":"oath lol u figur there excit lol",
    "cleaned_text":"oath lol u figur there excit lol",
    "normalized_text":"oath lol u figur there excit lol",
    "tokens":[
      "oath",
      "lol",
      "figur",
      "excit",
      "lol"
    ],
    "token_count":5,
    "processed_text":"oath lol figur excit lol"
  },
  {
    "label":4,
    "text":"mollykan ah good time",
    "cleaned_text":"mollykan ah good time",
    "normalized_text":"mollykan ah good time",
    "tokens":[
      "mollykan",
      "ah",
      "good",
      "time"
    ],
    "token_count":4,
    "processed_text":"mollykan ah good time"
  },
  {
    "label":0,
    "text":"itchi eyesgrass cutohoh",
    "cleaned_text":"itchi eyesgrass cutohoh",
    "normalized_text":"itchi eyesgrass cutohoh",
    "tokens":[
      "itchi",
      "eyesgrass",
      "cutohoh"
    ],
    "token_count":3,
    "processed_text":"itchi eyesgrass cutohoh"
  },
  {
    "label":4,
    "text":"gonna run sum quick eran joi charlott",
    "cleaned_text":"gonna run sum quick eran joi charlott",
    "normalized_text":"gonna run sum quick eran joi charlott",
    "tokens":[
      "gon",
      "na",
      "run",
      "sum",
      "quick",
      "eran",
      "joi",
      "charlott"
    ],
    "token_count":8,
    "processed_text":"gon na run sum quick eran joi charlott"
  },
  {
    "label":0,
    "text":"dnscoloscom report ns record domain mx mailserv look kaput mate",
    "cleaned_text":"dnscoloscom report ns record domain mx mailserv look kaput mate",
    "normalized_text":"dnscoloscom report ns record domain mx mailserv look kaput mate",
    "tokens":[
      "dnscoloscom",
      "report",
      "ns",
      "record",
      "domain",
      "mx",
      "mailserv",
      "look",
      "kaput",
      "mate"
    ],
    "token_count":10,
    "processed_text":"dnscoloscom report ns record domain mx mailserv look kaput mate"
  },
  {
    "label":0,
    "text":"im go epcot hollywood studio must pack travel home",
    "cleaned_text":"im go epcot hollywood studio must pack travel home",
    "normalized_text":"im go epcot hollywood studio must pack travel home",
    "tokens":[
      "im",
      "go",
      "epcot",
      "hollywood",
      "studio",
      "pack",
      "travel",
      "home"
    ],
    "token_count":8,
    "processed_text":"im go epcot hollywood studio pack travel home"
  },
  {
    "label":4,
    "text":"win instead easteuropian offenc pleas vote belgium next year depress haha that joke",
    "cleaned_text":"win instead easteuropian offenc pleas vote belgium next year depress haha that joke",
    "normalized_text":"win instead easteuropian offenc pleas vote belgium next year depress haha that joke",
    "tokens":[
      "win",
      "instead",
      "easteuropian",
      "offenc",
      "plea",
      "vote",
      "belgium",
      "next",
      "year",
      "depress",
      "haha",
      "joke"
    ],
    "token_count":12,
    "processed_text":"win instead easteuropian offenc plea vote belgium next year depress haha joke"
  },
  {
    "label":4,
    "text":"someon go buy batteri",
    "cleaned_text":"someon go buy batteri",
    "normalized_text":"someon go buy batteri",
    "tokens":[
      "someon",
      "go",
      "buy",
      "batteri"
    ],
    "token_count":4,
    "processed_text":"someon go buy batteri"
  },
  {
    "label":0,
    "text":"idlewild afew gd song aswel omg im guna cri tho cant go wolv anymor",
    "cleaned_text":"idlewild afew gd song aswel omg im guna cri tho cant go wolv anymor",
    "normalized_text":"idlewild afew gd song aswel omg im guna cri tho cant go wolv anymor",
    "tokens":[
      "idlewild",
      "afew",
      "gd",
      "song",
      "aswel",
      "omg",
      "im",
      "guna",
      "cri",
      "tho",
      "cant",
      "go",
      "wolv",
      "anymor"
    ],
    "token_count":14,
    "processed_text":"idlewild afew gd song aswel omg im guna cri tho cant go wolv anymor"
  },
  {
    "label":4,
    "text":"saw back atcha",
    "cleaned_text":"saw back atcha",
    "normalized_text":"saw back atcha",
    "tokens":[
      "saw",
      "back",
      "atcha"
    ],
    "token_count":3,
    "processed_text":"saw back atcha"
  },
  {
    "label":0,
    "text":"mine awkward offic sleein home nthing new old stori",
    "cleaned_text":"mine awkward offic sleein home nthing new old stori",
    "normalized_text":"mine awkward offic sleein home nthing new old stori",
    "tokens":[
      "mine",
      "awkward",
      "offic",
      "sleein",
      "home",
      "nthing",
      "new",
      "old",
      "stori"
    ],
    "token_count":9,
    "processed_text":"mine awkward offic sleein home nthing new old stori"
  },
  {
    "label":4,
    "text":"yaaaay soon im way home im sooooooo excit",
    "cleaned_text":"yaaaay soon im way home im sooooooo excit",
    "normalized_text":"yaaaay soon im way home im sooooooo excit",
    "tokens":[
      "yaaaay",
      "soon",
      "im",
      "way",
      "home",
      "im",
      "sooooooo",
      "excit"
    ],
    "token_count":8,
    "processed_text":"yaaaay soon im way home im sooooooo excit"
  },
  {
    "label":4,
    "text":"hope hear soon squarespac",
    "cleaned_text":"hope hear soon squarespac",
    "normalized_text":"hope hear soon squarespac",
    "tokens":[
      "hope",
      "hear",
      "soon",
      "squarespac"
    ],
    "token_count":4,
    "processed_text":"hope hear soon squarespac"
  },
  {
    "label":4,
    "text":"well told could decid also also environment issu",
    "cleaned_text":"well told could decid also also environment issu",
    "normalized_text":"well told could decid also also environment issu",
    "tokens":[
      "well",
      "told",
      "decid",
      "also",
      "also",
      "environ",
      "issu"
    ],
    "token_count":7,
    "processed_text":"well told decid also also environ issu"
  },
  {
    "label":4,
    "text":"cant wait see hey toni birthday tomorrow",
    "cleaned_text":"cant wait see hey toni birthday tomorrow",
    "normalized_text":"cant wait see hey toni birthday tomorrow",
    "tokens":[
      "cant",
      "wait",
      "see",
      "hey",
      "toni",
      "birthday",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"cant wait see hey toni birthday tomorrow"
  },
  {
    "label":0,
    "text":"hate hair",
    "cleaned_text":"hate hair",
    "normalized_text":"hate hair",
    "tokens":[
      "hate",
      "hair"
    ],
    "token_count":2,
    "processed_text":"hate hair"
  },
  {
    "label":0,
    "text":"focus tom much didnt see belov barack oh must vote poor presid vote",
    "cleaned_text":"focus tom much didnt see belov barack oh must vote poor presid vote",
    "normalized_text":"focus tom much didnt see belov barack oh must vote poor presid vote",
    "tokens":[
      "focu",
      "tom",
      "much",
      "didnt",
      "see",
      "belov",
      "barack",
      "oh",
      "vote",
      "poor",
      "presid",
      "vote"
    ],
    "token_count":12,
    "processed_text":"focu tom much didnt see belov barack oh vote poor presid vote"
  },
  {
    "label":4,
    "text":"improv take",
    "cleaned_text":"improv take",
    "normalized_text":"improv take",
    "tokens":[
      "improv",
      "take"
    ],
    "token_count":2,
    "processed_text":"improv take"
  },
  {
    "label":0,
    "text":"yeahbut think ate much feel bit quotsickquot haha",
    "cleaned_text":"yeahbut think ate much feel bit quotsickquot haha",
    "normalized_text":"yeahbut think ate much feel bit quotsickquot haha",
    "tokens":[
      "yeahbut",
      "think",
      "ate",
      "much",
      "feel",
      "bit",
      "quotsickquot",
      "haha"
    ],
    "token_count":8,
    "processed_text":"yeahbut think ate much feel bit quotsickquot haha"
  },
  {
    "label":4,
    "text":"good luck",
    "cleaned_text":"good luck",
    "normalized_text":"good luck",
    "tokens":[
      "good",
      "luck"
    ],
    "token_count":2,
    "processed_text":"good luck"
  },
  {
    "label":0,
    "text":"drag week cannot wake save soul coffe isnt work",
    "cleaned_text":"drag week cannot wake save soul coffe isnt work",
    "normalized_text":"drag week cannot wake save soul coffe isnt work",
    "tokens":[
      "drag",
      "week",
      "wake",
      "save",
      "soul",
      "coff",
      "isnt",
      "work"
    ],
    "token_count":8,
    "processed_text":"drag week wake save soul coff isnt work"
  },
  {
    "label":4,
    "text":"jennilynn hahaha",
    "cleaned_text":"jennilynn hahaha",
    "normalized_text":"jennilynn hahaha",
    "tokens":[
      "jennilynn",
      "hahaha"
    ],
    "token_count":2,
    "processed_text":"jennilynn hahaha"
  },
  {
    "label":0,
    "text":"mrsshedfir take pictur without shirt bleeeech",
    "cleaned_text":"mrsshedfir take pictur without shirt bleeeech",
    "normalized_text":"mrsshedfir take pictur without shirt bleeeech",
    "tokens":[
      "mrsshedfir",
      "take",
      "pictur",
      "without",
      "shirt",
      "bleeeech"
    ],
    "token_count":6,
    "processed_text":"mrsshedfir take pictur without shirt bleeeech"
  },
  {
    "label":0,
    "text":"poor horribl day work im guess",
    "cleaned_text":"poor horribl day work im guess",
    "normalized_text":"poor horribl day work im guess",
    "tokens":[
      "poor",
      "horribl",
      "day",
      "work",
      "im",
      "guess"
    ],
    "token_count":6,
    "processed_text":"poor horribl day work im guess"
  },
  {
    "label":0,
    "text":"leav relay earli unfortun homework take preced fun",
    "cleaned_text":"leav relay earli unfortun homework take preced fun",
    "normalized_text":"leav relay earli unfortun homework take preced fun",
    "tokens":[
      "leav",
      "relay",
      "earli",
      "unfortun",
      "homework",
      "take",
      "prece",
      "fun"
    ],
    "token_count":8,
    "processed_text":"leav relay earli unfortun homework take prece fun"
  },
  {
    "label":0,
    "text":"miss work terribl tonight",
    "cleaned_text":"miss work terribl tonight",
    "normalized_text":"miss work terribl tonight",
    "tokens":[
      "miss",
      "work",
      "terribl",
      "tonight"
    ],
    "token_count":4,
    "processed_text":"miss work terribl tonight"
  },
  {
    "label":0,
    "text":"late howev work realli happi",
    "cleaned_text":"late howev work realli happi",
    "normalized_text":"late howev work realli happi",
    "tokens":[
      "late",
      "howev",
      "work",
      "realli",
      "happi"
    ],
    "token_count":5,
    "processed_text":"late howev work realli happi"
  },
  {
    "label":4,
    "text":"hey bro what gna b track ep littl somyh album im planin put itun",
    "cleaned_text":"hey bro what gna b track ep littl somyh album im planin put itun",
    "normalized_text":"hey bro what gna b track ep littl somyh album im planin put itun",
    "tokens":[
      "hey",
      "bro",
      "gna",
      "track",
      "ep",
      "littl",
      "somyh",
      "album",
      "im",
      "planin",
      "put",
      "itun"
    ],
    "token_count":12,
    "processed_text":"hey bro gna track ep littl somyh album im planin put itun"
  },
  {
    "label":0,
    "text":"need miracl someth right",
    "cleaned_text":"need miracl someth right",
    "normalized_text":"need miracl someth right",
    "tokens":[
      "need",
      "miracl",
      "someth",
      "right"
    ],
    "token_count":4,
    "processed_text":"need miracl someth right"
  },
  {
    "label":0,
    "text":"got brace",
    "cleaned_text":"got brace",
    "normalized_text":"got brace",
    "tokens":[
      "got",
      "brace"
    ],
    "token_count":2,
    "processed_text":"got brace"
  },
  {
    "label":0,
    "text":"nerd upprocrastin physiotherapydread work tomorrow morn",
    "cleaned_text":"nerd upprocrastin physiotherapydread work tomorrow morn",
    "normalized_text":"nerd upprocrastin physiotherapydread work tomorrow morn",
    "tokens":[
      "nerd",
      "upprocrastin",
      "work",
      "tomorrow",
      "morn"
    ],
    "token_count":5,
    "processed_text":"nerd upprocrastin work tomorrow morn"
  },
  {
    "label":4,
    "text":"dont green avatar yet",
    "cleaned_text":"dont green avatar yet",
    "normalized_text":"dont green avatar yet",
    "tokens":[
      "dont",
      "green",
      "avatar",
      "yet"
    ],
    "token_count":4,
    "processed_text":"dont green avatar yet"
  },
  {
    "label":4,
    "text":"awe cute old nowb aunt amaz feelingi cant kid niec closest thing ive got",
    "cleaned_text":"awe cute old nowb aunt amaz feelingi cant kid niec closest thing ive got",
    "normalized_text":"awe cute old nowb aunt amaz feelingi cant kid niec closest thing ive got",
    "tokens":[
      "awe",
      "cute",
      "old",
      "nowb",
      "aunt",
      "amaz",
      "feelingi",
      "cant",
      "kid",
      "niec",
      "closest",
      "thing",
      "ive",
      "got"
    ],
    "token_count":14,
    "processed_text":"awe cute old nowb aunt amaz feelingi cant kid niec closest thing ive got"
  },
  {
    "label":0,
    "text":"babei cant see love tila tequila live gt",
    "cleaned_text":"babei cant see love tila tequila live gt",
    "normalized_text":"babei cant see love tila tequila live gt",
    "tokens":[
      "babei",
      "cant",
      "see",
      "love",
      "tila",
      "tequila",
      "live",
      "gt"
    ],
    "token_count":8,
    "processed_text":"babei cant see love tila tequila live gt"
  },
  {
    "label":4,
    "text":"gimm shout get realli desper jump",
    "cleaned_text":"gimm shout get realli desper jump",
    "normalized_text":"gimm shout get realli desper jump",
    "tokens":[
      "gimm",
      "shout",
      "get",
      "realli",
      "desper",
      "jump"
    ],
    "token_count":6,
    "processed_text":"gimm shout get realli desper jump"
  },
  {
    "label":4,
    "text":"got rad dress market fun",
    "cleaned_text":"got rad dress market fun",
    "normalized_text":"got rad dress market fun",
    "tokens":[
      "got",
      "rad",
      "dress",
      "market",
      "fun"
    ],
    "token_count":5,
    "processed_text":"got rad dress market fun"
  },
  {
    "label":0,
    "text":"everi followfriday peopl follow make like tweet sorri guy",
    "cleaned_text":"everi followfriday peopl follow make like tweet sorri guy",
    "normalized_text":"everi followfriday peopl follow make like tweet sorri guy",
    "tokens":[
      "everi",
      "followfriday",
      "peopl",
      "follow",
      "make",
      "like",
      "tweet",
      "sorri",
      "guy"
    ],
    "token_count":9,
    "processed_text":"everi followfriday peopl follow make like tweet sorri guy"
  },
  {
    "label":0,
    "text":"sambuca shot killer worst hangov yet",
    "cleaned_text":"sambuca shot killer worst hangov yet",
    "normalized_text":"sambuca shot killer worst hangov yet",
    "tokens":[
      "sambuca",
      "shot",
      "killer",
      "worst",
      "hangov",
      "yet"
    ],
    "token_count":6,
    "processed_text":"sambuca shot killer worst hangov yet"
  },
  {
    "label":0,
    "text":"long pizzl stuntz gunna last",
    "cleaned_text":"long pizzl stuntz gunna last",
    "normalized_text":"long pizzl stuntz gunna last",
    "tokens":[
      "long",
      "pizzl",
      "stuntz",
      "gunna",
      "last"
    ],
    "token_count":5,
    "processed_text":"long pizzl stuntz gunna last"
  },
  {
    "label":0,
    "text":"stomach hurt",
    "cleaned_text":"stomach hurt",
    "normalized_text":"stomach hurt",
    "tokens":[
      "stomach",
      "hurt"
    ],
    "token_count":2,
    "processed_text":"stomach hurt"
  },
  {
    "label":0,
    "text":"im sorri",
    "cleaned_text":"im sorri",
    "normalized_text":"im sorri",
    "tokens":[
      "im",
      "sorri"
    ],
    "token_count":2,
    "processed_text":"im sorri"
  },
  {
    "label":4,
    "text":"john hilari isnt swear contest wasnt fan bet ran could play",
    "cleaned_text":"john hilari isnt swear contest wasnt fan bet ran could play",
    "normalized_text":"john hilari isnt swear contest wasnt fan bet ran could play",
    "tokens":[
      "john",
      "hilari",
      "isnt",
      "swear",
      "contest",
      "wasnt",
      "fan",
      "bet",
      "ran",
      "play"
    ],
    "token_count":10,
    "processed_text":"john hilari isnt swear contest wasnt fan bet ran play"
  },
  {
    "label":4,
    "text":"love wake love text make warm amp mushi insid blush love daddi bless",
    "cleaned_text":"love wake love text make warm amp mushi insid blush love daddi bless",
    "normalized_text":"love wake love text make warm amp mushi insid blush love daddi bless",
    "tokens":[
      "love",
      "wake",
      "love",
      "text",
      "make",
      "warm",
      "amp",
      "mushi",
      "insid",
      "blush",
      "love",
      "daddi",
      "bless"
    ],
    "token_count":13,
    "processed_text":"love wake love text make warm amp mushi insid blush love daddi bless"
  },
  {
    "label":0,
    "text":"want bad want drum cant afford cash",
    "cleaned_text":"want bad want drum cant afford cash",
    "normalized_text":"want bad want drum cant afford cash",
    "tokens":[
      "want",
      "bad",
      "want",
      "drum",
      "cant",
      "afford",
      "cash"
    ],
    "token_count":7,
    "processed_text":"want bad want drum cant afford cash"
  },
  {
    "label":4,
    "text":"happi hurrican season everyon happi birthday mom",
    "cleaned_text":"happi hurrican season everyon happi birthday mom",
    "normalized_text":"happi hurrican season everyon happi birthday mom",
    "tokens":[
      "happi",
      "hurrican",
      "season",
      "everyon",
      "happi",
      "birthday",
      "mom"
    ],
    "token_count":7,
    "processed_text":"happi hurrican season everyon happi birthday mom"
  },
  {
    "label":4,
    "text":"love blog",
    "cleaned_text":"love blog",
    "normalized_text":"love blog",
    "tokens":[
      "love",
      "blog"
    ],
    "token_count":2,
    "processed_text":"love blog"
  },
  {
    "label":0,
    "text":"last time wash hair",
    "cleaned_text":"last time wash hair",
    "normalized_text":"last time wash hair",
    "tokens":[
      "last",
      "time",
      "wash",
      "hair"
    ],
    "token_count":4,
    "processed_text":"last time wash hair"
  },
  {
    "label":4,
    "text":"oh heart spanx realli miracl product",
    "cleaned_text":"oh heart spanx realli miracl product",
    "normalized_text":"oh heart spanx realli miracl product",
    "tokens":[
      "oh",
      "heart",
      "spanx",
      "realli",
      "miracl",
      "product"
    ],
    "token_count":6,
    "processed_text":"oh heart spanx realli miracl product"
  },
  {
    "label":4,
    "text":"definet im still celebr fact english speech need organis preset",
    "cleaned_text":"definet im still celebr fact english speech need organis preset",
    "normalized_text":"definet im still celebr fact english speech need organis preset",
    "tokens":[
      "definet",
      "im",
      "still",
      "celebr",
      "fact",
      "english",
      "speech",
      "need",
      "organi",
      "preset"
    ],
    "token_count":10,
    "processed_text":"definet im still celebr fact english speech need organi preset"
  },
  {
    "label":4,
    "text":"actual love show year amaz life revolv around mo haha",
    "cleaned_text":"actual love show year amaz life revolv around mo haha",
    "normalized_text":"actual love show year amaz life revolv around mo haha",
    "tokens":[
      "actual",
      "love",
      "show",
      "year",
      "amaz",
      "life",
      "revolv",
      "around",
      "mo",
      "haha"
    ],
    "token_count":10,
    "processed_text":"actual love show year amaz life revolv around mo haha"
  },
  {
    "label":4,
    "text":"rock babi get",
    "cleaned_text":"rock babi get",
    "normalized_text":"rock babi get",
    "tokens":[
      "rock",
      "babi",
      "get"
    ],
    "token_count":3,
    "processed_text":"rock babi get"
  },
  {
    "label":0,
    "text":"decid cant stand alltop spam longer",
    "cleaned_text":"decid cant stand alltop spam longer",
    "normalized_text":"decid cant stand alltop spam longer",
    "tokens":[
      "decid",
      "cant",
      "stand",
      "alltop",
      "spam",
      "longer"
    ],
    "token_count":6,
    "processed_text":"decid cant stand alltop spam longer"
  },
  {
    "label":4,
    "text":"your welcom awesom experi",
    "cleaned_text":"your welcom awesom experi",
    "normalized_text":"your welcom awesom experi",
    "tokens":[
      "welcom",
      "awesom",
      "experi"
    ],
    "token_count":3,
    "processed_text":"welcom awesom experi"
  },
  {
    "label":0,
    "text":"internet said lol instead actual laugh loud friend joke person",
    "cleaned_text":"internet said lol instead actual laugh loud friend joke person",
    "normalized_text":"internet said lol instead actual laugh loud friend joke person",
    "tokens":[
      "internet",
      "said",
      "lol",
      "instead",
      "actual",
      "laugh",
      "loud",
      "friend",
      "joke",
      "person"
    ],
    "token_count":10,
    "processed_text":"internet said lol instead actual laugh loud friend joke person"
  },
  {
    "label":4,
    "text":"time chang pic n show ur pretti face",
    "cleaned_text":"time chang pic n show ur pretti face",
    "normalized_text":"time chang pic n show ur pretti face",
    "tokens":[
      "time",
      "chang",
      "pic",
      "show",
      "ur",
      "pretti",
      "face"
    ],
    "token_count":7,
    "processed_text":"time chang pic show ur pretti face"
  },
  {
    "label":4,
    "text":"blah mine brokengotta buy new one",
    "cleaned_text":"blah mine brokengotta buy new one",
    "normalized_text":"blah mine brokengotta buy new one",
    "tokens":[
      "blah",
      "mine",
      "brokengotta",
      "buy",
      "new",
      "one"
    ],
    "token_count":6,
    "processed_text":"blah mine brokengotta buy new one"
  },
  {
    "label":4,
    "text":"one day",
    "cleaned_text":"one day",
    "normalized_text":"one day",
    "tokens":[
      "one",
      "day"
    ],
    "token_count":2,
    "processed_text":"one day"
  },
  {
    "label":0,
    "text":"shadi",
    "cleaned_text":"shadi",
    "normalized_text":"shadi",
    "tokens":[
      "shadi"
    ],
    "token_count":1,
    "processed_text":"shadi"
  },
  {
    "label":4,
    "text":"sound v nice home experi walk fluffydog english villag amp meet new dog well",
    "cleaned_text":"sound v nice home experi walk fluffydog english villag amp meet new dog well",
    "normalized_text":"sound v nice home experi walk fluffydog english villag amp meet new dog well",
    "tokens":[
      "sound",
      "nice",
      "home",
      "experi",
      "walk",
      "fluffydog",
      "english",
      "villag",
      "amp",
      "meet",
      "new",
      "dog",
      "well"
    ],
    "token_count":13,
    "processed_text":"sound nice home experi walk fluffydog english villag amp meet new dog well"
  },
  {
    "label":0,
    "text":"person gave assur itd make cri didnt though",
    "cleaned_text":"person gave assur itd make cri didnt though",
    "normalized_text":"person gave assur itd make cri didnt though",
    "tokens":[
      "person",
      "gave",
      "assur",
      "itd",
      "make",
      "cri",
      "didnt",
      "though"
    ],
    "token_count":8,
    "processed_text":"person gave assur itd make cri didnt though"
  },
  {
    "label":0,
    "text":"need scholarshiphelpi receiv fasfa award omgim senior",
    "cleaned_text":"need scholarshiphelpi receiv fasfa award omgim senior",
    "normalized_text":"need scholarshiphelpi receiv fasfa award omgim senior",
    "tokens":[
      "need",
      "receiv",
      "fasfa",
      "award",
      "omgim",
      "senior"
    ],
    "token_count":6,
    "processed_text":"need receiv fasfa award omgim senior"
  },
  {
    "label":0,
    "text":"would queu outsid cd store right till midnight get album im us",
    "cleaned_text":"would queu outsid cd store right till midnight get album im us",
    "normalized_text":"would queu outsid cd store right till midnight get album im us",
    "tokens":[
      "queu",
      "outsid",
      "cd",
      "store",
      "right",
      "till",
      "midnight",
      "get",
      "album",
      "im",
      "us"
    ],
    "token_count":11,
    "processed_text":"queu outsid cd store right till midnight get album im us"
  },
  {
    "label":0,
    "text":"think im get sick good",
    "cleaned_text":"think im get sick good",
    "normalized_text":"think im get sick good",
    "tokens":[
      "think",
      "im",
      "get",
      "sick",
      "good"
    ],
    "token_count":5,
    "processed_text":"think im get sick good"
  },
  {
    "label":4,
    "text":"hat true stori ill even post pictur minut",
    "cleaned_text":"hat true stori ill even post pictur minut",
    "normalized_text":"hat true stori ill even post pictur minut",
    "tokens":[
      "hat",
      "true",
      "stori",
      "ill",
      "even",
      "post",
      "pictur",
      "minut"
    ],
    "token_count":8,
    "processed_text":"hat true stori ill even post pictur minut"
  },
  {
    "label":0,
    "text":"leonard cohen red rock week go sad",
    "cleaned_text":"leonard cohen red rock week go sad",
    "normalized_text":"leonard cohen red rock week go sad",
    "tokens":[
      "leonard",
      "cohen",
      "red",
      "rock",
      "week",
      "go",
      "sad"
    ],
    "token_count":7,
    "processed_text":"leonard cohen red rock week go sad"
  },
  {
    "label":4,
    "text":"cool well see",
    "cleaned_text":"cool well see",
    "normalized_text":"cool well see",
    "tokens":[
      "cool",
      "well",
      "see"
    ],
    "token_count":3,
    "processed_text":"cool well see"
  },
  {
    "label":0,
    "text":"girl never heard highlif guess she",
    "cleaned_text":"girl never heard highlif guess she",
    "normalized_text":"girl never heard highlif guess she",
    "tokens":[
      "girl",
      "never",
      "heard",
      "highlif",
      "guess"
    ],
    "token_count":5,
    "processed_text":"girl never heard highlif guess"
  },
  {
    "label":0,
    "text":"think bronchiti cant breath well keep happen feel miser chest hurt gasp air",
    "cleaned_text":"think bronchiti cant breath well keep happen feel miser chest hurt gasp air",
    "normalized_text":"think bronchiti cant breath well keep happen feel miser chest hurt gasp air",
    "tokens":[
      "think",
      "bronchiti",
      "cant",
      "breath",
      "well",
      "keep",
      "happen",
      "feel",
      "miser",
      "chest",
      "hurt",
      "gasp",
      "air"
    ],
    "token_count":13,
    "processed_text":"think bronchiti cant breath well keep happen feel miser chest hurt gasp air"
  },
  {
    "label":0,
    "text":"wish dad could drop alreadi",
    "cleaned_text":"wish dad could drop alreadi",
    "normalized_text":"wish dad could drop alreadi",
    "tokens":[
      "wish",
      "dad",
      "drop",
      "alreadi"
    ],
    "token_count":4,
    "processed_text":"wish dad drop alreadi"
  },
  {
    "label":0,
    "text":"go watch show littl peopl show giant teenag boy break heart way",
    "cleaned_text":"go watch show littl peopl show giant teenag boy break heart way",
    "normalized_text":"go watch show littl peopl show giant teenag boy break heart way",
    "tokens":[
      "go",
      "watch",
      "show",
      "littl",
      "peopl",
      "show",
      "giant",
      "teenag",
      "boy",
      "break",
      "heart",
      "way"
    ],
    "token_count":12,
    "processed_text":"go watch show littl peopl show giant teenag boy break heart way"
  },
  {
    "label":0,
    "text":"ye amp arent first person think add credenc fact virtual selv extens us",
    "cleaned_text":"ye amp arent first person think add credenc fact virtual selv extens us",
    "normalized_text":"ye amp arent first person think add credenc fact virtual selv extens us",
    "tokens":[
      "ye",
      "amp",
      "arent",
      "first",
      "person",
      "think",
      "add",
      "credenc",
      "fact",
      "virtual",
      "selv",
      "exten",
      "us"
    ],
    "token_count":13,
    "processed_text":"ye amp arent first person think add credenc fact virtual selv exten us"
  },
  {
    "label":0,
    "text":"got big headach autschtri hold head high",
    "cleaned_text":"got big headach autschtri hold head high",
    "normalized_text":"got big headach autschtri hold head high",
    "tokens":[
      "got",
      "big",
      "headach",
      "autschtri",
      "hold",
      "head",
      "high"
    ],
    "token_count":7,
    "processed_text":"got big headach autschtri hold head high"
  },
  {
    "label":4,
    "text":"sound cool",
    "cleaned_text":"sound cool",
    "normalized_text":"sound cool",
    "tokens":[
      "sound",
      "cool"
    ],
    "token_count":2,
    "processed_text":"sound cool"
  },
  {
    "label":0,
    "text":"woke coffe mean get dress amp go latt stand fix",
    "cleaned_text":"woke coffe mean get dress amp go latt stand fix",
    "normalized_text":"woke coffe mean get dress amp go latt stand fix",
    "tokens":[
      "woke",
      "coff",
      "mean",
      "get",
      "dress",
      "amp",
      "go",
      "latt",
      "stand",
      "fix"
    ],
    "token_count":10,
    "processed_text":"woke coff mean get dress amp go latt stand fix"
  },
  {
    "label":0,
    "text":"morn issu",
    "cleaned_text":"morn issu",
    "normalized_text":"morn issu",
    "tokens":[
      "morn",
      "issu"
    ],
    "token_count":2,
    "processed_text":"morn issu"
  },
  {
    "label":4,
    "text":"omg dude your ok right your still tweet",
    "cleaned_text":"omg dude your ok right your still tweet",
    "normalized_text":"omg dude your ok right your still tweet",
    "tokens":[
      "omg",
      "dude",
      "ok",
      "right",
      "still",
      "tweet"
    ],
    "token_count":6,
    "processed_text":"omg dude ok right still tweet"
  },
  {
    "label":4,
    "text":"ever sinc met twitter make day brighter hope meet one day person go",
    "cleaned_text":"ever sinc met twitter make day brighter hope meet one day person go",
    "normalized_text":"ever sinc met twitter make day brighter hope meet one day person go",
    "tokens":[
      "ever",
      "sinc",
      "met",
      "twitter",
      "make",
      "day",
      "brighter",
      "hope",
      "meet",
      "one",
      "day",
      "person",
      "go"
    ],
    "token_count":13,
    "processed_text":"ever sinc met twitter make day brighter hope meet one day person go"
  },
  {
    "label":4,
    "text":"help get right frame mind",
    "cleaned_text":"help get right frame mind",
    "normalized_text":"help get right frame mind",
    "tokens":[
      "help",
      "get",
      "right",
      "frame",
      "mind"
    ],
    "token_count":5,
    "processed_text":"help get right frame mind"
  },
  {
    "label":0,
    "text":"lmao dont blame got breast dad side fam shame dident get eye colour though",
    "cleaned_text":"lmao dont blame got breast dad side fam shame dident get eye colour though",
    "normalized_text":"lmao dont blame got breast dad side fam shame dident get eye colour though",
    "tokens":[
      "lmao",
      "dont",
      "blame",
      "got",
      "breast",
      "dad",
      "side",
      "fam",
      "shame",
      "dident",
      "get",
      "eye",
      "colour",
      "though"
    ],
    "token_count":14,
    "processed_text":"lmao dont blame got breast dad side fam shame dident get eye colour though"
  },
  {
    "label":4,
    "text":"sound like long day well least hope get rest look like good caus gnite",
    "cleaned_text":"sound like long day well least hope get rest look like good caus gnite",
    "normalized_text":"sound like long day well least hope get rest look like good caus gnite",
    "tokens":[
      "sound",
      "like",
      "long",
      "day",
      "well",
      "least",
      "hope",
      "get",
      "rest",
      "look",
      "like",
      "good",
      "cau",
      "gnite"
    ],
    "token_count":14,
    "processed_text":"sound like long day well least hope get rest look like good cau gnite"
  },
  {
    "label":0,
    "text":"awak bodi think eastern time yet",
    "cleaned_text":"awak bodi think eastern time yet",
    "normalized_text":"awak bodi think eastern time yet",
    "tokens":[
      "awak",
      "bodi",
      "think",
      "eastern",
      "time",
      "yet"
    ],
    "token_count":6,
    "processed_text":"awak bodi think eastern time yet"
  },
  {
    "label":4,
    "text":"uhmmm realli want say annnd echo",
    "cleaned_text":"uhmmm realli want say annnd echo",
    "normalized_text":"uhmmm realli want say annnd echo",
    "tokens":[
      "uhmmm",
      "realli",
      "want",
      "say",
      "annnd",
      "echo"
    ],
    "token_count":6,
    "processed_text":"uhmmm realli want say annnd echo"
  },
  {
    "label":0,
    "text":"woke littl later origianlli plan need trim total bodi weight workout favorit late",
    "cleaned_text":"woke littl later origianlli plan need trim total bodi weight workout favorit late",
    "normalized_text":"woke littl later origianlli plan need trim total bodi weight workout favorit late",
    "tokens":[
      "woke",
      "littl",
      "later",
      "origianlli",
      "plan",
      "need",
      "trim",
      "total",
      "bodi",
      "weight",
      "workout",
      "favorit",
      "late"
    ],
    "token_count":13,
    "processed_text":"woke littl later origianlli plan need trim total bodi weight workout favorit late"
  },
  {
    "label":0,
    "text":"appar cat isnt miss mewow thought couldnt feel loneli",
    "cleaned_text":"appar cat isnt miss mewow thought couldnt feel loneli",
    "normalized_text":"appar cat isnt miss mewow thought couldnt feel loneli",
    "tokens":[
      "appar",
      "cat",
      "isnt",
      "miss",
      "mewow",
      "thought",
      "couldnt",
      "feel",
      "lone"
    ],
    "token_count":9,
    "processed_text":"appar cat isnt miss mewow thought couldnt feel lone"
  },
  {
    "label":4,
    "text":"oh welli hope guy could talk thing",
    "cleaned_text":"oh welli hope guy could talk thing",
    "normalized_text":"oh welli hope guy could talk thing",
    "tokens":[
      "oh",
      "welli",
      "hope",
      "guy",
      "talk",
      "thing"
    ],
    "token_count":6,
    "processed_text":"oh welli hope guy talk thing"
  },
  {
    "label":4,
    "text":"start anoth day tech good thing finish day",
    "cleaned_text":"start anoth day tech good thing finish day",
    "normalized_text":"start anoth day tech good thing finish day",
    "tokens":[
      "start",
      "anoth",
      "day",
      "tech",
      "good",
      "thing",
      "finish",
      "day"
    ],
    "token_count":8,
    "processed_text":"start anoth day tech good thing finish day"
  },
  {
    "label":0,
    "text":"horribl hiccup make difficult put makeup",
    "cleaned_text":"horribl hiccup make difficult put makeup",
    "normalized_text":"horribl hiccup make difficult put makeup",
    "tokens":[
      "horribl",
      "hiccup",
      "make",
      "difficult",
      "put",
      "makeup"
    ],
    "token_count":6,
    "processed_text":"horribl hiccup make difficult put makeup"
  },
  {
    "label":4,
    "text":"everyth seem like movi right like im dreamin someth pretti cool",
    "cleaned_text":"everyth seem like movi right like im dreamin someth pretti cool",
    "normalized_text":"everyth seem like movi right like im dreamin someth pretti cool",
    "tokens":[
      "everyth",
      "seem",
      "like",
      "movi",
      "right",
      "like",
      "im",
      "dreamin",
      "someth",
      "pretti",
      "cool"
    ],
    "token_count":11,
    "processed_text":"everyth seem like movi right like im dreamin someth pretti cool"
  },
  {
    "label":4,
    "text":"never realli understood rss work look thank",
    "cleaned_text":"never realli understood rss work look thank",
    "normalized_text":"never realli understood rss work look thank",
    "tokens":[
      "never",
      "realli",
      "understood",
      "rss",
      "work",
      "look",
      "thank"
    ],
    "token_count":7,
    "processed_text":"never realli understood rss work look thank"
  },
  {
    "label":0,
    "text":"big hug didnt see ball last night",
    "cleaned_text":"big hug didnt see ball last night",
    "normalized_text":"big hug didnt see ball last night",
    "tokens":[
      "big",
      "hug",
      "didnt",
      "see",
      "ball",
      "last",
      "night"
    ],
    "token_count":7,
    "processed_text":"big hug didnt see ball last night"
  },
  {
    "label":4,
    "text":"didnt expect otherwis cheer",
    "cleaned_text":"didnt expect otherwis cheer",
    "normalized_text":"didnt expect otherwis cheer",
    "tokens":[
      "didnt",
      "expect",
      "otherwi",
      "cheer"
    ],
    "token_count":4,
    "processed_text":"didnt expect otherwi cheer"
  },
  {
    "label":0,
    "text":"goin skewl sadli hey awesom thing there skewl thursday aww man im gunna get skewl late today",
    "cleaned_text":"goin skewl sadli hey awesom thing there skewl thursday aww man im gunna get skewl late today",
    "normalized_text":"goin skewl sadli hey awesom thing there skewl thursday aww man im gunna get skewl late today",
    "tokens":[
      "goin",
      "skewl",
      "sadli",
      "hey",
      "awesom",
      "thing",
      "skewl",
      "thursday",
      "aww",
      "man",
      "im",
      "gunna",
      "get",
      "skewl",
      "late",
      "today"
    ],
    "token_count":16,
    "processed_text":"goin skewl sadli hey awesom thing skewl thursday aww man im gunna get skewl late today"
  },
  {
    "label":4,
    "text":"well go comment one tweet earlier facebook nevermind lol",
    "cleaned_text":"well go comment one tweet earlier facebook nevermind lol",
    "normalized_text":"well go comment one tweet earlier facebook nevermind lol",
    "tokens":[
      "well",
      "go",
      "comment",
      "one",
      "tweet",
      "earlier",
      "facebook",
      "nevermind",
      "lol"
    ],
    "token_count":9,
    "processed_text":"well go comment one tweet earlier facebook nevermind lol"
  },
  {
    "label":4,
    "text":"buy idli vada breakfast",
    "cleaned_text":"buy idli vada breakfast",
    "normalized_text":"buy idli vada breakfast",
    "tokens":[
      "buy",
      "idli",
      "vada",
      "breakfast"
    ],
    "token_count":4,
    "processed_text":"buy idli vada breakfast"
  },
  {
    "label":4,
    "text":"sunday sunni sunday",
    "cleaned_text":"sunday sunni sunday",
    "normalized_text":"sunday sunni sunday",
    "tokens":[
      "sunday",
      "sunni",
      "sunday"
    ],
    "token_count":3,
    "processed_text":"sunday sunni sunday"
  },
  {
    "label":4,
    "text":"realli dont wanna look deep sens kind sarcast mother fuck tone",
    "cleaned_text":"realli dont wanna look deep sens kind sarcast mother fuck tone",
    "normalized_text":"realli dont wanna look deep sens kind sarcast mother fuck tone",
    "tokens":[
      "realli",
      "dont",
      "wan",
      "na",
      "look",
      "deep",
      "sen",
      "kind",
      "sarcast",
      "mother",
      "fuck",
      "tone"
    ],
    "token_count":12,
    "processed_text":"realli dont wan na look deep sen kind sarcast mother fuck tone"
  },
  {
    "label":0,
    "text":"helllloooo rashard lewi see ya nee nee deona miss guy back grind",
    "cleaned_text":"helllloooo rashard lewi see ya nee nee deona miss guy back grind",
    "normalized_text":"helllloooo rashard lewi see ya nee nee deona miss guy back grind",
    "tokens":[
      "helllloooo",
      "rashard",
      "lewi",
      "see",
      "ya",
      "nee",
      "nee",
      "deona",
      "miss",
      "guy",
      "back",
      "grind"
    ],
    "token_count":12,
    "processed_text":"helllloooo rashard lewi see ya nee nee deona miss guy back grind"
  },
  {
    "label":0,
    "text":"hate live feed need texa cspan",
    "cleaned_text":"hate live feed need texa cspan",
    "normalized_text":"hate live feed need texa cspan",
    "tokens":[
      "hate",
      "live",
      "feed",
      "need",
      "texa",
      "cspan"
    ],
    "token_count":6,
    "processed_text":"hate live feed need texa cspan"
  },
  {
    "label":0,
    "text":"god im suffer greatli hair sit blaze sunshin v v bad sunburn",
    "cleaned_text":"god im suffer greatli hair sit blaze sunshin v v bad sunburn",
    "normalized_text":"god im suffer greatli hair sit blaze sunshin v v bad sunburn",
    "tokens":[
      "god",
      "im",
      "suffer",
      "greatli",
      "hair",
      "sit",
      "blaze",
      "sunshin",
      "bad",
      "sunburn"
    ],
    "token_count":10,
    "processed_text":"god im suffer greatli hair sit blaze sunshin bad sunburn"
  },
  {
    "label":4,
    "text":"lt get lt shape latt",
    "cleaned_text":"lt get lt shape latt",
    "normalized_text":"lt get lt shape latt",
    "tokens":[
      "lt",
      "get",
      "lt",
      "shape",
      "latt"
    ],
    "token_count":5,
    "processed_text":"lt get lt shape latt"
  },
  {
    "label":0,
    "text":"wish live la aswel",
    "cleaned_text":"wish live la aswel",
    "normalized_text":"wish live la aswel",
    "tokens":[
      "wish",
      "live",
      "la",
      "aswel"
    ],
    "token_count":4,
    "processed_text":"wish live la aswel"
  },
  {
    "label":0,
    "text":"slow day",
    "cleaned_text":"slow day",
    "normalized_text":"slow day",
    "tokens":[
      "slow",
      "day"
    ],
    "token_count":2,
    "processed_text":"slow day"
  },
  {
    "label":0,
    "text":"hmmm text show symbol strang anyway interview went well thought gonna get job didnt",
    "cleaned_text":"hmmm text show symbol strang anyway interview went well thought gonna get job didnt",
    "normalized_text":"hmmm text show symbol strang anyway interview went well thought gonna get job didnt",
    "tokens":[
      "hmmm",
      "text",
      "show",
      "symbol",
      "strang",
      "anyway",
      "interview",
      "went",
      "well",
      "thought",
      "gon",
      "na",
      "get",
      "job",
      "didnt"
    ],
    "token_count":15,
    "processed_text":"hmmm text show symbol strang anyway interview went well thought gon na get job didnt"
  },
  {
    "label":0,
    "text":"watch last episod hillsgonna miss lauren citi word ook niet meerwhitney back la jaajaa newsflash",
    "cleaned_text":"watch last episod hillsgonna miss lauren citi word ook niet meerwhitney back la jaajaa newsflash",
    "normalized_text":"watch last episod hillsgonna miss lauren citi word ook niet meerwhitney back la jaajaa newsflash",
    "tokens":[
      "watch",
      "last",
      "episod",
      "hillsgonna",
      "miss",
      "lauren",
      "citi",
      "word",
      "ook",
      "niet",
      "meerwhitney",
      "back",
      "la",
      "jaajaa",
      "newsflash"
    ],
    "token_count":15,
    "processed_text":"watch last episod hillsgonna miss lauren citi word ook niet meerwhitney back la jaajaa newsflash"
  },
  {
    "label":0,
    "text":"rememb time long ago took everi thursday friday miss",
    "cleaned_text":"rememb time long ago took everi thursday friday miss",
    "normalized_text":"rememb time long ago took everi thursday friday miss",
    "tokens":[
      "rememb",
      "time",
      "long",
      "ago",
      "took",
      "everi",
      "thursday",
      "friday",
      "miss"
    ],
    "token_count":9,
    "processed_text":"rememb time long ago took everi thursday friday miss"
  },
  {
    "label":4,
    "text":"q im twitter rather gener updat im stick",
    "cleaned_text":"q im twitter rather gener updat im stick",
    "normalized_text":"q im twitter rather gener updat im stick",
    "tokens":[
      "im",
      "twitter",
      "rather",
      "gener",
      "updat",
      "im",
      "stick"
    ],
    "token_count":7,
    "processed_text":"im twitter rather gener updat im stick"
  },
  {
    "label":0,
    "text":"school im bore class im tire well couldnt sleep last night brother sick graduat rehers dayi",
    "cleaned_text":"school im bore class im tire well couldnt sleep last night brother sick graduat rehers dayi",
    "normalized_text":"school im bore class im tire well couldnt sleep last night brother sick graduat rehers dayi",
    "tokens":[
      "school",
      "im",
      "bore",
      "class",
      "im",
      "tire",
      "well",
      "couldnt",
      "sleep",
      "last",
      "night",
      "brother",
      "sick",
      "graduat",
      "reher",
      "dayi"
    ],
    "token_count":16,
    "processed_text":"school im bore class im tire well couldnt sleep last night brother sick graduat reher dayi"
  },
  {
    "label":4,
    "text":"send photo think match yeah send pictur",
    "cleaned_text":"send photo think match yeah send pictur",
    "normalized_text":"send photo think match yeah send pictur",
    "tokens":[
      "send",
      "photo",
      "think",
      "match",
      "yeah",
      "send",
      "pictur"
    ],
    "token_count":7,
    "processed_text":"send photo think match yeah send pictur"
  },
  {
    "label":0,
    "text":"aww know felt bad didnt see til way late rain check",
    "cleaned_text":"aww know felt bad didnt see til way late rain check",
    "normalized_text":"aww know felt bad didnt see til way late rain check",
    "tokens":[
      "aww",
      "know",
      "felt",
      "bad",
      "didnt",
      "see",
      "til",
      "way",
      "late",
      "rain",
      "check"
    ],
    "token_count":11,
    "processed_text":"aww know felt bad didnt see til way late rain check"
  },
  {
    "label":4,
    "text":"homeboredc wait go dadz hous iin hr yay llol",
    "cleaned_text":"homeboredc wait go dadz hous iin hr yay llol",
    "normalized_text":"homeboredc wait go dadz hous iin hr yay llol",
    "tokens":[
      "homeboredc",
      "wait",
      "go",
      "dadz",
      "hou",
      "iin",
      "hr",
      "yay",
      "llol"
    ],
    "token_count":9,
    "processed_text":"homeboredc wait go dadz hou iin hr yay llol"
  },
  {
    "label":4,
    "text":"nope usual prefer danc video soft video",
    "cleaned_text":"nope usual prefer danc video soft video",
    "normalized_text":"nope usual prefer danc video soft video",
    "tokens":[
      "nope",
      "usual",
      "prefer",
      "danc",
      "video",
      "soft",
      "video"
    ],
    "token_count":7,
    "processed_text":"nope usual prefer danc video soft video"
  },
  {
    "label":4,
    "text":"random post lol glad see tweet tweet back action",
    "cleaned_text":"random post lol glad see tweet tweet back action",
    "normalized_text":"random post lol glad see tweet tweet back action",
    "tokens":[
      "random",
      "post",
      "lol",
      "glad",
      "see",
      "tweet",
      "tweet",
      "back",
      "action"
    ],
    "token_count":9,
    "processed_text":"random post lol glad see tweet tweet back action"
  },
  {
    "label":0,
    "text":"aw lonli oh well",
    "cleaned_text":"aw lonli oh well",
    "normalized_text":"aw lonli oh well",
    "tokens":[
      "aw",
      "lonli",
      "oh",
      "well"
    ],
    "token_count":4,
    "processed_text":"aw lonli oh well"
  },
  {
    "label":4,
    "text":"anyth possibl",
    "cleaned_text":"anyth possibl",
    "normalized_text":"anyth possibl",
    "tokens":[
      "anyth",
      "possibl"
    ],
    "token_count":2,
    "processed_text":"anyth possibl"
  },
  {
    "label":4,
    "text":"awwww ador sleep foot guy bed kid",
    "cleaned_text":"awwww ador sleep foot guy bed kid",
    "normalized_text":"awwww ador sleep foot guy bed kid",
    "tokens":[
      "awwww",
      "ador",
      "sleep",
      "foot",
      "guy",
      "bed",
      "kid"
    ],
    "token_count":7,
    "processed_text":"awwww ador sleep foot guy bed kid"
  },
  {
    "label":0,
    "text":"go see",
    "cleaned_text":"go see",
    "normalized_text":"go see",
    "tokens":[
      "go",
      "see"
    ],
    "token_count":2,
    "processed_text":"go see"
  },
  {
    "label":0,
    "text":"dang doesnt seem right go whole month without jay leno tonight",
    "cleaned_text":"dang doesnt seem right go whole month without jay leno tonight",
    "normalized_text":"dang doesnt seem right go whole month without jay leno tonight",
    "tokens":[
      "dang",
      "doesnt",
      "seem",
      "right",
      "go",
      "whole",
      "month",
      "without",
      "jay",
      "leno",
      "tonight"
    ],
    "token_count":11,
    "processed_text":"dang doesnt seem right go whole month without jay leno tonight"
  },
  {
    "label":0,
    "text":"bed beach im jealou",
    "cleaned_text":"bed beach im jealou",
    "normalized_text":"bed beach im jealou",
    "tokens":[
      "bed",
      "beach",
      "im",
      "jealou"
    ],
    "token_count":4,
    "processed_text":"bed beach im jealou"
  },
  {
    "label":0,
    "text":"still stuf home nice hot train",
    "cleaned_text":"still stuf home nice hot train",
    "normalized_text":"still stuf home nice hot train",
    "tokens":[
      "still",
      "stuf",
      "home",
      "nice",
      "hot",
      "train"
    ],
    "token_count":6,
    "processed_text":"still stuf home nice hot train"
  },
  {
    "label":0,
    "text":"nightmar",
    "cleaned_text":"nightmar",
    "normalized_text":"nightmar",
    "tokens":[
      "nightmar"
    ],
    "token_count":1,
    "processed_text":"nightmar"
  },
  {
    "label":4,
    "text":"live stream mtv movi award wooo congrat ashley tisdal eff hater lol",
    "cleaned_text":"live stream mtv movi award wooo congrat ashley tisdal eff hater lol",
    "normalized_text":"live stream mtv movi award wooo congrat ashley tisdal eff hater lol",
    "tokens":[
      "live",
      "stream",
      "mtv",
      "movi",
      "award",
      "wooo",
      "congrat",
      "ashley",
      "tisdal",
      "eff",
      "hater",
      "lol"
    ],
    "token_count":12,
    "processed_text":"live stream mtv movi award wooo congrat ashley tisdal eff hater lol"
  },
  {
    "label":0,
    "text":"dont text back",
    "cleaned_text":"dont text back",
    "normalized_text":"dont text back",
    "tokens":[
      "dont",
      "text",
      "back"
    ],
    "token_count":3,
    "processed_text":"dont text back"
  },
  {
    "label":0,
    "text":"nope origin obvious play w diff type bup start almost manual morn",
    "cleaned_text":"nope origin obvious play w diff type bup start almost manual morn",
    "normalized_text":"nope origin obvious play w diff type bup start almost manual morn",
    "tokens":[
      "nope",
      "origin",
      "obviou",
      "play",
      "diff",
      "type",
      "bup",
      "start",
      "almost",
      "manual",
      "morn"
    ],
    "token_count":11,
    "processed_text":"nope origin obviou play diff type bup start almost manual morn"
  },
  {
    "label":0,
    "text":"think that sad",
    "cleaned_text":"think that sad",
    "normalized_text":"think that sad",
    "tokens":[
      "think",
      "sad"
    ],
    "token_count":2,
    "processed_text":"think sad"
  },
  {
    "label":0,
    "text":"never much traffic holcomb bridg luckili crap load would pull",
    "cleaned_text":"never much traffic holcomb bridg luckili crap load would pull",
    "normalized_text":"never much traffic holcomb bridg luckili crap load would pull",
    "tokens":[
      "never",
      "much",
      "traffic",
      "holcomb",
      "bridg",
      "luckili",
      "crap",
      "load",
      "pull"
    ],
    "token_count":9,
    "processed_text":"never much traffic holcomb bridg luckili crap load pull"
  },
  {
    "label":4,
    "text":"well good time tonight good show alway good see favorit twitter friend",
    "cleaned_text":"well good time tonight good show alway good see favorit twitter friend",
    "normalized_text":"well good time tonight good show alway good see favorit twitter friend",
    "tokens":[
      "well",
      "good",
      "time",
      "tonight",
      "good",
      "show",
      "alway",
      "good",
      "see",
      "favorit",
      "twitter",
      "friend"
    ],
    "token_count":12,
    "processed_text":"well good time tonight good show alway good see favorit twitter friend"
  },
  {
    "label":4,
    "text":"youv twitter vacuum good back",
    "cleaned_text":"youv twitter vacuum good back",
    "normalized_text":"youv twitter vacuum good back",
    "tokens":[
      "youv",
      "twitter",
      "vacuum",
      "good",
      "back"
    ],
    "token_count":5,
    "processed_text":"youv twitter vacuum good back"
  },
  {
    "label":0,
    "text":"your go see jb aww lucki want go im broke there way mom would let go pay much",
    "cleaned_text":"your go see jb aww lucki want go im broke there way mom would let go pay much",
    "normalized_text":"your go see jb aww lucki want go im broke there way mom would let go pay much",
    "tokens":[
      "go",
      "see",
      "jb",
      "aww",
      "lucki",
      "want",
      "go",
      "im",
      "broke",
      "way",
      "mom",
      "let",
      "go",
      "pay",
      "much"
    ],
    "token_count":15,
    "processed_text":"go see jb aww lucki want go im broke way mom let go pay much"
  },
  {
    "label":4,
    "text":"good night good day tomorrow sure much parti go",
    "cleaned_text":"good night good day tomorrow sure much parti go",
    "normalized_text":"good night good day tomorrow sure much parti go",
    "tokens":[
      "good",
      "night",
      "good",
      "day",
      "tomorrow",
      "sure",
      "much",
      "parti",
      "go"
    ],
    "token_count":9,
    "processed_text":"good night good day tomorrow sure much parti go"
  },
  {
    "label":0,
    "text":"map locat marker blue page go game",
    "cleaned_text":"map locat marker blue page go game",
    "normalized_text":"map locat marker blue page go game",
    "tokens":[
      "map",
      "locat",
      "marker",
      "blue",
      "page",
      "go",
      "game"
    ],
    "token_count":7,
    "processed_text":"map locat marker blue page go game"
  },
  {
    "label":4,
    "text":"wait till get go camp",
    "cleaned_text":"wait till get go camp",
    "normalized_text":"wait till get go camp",
    "tokens":[
      "wait",
      "till",
      "get",
      "go",
      "camp"
    ],
    "token_count":5,
    "processed_text":"wait till get go camp"
  },
  {
    "label":4,
    "text":"hear covert bailout loan common stock oooh make dork dont care",
    "cleaned_text":"hear covert bailout loan common stock oooh make dork dont care",
    "normalized_text":"hear covert bailout loan common stock oooh make dork dont care",
    "tokens":[
      "hear",
      "covert",
      "bailout",
      "loan",
      "common",
      "stock",
      "oooh",
      "make",
      "dork",
      "dont",
      "care"
    ],
    "token_count":11,
    "processed_text":"hear covert bailout loan common stock oooh make dork dont care"
  },
  {
    "label":4,
    "text":"goooood",
    "cleaned_text":"goooood",
    "normalized_text":"goooood",
    "tokens":[
      "goooood"
    ],
    "token_count":1,
    "processed_text":"goooood"
  },
  {
    "label":4,
    "text":"stand correct play two player duel",
    "cleaned_text":"stand correct play two player duel",
    "normalized_text":"stand correct play two player duel",
    "tokens":[
      "stand",
      "correct",
      "play",
      "two",
      "player",
      "duel"
    ],
    "token_count":6,
    "processed_text":"stand correct play two player duel"
  },
  {
    "label":0,
    "text":"im final pack go home problem idk im get",
    "cleaned_text":"im final pack go home problem idk im get",
    "normalized_text":"im final pack go home problem idk im get",
    "tokens":[
      "im",
      "final",
      "pack",
      "go",
      "home",
      "problem",
      "idk",
      "im",
      "get"
    ],
    "token_count":9,
    "processed_text":"im final pack go home problem idk im get"
  },
  {
    "label":0,
    "text":"cold sore realli hurt",
    "cleaned_text":"cold sore realli hurt",
    "normalized_text":"cold sore realli hurt",
    "tokens":[
      "cold",
      "sore",
      "realli",
      "hurt"
    ],
    "token_count":4,
    "processed_text":"cold sore realli hurt"
  },
  {
    "label":0,
    "text":"haha ya omg bad didnt meet friend though p",
    "cleaned_text":"haha ya omg bad didnt meet friend though p",
    "normalized_text":"haha ya omg bad didnt meet friend though p",
    "tokens":[
      "haha",
      "ya",
      "omg",
      "bad",
      "didnt",
      "meet",
      "friend",
      "though"
    ],
    "token_count":8,
    "processed_text":"haha ya omg bad didnt meet friend though"
  },
  {
    "label":4,
    "text":"need new criativ pictur mayb ill take tonight",
    "cleaned_text":"need new criativ pictur mayb ill take tonight",
    "normalized_text":"need new criativ pictur mayb ill take tonight",
    "tokens":[
      "need",
      "new",
      "criativ",
      "pictur",
      "mayb",
      "ill",
      "take",
      "tonight"
    ],
    "token_count":8,
    "processed_text":"need new criativ pictur mayb ill take tonight"
  },
  {
    "label":0,
    "text":"sore head day cant seem clear",
    "cleaned_text":"sore head day cant seem clear",
    "normalized_text":"sore head day cant seem clear",
    "tokens":[
      "sore",
      "head",
      "day",
      "cant",
      "seem",
      "clear"
    ],
    "token_count":6,
    "processed_text":"sore head day cant seem clear"
  },
  {
    "label":0,
    "text":"wooohcoffe man wish could chillhang longer option boo",
    "cleaned_text":"wooohcoffe man wish could chillhang longer option boo",
    "normalized_text":"wooohcoffe man wish could chillhang longer option boo",
    "tokens":[
      "wooohcoff",
      "man",
      "wish",
      "chillhang",
      "longer",
      "option",
      "boo"
    ],
    "token_count":7,
    "processed_text":"wooohcoff man wish chillhang longer option boo"
  },
  {
    "label":0,
    "text":"found last night dad tripl bypassheart diseas heriditarymom diabetesi got good gene",
    "cleaned_text":"found last night dad tripl bypassheart diseas heriditarymom diabetesi got good gene",
    "normalized_text":"found last night dad tripl bypassheart diseas heriditarymom diabetesi got good gene",
    "tokens":[
      "found",
      "last",
      "night",
      "dad",
      "tripl",
      "bypassheart",
      "disea",
      "heriditarymom",
      "diabetesi",
      "got",
      "good",
      "gene"
    ],
    "token_count":12,
    "processed_text":"found last night dad tripl bypassheart disea heriditarymom diabetesi got good gene"
  },
  {
    "label":0,
    "text":"sucki section",
    "cleaned_text":"sucki section",
    "normalized_text":"sucki section",
    "tokens":[
      "sucki",
      "section"
    ],
    "token_count":2,
    "processed_text":"sucki section"
  },
  {
    "label":4,
    "text":"put new elfa pantri three cheer organ",
    "cleaned_text":"put new elfa pantri three cheer organ",
    "normalized_text":"put new elfa pantri three cheer organ",
    "tokens":[
      "put",
      "new",
      "elfa",
      "pantri",
      "three",
      "cheer",
      "organ"
    ],
    "token_count":7,
    "processed_text":"put new elfa pantri three cheer organ"
  },
  {
    "label":4,
    "text":"followfriday one follow put wmi bad french good virgo send happi thought et respirfaut pa oublier",
    "cleaned_text":"followfriday one follow put wmi bad french good virgo send happi thought et respirfaut pa oublier",
    "normalized_text":"followfriday one follow put wmi bad french good virgo send happi thought et respirfaut pa oublier",
    "tokens":[
      "followfriday",
      "one",
      "follow",
      "put",
      "wmi",
      "bad",
      "french",
      "good",
      "virgo",
      "send",
      "happi",
      "thought",
      "et",
      "respirfaut",
      "pa",
      "oublier"
    ],
    "token_count":16,
    "processed_text":"followfriday one follow put wmi bad french good virgo send happi thought et respirfaut pa oublier"
  },
  {
    "label":0,
    "text":"yesterday day ok",
    "cleaned_text":"yesterday day ok",
    "normalized_text":"yesterday day ok",
    "tokens":[
      "yesterday",
      "day",
      "ok"
    ],
    "token_count":3,
    "processed_text":"yesterday day ok"
  },
  {
    "label":4,
    "text":"shop mamma",
    "cleaned_text":"shop mamma",
    "normalized_text":"shop mamma",
    "tokens":[
      "shop",
      "mamma"
    ],
    "token_count":2,
    "processed_text":"shop mamma"
  },
  {
    "label":4,
    "text":"love your tweet mom haha lt im come arizona see someday",
    "cleaned_text":"love your tweet mom haha lt im come arizona see someday",
    "normalized_text":"love your tweet mom haha lt im come arizona see someday",
    "tokens":[
      "love",
      "tweet",
      "mom",
      "haha",
      "lt",
      "im",
      "come",
      "arizona",
      "see",
      "someday"
    ],
    "token_count":10,
    "processed_text":"love tweet mom haha lt im come arizona see someday"
  },
  {
    "label":0,
    "text":"stuck rain boohoo",
    "cleaned_text":"stuck rain boohoo",
    "normalized_text":"stuck rain boohoo",
    "tokens":[
      "stuck",
      "rain",
      "boohoo"
    ],
    "token_count":3,
    "processed_text":"stuck rain boohoo"
  },
  {
    "label":4,
    "text":"everyon buy line vine tri time tomorrow",
    "cleaned_text":"everyon buy line vine tri time tomorrow",
    "normalized_text":"everyon buy line vine tri time tomorrow",
    "tokens":[
      "everyon",
      "buy",
      "line",
      "vine",
      "tri",
      "time",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"everyon buy line vine tri time tomorrow"
  },
  {
    "label":4,
    "text":"sleep well hope bedroom side hous main road",
    "cleaned_text":"sleep well hope bedroom side hous main road",
    "normalized_text":"sleep well hope bedroom side hous main road",
    "tokens":[
      "sleep",
      "well",
      "hope",
      "bedroom",
      "side",
      "hou",
      "main",
      "road"
    ],
    "token_count":8,
    "processed_text":"sleep well hope bedroom side hou main road"
  },
  {
    "label":4,
    "text":"chang rington ldn",
    "cleaned_text":"chang rington ldn",
    "normalized_text":"chang rington ldn",
    "tokens":[
      "chang",
      "rington",
      "ldn"
    ],
    "token_count":3,
    "processed_text":"chang rington ldn"
  },
  {
    "label":4,
    "text":"star trek guillermo del toro tonight oh work dont matter good",
    "cleaned_text":"star trek guillermo del toro tonight oh work dont matter good",
    "normalized_text":"star trek guillermo del toro tonight oh work dont matter good",
    "tokens":[
      "star",
      "trek",
      "guillermo",
      "del",
      "toro",
      "tonight",
      "oh",
      "work",
      "dont",
      "matter",
      "good"
    ],
    "token_count":11,
    "processed_text":"star trek guillermo del toro tonight oh work dont matter good"
  },
  {
    "label":4,
    "text":"te toca",
    "cleaned_text":"te toca",
    "normalized_text":"te toca",
    "tokens":[
      "te",
      "toca"
    ],
    "token_count":2,
    "processed_text":"te toca"
  },
  {
    "label":0,
    "text":"kitten pleas pleas stop attack arm scratch get realli bad",
    "cleaned_text":"kitten pleas pleas stop attack arm scratch get realli bad",
    "normalized_text":"kitten pleas pleas stop attack arm scratch get realli bad",
    "tokens":[
      "kitten",
      "plea",
      "plea",
      "stop",
      "attack",
      "arm",
      "scratch",
      "get",
      "realli",
      "bad"
    ],
    "token_count":10,
    "processed_text":"kitten plea plea stop attack arm scratch get realli bad"
  },
  {
    "label":4,
    "text":"cat snuggl donn offic chair glad got second cat",
    "cleaned_text":"cat snuggl donn offic chair glad got second cat",
    "normalized_text":"cat snuggl donn offic chair glad got second cat",
    "tokens":[
      "cat",
      "snuggl",
      "donn",
      "offic",
      "chair",
      "glad",
      "got",
      "second",
      "cat"
    ],
    "token_count":9,
    "processed_text":"cat snuggl donn offic chair glad got second cat"
  },
  {
    "label":4,
    "text":"swagga luv ya miss like crazi man haha dont forget marc j hahaah omg wont forget promis",
    "cleaned_text":"swagga luv ya miss like crazi man haha dont forget marc j hahaah omg wont forget promis",
    "normalized_text":"swagga luv ya miss like crazi man haha dont forget marc j hahaah omg wont forget promis",
    "tokens":[
      "swagga",
      "luv",
      "ya",
      "miss",
      "like",
      "crazi",
      "man",
      "haha",
      "dont",
      "forget",
      "marc",
      "hahaah",
      "omg",
      "wont",
      "forget",
      "promi"
    ],
    "token_count":16,
    "processed_text":"swagga luv ya miss like crazi man haha dont forget marc hahaah omg wont forget promi"
  },
  {
    "label":0,
    "text":"pleeeas repli",
    "cleaned_text":"pleeeas repli",
    "normalized_text":"pleeeas repli",
    "tokens":[
      "pleeea",
      "repli"
    ],
    "token_count":2,
    "processed_text":"pleeea repli"
  },
  {
    "label":4,
    "text":"finish mac photoshoot school",
    "cleaned_text":"finish mac photoshoot school",
    "normalized_text":"finish mac photoshoot school",
    "tokens":[
      "finish",
      "mac",
      "photoshoot",
      "school"
    ],
    "token_count":4,
    "processed_text":"finish mac photoshoot school"
  },
  {
    "label":0,
    "text":"read ur messagedont care ur hous sinc gonna abl parti lol",
    "cleaned_text":"read ur messagedont care ur hous sinc gonna abl parti lol",
    "normalized_text":"read ur messagedont care ur hous sinc gonna abl parti lol",
    "tokens":[
      "read",
      "ur",
      "messagedont",
      "care",
      "ur",
      "hou",
      "sinc",
      "gon",
      "na",
      "abl",
      "parti",
      "lol"
    ],
    "token_count":12,
    "processed_text":"read ur messagedont care ur hou sinc gon na abl parti lol"
  },
  {
    "label":4,
    "text":"realis quiet today twitter front work stuff realli get way websit dont build eat",
    "cleaned_text":"realis quiet today twitter front work stuff realli get way websit dont build eat",
    "normalized_text":"realis quiet today twitter front work stuff realli get way websit dont build eat",
    "tokens":[
      "reali",
      "quiet",
      "today",
      "twitter",
      "front",
      "work",
      "stuff",
      "realli",
      "get",
      "way",
      "websit",
      "dont",
      "build",
      "eat"
    ],
    "token_count":14,
    "processed_text":"reali quiet today twitter front work stuff realli get way websit dont build eat"
  },
  {
    "label":0,
    "text":"fk one best apart bcn got taken",
    "cleaned_text":"fk one best apart bcn got taken",
    "normalized_text":"fk one best apart bcn got taken",
    "tokens":[
      "fk",
      "one",
      "best",
      "apart",
      "bcn",
      "got",
      "taken"
    ],
    "token_count":7,
    "processed_text":"fk one best apart bcn got taken"
  },
  {
    "label":4,
    "text":"go bed peopl",
    "cleaned_text":"go bed peopl",
    "normalized_text":"go bed peopl",
    "tokens":[
      "go",
      "bed",
      "peopl"
    ],
    "token_count":3,
    "processed_text":"go bed peopl"
  },
  {
    "label":0,
    "text":"though",
    "cleaned_text":"though",
    "normalized_text":"though",
    "tokens":[
      "though"
    ],
    "token_count":1,
    "processed_text":"though"
  },
  {
    "label":0,
    "text":"hahahaha shame theyr expens",
    "cleaned_text":"hahahaha shame theyr expens",
    "normalized_text":"hahahaha shame theyr expens",
    "tokens":[
      "hahahaha",
      "shame",
      "theyr",
      "expen"
    ],
    "token_count":4,
    "processed_text":"hahahaha shame theyr expen"
  },
  {
    "label":4,
    "text":"puttin jammi tryna decid im gonna go sleep",
    "cleaned_text":"puttin jammi tryna decid im gonna go sleep",
    "normalized_text":"puttin jammi tryna decid im gonna go sleep",
    "tokens":[
      "puttin",
      "jammi",
      "tryna",
      "decid",
      "im",
      "gon",
      "na",
      "go",
      "sleep"
    ],
    "token_count":9,
    "processed_text":"puttin jammi tryna decid im gon na go sleep"
  },
  {
    "label":0,
    "text":"lol glad hear jealou bitch work still get want fire",
    "cleaned_text":"lol glad hear jealou bitch work still get want fire",
    "normalized_text":"lol glad hear jealou bitch work still get want fire",
    "tokens":[
      "lol",
      "glad",
      "hear",
      "jealou",
      "bitch",
      "work",
      "still",
      "get",
      "want",
      "fire"
    ],
    "token_count":10,
    "processed_text":"lol glad hear jealou bitch work still get want fire"
  },
  {
    "label":4,
    "text":"cant wait tonightmi new pc get home work",
    "cleaned_text":"cant wait tonightmi new pc get home work",
    "normalized_text":"cant wait tonightmi new pc get home work",
    "tokens":[
      "cant",
      "wait",
      "tonightmi",
      "new",
      "pc",
      "get",
      "home",
      "work"
    ],
    "token_count":8,
    "processed_text":"cant wait tonightmi new pc get home work"
  },
  {
    "label":0,
    "text":"srri best friend wasnt church today",
    "cleaned_text":"srri best friend wasnt church today",
    "normalized_text":"srri best friend wasnt church today",
    "tokens":[
      "srri",
      "best",
      "friend",
      "wasnt",
      "church",
      "today"
    ],
    "token_count":6,
    "processed_text":"srri best friend wasnt church today"
  },
  {
    "label":0,
    "text":"im bad fan didnt tragic mother didnt want take",
    "cleaned_text":"im bad fan didnt tragic mother didnt want take",
    "normalized_text":"im bad fan didnt tragic mother didnt want take",
    "tokens":[
      "im",
      "bad",
      "fan",
      "didnt",
      "tragic",
      "mother",
      "didnt",
      "want",
      "take"
    ],
    "token_count":9,
    "processed_text":"im bad fan didnt tragic mother didnt want take"
  },
  {
    "label":4,
    "text":"gonna attend mass email sendspac ym send fail thank",
    "cleaned_text":"gonna attend mass email sendspac ym send fail thank",
    "normalized_text":"gonna attend mass email sendspac ym send fail thank",
    "tokens":[
      "gon",
      "na",
      "attend",
      "mass",
      "email",
      "sendspac",
      "ym",
      "send",
      "fail",
      "thank"
    ],
    "token_count":10,
    "processed_text":"gon na attend mass email sendspac ym send fail thank"
  },
  {
    "label":4,
    "text":"im actual virgo leo day time birthday",
    "cleaned_text":"im actual virgo leo day time birthday",
    "normalized_text":"im actual virgo leo day time birthday",
    "tokens":[
      "im",
      "actual",
      "virgo",
      "leo",
      "day",
      "time",
      "birthday"
    ],
    "token_count":7,
    "processed_text":"im actual virgo leo day time birthday"
  },
  {
    "label":4,
    "text":"thank jessi",
    "cleaned_text":"thank jessi",
    "normalized_text":"thank jessi",
    "tokens":[
      "thank",
      "jessi"
    ],
    "token_count":2,
    "processed_text":"thank jessi"
  },
  {
    "label":4,
    "text":"actual stuck georgia",
    "cleaned_text":"actual stuck georgia",
    "normalized_text":"actual stuck georgia",
    "tokens":[
      "actual",
      "stuck",
      "georgia"
    ],
    "token_count":3,
    "processed_text":"actual stuck georgia"
  },
  {
    "label":0,
    "text":"nice amp beauti morningtoo bad work almost day",
    "cleaned_text":"nice amp beauti morningtoo bad work almost day",
    "normalized_text":"nice amp beauti morningtoo bad work almost day",
    "tokens":[
      "nice",
      "amp",
      "beauti",
      "morningtoo",
      "bad",
      "work",
      "almost",
      "day"
    ],
    "token_count":8,
    "processed_text":"nice amp beauti morningtoo bad work almost day"
  },
  {
    "label":4,
    "text":"old jedi funni",
    "cleaned_text":"old jedi funni",
    "normalized_text":"old jedi funni",
    "tokens":[
      "old",
      "jedi",
      "funni"
    ],
    "token_count":3,
    "processed_text":"old jedi funni"
  },
  {
    "label":4,
    "text":"noooooooooooo max go church wifey video get finish done church dont like",
    "cleaned_text":"noooooooooooo max go church wifey video get finish done church dont like",
    "normalized_text":"noooooooooooo max go church wifey video get finish done church dont like",
    "tokens":[
      "noooooooooooo",
      "max",
      "go",
      "church",
      "wifey",
      "video",
      "get",
      "finish",
      "done",
      "church",
      "dont",
      "like"
    ],
    "token_count":12,
    "processed_text":"noooooooooooo max go church wifey video get finish done church dont like"
  },
  {
    "label":0,
    "text":"omg fall asleep wait rental got im lame",
    "cleaned_text":"omg fall asleep wait rental got im lame",
    "normalized_text":"omg fall asleep wait rental got im lame",
    "tokens":[
      "omg",
      "fall",
      "asleep",
      "wait",
      "rental",
      "got",
      "im",
      "lame"
    ],
    "token_count":8,
    "processed_text":"omg fall asleep wait rental got im lame"
  },
  {
    "label":0,
    "text":"woke upp luke work make egg",
    "cleaned_text":"woke upp luke work make egg",
    "normalized_text":"woke upp luke work make egg",
    "tokens":[
      "woke",
      "upp",
      "luke",
      "work",
      "make",
      "egg"
    ],
    "token_count":6,
    "processed_text":"woke upp luke work make egg"
  },
  {
    "label":0,
    "text":"cant sleep feel lone need cuddl insomniac buddi hate alon night",
    "cleaned_text":"cant sleep feel lone need cuddl insomniac buddi hate alon night",
    "normalized_text":"cant sleep feel lone need cuddl insomniac buddi hate alon night",
    "tokens":[
      "cant",
      "sleep",
      "feel",
      "lone",
      "need",
      "cuddl",
      "insomniac",
      "buddi",
      "hate",
      "alon",
      "night"
    ],
    "token_count":11,
    "processed_text":"cant sleep feel lone need cuddl insomniac buddi hate alon night"
  },
  {
    "label":4,
    "text":"pleas pleas vote pictur websit realli want new macbook thank",
    "cleaned_text":"pleas pleas vote pictur websit realli want new macbook thank",
    "normalized_text":"pleas pleas vote pictur websit realli want new macbook thank",
    "tokens":[
      "plea",
      "plea",
      "vote",
      "pictur",
      "websit",
      "realli",
      "want",
      "new",
      "macbook",
      "thank"
    ],
    "token_count":10,
    "processed_text":"plea plea vote pictur websit realli want new macbook thank"
  },
  {
    "label":4,
    "text":"thank much answer question help find look",
    "cleaned_text":"thank much answer question help find look",
    "normalized_text":"thank much answer question help find look",
    "tokens":[
      "thank",
      "much",
      "answer",
      "question",
      "help",
      "find",
      "look"
    ],
    "token_count":7,
    "processed_text":"thank much answer question help find look"
  },
  {
    "label":4,
    "text":"im bore extrem bore car wait dad dinner chines yummm",
    "cleaned_text":"im bore extrem bore car wait dad dinner chines yummm",
    "normalized_text":"im bore extrem bore car wait dad dinner chines yummm",
    "tokens":[
      "im",
      "bore",
      "extrem",
      "bore",
      "car",
      "wait",
      "dad",
      "dinner",
      "chine",
      "yummm"
    ],
    "token_count":10,
    "processed_text":"im bore extrem bore car wait dad dinner chine yummm"
  },
  {
    "label":0,
    "text":"okc weather your make song play head",
    "cleaned_text":"okc weather your make song play head",
    "normalized_text":"okc weather your make song play head",
    "tokens":[
      "okc",
      "weather",
      "make",
      "song",
      "play",
      "head"
    ],
    "token_count":6,
    "processed_text":"okc weather make song play head"
  },
  {
    "label":4,
    "text":"yep learningampteachingi love life amaz day",
    "cleaned_text":"yep learningampteachingi love life amaz day",
    "normalized_text":"yep learningampteachingi love life amaz day",
    "tokens":[
      "yep",
      "love",
      "life",
      "amaz",
      "day"
    ],
    "token_count":5,
    "processed_text":"yep love life amaz day"
  },
  {
    "label":4,
    "text":"record first song hope step sing x",
    "cleaned_text":"record first song hope step sing x",
    "normalized_text":"record first song hope step sing x",
    "tokens":[
      "record",
      "first",
      "song",
      "hope",
      "step",
      "sing"
    ],
    "token_count":6,
    "processed_text":"record first song hope step sing"
  },
  {
    "label":4,
    "text":"noth realli help write ot talk get right way",
    "cleaned_text":"noth realli help write ot talk get right way",
    "normalized_text":"noth realli help write ot talk get right way",
    "tokens":[
      "noth",
      "realli",
      "help",
      "write",
      "ot",
      "talk",
      "get",
      "right",
      "way"
    ],
    "token_count":9,
    "processed_text":"noth realli help write ot talk get right way"
  },
  {
    "label":4,
    "text":"he back assist coach hell make good one imo",
    "cleaned_text":"he back assist coach hell make good one imo",
    "normalized_text":"he back assist coach hell make good one imo",
    "tokens":[
      "back",
      "assist",
      "coach",
      "hell",
      "make",
      "good",
      "one",
      "imo"
    ],
    "token_count":8,
    "processed_text":"back assist coach hell make good one imo"
  },
  {
    "label":0,
    "text":"awww im sorri im defin gonna miss u",
    "cleaned_text":"awww im sorri im defin gonna miss u",
    "normalized_text":"awww im sorri im defin gonna miss u",
    "tokens":[
      "awww",
      "im",
      "sorri",
      "im",
      "defin",
      "gon",
      "na",
      "miss"
    ],
    "token_count":8,
    "processed_text":"awww im sorri im defin gon na miss"
  },
  {
    "label":0,
    "text":"git job",
    "cleaned_text":"git job",
    "normalized_text":"git job",
    "tokens":[
      "git",
      "job"
    ],
    "token_count":2,
    "processed_text":"git job"
  },
  {
    "label":0,
    "text":"error hope work later",
    "cleaned_text":"error hope work later",
    "normalized_text":"error hope work later",
    "tokens":[
      "error",
      "hope",
      "work",
      "later"
    ],
    "token_count":4,
    "processed_text":"error hope work later"
  },
  {
    "label":4,
    "text":"headi aroma bake bread fresh coffe waft direct yum",
    "cleaned_text":"headi aroma bake bread fresh coffe waft direct yum",
    "normalized_text":"headi aroma bake bread fresh coffe waft direct yum",
    "tokens":[
      "headi",
      "aroma",
      "bake",
      "bread",
      "fresh",
      "coff",
      "waft",
      "direct",
      "yum"
    ],
    "token_count":9,
    "processed_text":"headi aroma bake bread fresh coff waft direct yum"
  },
  {
    "label":0,
    "text":"cooliesit fair us get discount dont",
    "cleaned_text":"cooliesit fair us get discount dont",
    "normalized_text":"cooliesit fair us get discount dont",
    "tokens":[
      "cooliesit",
      "fair",
      "us",
      "get",
      "discount",
      "dont"
    ],
    "token_count":6,
    "processed_text":"cooliesit fair us get discount dont"
  },
  {
    "label":4,
    "text":"thanx cyber love randomsfeel free join love fest accept hug",
    "cleaned_text":"thanx cyber love randomsfeel free join love fest accept hug",
    "normalized_text":"thanx cyber love randomsfeel free join love fest accept hug",
    "tokens":[
      "thanx",
      "cyber",
      "love",
      "randomsfeel",
      "free",
      "join",
      "love",
      "fest",
      "accept",
      "hug"
    ],
    "token_count":10,
    "processed_text":"thanx cyber love randomsfeel free join love fest accept hug"
  },
  {
    "label":4,
    "text":"followfriday great stock market info thank good morn",
    "cleaned_text":"followfriday great stock market info thank good morn",
    "normalized_text":"followfriday great stock market info thank good morn",
    "tokens":[
      "followfriday",
      "great",
      "stock",
      "market",
      "info",
      "thank",
      "good",
      "morn"
    ],
    "token_count":8,
    "processed_text":"followfriday great stock market info thank good morn"
  },
  {
    "label":0,
    "text":"make trip bay area wknd visit husband gparent mean need get laundri earlier boo busi week",
    "cleaned_text":"make trip bay area wknd visit husband gparent mean need get laundri earlier boo busi week",
    "normalized_text":"make trip bay area wknd visit husband gparent mean need get laundri earlier boo busi week",
    "tokens":[
      "make",
      "trip",
      "bay",
      "area",
      "wknd",
      "visit",
      "husband",
      "gparent",
      "mean",
      "need",
      "get",
      "laundri",
      "earlier",
      "boo",
      "busi",
      "week"
    ],
    "token_count":16,
    "processed_text":"make trip bay area wknd visit husband gparent mean need get laundri earlier boo busi week"
  },
  {
    "label":0,
    "text":"left librari amp book want read r check pooie",
    "cleaned_text":"left librari amp book want read r check pooie",
    "normalized_text":"left librari amp book want read r check pooie",
    "tokens":[
      "left",
      "librari",
      "amp",
      "book",
      "want",
      "read",
      "check",
      "pooie"
    ],
    "token_count":8,
    "processed_text":"left librari amp book want read check pooie"
  },
  {
    "label":0,
    "text":"wont see fox afghan children left die us bomb",
    "cleaned_text":"wont see fox afghan children left die us bomb",
    "normalized_text":"wont see fox afghan children left die us bomb",
    "tokens":[
      "wont",
      "see",
      "fox",
      "afghan",
      "children",
      "left",
      "die",
      "us",
      "bomb"
    ],
    "token_count":9,
    "processed_text":"wont see fox afghan children left die us bomb"
  },
  {
    "label":4,
    "text":"um suck your crazi lol",
    "cleaned_text":"um suck your crazi lol",
    "normalized_text":"um suck your crazi lol",
    "tokens":[
      "um",
      "suck",
      "crazi",
      "lol"
    ],
    "token_count":4,
    "processed_text":"um suck crazi lol"
  },
  {
    "label":0,
    "text":"hope didnt make even upset",
    "cleaned_text":"hope didnt make even upset",
    "normalized_text":"hope didnt make even upset",
    "tokens":[
      "hope",
      "didnt",
      "make",
      "even",
      "upset"
    ],
    "token_count":5,
    "processed_text":"hope didnt make even upset"
  },
  {
    "label":0,
    "text":"flat screen broken",
    "cleaned_text":"flat screen broken",
    "normalized_text":"flat screen broken",
    "tokens":[
      "flat",
      "screen",
      "broken"
    ],
    "token_count":3,
    "processed_text":"flat screen broken"
  },
  {
    "label":0,
    "text":"anyon know make pic smaller cant load one",
    "cleaned_text":"anyon know make pic smaller cant load one",
    "normalized_text":"anyon know make pic smaller cant load one",
    "tokens":[
      "anyon",
      "know",
      "make",
      "pic",
      "smaller",
      "cant",
      "load",
      "one"
    ],
    "token_count":8,
    "processed_text":"anyon know make pic smaller cant load one"
  },
  {
    "label":4,
    "text":"oh fantast set film robin great choic love funni great guy",
    "cleaned_text":"oh fantast set film robin great choic love funni great guy",
    "normalized_text":"oh fantast set film robin great choic love funni great guy",
    "tokens":[
      "oh",
      "fantast",
      "set",
      "film",
      "robin",
      "great",
      "choic",
      "love",
      "funni",
      "great",
      "guy"
    ],
    "token_count":11,
    "processed_text":"oh fantast set film robin great choic love funni great guy"
  },
  {
    "label":4,
    "text":"basketbal go becom realli famou someday though know right",
    "cleaned_text":"basketbal go becom realli famou someday though know right",
    "normalized_text":"basketbal go becom realli famou someday though know right",
    "tokens":[
      "basketb",
      "go",
      "becom",
      "realli",
      "famou",
      "someday",
      "though",
      "know",
      "right"
    ],
    "token_count":9,
    "processed_text":"basketb go becom realli famou someday though know right"
  },
  {
    "label":0,
    "text":"plan beauti day studi",
    "cleaned_text":"plan beauti day studi",
    "normalized_text":"plan beauti day studi",
    "tokens":[
      "plan",
      "beauti",
      "day",
      "studi"
    ],
    "token_count":4,
    "processed_text":"plan beauti day studi"
  },
  {
    "label":0,
    "text":"hell freak weekend even yet",
    "cleaned_text":"hell freak weekend even yet",
    "normalized_text":"hell freak weekend even yet",
    "tokens":[
      "hell",
      "freak",
      "weekend",
      "even",
      "yet"
    ],
    "token_count":5,
    "processed_text":"hell freak weekend even yet"
  },
  {
    "label":4,
    "text":"post morn",
    "cleaned_text":"post morn",
    "normalized_text":"post morn",
    "tokens":[
      "post",
      "morn"
    ],
    "token_count":2,
    "processed_text":"post morn"
  },
  {
    "label":0,
    "text":"moment silenc celtic",
    "cleaned_text":"moment silenc celtic",
    "normalized_text":"moment silenc celtic",
    "tokens":[
      "moment",
      "silenc",
      "celtic"
    ],
    "token_count":3,
    "processed_text":"moment silenc celtic"
  },
  {
    "label":0,
    "text":"bore studi realli anyon got idea studi",
    "cleaned_text":"bore studi realli anyon got idea studi",
    "normalized_text":"bore studi realli anyon got idea studi",
    "tokens":[
      "bore",
      "studi",
      "realli",
      "anyon",
      "got",
      "idea",
      "studi"
    ],
    "token_count":7,
    "processed_text":"bore studi realli anyon got idea studi"
  },
  {
    "label":4,
    "text":"got back palm gelli amp nessa saw night museum love movi rock",
    "cleaned_text":"got back palm gelli amp nessa saw night museum love movi rock",
    "normalized_text":"got back palm gelli amp nessa saw night museum love movi rock",
    "tokens":[
      "got",
      "back",
      "palm",
      "gelli",
      "amp",
      "nessa",
      "saw",
      "night",
      "museum",
      "love",
      "movi",
      "rock"
    ],
    "token_count":12,
    "processed_text":"got back palm gelli amp nessa saw night museum love movi rock"
  },
  {
    "label":4,
    "text":"train love",
    "cleaned_text":"train love",
    "normalized_text":"train love",
    "tokens":[
      "train",
      "love"
    ],
    "token_count":2,
    "processed_text":"train love"
  },
  {
    "label":0,
    "text":"realli miss time share famili ill never forget memori",
    "cleaned_text":"realli miss time share famili ill never forget memori",
    "normalized_text":"realli miss time share famili ill never forget memori",
    "tokens":[
      "realli",
      "miss",
      "time",
      "share",
      "famili",
      "ill",
      "never",
      "forget",
      "memori"
    ],
    "token_count":9,
    "processed_text":"realli miss time share famili ill never forget memori"
  },
  {
    "label":4,
    "text":"your cool hahah pleas repli would cool",
    "cleaned_text":"your cool hahah pleas repli would cool",
    "normalized_text":"your cool hahah pleas repli would cool",
    "tokens":[
      "cool",
      "hahah",
      "plea",
      "repli",
      "cool"
    ],
    "token_count":5,
    "processed_text":"cool hahah plea repli cool"
  },
  {
    "label":0,
    "text":"nat im go miss badli",
    "cleaned_text":"nat im go miss badli",
    "normalized_text":"nat im go miss badli",
    "tokens":[
      "nat",
      "im",
      "go",
      "miss",
      "badli"
    ],
    "token_count":5,
    "processed_text":"nat im go miss badli"
  },
  {
    "label":4,
    "text":"yeah know",
    "cleaned_text":"yeah know",
    "normalized_text":"yeah know",
    "tokens":[
      "yeah",
      "know"
    ],
    "token_count":2,
    "processed_text":"yeah know"
  },
  {
    "label":0,
    "text":"wow kid realli fuck stupid sometim poor puppi",
    "cleaned_text":"wow kid realli fuck stupid sometim poor puppi",
    "normalized_text":"wow kid realli fuck stupid sometim poor puppi",
    "tokens":[
      "wow",
      "kid",
      "realli",
      "fuck",
      "stupid",
      "sometim",
      "poor",
      "puppi"
    ],
    "token_count":8,
    "processed_text":"wow kid realli fuck stupid sometim poor puppi"
  },
  {
    "label":4,
    "text":"got love picturetak moment x",
    "cleaned_text":"got love picturetak moment x",
    "normalized_text":"got love picturetak moment x",
    "tokens":[
      "got",
      "love",
      "picturetak",
      "moment"
    ],
    "token_count":4,
    "processed_text":"got love picturetak moment"
  },
  {
    "label":0,
    "text":"im soooo sad right",
    "cleaned_text":"im soooo sad right",
    "normalized_text":"im soooo sad right",
    "tokens":[
      "im",
      "soooo",
      "sad",
      "right"
    ],
    "token_count":4,
    "processed_text":"im soooo sad right"
  },
  {
    "label":0,
    "text":"duuud ring",
    "cleaned_text":"duuud ring",
    "normalized_text":"duuud ring",
    "tokens":[
      "duuud",
      "ring"
    ],
    "token_count":2,
    "processed_text":"duuud ring"
  },
  {
    "label":4,
    "text":"ah well miss cazzzl long",
    "cleaned_text":"ah well miss cazzzl long",
    "normalized_text":"ah well miss cazzzl long",
    "tokens":[
      "ah",
      "well",
      "miss",
      "cazzzl",
      "long"
    ],
    "token_count":5,
    "processed_text":"ah well miss cazzzl long"
  },
  {
    "label":0,
    "text":"chill chillin rain",
    "cleaned_text":"chill chillin rain",
    "normalized_text":"chill chillin rain",
    "tokens":[
      "chill",
      "chillin",
      "rain"
    ],
    "token_count":3,
    "processed_text":"chill chillin rain"
  },
  {
    "label":4,
    "text":"ha ill call franki work nighter movi set hard sleep",
    "cleaned_text":"ha ill call franki work nighter movi set hard sleep",
    "normalized_text":"ha ill call franki work nighter movi set hard sleep",
    "tokens":[
      "ha",
      "ill",
      "call",
      "franki",
      "work",
      "nighter",
      "movi",
      "set",
      "hard",
      "sleep"
    ],
    "token_count":10,
    "processed_text":"ha ill call franki work nighter movi set hard sleep"
  },
  {
    "label":0,
    "text":"tell",
    "cleaned_text":"tell",
    "normalized_text":"tell",
    "tokens":[
      "tell"
    ],
    "token_count":1,
    "processed_text":"tell"
  },
  {
    "label":4,
    "text":"hello sound good count follow",
    "cleaned_text":"hello sound good count follow",
    "normalized_text":"hello sound good count follow",
    "tokens":[
      "hello",
      "sound",
      "good",
      "count",
      "follow"
    ],
    "token_count":5,
    "processed_text":"hello sound good count follow"
  },
  {
    "label":4,
    "text":"im im go parad",
    "cleaned_text":"im im go parad",
    "normalized_text":"im im go parad",
    "tokens":[
      "im",
      "im",
      "go",
      "parad"
    ],
    "token_count":4,
    "processed_text":"im im go parad"
  },
  {
    "label":4,
    "text":"everyon love soo much",
    "cleaned_text":"everyon love soo much",
    "normalized_text":"everyon love soo much",
    "tokens":[
      "everyon",
      "love",
      "soo",
      "much"
    ],
    "token_count":4,
    "processed_text":"everyon love soo much"
  },
  {
    "label":0,
    "text":"titan love movi sad",
    "cleaned_text":"titan love movi sad",
    "normalized_text":"titan love movi sad",
    "tokens":[
      "titan",
      "love",
      "movi",
      "sad"
    ],
    "token_count":4,
    "processed_text":"titan love movi sad"
  },
  {
    "label":4,
    "text":"go movi mom dad haha im lame",
    "cleaned_text":"go movi mom dad haha im lame",
    "normalized_text":"go movi mom dad haha im lame",
    "tokens":[
      "go",
      "movi",
      "mom",
      "dad",
      "haha",
      "im",
      "lame"
    ],
    "token_count":7,
    "processed_text":"go movi mom dad haha im lame"
  },
  {
    "label":0,
    "text":"majorett show tomorrow fuck im gonna tire consid perform hour long hour",
    "cleaned_text":"majorett show tomorrow fuck im gonna tire consid perform hour long hour",
    "normalized_text":"majorett show tomorrow fuck im gonna tire consid perform hour long hour",
    "tokens":[
      "majorett",
      "show",
      "tomorrow",
      "fuck",
      "im",
      "gon",
      "na",
      "tire",
      "consid",
      "perform",
      "hour",
      "long",
      "hour"
    ],
    "token_count":13,
    "processed_text":"majorett show tomorrow fuck im gon na tire consid perform hour long hour"
  },
  {
    "label":0,
    "text":"think may lost job fml",
    "cleaned_text":"think may lost job fml",
    "normalized_text":"think may lost job fml",
    "tokens":[
      "think",
      "may",
      "lost",
      "job",
      "fml"
    ],
    "token_count":5,
    "processed_text":"think may lost job fml"
  },
  {
    "label":0,
    "text":"sick still drive work grab laptop could work home",
    "cleaned_text":"sick still drive work grab laptop could work home",
    "normalized_text":"sick still drive work grab laptop could work home",
    "tokens":[
      "sick",
      "still",
      "drive",
      "work",
      "grab",
      "laptop",
      "work",
      "home"
    ],
    "token_count":8,
    "processed_text":"sick still drive work grab laptop work home"
  },
  {
    "label":4,
    "text":"allo watch friend eat cheerio sunday alway bore day",
    "cleaned_text":"allo watch friend eat cheerio sunday alway bore day",
    "normalized_text":"allo watch friend eat cheerio sunday alway bore day",
    "tokens":[
      "allo",
      "watch",
      "friend",
      "eat",
      "cheerio",
      "sunday",
      "alway",
      "bore",
      "day"
    ],
    "token_count":9,
    "processed_text":"allo watch friend eat cheerio sunday alway bore day"
  },
  {
    "label":0,
    "text":"jon amp kate ish controli sick itfeel bad prbli gonna tell allison cant watch anymor",
    "cleaned_text":"jon amp kate ish controli sick itfeel bad prbli gonna tell allison cant watch anymor",
    "normalized_text":"jon amp kate ish controli sick itfeel bad prbli gonna tell allison cant watch anymor",
    "tokens":[
      "jon",
      "amp",
      "kate",
      "ish",
      "controli",
      "sick",
      "itfeel",
      "bad",
      "prbli",
      "gon",
      "na",
      "tell",
      "allison",
      "cant",
      "watch",
      "anymor"
    ],
    "token_count":16,
    "processed_text":"jon amp kate ish controli sick itfeel bad prbli gon na tell allison cant watch anymor"
  },
  {
    "label":0,
    "text":"ugghmi poor puppi still limp acl surgeri poor littl girl",
    "cleaned_text":"ugghmi poor puppi still limp acl surgeri poor littl girl",
    "normalized_text":"ugghmi poor puppi still limp acl surgeri poor littl girl",
    "tokens":[
      "ugghmi",
      "poor",
      "puppi",
      "still",
      "limp",
      "acl",
      "surgeri",
      "poor",
      "littl",
      "girl"
    ],
    "token_count":10,
    "processed_text":"ugghmi poor puppi still limp acl surgeri poor littl girl"
  },
  {
    "label":4,
    "text":"man saturday night tv bore gonna go read",
    "cleaned_text":"man saturday night tv bore gonna go read",
    "normalized_text":"man saturday night tv bore gonna go read",
    "tokens":[
      "man",
      "saturday",
      "night",
      "tv",
      "bore",
      "gon",
      "na",
      "go",
      "read"
    ],
    "token_count":9,
    "processed_text":"man saturday night tv bore gon na go read"
  },
  {
    "label":0,
    "text":"peopl txting sorri phone dead cant resurrect hope verizon replac ill back tonight",
    "cleaned_text":"peopl txting sorri phone dead cant resurrect hope verizon replac ill back tonight",
    "normalized_text":"peopl txting sorri phone dead cant resurrect hope verizon replac ill back tonight",
    "tokens":[
      "peopl",
      "txting",
      "sorri",
      "phone",
      "dead",
      "cant",
      "resurrect",
      "hope",
      "verizon",
      "replac",
      "ill",
      "back",
      "tonight"
    ],
    "token_count":13,
    "processed_text":"peopl txting sorri phone dead cant resurrect hope verizon replac ill back tonight"
  },
  {
    "label":4,
    "text":"one never mani gadget",
    "cleaned_text":"one never mani gadget",
    "normalized_text":"one never mani gadget",
    "tokens":[
      "one",
      "never",
      "mani",
      "gadget"
    ],
    "token_count":4,
    "processed_text":"one never mani gadget"
  },
  {
    "label":0,
    "text":"site sale time finish price minimum",
    "cleaned_text":"site sale time finish price minimum",
    "normalized_text":"site sale time finish price minimum",
    "tokens":[
      "site",
      "sale",
      "time",
      "finish",
      "price",
      "minimum"
    ],
    "token_count":6,
    "processed_text":"site sale time finish price minimum"
  },
  {
    "label":0,
    "text":"hope go date plan leav jacksonvil came earli",
    "cleaned_text":"hope go date plan leav jacksonvil came earli",
    "normalized_text":"hope go date plan leav jacksonvil came earli",
    "tokens":[
      "hope",
      "go",
      "date",
      "plan",
      "leav",
      "jacksonvil",
      "came",
      "earli"
    ],
    "token_count":8,
    "processed_text":"hope go date plan leav jacksonvil came earli"
  },
  {
    "label":4,
    "text":"love georg sampson last night sooo well he get fitter everi dayi yummm",
    "cleaned_text":"love georg sampson last night sooo well he get fitter everi dayi yummm",
    "normalized_text":"love georg sampson last night sooo well he get fitter everi dayi yummm",
    "tokens":[
      "love",
      "georg",
      "sampson",
      "last",
      "night",
      "sooo",
      "well",
      "get",
      "fitter",
      "everi",
      "dayi",
      "yummm"
    ],
    "token_count":12,
    "processed_text":"love georg sampson last night sooo well get fitter everi dayi yummm"
  },
  {
    "label":0,
    "text":"ahhhh wanna enjoy welllllll yea bad weather",
    "cleaned_text":"ahhhh wanna enjoy welllllll yea bad weather",
    "normalized_text":"ahhhh wanna enjoy welllllll yea bad weather",
    "tokens":[
      "ahhhh",
      "wan",
      "na",
      "enjoy",
      "welllllll",
      "yea",
      "bad",
      "weather"
    ],
    "token_count":8,
    "processed_text":"ahhhh wan na enjoy welllllll yea bad weather"
  },
  {
    "label":4,
    "text":"awww rock damn hard super grin action go",
    "cleaned_text":"awww rock damn hard super grin action go",
    "normalized_text":"awww rock damn hard super grin action go",
    "tokens":[
      "awww",
      "rock",
      "damn",
      "hard",
      "super",
      "grin",
      "action",
      "go"
    ],
    "token_count":8,
    "processed_text":"awww rock damn hard super grin action go"
  },
  {
    "label":4,
    "text":"still show great bet",
    "cleaned_text":"still show great bet",
    "normalized_text":"still show great bet",
    "tokens":[
      "still",
      "show",
      "great",
      "bet"
    ],
    "token_count":4,
    "processed_text":"still show great bet"
  },
  {
    "label":0,
    "text":"doc appt afternoon shoulder still sore tho much hope",
    "cleaned_text":"doc appt afternoon shoulder still sore tho much hope",
    "normalized_text":"doc appt afternoon shoulder still sore tho much hope",
    "tokens":[
      "doc",
      "appt",
      "afternoon",
      "shoulder",
      "still",
      "sore",
      "tho",
      "much",
      "hope"
    ],
    "token_count":9,
    "processed_text":"doc appt afternoon shoulder still sore tho much hope"
  },
  {
    "label":0,
    "text":"hahahaha yeah big time joke asid old partner crazi",
    "cleaned_text":"hahahaha yeah big time joke asid old partner crazi",
    "normalized_text":"hahahaha yeah big time joke asid old partner crazi",
    "tokens":[
      "hahahaha",
      "yeah",
      "big",
      "time",
      "joke",
      "asid",
      "old",
      "partner",
      "crazi"
    ],
    "token_count":9,
    "processed_text":"hahahaha yeah big time joke asid old partner crazi"
  },
  {
    "label":4,
    "text":"sure thing ill second latt honor",
    "cleaned_text":"sure thing ill second latt honor",
    "normalized_text":"sure thing ill second latt honor",
    "tokens":[
      "sure",
      "thing",
      "ill",
      "second",
      "latt",
      "honor"
    ],
    "token_count":6,
    "processed_text":"sure thing ill second latt honor"
  },
  {
    "label":4,
    "text":"awesom birthday morn final watch entir season dexter get haircut expens one thank dad",
    "cleaned_text":"awesom birthday morn final watch entir season dexter get haircut expens one thank dad",
    "normalized_text":"awesom birthday morn final watch entir season dexter get haircut expens one thank dad",
    "tokens":[
      "awesom",
      "birthday",
      "morn",
      "final",
      "watch",
      "entir",
      "season",
      "dexter",
      "get",
      "haircut",
      "expen",
      "one",
      "thank",
      "dad"
    ],
    "token_count":14,
    "processed_text":"awesom birthday morn final watch entir season dexter get haircut expen one thank dad"
  },
  {
    "label":0,
    "text":"nashvil wonder pa suck",
    "cleaned_text":"nashvil wonder pa suck",
    "normalized_text":"nashvil wonder pa suck",
    "tokens":[
      "nashvil",
      "wonder",
      "pa",
      "suck"
    ],
    "token_count":4,
    "processed_text":"nashvil wonder pa suck"
  },
  {
    "label":0,
    "text":"didnt get go choic either cigarett catfood priestess ticket",
    "cleaned_text":"didnt get go choic either cigarett catfood priestess ticket",
    "normalized_text":"didnt get go choic either cigarett catfood priestess ticket",
    "tokens":[
      "didnt",
      "get",
      "go",
      "choic",
      "either",
      "cigarett",
      "catfood",
      "priestess",
      "ticket"
    ],
    "token_count":9,
    "processed_text":"didnt get go choic either cigarett catfood priestess ticket"
  },
  {
    "label":0,
    "text":"still bake keep away kathi slowli die miss zachari",
    "cleaned_text":"still bake keep away kathi slowli die miss zachari",
    "normalized_text":"still bake keep away kathi slowli die miss zachari",
    "tokens":[
      "still",
      "bake",
      "keep",
      "away",
      "kathi",
      "slowli",
      "die",
      "miss",
      "zachari"
    ],
    "token_count":9,
    "processed_text":"still bake keep away kathi slowli die miss zachari"
  },
  {
    "label":0,
    "text":"hit squirrel felt bad",
    "cleaned_text":"hit squirrel felt bad",
    "normalized_text":"hit squirrel felt bad",
    "tokens":[
      "hit",
      "squirrel",
      "felt",
      "bad"
    ],
    "token_count":4,
    "processed_text":"hit squirrel felt bad"
  },
  {
    "label":4,
    "text":"pleas get shot",
    "cleaned_text":"pleas get shot",
    "normalized_text":"pleas get shot",
    "tokens":[
      "plea",
      "get",
      "shot"
    ],
    "token_count":3,
    "processed_text":"plea get shot"
  },
  {
    "label":4,
    "text":"wonder decid danc storm workout feel burn hmmm kinda like",
    "cleaned_text":"wonder decid danc storm workout feel burn hmmm kinda like",
    "normalized_text":"wonder decid danc storm workout feel burn hmmm kinda like",
    "tokens":[
      "wonder",
      "decid",
      "danc",
      "storm",
      "workout",
      "feel",
      "burn",
      "hmmm",
      "kinda",
      "like"
    ],
    "token_count":10,
    "processed_text":"wonder decid danc storm workout feel burn hmmm kinda like"
  },
  {
    "label":0,
    "text":"bachelorett tonight want kiptyn reid jake robbi jess make final five know that gonna happen",
    "cleaned_text":"bachelorett tonight want kiptyn reid jake robbi jess make final five know that gonna happen",
    "normalized_text":"bachelorett tonight want kiptyn reid jake robbi jess make final five know that gonna happen",
    "tokens":[
      "bachelorett",
      "tonight",
      "want",
      "kiptyn",
      "reid",
      "jake",
      "robbi",
      "jess",
      "make",
      "final",
      "five",
      "know",
      "gon",
      "na",
      "happen"
    ],
    "token_count":15,
    "processed_text":"bachelorett tonight want kiptyn reid jake robbi jess make final five know gon na happen"
  },
  {
    "label":0,
    "text":"villag sunday miss obama day",
    "cleaned_text":"villag sunday miss obama day",
    "normalized_text":"villag sunday miss obama day",
    "tokens":[
      "villag",
      "sunday",
      "miss",
      "obama",
      "day"
    ],
    "token_count":5,
    "processed_text":"villag sunday miss obama day"
  },
  {
    "label":4,
    "text":"new ghd straightner senior parti nice",
    "cleaned_text":"new ghd straightner senior parti nice",
    "normalized_text":"new ghd straightner senior parti nice",
    "tokens":[
      "new",
      "ghd",
      "straightner",
      "senior",
      "parti",
      "nice"
    ],
    "token_count":6,
    "processed_text":"new ghd straightner senior parti nice"
  },
  {
    "label":4,
    "text":"awesom day amp night man love life night",
    "cleaned_text":"awesom day amp night man love life night",
    "normalized_text":"awesom day amp night man love life night",
    "tokens":[
      "awesom",
      "day",
      "amp",
      "night",
      "man",
      "love",
      "life",
      "night"
    ],
    "token_count":8,
    "processed_text":"awesom day amp night man love life night"
  },
  {
    "label":0,
    "text":"still eat nugget though wish share",
    "cleaned_text":"still eat nugget though wish share",
    "normalized_text":"still eat nugget though wish share",
    "tokens":[
      "still",
      "eat",
      "nugget",
      "though",
      "wish",
      "share"
    ],
    "token_count":6,
    "processed_text":"still eat nugget though wish share"
  },
  {
    "label":0,
    "text":"quottilaquot pray tht ur shirt dnt sold reallli want thm",
    "cleaned_text":"quottilaquot pray tht ur shirt dnt sold reallli want thm",
    "normalized_text":"quottilaquot pray tht ur shirt dnt sold reallli want thm",
    "tokens":[
      "quottilaquot",
      "pray",
      "tht",
      "ur",
      "shirt",
      "dnt",
      "sold",
      "reallli",
      "want",
      "thm"
    ],
    "token_count":10,
    "processed_text":"quottilaquot pray tht ur shirt dnt sold reallli want thm"
  },
  {
    "label":4,
    "text":"sort thing gazebo rib oven dorset",
    "cleaned_text":"sort thing gazebo rib oven dorset",
    "normalized_text":"sort thing gazebo rib oven dorset",
    "tokens":[
      "sort",
      "thing",
      "gazebo",
      "rib",
      "oven",
      "dorset"
    ],
    "token_count":6,
    "processed_text":"sort thing gazebo rib oven dorset"
  },
  {
    "label":0,
    "text":"miss u death im go futur hous like minut dont realli time talk",
    "cleaned_text":"miss u death im go futur hous like minut dont realli time talk",
    "normalized_text":"miss u death im go futur hous like minut dont realli time talk",
    "tokens":[
      "miss",
      "death",
      "im",
      "go",
      "futur",
      "hou",
      "like",
      "minut",
      "dont",
      "realli",
      "time",
      "talk"
    ],
    "token_count":12,
    "processed_text":"miss death im go futur hou like minut dont realli time talk"
  },
  {
    "label":4,
    "text":"day sunday got toooo much stuff b tuesdaytweet ya lter",
    "cleaned_text":"day sunday got toooo much stuff b tuesdaytweet ya lter",
    "normalized_text":"day sunday got toooo much stuff b tuesdaytweet ya lter",
    "tokens":[
      "day",
      "sunday",
      "got",
      "toooo",
      "much",
      "stuff",
      "tuesdaytweet",
      "ya",
      "lter"
    ],
    "token_count":9,
    "processed_text":"day sunday got toooo much stuff tuesdaytweet ya lter"
  },
  {
    "label":4,
    "text":"gten tag decent night restwok mind",
    "cleaned_text":"gten tag decent night restwok mind",
    "normalized_text":"gten tag decent night restwok mind",
    "tokens":[
      "gten",
      "tag",
      "decent",
      "night",
      "restwok",
      "mind"
    ],
    "token_count":6,
    "processed_text":"gten tag decent night restwok mind"
  },
  {
    "label":0,
    "text":"man get freak graduat posit",
    "cleaned_text":"man get freak graduat posit",
    "normalized_text":"man get freak graduat posit",
    "tokens":[
      "man",
      "get",
      "freak",
      "graduat",
      "posit"
    ],
    "token_count":5,
    "processed_text":"man get freak graduat posit"
  },
  {
    "label":4,
    "text":"chill sofa broadcast live",
    "cleaned_text":"chill sofa broadcast live",
    "normalized_text":"chill sofa broadcast live",
    "tokens":[
      "chill",
      "sofa",
      "broadcast",
      "live"
    ],
    "token_count":4,
    "processed_text":"chill sofa broadcast live"
  },
  {
    "label":0,
    "text":"haha wish dont think chang desktop hey dream work nyc def worth",
    "cleaned_text":"haha wish dont think chang desktop hey dream work nyc def worth",
    "normalized_text":"haha wish dont think chang desktop hey dream work nyc def worth",
    "tokens":[
      "haha",
      "wish",
      "dont",
      "think",
      "chang",
      "desktop",
      "hey",
      "dream",
      "work",
      "nyc",
      "def",
      "worth"
    ],
    "token_count":12,
    "processed_text":"haha wish dont think chang desktop hey dream work nyc def worth"
  },
  {
    "label":4,
    "text":"feel bad time low new cd leak also dont want wait juli ill still buy though even",
    "cleaned_text":"feel bad time low new cd leak also dont want wait juli ill still buy though even",
    "normalized_text":"feel bad time low new cd leak also dont want wait juli ill still buy though even",
    "tokens":[
      "feel",
      "bad",
      "time",
      "low",
      "new",
      "cd",
      "leak",
      "also",
      "dont",
      "want",
      "wait",
      "juli",
      "ill",
      "still",
      "buy",
      "though",
      "even"
    ],
    "token_count":17,
    "processed_text":"feel bad time low new cd leak also dont want wait juli ill still buy though even"
  },
  {
    "label":0,
    "text":"never",
    "cleaned_text":"never",
    "normalized_text":"never",
    "tokens":[
      "never"
    ],
    "token_count":1,
    "processed_text":"never"
  },
  {
    "label":4,
    "text":"cool peopl dont know cool beacus way much think ps stop eat pizza",
    "cleaned_text":"cool peopl dont know cool beacus way much think ps stop eat pizza",
    "normalized_text":"cool peopl dont know cool beacus way much think ps stop eat pizza",
    "tokens":[
      "cool",
      "peopl",
      "dont",
      "know",
      "cool",
      "beacu",
      "way",
      "much",
      "think",
      "ps",
      "stop",
      "eat",
      "pizza"
    ],
    "token_count":13,
    "processed_text":"cool peopl dont know cool beacu way much think ps stop eat pizza"
  },
  {
    "label":0,
    "text":"still sickish second citi lesson today hope dont infect everyonelol",
    "cleaned_text":"still sickish second citi lesson today hope dont infect everyonelol",
    "normalized_text":"still sickish second citi lesson today hope dont infect everyonelol",
    "tokens":[
      "still",
      "sickish",
      "second",
      "citi",
      "lesson",
      "today",
      "hope",
      "dont",
      "infect",
      "everyonelol"
    ],
    "token_count":10,
    "processed_text":"still sickish second citi lesson today hope dont infect everyonelol"
  },
  {
    "label":0,
    "text":"lol actual tri write someth crazi cloud contest ive hate everyth ive written far",
    "cleaned_text":"lol actual tri write someth crazi cloud contest ive hate everyth ive written far",
    "normalized_text":"lol actual tri write someth crazi cloud contest ive hate everyth ive written far",
    "tokens":[
      "lol",
      "actual",
      "tri",
      "write",
      "someth",
      "crazi",
      "cloud",
      "contest",
      "ive",
      "hate",
      "everyth",
      "ive",
      "written",
      "far"
    ],
    "token_count":14,
    "processed_text":"lol actual tri write someth crazi cloud contest ive hate everyth ive written far"
  },
  {
    "label":4,
    "text":"home depot padr",
    "cleaned_text":"home depot padr",
    "normalized_text":"home depot padr",
    "tokens":[
      "home",
      "depot",
      "padr"
    ],
    "token_count":3,
    "processed_text":"home depot padr"
  },
  {
    "label":4,
    "text":"tast good tri nearli done",
    "cleaned_text":"tast good tri nearli done",
    "normalized_text":"tast good tri nearli done",
    "tokens":[
      "tast",
      "good",
      "tri",
      "nearli",
      "done"
    ],
    "token_count":5,
    "processed_text":"tast good tri nearli done"
  },
  {
    "label":0,
    "text":"ok still aint found momma wtf umm myspac facebook much easier",
    "cleaned_text":"ok still aint found momma wtf umm myspac facebook much easier",
    "normalized_text":"ok still aint found momma wtf umm myspac facebook much easier",
    "tokens":[
      "ok",
      "still",
      "aint",
      "found",
      "momma",
      "wtf",
      "umm",
      "myspac",
      "facebook",
      "much",
      "easier"
    ],
    "token_count":11,
    "processed_text":"ok still aint found momma wtf umm myspac facebook much easier"
  },
  {
    "label":4,
    "text":"look forward tomorrow first full day back beauti vancouv famili elbow room boy sushi canuck game",
    "cleaned_text":"look forward tomorrow first full day back beauti vancouv famili elbow room boy sushi canuck game",
    "normalized_text":"look forward tomorrow first full day back beauti vancouv famili elbow room boy sushi canuck game",
    "tokens":[
      "look",
      "forward",
      "tomorrow",
      "first",
      "full",
      "day",
      "back",
      "beauti",
      "vancouv",
      "famili",
      "elbow",
      "room",
      "boy",
      "sushi",
      "canuck",
      "game"
    ],
    "token_count":16,
    "processed_text":"look forward tomorrow first full day back beauti vancouv famili elbow room boy sushi canuck game"
  },
  {
    "label":4,
    "text":"miss anthoni follow back",
    "cleaned_text":"miss anthoni follow back",
    "normalized_text":"miss anthoni follow back",
    "tokens":[
      "miss",
      "anthoni",
      "follow",
      "back"
    ],
    "token_count":4,
    "processed_text":"miss anthoni follow back"
  },
  {
    "label":4,
    "text":"ep blog soon updat soon audrey take time give day sever excit stori come check",
    "cleaned_text":"ep blog soon updat soon audrey take time give day sever excit stori come check",
    "normalized_text":"ep blog soon updat soon audrey take time give day sever excit stori come check",
    "tokens":[
      "ep",
      "blog",
      "soon",
      "updat",
      "soon",
      "audrey",
      "take",
      "time",
      "give",
      "day",
      "sever",
      "excit",
      "stori",
      "come",
      "check"
    ],
    "token_count":15,
    "processed_text":"ep blog soon updat soon audrey take time give day sever excit stori come check"
  },
  {
    "label":0,
    "text":"drink one miss badli",
    "cleaned_text":"drink one miss badli",
    "normalized_text":"drink one miss badli",
    "tokens":[
      "drink",
      "one",
      "miss",
      "badli"
    ],
    "token_count":4,
    "processed_text":"drink one miss badli"
  },
  {
    "label":0,
    "text":"im go visit dad law hospit",
    "cleaned_text":"im go visit dad law hospit",
    "normalized_text":"im go visit dad law hospit",
    "tokens":[
      "im",
      "go",
      "visit",
      "dad",
      "law",
      "hospit"
    ],
    "token_count":6,
    "processed_text":"im go visit dad law hospit"
  },
  {
    "label":0,
    "text":"nice day bad im stuck work go live today enough us",
    "cleaned_text":"nice day bad im stuck work go live today enough us",
    "normalized_text":"nice day bad im stuck work go live today enough us",
    "tokens":[
      "nice",
      "day",
      "bad",
      "im",
      "stuck",
      "work",
      "go",
      "live",
      "today",
      "enough",
      "us"
    ],
    "token_count":11,
    "processed_text":"nice day bad im stuck work go live today enough us"
  },
  {
    "label":0,
    "text":"anyway im alreadi tomorrow anoth day wish better",
    "cleaned_text":"anyway im alreadi tomorrow anoth day wish better",
    "normalized_text":"anyway im alreadi tomorrow anoth day wish better",
    "tokens":[
      "anyway",
      "im",
      "alreadi",
      "tomorrow",
      "anoth",
      "day",
      "wish",
      "better"
    ],
    "token_count":8,
    "processed_text":"anyway im alreadi tomorrow anoth day wish better"
  },
  {
    "label":4,
    "text":"go guy right front gate ask give",
    "cleaned_text":"go guy right front gate ask give",
    "normalized_text":"go guy right front gate ask give",
    "tokens":[
      "go",
      "guy",
      "right",
      "front",
      "gate",
      "ask",
      "give"
    ],
    "token_count":7,
    "processed_text":"go guy right front gate ask give"
  },
  {
    "label":0,
    "text":"royal marlin got amp hand us",
    "cleaned_text":"royal marlin got amp hand us",
    "normalized_text":"royal marlin got amp hand us",
    "tokens":[
      "royal",
      "marlin",
      "got",
      "amp",
      "hand",
      "us"
    ],
    "token_count":6,
    "processed_text":"royal marlin got amp hand us"
  },
  {
    "label":4,
    "text":"rock superheist level feel like back high school",
    "cleaned_text":"rock superheist level feel like back high school",
    "normalized_text":"rock superheist level feel like back high school",
    "tokens":[
      "rock",
      "superheist",
      "level",
      "feel",
      "like",
      "back",
      "high",
      "school"
    ],
    "token_count":8,
    "processed_text":"rock superheist level feel like back high school"
  },
  {
    "label":0,
    "text":"miss pic",
    "cleaned_text":"miss pic",
    "normalized_text":"miss pic",
    "tokens":[
      "miss",
      "pic"
    ],
    "token_count":2,
    "processed_text":"miss pic"
  },
  {
    "label":0,
    "text":"know everyon quotpsht id total leavequot im like quotno wouldnt toolquot need puppi",
    "cleaned_text":"know everyon quotpsht id total leavequot im like quotno wouldnt toolquot need puppi",
    "normalized_text":"know everyon quotpsht id total leavequot im like quotno wouldnt toolquot need puppi",
    "tokens":[
      "know",
      "everyon",
      "quotpsht",
      "id",
      "total",
      "leavequot",
      "im",
      "like",
      "quotno",
      "wouldnt",
      "toolquot",
      "need",
      "puppi"
    ],
    "token_count":13,
    "processed_text":"know everyon quotpsht id total leavequot im like quotno wouldnt toolquot need puppi"
  },
  {
    "label":0,
    "text":"want friend bbm",
    "cleaned_text":"want friend bbm",
    "normalized_text":"want friend bbm",
    "tokens":[
      "want",
      "friend",
      "bbm"
    ],
    "token_count":3,
    "processed_text":"want friend bbm"
  },
  {
    "label":4,
    "text":"oh yeah watch find nemo anoth puzzl tonight love boy",
    "cleaned_text":"oh yeah watch find nemo anoth puzzl tonight love boy",
    "normalized_text":"oh yeah watch find nemo anoth puzzl tonight love boy",
    "tokens":[
      "oh",
      "yeah",
      "watch",
      "find",
      "nemo",
      "anoth",
      "puzzl",
      "tonight",
      "love",
      "boy"
    ],
    "token_count":10,
    "processed_text":"oh yeah watch find nemo anoth puzzl tonight love boy"
  },
  {
    "label":0,
    "text":"mani thing want littl money",
    "cleaned_text":"mani thing want littl money",
    "normalized_text":"mani thing want littl money",
    "tokens":[
      "mani",
      "thing",
      "want",
      "littl",
      "money"
    ],
    "token_count":5,
    "processed_text":"mani thing want littl money"
  },
  {
    "label":0,
    "text":"goodnight ipod still found old mp playerwel see long hold xxx",
    "cleaned_text":"goodnight ipod still found old mp playerwel see long hold xxx",
    "normalized_text":"goodnight ipod still found old mp playerwel see long hold xxx",
    "tokens":[
      "goodnight",
      "ipod",
      "still",
      "found",
      "old",
      "mp",
      "playerwel",
      "see",
      "long",
      "hold",
      "xxx"
    ],
    "token_count":11,
    "processed_text":"goodnight ipod still found old mp playerwel see long hold xxx"
  },
  {
    "label":4,
    "text":"happi see",
    "cleaned_text":"happi see",
    "normalized_text":"happi see",
    "tokens":[
      "happi",
      "see"
    ],
    "token_count":2,
    "processed_text":"happi see"
  },
  {
    "label":0,
    "text":"worst headach allergi",
    "cleaned_text":"worst headach allergi",
    "normalized_text":"worst headach allergi",
    "tokens":[
      "worst",
      "headach",
      "allergi"
    ],
    "token_count":3,
    "processed_text":"worst headach allergi"
  },
  {
    "label":4,
    "text":"cant take credit xd talk she mastermind",
    "cleaned_text":"cant take credit xd talk she mastermind",
    "normalized_text":"cant take credit xd talk she mastermind",
    "tokens":[
      "cant",
      "take",
      "credit",
      "xd",
      "talk",
      "mastermind"
    ],
    "token_count":6,
    "processed_text":"cant take credit xd talk mastermind"
  },
  {
    "label":4,
    "text":"love site guy take american skateboard find make miss london littl bit less",
    "cleaned_text":"love site guy take american skateboard find make miss london littl bit less",
    "normalized_text":"love site guy take american skateboard find make miss london littl bit less",
    "tokens":[
      "love",
      "site",
      "guy",
      "take",
      "american",
      "skateboard",
      "find",
      "make",
      "miss",
      "london",
      "littl",
      "bit",
      "less"
    ],
    "token_count":13,
    "processed_text":"love site guy take american skateboard find make miss london littl bit less"
  },
  {
    "label":0,
    "text":"hate di train station wit passion right got di lil ounc scare im g ight idk wat wrong wit nowah",
    "cleaned_text":"hate di train station wit passion right got di lil ounc scare im g ight idk wat wrong wit nowah",
    "normalized_text":"hate di train station wit passion right got di lil ounc scare im g ight idk wat wrong wit nowah",
    "tokens":[
      "hate",
      "di",
      "train",
      "station",
      "wit",
      "passion",
      "right",
      "got",
      "di",
      "lil",
      "ounc",
      "scare",
      "im",
      "ight",
      "idk",
      "wat",
      "wrong",
      "wit",
      "nowah"
    ],
    "token_count":19,
    "processed_text":"hate di train station wit passion right got di lil ounc scare im ight idk wat wrong wit nowah"
  },
  {
    "label":0,
    "text":"wish live somewher felt bigger box",
    "cleaned_text":"wish live somewher felt bigger box",
    "normalized_text":"wish live somewher felt bigger box",
    "tokens":[
      "wish",
      "live",
      "somewh",
      "felt",
      "bigger",
      "box"
    ],
    "token_count":6,
    "processed_text":"wish live somewh felt bigger box"
  },
  {
    "label":0,
    "text":"think follow first wont allow send dm",
    "cleaned_text":"think follow first wont allow send dm",
    "normalized_text":"think follow first wont allow send dm",
    "tokens":[
      "think",
      "follow",
      "first",
      "wont",
      "allow",
      "send",
      "dm"
    ],
    "token_count":7,
    "processed_text":"think follow first wont allow send dm"
  },
  {
    "label":4,
    "text":"listen hey monday month new favorit song",
    "cleaned_text":"listen hey monday month new favorit song",
    "normalized_text":"listen hey monday month new favorit song",
    "tokens":[
      "listen",
      "hey",
      "monday",
      "month",
      "new",
      "favorit",
      "song"
    ],
    "token_count":7,
    "processed_text":"listen hey monday month new favorit song"
  },
  {
    "label":0,
    "text":"think cat might come back",
    "cleaned_text":"think cat might come back",
    "normalized_text":"think cat might come back",
    "tokens":[
      "think",
      "cat",
      "come",
      "back"
    ],
    "token_count":4,
    "processed_text":"think cat come back"
  },
  {
    "label":4,
    "text":"lot folk come see coldplay think im black person",
    "cleaned_text":"lot folk come see coldplay think im black person",
    "normalized_text":"lot folk come see coldplay think im black person",
    "tokens":[
      "lot",
      "folk",
      "come",
      "see",
      "coldplay",
      "think",
      "im",
      "black",
      "person"
    ],
    "token_count":9,
    "processed_text":"lot folk come see coldplay think im black person"
  },
  {
    "label":4,
    "text":"end day diserv new song thank everyth cant express love admir night night xx",
    "cleaned_text":"end day diserv new song thank everyth cant express love admir night night xx",
    "normalized_text":"end day diserv new song thank everyth cant express love admir night night xx",
    "tokens":[
      "end",
      "day",
      "diserv",
      "new",
      "song",
      "thank",
      "everyth",
      "cant",
      "express",
      "love",
      "admir",
      "night",
      "night",
      "xx"
    ],
    "token_count":14,
    "processed_text":"end day diserv new song thank everyth cant express love admir night night xx"
  },
  {
    "label":4,
    "text":"hi nice blog enjoy articl twitter market everpres anim pictur tho",
    "cleaned_text":"hi nice blog enjoy articl twitter market everpres anim pictur tho",
    "normalized_text":"hi nice blog enjoy articl twitter market everpres anim pictur tho",
    "tokens":[
      "hi",
      "nice",
      "blog",
      "enjoy",
      "articl",
      "twitter",
      "market",
      "everpr",
      "anim",
      "pictur",
      "tho"
    ],
    "token_count":11,
    "processed_text":"hi nice blog enjoy articl twitter market everpr anim pictur tho"
  },
  {
    "label":4,
    "text":"ok thank",
    "cleaned_text":"ok thank",
    "normalized_text":"ok thank",
    "tokens":[
      "ok",
      "thank"
    ],
    "token_count":2,
    "processed_text":"ok thank"
  },
  {
    "label":4,
    "text":"definit true statement",
    "cleaned_text":"definit true statement",
    "normalized_text":"definit true statement",
    "tokens":[
      "definit",
      "true",
      "statement"
    ],
    "token_count":3,
    "processed_text":"definit true statement"
  },
  {
    "label":0,
    "text":"fun car show steve marc two mini cooper though",
    "cleaned_text":"fun car show steve marc two mini cooper though",
    "normalized_text":"fun car show steve marc two mini cooper though",
    "tokens":[
      "fun",
      "car",
      "show",
      "steve",
      "marc",
      "two",
      "mini",
      "cooper",
      "though"
    ],
    "token_count":9,
    "processed_text":"fun car show steve marc two mini cooper though"
  },
  {
    "label":4,
    "text":"lmao notic",
    "cleaned_text":"lmao notic",
    "normalized_text":"lmao notic",
    "tokens":[
      "lmao",
      "notic"
    ],
    "token_count":2,
    "processed_text":"lmao notic"
  },
  {
    "label":4,
    "text":"admit took minut figur card trick",
    "cleaned_text":"admit took minut figur card trick",
    "normalized_text":"admit took minut figur card trick",
    "tokens":[
      "admit",
      "took",
      "minut",
      "figur",
      "card",
      "trick"
    ],
    "token_count":6,
    "processed_text":"admit took minut figur card trick"
  },
  {
    "label":4,
    "text":"havent seen movi yet mayb todayi miss termin salvat",
    "cleaned_text":"havent seen movi yet mayb todayi miss termin salvat",
    "normalized_text":"havent seen movi yet mayb todayi miss termin salvat",
    "tokens":[
      "havent",
      "seen",
      "movi",
      "yet",
      "mayb",
      "todayi",
      "miss",
      "termin",
      "salvat"
    ],
    "token_count":9,
    "processed_text":"havent seen movi yet mayb todayi miss termin salvat"
  },
  {
    "label":0,
    "text":"ugh english project im lazi",
    "cleaned_text":"ugh english project im lazi",
    "normalized_text":"ugh english project im lazi",
    "tokens":[
      "ugh",
      "english",
      "project",
      "im",
      "lazi"
    ],
    "token_count":5,
    "processed_text":"ugh english project im lazi"
  },
  {
    "label":0,
    "text":"bit hung didnt much yesterday",
    "cleaned_text":"bit hung didnt much yesterday",
    "normalized_text":"bit hung didnt much yesterday",
    "tokens":[
      "bit",
      "hung",
      "didnt",
      "much",
      "yesterday"
    ],
    "token_count":5,
    "processed_text":"bit hung didnt much yesterday"
  },
  {
    "label":0,
    "text":"myweak gener music music guitar synth",
    "cleaned_text":"myweak gener music music guitar synth",
    "normalized_text":"myweak gener music music guitar synth",
    "tokens":[
      "myweak",
      "gener",
      "music",
      "music",
      "guitar",
      "synth"
    ],
    "token_count":6,
    "processed_text":"myweak gener music music guitar synth"
  },
  {
    "label":4,
    "text":"set proxi pleas",
    "cleaned_text":"set proxi pleas",
    "normalized_text":"set proxi pleas",
    "tokens":[
      "set",
      "proxi",
      "plea"
    ],
    "token_count":3,
    "processed_text":"set proxi plea"
  },
  {
    "label":0,
    "text":"wanna see yo beauti face pic sum white dude holdin white girl lmao",
    "cleaned_text":"wanna see yo beauti face pic sum white dude holdin white girl lmao",
    "normalized_text":"wanna see yo beauti face pic sum white dude holdin white girl lmao",
    "tokens":[
      "wan",
      "na",
      "see",
      "yo",
      "beauti",
      "face",
      "pic",
      "sum",
      "white",
      "dude",
      "holdin",
      "white",
      "girl",
      "lmao"
    ],
    "token_count":14,
    "processed_text":"wan na see yo beauti face pic sum white dude holdin white girl lmao"
  },
  {
    "label":4,
    "text":"go sleep tweeti lol cant type sorri",
    "cleaned_text":"go sleep tweeti lol cant type sorri",
    "normalized_text":"go sleep tweeti lol cant type sorri",
    "tokens":[
      "go",
      "sleep",
      "tweeti",
      "lol",
      "cant",
      "type",
      "sorri"
    ],
    "token_count":7,
    "processed_text":"go sleep tweeti lol cant type sorri"
  },
  {
    "label":0,
    "text":"wish newport",
    "cleaned_text":"wish newport",
    "normalized_text":"wish newport",
    "tokens":[
      "wish",
      "newport"
    ],
    "token_count":2,
    "processed_text":"wish newport"
  },
  {
    "label":0,
    "text":"cant tonight",
    "cleaned_text":"cant tonight",
    "normalized_text":"cant tonight",
    "tokens":[
      "cant",
      "tonight"
    ],
    "token_count":2,
    "processed_text":"cant tonight"
  },
  {
    "label":4,
    "text":"birthday came gonelov",
    "cleaned_text":"birthday came gonelov",
    "normalized_text":"birthday came gonelov",
    "tokens":[
      "birthday",
      "came",
      "gonelov"
    ],
    "token_count":3,
    "processed_text":"birthday came gonelov"
  },
  {
    "label":0,
    "text":"ex motherinlaw come visit kid joy joy fuckin fantast",
    "cleaned_text":"ex motherinlaw come visit kid joy joy fuckin fantast",
    "normalized_text":"ex motherinlaw come visit kid joy joy fuckin fantast",
    "tokens":[
      "ex",
      "motherinlaw",
      "come",
      "visit",
      "kid",
      "joy",
      "joy",
      "fuckin",
      "fantast"
    ],
    "token_count":9,
    "processed_text":"ex motherinlaw come visit kid joy joy fuckin fantast"
  },
  {
    "label":0,
    "text":"ill work ill get updat get receiv updat",
    "cleaned_text":"ill work ill get updat get receiv updat",
    "normalized_text":"ill work ill get updat get receiv updat",
    "tokens":[
      "ill",
      "work",
      "ill",
      "get",
      "updat",
      "get",
      "receiv",
      "updat"
    ],
    "token_count":8,
    "processed_text":"ill work ill get updat get receiv updat"
  },
  {
    "label":0,
    "text":"doctor appt",
    "cleaned_text":"doctor appt",
    "normalized_text":"doctor appt",
    "tokens":[
      "doctor",
      "appt"
    ],
    "token_count":2,
    "processed_text":"doctor appt"
  },
  {
    "label":0,
    "text":"ive sent tesco someon forgot buy bread yesterday",
    "cleaned_text":"ive sent tesco someon forgot buy bread yesterday",
    "normalized_text":"ive sent tesco someon forgot buy bread yesterday",
    "tokens":[
      "ive",
      "sent",
      "tesco",
      "someon",
      "forgot",
      "buy",
      "bread",
      "yesterday"
    ],
    "token_count":8,
    "processed_text":"ive sent tesco someon forgot buy bread yesterday"
  },
  {
    "label":0,
    "text":"miss old hous baton roug",
    "cleaned_text":"miss old hous baton roug",
    "normalized_text":"miss old hous baton roug",
    "tokens":[
      "miss",
      "old",
      "hou",
      "baton",
      "roug"
    ],
    "token_count":5,
    "processed_text":"miss old hou baton roug"
  },
  {
    "label":0,
    "text":"u guy need make anoth batch social distort high top miss last time",
    "cleaned_text":"u guy need make anoth batch social distort high top miss last time",
    "normalized_text":"u guy need make anoth batch social distort high top miss last time",
    "tokens":[
      "guy",
      "need",
      "make",
      "anoth",
      "batch",
      "social",
      "distort",
      "high",
      "top",
      "miss",
      "last",
      "time"
    ],
    "token_count":12,
    "processed_text":"guy need make anoth batch social distort high top miss last time"
  },
  {
    "label":4,
    "text":"get job hear your head india cool im bollywood fan",
    "cleaned_text":"get job hear your head india cool im bollywood fan",
    "normalized_text":"get job hear your head india cool im bollywood fan",
    "tokens":[
      "get",
      "job",
      "hear",
      "head",
      "india",
      "cool",
      "im",
      "bollywood",
      "fan"
    ],
    "token_count":9,
    "processed_text":"get job hear head india cool im bollywood fan"
  },
  {
    "label":4,
    "text":"realli get",
    "cleaned_text":"realli get",
    "normalized_text":"realli get",
    "tokens":[
      "realli",
      "get"
    ],
    "token_count":2,
    "processed_text":"realli get"
  },
  {
    "label":4,
    "text":"use make collag favourit thing use photoshop school mayb pass time haha x",
    "cleaned_text":"use make collag favourit thing use photoshop school mayb pass time haha x",
    "normalized_text":"use make collag favourit thing use photoshop school mayb pass time haha x",
    "tokens":[
      "use",
      "make",
      "collag",
      "favourit",
      "thing",
      "use",
      "photoshop",
      "school",
      "mayb",
      "pass",
      "time",
      "haha"
    ],
    "token_count":12,
    "processed_text":"use make collag favourit thing use photoshop school mayb pass time haha"
  },
  {
    "label":0,
    "text":"kind power cut need gate main power suppli least duti cycl improv",
    "cleaned_text":"kind power cut need gate main power suppli least duti cycl improv",
    "normalized_text":"kind power cut need gate main power suppli least duti cycl improv",
    "tokens":[
      "kind",
      "power",
      "cut",
      "need",
      "gate",
      "main",
      "power",
      "suppli",
      "least",
      "duti",
      "cycl",
      "improv"
    ],
    "token_count":12,
    "processed_text":"kind power cut need gate main power suppli least duti cycl improv"
  },
  {
    "label":4,
    "text":"wait sara get leav soon listen oasi grate music back",
    "cleaned_text":"wait sara get leav soon listen oasi grate music back",
    "normalized_text":"wait sara get leav soon listen oasi grate music back",
    "tokens":[
      "wait",
      "sara",
      "get",
      "leav",
      "soon",
      "listen",
      "oasi",
      "grate",
      "music",
      "back"
    ],
    "token_count":10,
    "processed_text":"wait sara get leav soon listen oasi grate music back"
  },
  {
    "label":0,
    "text":"got wisdom teeth growin hurt like bitch",
    "cleaned_text":"got wisdom teeth growin hurt like bitch",
    "normalized_text":"got wisdom teeth growin hurt like bitch",
    "tokens":[
      "got",
      "wisdom",
      "teeth",
      "growin",
      "hurt",
      "like",
      "bitch"
    ],
    "token_count":7,
    "processed_text":"got wisdom teeth growin hurt like bitch"
  },
  {
    "label":0,
    "text":"wifi nh h ng xm u ri nh fb",
    "cleaned_text":"wifi nh h ng xm u ri nh fb",
    "normalized_text":"wifi nh h ng xm u ri nh fb",
    "tokens":[
      "wifi",
      "nh",
      "ng",
      "xm",
      "ri",
      "nh",
      "fb"
    ],
    "token_count":7,
    "processed_text":"wifi nh ng xm ri nh fb"
  },
  {
    "label":4,
    "text":"head back dalla day home europ great start pack get home",
    "cleaned_text":"head back dalla day home europ great start pack get home",
    "normalized_text":"head back dalla day home europ great start pack get home",
    "tokens":[
      "head",
      "back",
      "dalla",
      "day",
      "home",
      "europ",
      "great",
      "start",
      "pack",
      "get",
      "home"
    ],
    "token_count":11,
    "processed_text":"head back dalla day home europ great start pack get home"
  },
  {
    "label":0,
    "text":"oki look like drownd rat",
    "cleaned_text":"oki look like drownd rat",
    "normalized_text":"oki look like drownd rat",
    "tokens":[
      "oki",
      "look",
      "like",
      "drownd",
      "rat"
    ],
    "token_count":5,
    "processed_text":"oki look like drownd rat"
  },
  {
    "label":4,
    "text":"welcom mom year survivor close heart",
    "cleaned_text":"welcom mom year survivor close heart",
    "normalized_text":"welcom mom year survivor close heart",
    "tokens":[
      "welcom",
      "mom",
      "year",
      "survivor",
      "close",
      "heart"
    ],
    "token_count":6,
    "processed_text":"welcom mom year survivor close heart"
  },
  {
    "label":0,
    "text":"hmm seem everi silver line cloud",
    "cleaned_text":"hmm seem everi silver line cloud",
    "normalized_text":"hmm seem everi silver line cloud",
    "tokens":[
      "hmm",
      "seem",
      "everi",
      "silver",
      "line",
      "cloud"
    ],
    "token_count":6,
    "processed_text":"hmm seem everi silver line cloud"
  },
  {
    "label":4,
    "text":"relax beauti stratford connecticut pool think",
    "cleaned_text":"relax beauti stratford connecticut pool think",
    "normalized_text":"relax beauti stratford connecticut pool think",
    "tokens":[
      "relax",
      "beauti",
      "stratford",
      "connecticut",
      "pool",
      "think"
    ],
    "token_count":6,
    "processed_text":"relax beauti stratford connecticut pool think"
  },
  {
    "label":4,
    "text":"confid weekend footbal nervou yet",
    "cleaned_text":"confid weekend footbal nervou yet",
    "normalized_text":"confid weekend footbal nervou yet",
    "tokens":[
      "confid",
      "weekend",
      "footbal",
      "nervou",
      "yet"
    ],
    "token_count":5,
    "processed_text":"confid weekend footbal nervou yet"
  },
  {
    "label":0,
    "text":"prais lordher brother almost got train collid prayer go famili affect",
    "cleaned_text":"prais lordher brother almost got train collid prayer go famili affect",
    "normalized_text":"prais lordher brother almost got train collid prayer go famili affect",
    "tokens":[
      "prai",
      "lordher",
      "brother",
      "almost",
      "got",
      "train",
      "collid",
      "prayer",
      "go",
      "famili",
      "affect"
    ],
    "token_count":11,
    "processed_text":"prai lordher brother almost got train collid prayer go famili affect"
  },
  {
    "label":0,
    "text":"serious hope swine flu get well soon gtlt",
    "cleaned_text":"serious hope swine flu get well soon gtlt",
    "normalized_text":"serious hope swine flu get well soon gtlt",
    "tokens":[
      "seriou",
      "hope",
      "swine",
      "flu",
      "get",
      "well",
      "soon",
      "gtlt"
    ],
    "token_count":8,
    "processed_text":"seriou hope swine flu get well soon gtlt"
  },
  {
    "label":4,
    "text":"paint avail print onlin wwwmpattinsonsmugmugcom",
    "cleaned_text":"paint avail print onlin wwwmpattinsonsmugmugcom",
    "normalized_text":"paint avail print onlin wwwmpattinsonsmugmugcom",
    "tokens":[
      "paint",
      "avail",
      "print",
      "onlin"
    ],
    "token_count":4,
    "processed_text":"paint avail print onlin"
  },
  {
    "label":0,
    "text":"im cri bco im listen song amp sooo sad singer cant sing",
    "cleaned_text":"im cri bco im listen song amp sooo sad singer cant sing",
    "normalized_text":"im cri bco im listen song amp sooo sad singer cant sing",
    "tokens":[
      "im",
      "cri",
      "bco",
      "im",
      "listen",
      "song",
      "amp",
      "sooo",
      "sad",
      "singer",
      "cant",
      "sing"
    ],
    "token_count":12,
    "processed_text":"im cri bco im listen song amp sooo sad singer cant sing"
  },
  {
    "label":0,
    "text":"wonder anyth go tomorrow nada",
    "cleaned_text":"wonder anyth go tomorrow nada",
    "normalized_text":"wonder anyth go tomorrow nada",
    "tokens":[
      "wonder",
      "anyth",
      "go",
      "tomorrow",
      "nada"
    ],
    "token_count":5,
    "processed_text":"wonder anyth go tomorrow nada"
  },
  {
    "label":4,
    "text":"come live sydney",
    "cleaned_text":"come live sydney",
    "normalized_text":"come live sydney",
    "tokens":[
      "come",
      "live",
      "sydney"
    ],
    "token_count":3,
    "processed_text":"come live sydney"
  },
  {
    "label":0,
    "text":"oh jennif lust kindl unfortun sale canada",
    "cleaned_text":"oh jennif lust kindl unfortun sale canada",
    "normalized_text":"oh jennif lust kindl unfortun sale canada",
    "tokens":[
      "oh",
      "jennif",
      "lust",
      "kindl",
      "unfortun",
      "sale",
      "canada"
    ],
    "token_count":7,
    "processed_text":"oh jennif lust kindl unfortun sale canada"
  },
  {
    "label":0,
    "text":"play heartbeat acoust version keyboard want beabl play propper version",
    "cleaned_text":"play heartbeat acoust version keyboard want beabl play propper version",
    "normalized_text":"play heartbeat acoust version keyboard want beabl play propper version",
    "tokens":[
      "play",
      "heartbeat",
      "acoust",
      "version",
      "keyboard",
      "want",
      "beabl",
      "play",
      "propper",
      "version"
    ],
    "token_count":10,
    "processed_text":"play heartbeat acoust version keyboard want beabl play propper version"
  },
  {
    "label":0,
    "text":"leav im sad kitti",
    "cleaned_text":"leav im sad kitti",
    "normalized_text":"leav im sad kitti",
    "tokens":[
      "leav",
      "im",
      "sad",
      "kitti"
    ],
    "token_count":4,
    "processed_text":"leav im sad kitti"
  },
  {
    "label":0,
    "text":"jeeeee love twit nn math test tomorrow right im stress test tuesday",
    "cleaned_text":"jeeeee love twit nn math test tomorrow right im stress test tuesday",
    "normalized_text":"jeeeee love twit nn math test tomorrow right im stress test tuesday",
    "tokens":[
      "jeeeee",
      "love",
      "twit",
      "nn",
      "math",
      "test",
      "tomorrow",
      "right",
      "im",
      "stress",
      "test",
      "tuesday"
    ],
    "token_count":12,
    "processed_text":"jeeeee love twit nn math test tomorrow right im stress test tuesday"
  },
  {
    "label":4,
    "text":"final countdown quotth never againquot good way",
    "cleaned_text":"final countdown quotth never againquot good way",
    "normalized_text":"final countdown quotth never againquot good way",
    "tokens":[
      "final",
      "countdown",
      "quotth",
      "never",
      "againquot",
      "good",
      "way"
    ],
    "token_count":7,
    "processed_text":"final countdown quotth never againquot good way"
  },
  {
    "label":4,
    "text":"sister gave us american express gift card dinner",
    "cleaned_text":"sister gave us american express gift card dinner",
    "normalized_text":"sister gave us american express gift card dinner",
    "tokens":[
      "sister",
      "gave",
      "us",
      "american",
      "express",
      "gift",
      "card",
      "dinner"
    ],
    "token_count":8,
    "processed_text":"sister gave us american express gift card dinner"
  },
  {
    "label":4,
    "text":"youll see like make circular cake may notic",
    "cleaned_text":"youll see like make circular cake may notic",
    "normalized_text":"youll see like make circular cake may notic",
    "tokens":[
      "youll",
      "see",
      "like",
      "make",
      "circular",
      "cake",
      "may",
      "notic"
    ],
    "token_count":8,
    "processed_text":"youll see like make circular cake may notic"
  },
  {
    "label":0,
    "text":"came back studio jeff lah jeff interview came kpo mr delay wont get see",
    "cleaned_text":"came back studio jeff lah jeff interview came kpo mr delay wont get see",
    "normalized_text":"came back studio jeff lah jeff interview came kpo mr delay wont get see",
    "tokens":[
      "came",
      "back",
      "studio",
      "jeff",
      "lah",
      "jeff",
      "interview",
      "came",
      "kpo",
      "mr",
      "delay",
      "wont",
      "get",
      "see"
    ],
    "token_count":14,
    "processed_text":"came back studio jeff lah jeff interview came kpo mr delay wont get see"
  },
  {
    "label":4,
    "text":"would love get iphon play friend never seen one today",
    "cleaned_text":"would love get iphon play friend never seen one today",
    "normalized_text":"would love get iphon play friend never seen one today",
    "tokens":[
      "love",
      "get",
      "iphon",
      "play",
      "friend",
      "never",
      "seen",
      "one",
      "today"
    ],
    "token_count":9,
    "processed_text":"love get iphon play friend never seen one today"
  },
  {
    "label":4,
    "text":"lot mind goin bed earli gettin hair done tomorrow",
    "cleaned_text":"lot mind goin bed earli gettin hair done tomorrow",
    "normalized_text":"lot mind goin bed earli gettin hair done tomorrow",
    "tokens":[
      "lot",
      "mind",
      "goin",
      "bed",
      "earli",
      "gettin",
      "hair",
      "done",
      "tomorrow"
    ],
    "token_count":9,
    "processed_text":"lot mind goin bed earli gettin hair done tomorrow"
  },
  {
    "label":4,
    "text":"nice day hope weather stay sunni",
    "cleaned_text":"nice day hope weather stay sunni",
    "normalized_text":"nice day hope weather stay sunni",
    "tokens":[
      "nice",
      "day",
      "hope",
      "weather",
      "stay",
      "sunni"
    ],
    "token_count":6,
    "processed_text":"nice day hope weather stay sunni"
  },
  {
    "label":4,
    "text":"found date premier new moon launch nm trailer wont hurt thing",
    "cleaned_text":"found date premier new moon launch nm trailer wont hurt thing",
    "normalized_text":"found date premier new moon launch nm trailer wont hurt thing",
    "tokens":[
      "found",
      "date",
      "premier",
      "new",
      "moon",
      "launch",
      "nm",
      "trailer",
      "wont",
      "hurt",
      "thing"
    ],
    "token_count":11,
    "processed_text":"found date premier new moon launch nm trailer wont hurt thing"
  },
  {
    "label":4,
    "text":"hahah dont worri",
    "cleaned_text":"hahah dont worri",
    "normalized_text":"hahah dont worri",
    "tokens":[
      "hahah",
      "dont",
      "worri"
    ],
    "token_count":3,
    "processed_text":"hahah dont worri"
  },
  {
    "label":0,
    "text":"ok holli bahahah wish come near",
    "cleaned_text":"ok holli bahahah wish come near",
    "normalized_text":"ok holli bahahah wish come near",
    "tokens":[
      "ok",
      "holli",
      "bahahah",
      "wish",
      "come",
      "near"
    ],
    "token_count":6,
    "processed_text":"ok holli bahahah wish come near"
  },
  {
    "label":0,
    "text":"morn bugsveri sickno skewl todayand cancel plan weekend happybut got peb",
    "cleaned_text":"morn bugsveri sickno skewl todayand cancel plan weekend happybut got peb",
    "normalized_text":"morn bugsveri sickno skewl todayand cancel plan weekend happybut got peb",
    "tokens":[
      "morn",
      "bugsveri",
      "sickno",
      "skewl",
      "todayand",
      "cancel",
      "plan",
      "weekend",
      "happybut",
      "got",
      "peb"
    ],
    "token_count":11,
    "processed_text":"morn bugsveri sickno skewl todayand cancel plan weekend happybut got peb"
  },
  {
    "label":0,
    "text":"depress mani turnov hawthorn afl",
    "cleaned_text":"depress mani turnov hawthorn afl",
    "normalized_text":"depress mani turnov hawthorn afl",
    "tokens":[
      "depress",
      "mani",
      "turnov",
      "hawthorn",
      "afl"
    ],
    "token_count":5,
    "processed_text":"depress mani turnov hawthorn afl"
  },
  {
    "label":0,
    "text":"tummi hurt want soft toy",
    "cleaned_text":"tummi hurt want soft toy",
    "normalized_text":"tummi hurt want soft toy",
    "tokens":[
      "tummi",
      "hurt",
      "want",
      "soft",
      "toy"
    ],
    "token_count":5,
    "processed_text":"tummi hurt want soft toy"
  },
  {
    "label":0,
    "text":"unit v citi f nice littl combo methink still ill",
    "cleaned_text":"unit v citi f nice littl combo methink still ill",
    "normalized_text":"unit v citi f nice littl combo methink still ill",
    "tokens":[
      "unit",
      "citi",
      "nice",
      "littl",
      "combo",
      "methink",
      "still",
      "ill"
    ],
    "token_count":8,
    "processed_text":"unit citi nice littl combo methink still ill"
  },
  {
    "label":4,
    "text":"beauti day park",
    "cleaned_text":"beauti day park",
    "normalized_text":"beauti day park",
    "tokens":[
      "beauti",
      "day",
      "park"
    ],
    "token_count":3,
    "processed_text":"beauti day park"
  },
  {
    "label":0,
    "text":"cc compani got sinc paid everi wk made payment one month amp next even though paid biweekli",
    "cleaned_text":"cc compani got sinc paid everi wk made payment one month amp next even though paid biweekli",
    "normalized_text":"cc compani got sinc paid everi wk made payment one month amp next even though paid biweekli",
    "tokens":[
      "cc",
      "compani",
      "got",
      "sinc",
      "paid",
      "everi",
      "wk",
      "made",
      "payment",
      "one",
      "month",
      "amp",
      "next",
      "even",
      "though",
      "paid",
      "biweekli"
    ],
    "token_count":17,
    "processed_text":"cc compani got sinc paid everi wk made payment one month amp next even though paid biweekli"
  },
  {
    "label":0,
    "text":"dizzi wtf ugh lay excit",
    "cleaned_text":"dizzi wtf ugh lay excit",
    "normalized_text":"dizzi wtf ugh lay excit",
    "tokens":[
      "dizzi",
      "wtf",
      "ugh",
      "lay",
      "excit"
    ],
    "token_count":5,
    "processed_text":"dizzi wtf ugh lay excit"
  },
  {
    "label":0,
    "text":"got play mario kart today n bowl gamehahaha went parti find someon luck",
    "cleaned_text":"got play mario kart today n bowl gamehahaha went parti find someon luck",
    "normalized_text":"got play mario kart today n bowl gamehahaha went parti find someon luck",
    "tokens":[
      "got",
      "play",
      "mario",
      "kart",
      "today",
      "bowl",
      "gamehahaha",
      "went",
      "parti",
      "find",
      "someon",
      "luck"
    ],
    "token_count":12,
    "processed_text":"got play mario kart today bowl gamehahaha went parti find someon luck"
  },
  {
    "label":4,
    "text":"found marmoset",
    "cleaned_text":"found marmoset",
    "normalized_text":"found marmoset",
    "tokens":[
      "found",
      "marmoset"
    ],
    "token_count":2,
    "processed_text":"found marmoset"
  },
  {
    "label":4,
    "text":"religiontim catch sleepz thank you jesu",
    "cleaned_text":"religiontim catch sleepz thank you jesu",
    "normalized_text":"religiontim catch sleepz thank you jesu",
    "tokens":[
      "religiontim",
      "catch",
      "sleepz",
      "thank",
      "jesu"
    ],
    "token_count":5,
    "processed_text":"religiontim catch sleepz thank jesu"
  },
  {
    "label":0,
    "text":"woken upthought sleep magic minutesnow feel like death",
    "cleaned_text":"woken upthought sleep magic minutesnow feel like death",
    "normalized_text":"woken upthought sleep magic minutesnow feel like death",
    "tokens":[
      "woken",
      "upthought",
      "sleep",
      "magic",
      "minutesnow",
      "feel",
      "like",
      "death"
    ],
    "token_count":8,
    "processed_text":"woken upthought sleep magic minutesnow feel like death"
  },
  {
    "label":0,
    "text":"omg ow put retain first time like three day",
    "cleaned_text":"omg ow put retain first time like three day",
    "normalized_text":"omg ow put retain first time like three day",
    "tokens":[
      "omg",
      "ow",
      "put",
      "retain",
      "first",
      "time",
      "like",
      "three",
      "day"
    ],
    "token_count":9,
    "processed_text":"omg ow put retain first time like three day"
  },
  {
    "label":4,
    "text":"tenaci sensit like",
    "cleaned_text":"tenaci sensit like",
    "normalized_text":"tenaci sensit like",
    "tokens":[
      "tenaci",
      "sensit",
      "like"
    ],
    "token_count":3,
    "processed_text":"tenaci sensit like"
  },
  {
    "label":4,
    "text":"oh hola right back ya",
    "cleaned_text":"oh hola right back ya",
    "normalized_text":"oh hola right back ya",
    "tokens":[
      "oh",
      "hola",
      "right",
      "back",
      "ya"
    ],
    "token_count":5,
    "processed_text":"oh hola right back ya"
  },
  {
    "label":0,
    "text":"english essay cant think anyth write hard soccer lost let goal",
    "cleaned_text":"english essay cant think anyth write hard soccer lost let goal",
    "normalized_text":"english essay cant think anyth write hard soccer lost let goal",
    "tokens":[
      "english",
      "essay",
      "cant",
      "think",
      "anyth",
      "write",
      "hard",
      "soccer",
      "lost",
      "let",
      "goal"
    ],
    "token_count":11,
    "processed_text":"english essay cant think anyth write hard soccer lost let goal"
  },
  {
    "label":0,
    "text":"cant get stupid tabl stand right keep fall",
    "cleaned_text":"cant get stupid tabl stand right keep fall",
    "normalized_text":"cant get stupid tabl stand right keep fall",
    "tokens":[
      "cant",
      "get",
      "stupid",
      "tabl",
      "stand",
      "right",
      "keep",
      "fall"
    ],
    "token_count":8,
    "processed_text":"cant get stupid tabl stand right keep fall"
  },
  {
    "label":0,
    "text":"beauti day bad there sidehandshak amp awkward moment occur water serv cup form",
    "cleaned_text":"beauti day bad there sidehandshak amp awkward moment occur water serv cup form",
    "normalized_text":"beauti day bad there sidehandshak amp awkward moment occur water serv cup form",
    "tokens":[
      "beauti",
      "day",
      "bad",
      "sidehandshak",
      "amp",
      "awkward",
      "moment",
      "occur",
      "water",
      "serv",
      "cup",
      "form"
    ],
    "token_count":12,
    "processed_text":"beauti day bad sidehandshak amp awkward moment occur water serv cup form"
  },
  {
    "label":4,
    "text":"hangov tini vodka",
    "cleaned_text":"hangov tini vodka",
    "normalized_text":"hangov tini vodka",
    "tokens":[
      "hangov",
      "tini",
      "vodka"
    ],
    "token_count":3,
    "processed_text":"hangov tini vodka"
  },
  {
    "label":4,
    "text":"ive alreadi made pancak kick kid hous day",
    "cleaned_text":"ive alreadi made pancak kick kid hous day",
    "normalized_text":"ive alreadi made pancak kick kid hous day",
    "tokens":[
      "ive",
      "alreadi",
      "made",
      "pancak",
      "kick",
      "kid",
      "hou",
      "day"
    ],
    "token_count":8,
    "processed_text":"ive alreadi made pancak kick kid hou day"
  },
  {
    "label":0,
    "text":"disregard last post state avail g version",
    "cleaned_text":"disregard last post state avail g version",
    "normalized_text":"disregard last post state avail g version",
    "tokens":[
      "disregard",
      "last",
      "post",
      "state",
      "avail",
      "version"
    ],
    "token_count":6,
    "processed_text":"disregard last post state avail version"
  },
  {
    "label":4,
    "text":"oh right sorri got wrong lol",
    "cleaned_text":"oh right sorri got wrong lol",
    "normalized_text":"oh right sorri got wrong lol",
    "tokens":[
      "oh",
      "right",
      "sorri",
      "got",
      "wrong",
      "lol"
    ],
    "token_count":6,
    "processed_text":"oh right sorri got wrong lol"
  },
  {
    "label":0,
    "text":"olney md store use chai diet tea diet white tea acai none",
    "cleaned_text":"olney md store use chai diet tea diet white tea acai none",
    "normalized_text":"olney md store use chai diet tea diet white tea acai none",
    "tokens":[
      "olney",
      "md",
      "store",
      "use",
      "chai",
      "diet",
      "tea",
      "diet",
      "white",
      "tea",
      "acai",
      "none"
    ],
    "token_count":12,
    "processed_text":"olney md store use chai diet tea diet white tea acai none"
  },
  {
    "label":0,
    "text":"omg friday must cri think",
    "cleaned_text":"omg friday must cri think",
    "normalized_text":"omg friday must cri think",
    "tokens":[
      "omg",
      "friday",
      "cri",
      "think"
    ],
    "token_count":4,
    "processed_text":"omg friday cri think"
  },
  {
    "label":0,
    "text":"sit offic wait kid playingyellingcri offic next door look clock min pass",
    "cleaned_text":"sit offic wait kid playingyellingcri offic next door look clock min pass",
    "normalized_text":"sit offic wait kid playingyellingcri offic next door look clock min pass",
    "tokens":[
      "sit",
      "offic",
      "wait",
      "kid",
      "offic",
      "next",
      "door",
      "look",
      "clock",
      "min",
      "pass"
    ],
    "token_count":11,
    "processed_text":"sit offic wait kid offic next door look clock min pass"
  },
  {
    "label":0,
    "text":"sad hill wont lauren anymor probabl wont watch show anymor",
    "cleaned_text":"sad hill wont lauren anymor probabl wont watch show anymor",
    "normalized_text":"sad hill wont lauren anymor probabl wont watch show anymor",
    "tokens":[
      "sad",
      "hill",
      "wont",
      "lauren",
      "anymor",
      "probabl",
      "wont",
      "watch",
      "show",
      "anymor"
    ],
    "token_count":10,
    "processed_text":"sad hill wont lauren anymor probabl wont watch show anymor"
  },
  {
    "label":4,
    "text":"hungri go bed goodnight",
    "cleaned_text":"hungri go bed goodnight",
    "normalized_text":"hungri go bed goodnight",
    "tokens":[
      "hungri",
      "go",
      "bed",
      "goodnight"
    ],
    "token_count":4,
    "processed_text":"hungri go bed goodnight"
  },
  {
    "label":0,
    "text":"want revis want tidi also wrong",
    "cleaned_text":"want revis want tidi also wrong",
    "normalized_text":"want revis want tidi also wrong",
    "tokens":[
      "want",
      "revi",
      "want",
      "tidi",
      "also",
      "wrong"
    ],
    "token_count":6,
    "processed_text":"want revi want tidi also wrong"
  },
  {
    "label":0,
    "text":"blahh anoth bore day michel parti",
    "cleaned_text":"blahh anoth bore day michel parti",
    "normalized_text":"blahh anoth bore day michel parti",
    "tokens":[
      "blahh",
      "anoth",
      "bore",
      "day",
      "michel",
      "parti"
    ],
    "token_count":6,
    "processed_text":"blahh anoth bore day michel parti"
  },
  {
    "label":4,
    "text":"go get haircut today get guest uk",
    "cleaned_text":"go get haircut today get guest uk",
    "normalized_text":"go get haircut today get guest uk",
    "tokens":[
      "go",
      "get",
      "haircut",
      "today",
      "get",
      "guest",
      "uk"
    ],
    "token_count":7,
    "processed_text":"go get haircut today get guest uk"
  },
  {
    "label":0,
    "text":"go bed anoth day help rooki week tomorrow fun",
    "cleaned_text":"go bed anoth day help rooki week tomorrow fun",
    "normalized_text":"go bed anoth day help rooki week tomorrow fun",
    "tokens":[
      "go",
      "bed",
      "anoth",
      "day",
      "help",
      "rooki",
      "week",
      "tomorrow",
      "fun"
    ],
    "token_count":9,
    "processed_text":"go bed anoth day help rooki week tomorrow fun"
  },
  {
    "label":4,
    "text":"go outsid see friend",
    "cleaned_text":"go outsid see friend",
    "normalized_text":"go outsid see friend",
    "tokens":[
      "go",
      "outsid",
      "see",
      "friend"
    ],
    "token_count":4,
    "processed_text":"go outsid see friend"
  },
  {
    "label":4,
    "text":"watch green green grass boyci bald",
    "cleaned_text":"watch green green grass boyci bald",
    "normalized_text":"watch green green grass boyci bald",
    "tokens":[
      "watch",
      "green",
      "green",
      "grass",
      "boyci",
      "bald"
    ],
    "token_count":6,
    "processed_text":"watch green green grass boyci bald"
  },
  {
    "label":4,
    "text":"church",
    "cleaned_text":"church",
    "normalized_text":"church",
    "tokens":[
      "church"
    ],
    "token_count":1,
    "processed_text":"church"
  },
  {
    "label":0,
    "text":"ripley miss sarah",
    "cleaned_text":"ripley miss sarah",
    "normalized_text":"ripley miss sarah",
    "tokens":[
      "ripley",
      "miss",
      "sarah"
    ],
    "token_count":3,
    "processed_text":"ripley miss sarah"
  },
  {
    "label":0,
    "text":"piss wknd didnt turn right",
    "cleaned_text":"piss wknd didnt turn right",
    "normalized_text":"piss wknd didnt turn right",
    "tokens":[
      "piss",
      "wknd",
      "didnt",
      "turn",
      "right"
    ],
    "token_count":5,
    "processed_text":"piss wknd didnt turn right"
  },
  {
    "label":4,
    "text":"wooot turn blogg",
    "cleaned_text":"wooot turn blogg",
    "normalized_text":"wooot turn blogg",
    "tokens":[
      "wooot",
      "turn",
      "blogg"
    ],
    "token_count":3,
    "processed_text":"wooot turn blogg"
  },
  {
    "label":0,
    "text":"fuck miss sorri today",
    "cleaned_text":"fuck miss sorri today",
    "normalized_text":"fuck miss sorri today",
    "tokens":[
      "fuck",
      "miss",
      "sorri",
      "today"
    ],
    "token_count":4,
    "processed_text":"fuck miss sorri today"
  },
  {
    "label":4,
    "text":"alrighttti send pictur finish come work muahahaha",
    "cleaned_text":"alrighttti send pictur finish come work muahahaha",
    "normalized_text":"alrighttti send pictur finish come work muahahaha",
    "tokens":[
      "alrighttti",
      "send",
      "pictur",
      "finish",
      "come",
      "work",
      "muahahaha"
    ],
    "token_count":7,
    "processed_text":"alrighttti send pictur finish come work muahahaha"
  },
  {
    "label":4,
    "text":"confus everyth watch juno strawberri amp banana smoothi",
    "cleaned_text":"confus everyth watch juno strawberri amp banana smoothi",
    "normalized_text":"confus everyth watch juno strawberri amp banana smoothi",
    "tokens":[
      "confu",
      "everyth",
      "watch",
      "juno",
      "strawberri",
      "amp",
      "banana",
      "smoothi"
    ],
    "token_count":8,
    "processed_text":"confu everyth watch juno strawberri amp banana smoothi"
  },
  {
    "label":4,
    "text":"good wasnt girl abl start nonprofit event",
    "cleaned_text":"good wasnt girl abl start nonprofit event",
    "normalized_text":"good wasnt girl abl start nonprofit event",
    "tokens":[
      "good",
      "wasnt",
      "girl",
      "abl",
      "start",
      "nonprofit",
      "event"
    ],
    "token_count":7,
    "processed_text":"good wasnt girl abl start nonprofit event"
  },
  {
    "label":0,
    "text":"guh ok go bed yay",
    "cleaned_text":"guh ok go bed yay",
    "normalized_text":"guh ok go bed yay",
    "tokens":[
      "guh",
      "ok",
      "go",
      "bed",
      "yay"
    ],
    "token_count":5,
    "processed_text":"guh ok go bed yay"
  },
  {
    "label":0,
    "text":"hate hous im increasinglymorebroken internet feel slightli crippl",
    "cleaned_text":"hate hous im increasinglymorebroken internet feel slightli crippl",
    "normalized_text":"hate hous im increasinglymorebroken internet feel slightli crippl",
    "tokens":[
      "hate",
      "hou",
      "im",
      "internet",
      "feel",
      "slightli",
      "crippl"
    ],
    "token_count":7,
    "processed_text":"hate hou im internet feel slightli crippl"
  },
  {
    "label":4,
    "text":"ill look largest break may call",
    "cleaned_text":"ill look largest break may call",
    "normalized_text":"ill look largest break may call",
    "tokens":[
      "ill",
      "look",
      "largest",
      "break",
      "may",
      "call"
    ],
    "token_count":6,
    "processed_text":"ill look largest break may call"
  },
  {
    "label":4,
    "text":"ocean eleven one perfect movi quotfella fella redsquot",
    "cleaned_text":"ocean eleven one perfect movi quotfella fella redsquot",
    "normalized_text":"ocean eleven one perfect movi quotfella fella redsquot",
    "tokens":[
      "ocean",
      "eleven",
      "one",
      "perfect",
      "movi",
      "quotfella",
      "fella",
      "redsquot"
    ],
    "token_count":8,
    "processed_text":"ocean eleven one perfect movi quotfella fella redsquot"
  },
  {
    "label":4,
    "text":"would like cult movi wouldnt watch instead",
    "cleaned_text":"would like cult movi wouldnt watch instead",
    "normalized_text":"would like cult movi wouldnt watch instead",
    "tokens":[
      "like",
      "cult",
      "movi",
      "wouldnt",
      "watch",
      "instead"
    ],
    "token_count":6,
    "processed_text":"like cult movi wouldnt watch instead"
  },
  {
    "label":0,
    "text":"mayb work store like game voucher",
    "cleaned_text":"mayb work store like game voucher",
    "normalized_text":"mayb work store like game voucher",
    "tokens":[
      "mayb",
      "work",
      "store",
      "like",
      "game",
      "voucher"
    ],
    "token_count":6,
    "processed_text":"mayb work store like game voucher"
  },
  {
    "label":0,
    "text":"live matrix call quotth windowcentr worldquot sad true",
    "cleaned_text":"live matrix call quotth windowcentr worldquot sad true",
    "normalized_text":"live matrix call quotth windowcentr worldquot sad true",
    "tokens":[
      "live",
      "matrix",
      "call",
      "quotth",
      "windowcentr",
      "worldquot",
      "sad",
      "true"
    ],
    "token_count":8,
    "processed_text":"live matrix call quotth windowcentr worldquot sad true"
  },
  {
    "label":4,
    "text":"dane cook sexi boy cuterr know im talk abouttt",
    "cleaned_text":"dane cook sexi boy cuterr know im talk abouttt",
    "normalized_text":"dane cook sexi boy cuterr know im talk abouttt",
    "tokens":[
      "dane",
      "cook",
      "sexi",
      "boy",
      "cuterr",
      "know",
      "im",
      "talk",
      "abouttt"
    ],
    "token_count":9,
    "processed_text":"dane cook sexi boy cuterr know im talk abouttt"
  },
  {
    "label":0,
    "text":"idk home",
    "cleaned_text":"idk home",
    "normalized_text":"idk home",
    "tokens":[
      "idk",
      "home"
    ],
    "token_count":2,
    "processed_text":"idk home"
  },
  {
    "label":4,
    "text":"webpi download bzr android updat hope doesnt break phone g",
    "cleaned_text":"webpi download bzr android updat hope doesnt break phone g",
    "normalized_text":"webpi download bzr android updat hope doesnt break phone g",
    "tokens":[
      "webpi",
      "download",
      "bzr",
      "android",
      "updat",
      "hope",
      "doesnt",
      "break",
      "phone"
    ],
    "token_count":9,
    "processed_text":"webpi download bzr android updat hope doesnt break phone"
  },
  {
    "label":0,
    "text":"lol that",
    "cleaned_text":"lol that",
    "normalized_text":"lol that",
    "tokens":[
      "lol"
    ],
    "token_count":1,
    "processed_text":"lol"
  },
  {
    "label":0,
    "text":"big swim nightgirl pool alon work out get big",
    "cleaned_text":"big swim nightgirl pool alon work out get big",
    "normalized_text":"big swim nightgirl pool alon work out get big",
    "tokens":[
      "big",
      "swim",
      "nightgirl",
      "pool",
      "alon",
      "work",
      "get",
      "big"
    ],
    "token_count":8,
    "processed_text":"big swim nightgirl pool alon work get big"
  },
  {
    "label":4,
    "text":"lol im glad averag connect speed usa alreadi plot get visa",
    "cleaned_text":"lol im glad averag connect speed usa alreadi plot get visa",
    "normalized_text":"lol im glad averag connect speed usa alreadi plot get visa",
    "tokens":[
      "lol",
      "im",
      "glad",
      "averag",
      "connect",
      "speed",
      "usa",
      "alreadi",
      "plot",
      "get",
      "visa"
    ],
    "token_count":11,
    "processed_text":"lol im glad averag connect speed usa alreadi plot get visa"
  },
  {
    "label":4,
    "text":"realli see daddi god wonder work exam",
    "cleaned_text":"realli see daddi god wonder work exam",
    "normalized_text":"realli see daddi god wonder work exam",
    "tokens":[
      "realli",
      "see",
      "daddi",
      "god",
      "wonder",
      "work",
      "exam"
    ],
    "token_count":7,
    "processed_text":"realli see daddi god wonder work exam"
  },
  {
    "label":0,
    "text":"fun atl sadli ur boy may miss",
    "cleaned_text":"fun atl sadli ur boy may miss",
    "normalized_text":"fun atl sadli ur boy may miss",
    "tokens":[
      "fun",
      "atl",
      "sadli",
      "ur",
      "boy",
      "may",
      "miss"
    ],
    "token_count":7,
    "processed_text":"fun atl sadli ur boy may miss"
  },
  {
    "label":0,
    "text":"im lobster week",
    "cleaned_text":"im lobster week",
    "normalized_text":"im lobster week",
    "tokens":[
      "im",
      "lobster",
      "week"
    ],
    "token_count":3,
    "processed_text":"im lobster week"
  },
  {
    "label":4,
    "text":"went shoppin merri hill shirt n short yey bt still cant find cardigan got clean tip room l",
    "cleaned_text":"went shoppin merri hill shirt n short yey bt still cant find cardigan got clean tip room l",
    "normalized_text":"went shoppin merri hill shirt n short yey bt still cant find cardigan got clean tip room l",
    "tokens":[
      "went",
      "shoppin",
      "merri",
      "hill",
      "shirt",
      "short",
      "yey",
      "bt",
      "still",
      "cant",
      "find",
      "cardigan",
      "got",
      "clean",
      "tip",
      "room"
    ],
    "token_count":16,
    "processed_text":"went shoppin merri hill shirt short yey bt still cant find cardigan got clean tip room"
  },
  {
    "label":4,
    "text":"sunshin yay",
    "cleaned_text":"sunshin yay",
    "normalized_text":"sunshin yay",
    "tokens":[
      "sunshin",
      "yay"
    ],
    "token_count":2,
    "processed_text":"sunshin yay"
  },
  {
    "label":0,
    "text":"whoaa drunken mistak",
    "cleaned_text":"whoaa drunken mistak",
    "normalized_text":"whoaa drunken mistak",
    "tokens":[
      "whoaa",
      "drunken",
      "mistak"
    ],
    "token_count":3,
    "processed_text":"whoaa drunken mistak"
  },
  {
    "label":4,
    "text":"realli interest",
    "cleaned_text":"realli interest",
    "normalized_text":"realli interest",
    "tokens":[
      "realli",
      "interest"
    ],
    "token_count":2,
    "processed_text":"realli interest"
  },
  {
    "label":4,
    "text":"here get cold got coffe break minut ago enjoy drink",
    "cleaned_text":"here get cold got coffe break minut ago enjoy drink",
    "normalized_text":"here get cold got coffe break minut ago enjoy drink",
    "tokens":[
      "get",
      "cold",
      "got",
      "coff",
      "break",
      "minut",
      "ago",
      "enjoy",
      "drink"
    ],
    "token_count":9,
    "processed_text":"get cold got coff break minut ago enjoy drink"
  },
  {
    "label":0,
    "text":"ye piraci big issu sinc one right control internet noth realli done issu",
    "cleaned_text":"ye piraci big issu sinc one right control internet noth realli done issu",
    "normalized_text":"ye piraci big issu sinc one right control internet noth realli done issu",
    "tokens":[
      "ye",
      "piraci",
      "big",
      "issu",
      "sinc",
      "one",
      "right",
      "control",
      "internet",
      "noth",
      "realli",
      "done",
      "issu"
    ],
    "token_count":13,
    "processed_text":"ye piraci big issu sinc one right control internet noth realli done issu"
  },
  {
    "label":0,
    "text":"total would wasnt suppos work right mayb anyway",
    "cleaned_text":"total would wasnt suppos work right mayb anyway",
    "normalized_text":"total would wasnt suppos work right mayb anyway",
    "tokens":[
      "total",
      "wasnt",
      "suppo",
      "work",
      "right",
      "mayb",
      "anyway"
    ],
    "token_count":7,
    "processed_text":"total wasnt suppo work right mayb anyway"
  },
  {
    "label":0,
    "text":"also want rat",
    "cleaned_text":"also want rat",
    "normalized_text":"also want rat",
    "tokens":[
      "also",
      "want",
      "rat"
    ],
    "token_count":3,
    "processed_text":"also want rat"
  },
  {
    "label":0,
    "text":"cant believ possibl bore",
    "cleaned_text":"cant believ possibl bore",
    "normalized_text":"cant believ possibl bore",
    "tokens":[
      "cant",
      "believ",
      "possibl",
      "bore"
    ],
    "token_count":4,
    "processed_text":"cant believ possibl bore"
  },
  {
    "label":4,
    "text":"im amp im work would honour keep u companybedtim stori cute movi btw",
    "cleaned_text":"im amp im work would honour keep u companybedtim stori cute movi btw",
    "normalized_text":"im amp im work would honour keep u companybedtim stori cute movi btw",
    "tokens":[
      "im",
      "amp",
      "im",
      "work",
      "honour",
      "keep",
      "companybedtim",
      "stori",
      "cute",
      "movi",
      "btw"
    ],
    "token_count":11,
    "processed_text":"im amp im work honour keep companybedtim stori cute movi btw"
  },
  {
    "label":4,
    "text":"im realli horribl studi come exam cuz dont studi melani graduat tonight",
    "cleaned_text":"im realli horribl studi come exam cuz dont studi melani graduat tonight",
    "normalized_text":"im realli horribl studi come exam cuz dont studi melani graduat tonight",
    "tokens":[
      "im",
      "realli",
      "horribl",
      "studi",
      "come",
      "exam",
      "cuz",
      "dont",
      "studi",
      "melani",
      "graduat",
      "tonight"
    ],
    "token_count":12,
    "processed_text":"im realli horribl studi come exam cuz dont studi melani graduat tonight"
  },
  {
    "label":0,
    "text":"pull intak lump filthi",
    "cleaned_text":"pull intak lump filthi",
    "normalized_text":"pull intak lump filthi",
    "tokens":[
      "pull",
      "intak",
      "lump",
      "filthi"
    ],
    "token_count":4,
    "processed_text":"pull intak lump filthi"
  },
  {
    "label":0,
    "text":"brother leav tomorrow he two day ugh",
    "cleaned_text":"brother leav tomorrow he two day ugh",
    "normalized_text":"brother leav tomorrow he two day ugh",
    "tokens":[
      "brother",
      "leav",
      "tomorrow",
      "two",
      "day",
      "ugh"
    ],
    "token_count":6,
    "processed_text":"brother leav tomorrow two day ugh"
  },
  {
    "label":0,
    "text":"clean window metho water sure tell clean though",
    "cleaned_text":"clean window metho water sure tell clean though",
    "normalized_text":"clean window metho water sure tell clean though",
    "tokens":[
      "clean",
      "window",
      "metho",
      "water",
      "sure",
      "tell",
      "clean",
      "though"
    ],
    "token_count":8,
    "processed_text":"clean window metho water sure tell clean though"
  },
  {
    "label":0,
    "text":"im dissapoint quid save ps ps ps ps ps hopin quid",
    "cleaned_text":"im dissapoint quid save ps ps ps ps ps hopin quid",
    "normalized_text":"im dissapoint quid save ps ps ps ps ps hopin quid",
    "tokens":[
      "im",
      "dissapoint",
      "quid",
      "save",
      "ps",
      "ps",
      "ps",
      "ps",
      "ps",
      "hopin",
      "quid"
    ],
    "token_count":11,
    "processed_text":"im dissapoint quid save ps ps ps ps ps hopin quid"
  },
  {
    "label":4,
    "text":"nope serv buffett couldnt eat anyth",
    "cleaned_text":"nope serv buffett couldnt eat anyth",
    "normalized_text":"nope serv buffett couldnt eat anyth",
    "tokens":[
      "nope",
      "serv",
      "buffett",
      "couldnt",
      "eat",
      "anyth"
    ],
    "token_count":6,
    "processed_text":"nope serv buffett couldnt eat anyth"
  },
  {
    "label":4,
    "text":"dont like hope hous next agenda",
    "cleaned_text":"dont like hope hous next agenda",
    "normalized_text":"dont like hope hous next agenda",
    "tokens":[
      "dont",
      "like",
      "hope",
      "hou",
      "next",
      "agenda"
    ],
    "token_count":6,
    "processed_text":"dont like hope hou next agenda"
  },
  {
    "label":4,
    "text":"tattoo pretti",
    "cleaned_text":"tattoo pretti",
    "normalized_text":"tattoo pretti",
    "tokens":[
      "tattoo",
      "pretti"
    ],
    "token_count":2,
    "processed_text":"tattoo pretti"
  },
  {
    "label":4,
    "text":"haha full schedul fall love",
    "cleaned_text":"haha full schedul fall love",
    "normalized_text":"haha full schedul fall love",
    "tokens":[
      "haha",
      "full",
      "schedul",
      "fall",
      "love"
    ],
    "token_count":5,
    "processed_text":"haha full schedul fall love"
  },
  {
    "label":4,
    "text":"wow that pretti awesom",
    "cleaned_text":"wow that pretti awesom",
    "normalized_text":"wow that pretti awesom",
    "tokens":[
      "wow",
      "pretti",
      "awesom"
    ],
    "token_count":3,
    "processed_text":"wow pretti awesom"
  },
  {
    "label":0,
    "text":"ahhh sang part mansfield",
    "cleaned_text":"ahhh sang part mansfield",
    "normalized_text":"ahhh sang part mansfield",
    "tokens":[
      "ahhh",
      "sang",
      "part",
      "mansfield"
    ],
    "token_count":4,
    "processed_text":"ahhh sang part mansfield"
  },
  {
    "label":4,
    "text":"lake peep get tan ice cream man gave free ice creamsick",
    "cleaned_text":"lake peep get tan ice cream man gave free ice creamsick",
    "normalized_text":"lake peep get tan ice cream man gave free ice creamsick",
    "tokens":[
      "lake",
      "peep",
      "get",
      "tan",
      "ice",
      "cream",
      "man",
      "gave",
      "free",
      "ice",
      "creamsick"
    ],
    "token_count":11,
    "processed_text":"lake peep get tan ice cream man gave free ice creamsick"
  },
  {
    "label":0,
    "text":"math exam aaa aaaa aaaaaaaa",
    "cleaned_text":"math exam aaa aaaa aaaaaaaa",
    "normalized_text":"math exam aaa aaaa aaaaaaaa",
    "tokens":[
      "math",
      "exam",
      "aaa",
      "aaaa",
      "aaaaaaaa"
    ],
    "token_count":5,
    "processed_text":"math exam aaa aaaa aaaaaaaa"
  },
  {
    "label":4,
    "text":"hello good morn sheila",
    "cleaned_text":"hello good morn sheila",
    "normalized_text":"hello good morn sheila",
    "tokens":[
      "hello",
      "good",
      "morn",
      "sheila"
    ],
    "token_count":4,
    "processed_text":"hello good morn sheila"
  },
  {
    "label":4,
    "text":"practic test tml best everyon amp haha ppl pleas take care health faster recov",
    "cleaned_text":"practic test tml best everyon amp haha ppl pleas take care health faster recov",
    "normalized_text":"practic test tml best everyon amp haha ppl pleas take care health faster recov",
    "tokens":[
      "practic",
      "test",
      "tml",
      "best",
      "everyon",
      "amp",
      "haha",
      "ppl",
      "plea",
      "take",
      "care",
      "health",
      "faster",
      "recov"
    ],
    "token_count":14,
    "processed_text":"practic test tml best everyon amp haha ppl plea take care health faster recov"
  },
  {
    "label":4,
    "text":"yea fave actorssing jame marster im huug buffyangel fan haha u jojo x",
    "cleaned_text":"yea fave actorssing jame marster im huug buffyangel fan haha u jojo x",
    "normalized_text":"yea fave actorssing jame marster im huug buffyangel fan haha u jojo x",
    "tokens":[
      "yea",
      "fave",
      "actorss",
      "jame",
      "marster",
      "im",
      "huug",
      "buffyangel",
      "fan",
      "haha",
      "jojo"
    ],
    "token_count":11,
    "processed_text":"yea fave actorss jame marster im huug buffyangel fan haha jojo"
  },
  {
    "label":0,
    "text":"neighbor dont play beer pong anymor",
    "cleaned_text":"neighbor dont play beer pong anymor",
    "normalized_text":"neighbor dont play beer pong anymor",
    "tokens":[
      "neighbor",
      "dont",
      "play",
      "beer",
      "pong",
      "anymor"
    ],
    "token_count":6,
    "processed_text":"neighbor dont play beer pong anymor"
  },
  {
    "label":4,
    "text":"watchin nw jakk citi listnin z njc ampamp ju got da salon flinn grattttt",
    "cleaned_text":"watchin nw jakk citi listnin z njc ampamp ju got da salon flinn grattttt",
    "normalized_text":"watchin nw jakk citi listnin z njc ampamp ju got da salon flinn grattttt",
    "tokens":[
      "watchin",
      "nw",
      "jakk",
      "citi",
      "listnin",
      "njc",
      "ampamp",
      "ju",
      "got",
      "da",
      "salon",
      "flinn",
      "grattttt"
    ],
    "token_count":13,
    "processed_text":"watchin nw jakk citi listnin njc ampamp ju got da salon flinn grattttt"
  },
  {
    "label":4,
    "text":"sent direct messag dm offer opportun",
    "cleaned_text":"sent direct messag dm offer opportun",
    "normalized_text":"sent direct messag dm offer opportun",
    "tokens":[
      "sent",
      "direct",
      "messag",
      "dm",
      "offer",
      "opportun"
    ],
    "token_count":6,
    "processed_text":"sent direct messag dm offer opportun"
  },
  {
    "label":0,
    "text":"didnt receiv mail resend sampl",
    "cleaned_text":"didnt receiv mail resend sampl",
    "normalized_text":"didnt receiv mail resend sampl",
    "tokens":[
      "didnt",
      "receiv",
      "mail",
      "resend",
      "sampl"
    ],
    "token_count":5,
    "processed_text":"didnt receiv mail resend sampl"
  },
  {
    "label":4,
    "text":"also weekend good",
    "cleaned_text":"also weekend good",
    "normalized_text":"also weekend good",
    "tokens":[
      "also",
      "weekend",
      "good"
    ],
    "token_count":3,
    "processed_text":"also weekend good"
  },
  {
    "label":0,
    "text":"ooh mannn burn cant eat",
    "cleaned_text":"ooh mannn burn cant eat",
    "normalized_text":"ooh mannn burn cant eat",
    "tokens":[
      "ooh",
      "mannn",
      "burn",
      "cant",
      "eat"
    ],
    "token_count":5,
    "processed_text":"ooh mannn burn cant eat"
  },
  {
    "label":4,
    "text":"spent day beach friend eat lot good food still camp",
    "cleaned_text":"spent day beach friend eat lot good food still camp",
    "normalized_text":"spent day beach friend eat lot good food still camp",
    "tokens":[
      "spent",
      "day",
      "beach",
      "friend",
      "eat",
      "lot",
      "good",
      "food",
      "still",
      "camp"
    ],
    "token_count":10,
    "processed_text":"spent day beach friend eat lot good food still camp"
  },
  {
    "label":0,
    "text":"know miss",
    "cleaned_text":"know miss",
    "normalized_text":"know miss",
    "tokens":[
      "know",
      "miss"
    ],
    "token_count":2,
    "processed_text":"know miss"
  },
  {
    "label":4,
    "text":"feel bit isol well spent bunch time schedul hope help thing",
    "cleaned_text":"feel bit isol well spent bunch time schedul hope help thing",
    "normalized_text":"feel bit isol well spent bunch time schedul hope help thing",
    "tokens":[
      "feel",
      "bit",
      "isol",
      "well",
      "spent",
      "bunch",
      "time",
      "schedul",
      "hope",
      "help",
      "thing"
    ],
    "token_count":11,
    "processed_text":"feel bit isol well spent bunch time schedul hope help thing"
  },
  {
    "label":4,
    "text":"realis someth last night good",
    "cleaned_text":"realis someth last night good",
    "normalized_text":"realis someth last night good",
    "tokens":[
      "reali",
      "someth",
      "last",
      "night",
      "good"
    ],
    "token_count":5,
    "processed_text":"reali someth last night good"
  },
  {
    "label":0,
    "text":"ive hr sleepthi cool",
    "cleaned_text":"ive hr sleepthi cool",
    "normalized_text":"ive hr sleepthi cool",
    "tokens":[
      "ive",
      "hr",
      "sleepthi",
      "cool"
    ],
    "token_count":4,
    "processed_text":"ive hr sleepthi cool"
  },
  {
    "label":0,
    "text":"awwwww would virtual high five make better",
    "cleaned_text":"awwwww would virtual high five make better",
    "normalized_text":"awwwww would virtual high five make better",
    "tokens":[
      "awwwww",
      "virtual",
      "high",
      "five",
      "make",
      "better"
    ],
    "token_count":6,
    "processed_text":"awwwww virtual high five make better"
  },
  {
    "label":0,
    "text":"want becom vegetarian go hard",
    "cleaned_text":"want becom vegetarian go hard",
    "normalized_text":"want becom vegetarian go hard",
    "tokens":[
      "want",
      "becom",
      "vegetarian",
      "go",
      "hard"
    ],
    "token_count":5,
    "processed_text":"want becom vegetarian go hard"
  },
  {
    "label":4,
    "text":"wat upyou know doa tornado could come throught show would hold pole keep watch show",
    "cleaned_text":"wat upyou know doa tornado could come throught show would hold pole keep watch show",
    "normalized_text":"wat upyou know doa tornado could come throught show would hold pole keep watch show",
    "tokens":[
      "wat",
      "upyou",
      "know",
      "doa",
      "tornado",
      "come",
      "throught",
      "show",
      "hold",
      "pole",
      "keep",
      "watch",
      "show"
    ],
    "token_count":13,
    "processed_text":"wat upyou know doa tornado come throught show hold pole keep watch show"
  },
  {
    "label":0,
    "text":"missin bfffffffffffffffffffff",
    "cleaned_text":"missin bfffffffffffffffffffff",
    "normalized_text":"missin bfffffffffffffffffffff",
    "tokens":[
      "missin"
    ],
    "token_count":1,
    "processed_text":"missin"
  },
  {
    "label":4,
    "text":"charliiii got twitter",
    "cleaned_text":"charliiii got twitter",
    "normalized_text":"charliiii got twitter",
    "tokens":[
      "charliiii",
      "got",
      "twitter"
    ],
    "token_count":3,
    "processed_text":"charliiii got twitter"
  },
  {
    "label":0,
    "text":"tkd show sick dont want go school work tomorrow",
    "cleaned_text":"tkd show sick dont want go school work tomorrow",
    "normalized_text":"tkd show sick dont want go school work tomorrow",
    "tokens":[
      "tkd",
      "show",
      "sick",
      "dont",
      "want",
      "go",
      "school",
      "work",
      "tomorrow"
    ],
    "token_count":9,
    "processed_text":"tkd show sick dont want go school work tomorrow"
  },
  {
    "label":4,
    "text":"lol woke hour ago hehe nope work weekend boo lol well hope itll get hotter weekend",
    "cleaned_text":"lol woke hour ago hehe nope work weekend boo lol well hope itll get hotter weekend",
    "normalized_text":"lol woke hour ago hehe nope work weekend boo lol well hope itll get hotter weekend",
    "tokens":[
      "lol",
      "woke",
      "hour",
      "ago",
      "hehe",
      "nope",
      "work",
      "weekend",
      "boo",
      "lol",
      "well",
      "hope",
      "itll",
      "get",
      "hotter",
      "weekend"
    ],
    "token_count":16,
    "processed_text":"lol woke hour ago hehe nope work weekend boo lol well hope itll get hotter weekend"
  },
  {
    "label":0,
    "text":"drunk skunk get im hung",
    "cleaned_text":"drunk skunk get im hung",
    "normalized_text":"drunk skunk get im hung",
    "tokens":[
      "drunk",
      "skunk",
      "get",
      "im",
      "hung"
    ],
    "token_count":5,
    "processed_text":"drunk skunk get im hung"
  },
  {
    "label":0,
    "text":"appl juic gave heart burn",
    "cleaned_text":"appl juic gave heart burn",
    "normalized_text":"appl juic gave heart burn",
    "tokens":[
      "appl",
      "juic",
      "gave",
      "heart",
      "burn"
    ],
    "token_count":5,
    "processed_text":"appl juic gave heart burn"
  },
  {
    "label":4,
    "text":"got mbp back never buy regular laptop",
    "cleaned_text":"got mbp back never buy regular laptop",
    "normalized_text":"got mbp back never buy regular laptop",
    "tokens":[
      "got",
      "mbp",
      "back",
      "never",
      "buy",
      "regular",
      "laptop"
    ],
    "token_count":7,
    "processed_text":"got mbp back never buy regular laptop"
  },
  {
    "label":4,
    "text":"lol ye sweetheart real person",
    "cleaned_text":"lol ye sweetheart real person",
    "normalized_text":"lol ye sweetheart real person",
    "tokens":[
      "lol",
      "ye",
      "sweetheart",
      "real",
      "person"
    ],
    "token_count":5,
    "processed_text":"lol ye sweetheart real person"
  },
  {
    "label":4,
    "text":"gonna lunch get money go surf",
    "cleaned_text":"gonna lunch get money go surf",
    "normalized_text":"gonna lunch get money go surf",
    "tokens":[
      "gon",
      "na",
      "lunch",
      "get",
      "money",
      "go",
      "surf"
    ],
    "token_count":7,
    "processed_text":"gon na lunch get money go surf"
  },
  {
    "label":4,
    "text":"ye full day",
    "cleaned_text":"ye full day",
    "normalized_text":"ye full day",
    "tokens":[
      "ye",
      "full",
      "day"
    ],
    "token_count":3,
    "processed_text":"ye full day"
  },
  {
    "label":0,
    "text":"wish real peopl would follow",
    "cleaned_text":"wish real peopl would follow",
    "normalized_text":"wish real peopl would follow",
    "tokens":[
      "wish",
      "real",
      "peopl",
      "follow"
    ],
    "token_count":4,
    "processed_text":"wish real peopl follow"
  },
  {
    "label":4,
    "text":"sure bring",
    "cleaned_text":"sure bring",
    "normalized_text":"sure bring",
    "tokens":[
      "sure",
      "bring"
    ],
    "token_count":2,
    "processed_text":"sure bring"
  },
  {
    "label":4,
    "text":"busybusybusyy empireampampmmlampampbiancosampampsleepoverampampwarehousethingyy funfunfunn xx",
    "cleaned_text":"busybusybusyy empireampampmmlampampbiancosampampsleepoverampampwarehousethingyy funfunfunn xx",
    "normalized_text":"busybusybusyy empireampampmmlampampbiancosampampsleepoverampampwarehousethingyy funfunfunn xx",
    "tokens":[
      "busybusybusyy",
      "funfunfunn",
      "xx"
    ],
    "token_count":3,
    "processed_text":"busybusybusyy funfunfunn xx"
  },
  {
    "label":4,
    "text":"haha neither nonstud life guess mahin ek baar expens citi",
    "cleaned_text":"haha neither nonstud life guess mahin ek baar expens citi",
    "normalized_text":"haha neither nonstud life guess mahin ek baar expens citi",
    "tokens":[
      "haha",
      "neither",
      "nonstud",
      "life",
      "guess",
      "mahin",
      "ek",
      "baar",
      "expen",
      "citi"
    ],
    "token_count":10,
    "processed_text":"haha neither nonstud life guess mahin ek baar expen citi"
  },
  {
    "label":0,
    "text":"haha squid oooo ya im ganna tri get figur tomorrow im like freakin die without phone",
    "cleaned_text":"haha squid oooo ya im ganna tri get figur tomorrow im like freakin die without phone",
    "normalized_text":"haha squid oooo ya im ganna tri get figur tomorrow im like freakin die without phone",
    "tokens":[
      "haha",
      "squid",
      "oooo",
      "ya",
      "im",
      "ganna",
      "tri",
      "get",
      "figur",
      "tomorrow",
      "im",
      "like",
      "freakin",
      "die",
      "without",
      "phone"
    ],
    "token_count":16,
    "processed_text":"haha squid oooo ya im ganna tri get figur tomorrow im like freakin die without phone"
  },
  {
    "label":4,
    "text":"consid us pay fortun healthcar let kiddi mani sticker want",
    "cleaned_text":"consid us pay fortun healthcar let kiddi mani sticker want",
    "normalized_text":"consid us pay fortun healthcar let kiddi mani sticker want",
    "tokens":[
      "consid",
      "us",
      "pay",
      "fortun",
      "healthcar",
      "let",
      "kiddi",
      "mani",
      "sticker",
      "want"
    ],
    "token_count":10,
    "processed_text":"consid us pay fortun healthcar let kiddi mani sticker want"
  },
  {
    "label":0,
    "text":"hate cat",
    "cleaned_text":"hate cat",
    "normalized_text":"hate cat",
    "tokens":[
      "hate",
      "cat"
    ],
    "token_count":2,
    "processed_text":"hate cat"
  },
  {
    "label":0,
    "text":"bluetooth mous failur blue tooth definit mous",
    "cleaned_text":"bluetooth mous failur blue tooth definit mous",
    "normalized_text":"bluetooth mous failur blue tooth definit mous",
    "tokens":[
      "bluetooth",
      "mou",
      "failur",
      "blue",
      "tooth",
      "definit",
      "mou"
    ],
    "token_count":7,
    "processed_text":"bluetooth mou failur blue tooth definit mou"
  },
  {
    "label":4,
    "text":"practic danc present",
    "cleaned_text":"practic danc present",
    "normalized_text":"practic danc present",
    "tokens":[
      "practic",
      "danc",
      "present"
    ],
    "token_count":3,
    "processed_text":"practic danc present"
  },
  {
    "label":4,
    "text":"ye nice slept window open",
    "cleaned_text":"ye nice slept window open",
    "normalized_text":"ye nice slept window open",
    "tokens":[
      "ye",
      "nice",
      "slept",
      "window",
      "open"
    ],
    "token_count":5,
    "processed_text":"ye nice slept window open"
  },
  {
    "label":4,
    "text":"ye htp race take pole aston martin dutch gt championship junior strou let win race",
    "cleaned_text":"ye htp race take pole aston martin dutch gt championship junior strou let win race",
    "normalized_text":"ye htp race take pole aston martin dutch gt championship junior strou let win race",
    "tokens":[
      "ye",
      "htp",
      "race",
      "take",
      "pole",
      "aston",
      "martin",
      "dutch",
      "gt",
      "championship",
      "junior",
      "strou",
      "let",
      "win",
      "race"
    ],
    "token_count":15,
    "processed_text":"ye htp race take pole aston martin dutch gt championship junior strou let win race"
  },
  {
    "label":4,
    "text":"refersawesomemucha gracia man",
    "cleaned_text":"refersawesomemucha gracia man",
    "normalized_text":"refersawesomemucha gracia man",
    "tokens":[
      "gracia",
      "man"
    ],
    "token_count":2,
    "processed_text":"gracia man"
  },
  {
    "label":0,
    "text":"go bank real dont want go",
    "cleaned_text":"go bank real dont want go",
    "normalized_text":"go bank real dont want go",
    "tokens":[
      "go",
      "bank",
      "real",
      "dont",
      "want",
      "go"
    ],
    "token_count":6,
    "processed_text":"go bank real dont want go"
  },
  {
    "label":4,
    "text":"woooowww cant believ your spain love demi",
    "cleaned_text":"woooowww cant believ your spain love demi",
    "normalized_text":"woooowww cant believ your spain love demi",
    "tokens":[
      "woooowww",
      "cant",
      "believ",
      "spain",
      "love",
      "demi"
    ],
    "token_count":6,
    "processed_text":"woooowww cant believ spain love demi"
  },
  {
    "label":4,
    "text":"havent starbuck hope dont get shaki that prefer coffe bean",
    "cleaned_text":"havent starbuck hope dont get shaki that prefer coffe bean",
    "normalized_text":"havent starbuck hope dont get shaki that prefer coffe bean",
    "tokens":[
      "havent",
      "starbuck",
      "hope",
      "dont",
      "get",
      "shaki",
      "prefer",
      "coff",
      "bean"
    ],
    "token_count":9,
    "processed_text":"havent starbuck hope dont get shaki prefer coff bean"
  },
  {
    "label":0,
    "text":"arent divorcedyet poor kid",
    "cleaned_text":"arent divorcedyet poor kid",
    "normalized_text":"arent divorcedyet poor kid",
    "tokens":[
      "arent",
      "divorcedyet",
      "poor",
      "kid"
    ],
    "token_count":4,
    "processed_text":"arent divorcedyet poor kid"
  },
  {
    "label":4,
    "text":"hug sam go floor tomorrow night well readi mil pretti much finish offic",
    "cleaned_text":"hug sam go floor tomorrow night well readi mil pretti much finish offic",
    "normalized_text":"hug sam go floor tomorrow night well readi mil pretti much finish offic",
    "tokens":[
      "hug",
      "sam",
      "go",
      "floor",
      "tomorrow",
      "night",
      "well",
      "readi",
      "mil",
      "pretti",
      "much",
      "finish",
      "offic"
    ],
    "token_count":13,
    "processed_text":"hug sam go floor tomorrow night well readi mil pretti much finish offic"
  },
  {
    "label":0,
    "text":"didnt get job stuff magazin dammit would amaz oh well",
    "cleaned_text":"didnt get job stuff magazin dammit would amaz oh well",
    "normalized_text":"didnt get job stuff magazin dammit would amaz oh well",
    "tokens":[
      "didnt",
      "get",
      "job",
      "stuff",
      "magazin",
      "dammit",
      "amaz",
      "oh",
      "well"
    ],
    "token_count":9,
    "processed_text":"didnt get job stuff magazin dammit amaz oh well"
  },
  {
    "label":4,
    "text":"dollar footlong",
    "cleaned_text":"dollar footlong",
    "normalized_text":"dollar footlong",
    "tokens":[
      "dollar",
      "footlong"
    ],
    "token_count":2,
    "processed_text":"dollar footlong"
  },
  {
    "label":0,
    "text":"explain wonder wasnt invit got pajama got tooth brush",
    "cleaned_text":"explain wonder wasnt invit got pajama got tooth brush",
    "normalized_text":"explain wonder wasnt invit got pajama got tooth brush",
    "tokens":[
      "explain",
      "wonder",
      "wasnt",
      "invit",
      "got",
      "pajama",
      "got",
      "tooth",
      "brush"
    ],
    "token_count":9,
    "processed_text":"explain wonder wasnt invit got pajama got tooth brush"
  },
  {
    "label":0,
    "text":"dont even want know",
    "cleaned_text":"dont even want know",
    "normalized_text":"dont even want know",
    "tokens":[
      "dont",
      "even",
      "want",
      "know"
    ],
    "token_count":4,
    "processed_text":"dont even want know"
  },
  {
    "label":4,
    "text":"u watch kungfu panda big pictur small pictur pictur see",
    "cleaned_text":"u watch kungfu panda big pictur small pictur pictur see",
    "normalized_text":"u watch kungfu panda big pictur small pictur pictur see",
    "tokens":[
      "watch",
      "kungfu",
      "panda",
      "big",
      "pictur",
      "small",
      "pictur",
      "pictur",
      "see"
    ],
    "token_count":9,
    "processed_text":"watch kungfu panda big pictur small pictur pictur see"
  },
  {
    "label":0,
    "text":"realli want go tonight stupid work tomorrow",
    "cleaned_text":"realli want go tonight stupid work tomorrow",
    "normalized_text":"realli want go tonight stupid work tomorrow",
    "tokens":[
      "realli",
      "want",
      "go",
      "tonight",
      "stupid",
      "work",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"realli want go tonight stupid work tomorrow"
  },
  {
    "label":0,
    "text":"laid goodi noth like feel like complet failur loser get blood flow",
    "cleaned_text":"laid goodi noth like feel like complet failur loser get blood flow",
    "normalized_text":"laid goodi noth like feel like complet failur loser get blood flow",
    "tokens":[
      "laid",
      "goodi",
      "noth",
      "like",
      "feel",
      "like",
      "complet",
      "failur",
      "loser",
      "get",
      "blood",
      "flow"
    ],
    "token_count":12,
    "processed_text":"laid goodi noth like feel like complet failur loser get blood flow"
  },
  {
    "label":4,
    "text":"school season end wednesday think go well juli summer basebal year",
    "cleaned_text":"school season end wednesday think go well juli summer basebal year",
    "normalized_text":"school season end wednesday think go well juli summer basebal year",
    "tokens":[
      "school",
      "season",
      "end",
      "wednesday",
      "think",
      "go",
      "well",
      "juli",
      "summer",
      "baseb",
      "year"
    ],
    "token_count":11,
    "processed_text":"school season end wednesday think go well juli summer baseb year"
  },
  {
    "label":4,
    "text":"see respons",
    "cleaned_text":"see respons",
    "normalized_text":"see respons",
    "tokens":[
      "see",
      "respon"
    ],
    "token_count":2,
    "processed_text":"see respon"
  },
  {
    "label":0,
    "text":"boredboredbor want watch gossip girl fam taken tv never get watch gg suck",
    "cleaned_text":"boredboredbor want watch gossip girl fam taken tv never get watch gg suck",
    "normalized_text":"boredboredbor want watch gossip girl fam taken tv never get watch gg suck",
    "tokens":[
      "boredboredbor",
      "want",
      "watch",
      "gossip",
      "girl",
      "fam",
      "taken",
      "tv",
      "never",
      "get",
      "watch",
      "gg",
      "suck"
    ],
    "token_count":13,
    "processed_text":"boredboredbor want watch gossip girl fam taken tv never get watch gg suck"
  },
  {
    "label":4,
    "text":"ye im selfish see eye want see feel heart want feel morn",
    "cleaned_text":"ye im selfish see eye want see feel heart want feel morn",
    "normalized_text":"ye im selfish see eye want see feel heart want feel morn",
    "tokens":[
      "ye",
      "im",
      "selfish",
      "see",
      "eye",
      "want",
      "see",
      "feel",
      "heart",
      "want",
      "feel",
      "morn"
    ],
    "token_count":12,
    "processed_text":"ye im selfish see eye want see feel heart want feel morn"
  },
  {
    "label":0,
    "text":"cranki today car accid",
    "cleaned_text":"cranki today car accid",
    "normalized_text":"cranki today car accid",
    "tokens":[
      "cranki",
      "today",
      "car",
      "accid"
    ],
    "token_count":4,
    "processed_text":"cranki today car accid"
  },
  {
    "label":0,
    "text":"itchi eye",
    "cleaned_text":"itchi eye",
    "normalized_text":"itchi eye",
    "tokens":[
      "itchi",
      "eye"
    ],
    "token_count":2,
    "processed_text":"itchi eye"
  },
  {
    "label":0,
    "text":"mani work miss sum special",
    "cleaned_text":"mani work miss sum special",
    "normalized_text":"mani work miss sum special",
    "tokens":[
      "mani",
      "work",
      "miss",
      "sum",
      "special"
    ],
    "token_count":5,
    "processed_text":"mani work miss sum special"
  },
  {
    "label":0,
    "text":"hurt wrist pleas pleas pleas ok tomorrow",
    "cleaned_text":"hurt wrist pleas pleas pleas ok tomorrow",
    "normalized_text":"hurt wrist pleas pleas pleas ok tomorrow",
    "tokens":[
      "hurt",
      "wrist",
      "plea",
      "plea",
      "plea",
      "ok",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"hurt wrist plea plea plea ok tomorrow"
  },
  {
    "label":4,
    "text":"hmm alright well case im who pay ticket lol messin thatd fun though",
    "cleaned_text":"hmm alright well case im who pay ticket lol messin thatd fun though",
    "normalized_text":"hmm alright well case im who pay ticket lol messin thatd fun though",
    "tokens":[
      "hmm",
      "alright",
      "well",
      "case",
      "im",
      "pay",
      "ticket",
      "lol",
      "messin",
      "thatd",
      "fun",
      "though"
    ],
    "token_count":12,
    "processed_text":"hmm alright well case im pay ticket lol messin thatd fun though"
  },
  {
    "label":4,
    "text":"happi birthday",
    "cleaned_text":"happi birthday",
    "normalized_text":"happi birthday",
    "tokens":[
      "happi",
      "birthday"
    ],
    "token_count":2,
    "processed_text":"happi birthday"
  },
  {
    "label":0,
    "text":"lol idk bb im kinda everyth atm lol p",
    "cleaned_text":"lol idk bb im kinda everyth atm lol p",
    "normalized_text":"lol idk bb im kinda everyth atm lol p",
    "tokens":[
      "lol",
      "idk",
      "bb",
      "im",
      "kinda",
      "everyth",
      "atm",
      "lol"
    ],
    "token_count":8,
    "processed_text":"lol idk bb im kinda everyth atm lol"
  },
  {
    "label":0,
    "text":"dont think rain much harder",
    "cleaned_text":"dont think rain much harder",
    "normalized_text":"dont think rain much harder",
    "tokens":[
      "dont",
      "think",
      "rain",
      "much",
      "harder"
    ],
    "token_count":5,
    "processed_text":"dont think rain much harder"
  },
  {
    "label":0,
    "text":"man make feel like fuck shit fuck life hardcor",
    "cleaned_text":"man make feel like fuck shit fuck life hardcor",
    "normalized_text":"man make feel like fuck shit fuck life hardcor",
    "tokens":[
      "man",
      "make",
      "feel",
      "like",
      "fuck",
      "shit",
      "fuck",
      "life",
      "hardcor"
    ],
    "token_count":9,
    "processed_text":"man make feel like fuck shit fuck life hardcor"
  },
  {
    "label":0,
    "text":"awesom nooner run sad miss race",
    "cleaned_text":"awesom nooner run sad miss race",
    "normalized_text":"awesom nooner run sad miss race",
    "tokens":[
      "awesom",
      "nooner",
      "run",
      "sad",
      "miss",
      "race"
    ],
    "token_count":6,
    "processed_text":"awesom nooner run sad miss race"
  },
  {
    "label":0,
    "text":"tivo remot far away",
    "cleaned_text":"tivo remot far away",
    "normalized_text":"tivo remot far away",
    "tokens":[
      "tivo",
      "remot",
      "far",
      "away"
    ],
    "token_count":4,
    "processed_text":"tivo remot far away"
  },
  {
    "label":0,
    "text":"wonder much twat must come today",
    "cleaned_text":"wonder much twat must come today",
    "normalized_text":"wonder much twat must come today",
    "tokens":[
      "wonder",
      "much",
      "twat",
      "come",
      "today"
    ],
    "token_count":5,
    "processed_text":"wonder much twat come today"
  },
  {
    "label":4,
    "text":"walk dog traci pick kid",
    "cleaned_text":"walk dog traci pick kid",
    "normalized_text":"walk dog traci pick kid",
    "tokens":[
      "walk",
      "dog",
      "traci",
      "pick",
      "kid"
    ],
    "token_count":5,
    "processed_text":"walk dog traci pick kid"
  },
  {
    "label":4,
    "text":"enjoy sun often u say uk",
    "cleaned_text":"enjoy sun often u say uk",
    "normalized_text":"enjoy sun often u say uk",
    "tokens":[
      "enjoy",
      "sun",
      "often",
      "say",
      "uk"
    ],
    "token_count":5,
    "processed_text":"enjoy sun often say uk"
  },
  {
    "label":4,
    "text":"nope noth tv gonna watch dvd dont know im mood give suggest xx",
    "cleaned_text":"nope noth tv gonna watch dvd dont know im mood give suggest xx",
    "normalized_text":"nope noth tv gonna watch dvd dont know im mood give suggest xx",
    "tokens":[
      "nope",
      "noth",
      "tv",
      "gon",
      "na",
      "watch",
      "dvd",
      "dont",
      "know",
      "im",
      "mood",
      "give",
      "suggest",
      "xx"
    ],
    "token_count":14,
    "processed_text":"nope noth tv gon na watch dvd dont know im mood give suggest xx"
  },
  {
    "label":4,
    "text":"knowww tell what deal red wing",
    "cleaned_text":"knowww tell what deal red wing",
    "normalized_text":"knowww tell what deal red wing",
    "tokens":[
      "knowww",
      "tell",
      "deal",
      "red",
      "wing"
    ],
    "token_count":5,
    "processed_text":"knowww tell deal red wing"
  },
  {
    "label":4,
    "text":"proud littlest loskutoff graduat well",
    "cleaned_text":"proud littlest loskutoff graduat well",
    "normalized_text":"proud littlest loskutoff graduat well",
    "tokens":[
      "proud",
      "littlest",
      "loskutoff",
      "graduat",
      "well"
    ],
    "token_count":5,
    "processed_text":"proud littlest loskutoff graduat well"
  },
  {
    "label":4,
    "text":"pretti much watch tv today decid like",
    "cleaned_text":"pretti much watch tv today decid like",
    "normalized_text":"pretti much watch tv today decid like",
    "tokens":[
      "pretti",
      "much",
      "watch",
      "tv",
      "today",
      "decid",
      "like"
    ],
    "token_count":7,
    "processed_text":"pretti much watch tv today decid like"
  },
  {
    "label":0,
    "text":"hard time watch law amp order svu ep deal child sex slave uganda realli depress",
    "cleaned_text":"hard time watch law amp order svu ep deal child sex slave uganda realli depress",
    "normalized_text":"hard time watch law amp order svu ep deal child sex slave uganda realli depress",
    "tokens":[
      "hard",
      "time",
      "watch",
      "law",
      "amp",
      "order",
      "svu",
      "ep",
      "deal",
      "child",
      "sex",
      "slave",
      "uganda",
      "realli",
      "depress"
    ],
    "token_count":15,
    "processed_text":"hard time watch law amp order svu ep deal child sex slave uganda realli depress"
  },
  {
    "label":0,
    "text":"bawl eye marley",
    "cleaned_text":"bawl eye marley",
    "normalized_text":"bawl eye marley",
    "tokens":[
      "bawl",
      "eye",
      "marley"
    ],
    "token_count":3,
    "processed_text":"bawl eye marley"
  },
  {
    "label":4,
    "text":"ye im def geeki girli chic addict work dont sleep much",
    "cleaned_text":"ye im def geeki girli chic addict work dont sleep much",
    "normalized_text":"ye im def geeki girli chic addict work dont sleep much",
    "tokens":[
      "ye",
      "im",
      "def",
      "geeki",
      "girli",
      "chic",
      "addict",
      "work",
      "dont",
      "sleep",
      "much"
    ],
    "token_count":11,
    "processed_text":"ye im def geeki girli chic addict work dont sleep much"
  },
  {
    "label":0,
    "text":"oh im sorri thought beauti dog ill hug lab tight tonight",
    "cleaned_text":"oh im sorri thought beauti dog ill hug lab tight tonight",
    "normalized_text":"oh im sorri thought beauti dog ill hug lab tight tonight",
    "tokens":[
      "oh",
      "im",
      "sorri",
      "thought",
      "beauti",
      "dog",
      "ill",
      "hug",
      "lab",
      "tight",
      "tonight"
    ],
    "token_count":11,
    "processed_text":"oh im sorri thought beauti dog ill hug lab tight tonight"
  },
  {
    "label":0,
    "text":"sorri folk meet went longer anticip much look like miss parti knittertwitt",
    "cleaned_text":"sorri folk meet went longer anticip much look like miss parti knittertwitt",
    "normalized_text":"sorri folk meet went longer anticip much look like miss parti knittertwitt",
    "tokens":[
      "sorri",
      "folk",
      "meet",
      "went",
      "longer",
      "anticip",
      "much",
      "look",
      "like",
      "miss",
      "parti",
      "knittertwitt"
    ],
    "token_count":12,
    "processed_text":"sorri folk meet went longer anticip much look like miss parti knittertwitt"
  },
  {
    "label":0,
    "text":"think ish r u",
    "cleaned_text":"think ish r u",
    "normalized_text":"think ish r u",
    "tokens":[
      "think",
      "ish"
    ],
    "token_count":2,
    "processed_text":"think ish"
  },
  {
    "label":4,
    "text":"go downstair coffe socialis wow give time french later",
    "cleaned_text":"go downstair coffe socialis wow give time french later",
    "normalized_text":"go downstair coffe socialis wow give time french later",
    "tokens":[
      "go",
      "downstair",
      "coff",
      "sociali",
      "wow",
      "give",
      "time",
      "french",
      "later"
    ],
    "token_count":9,
    "processed_text":"go downstair coff sociali wow give time french later"
  },
  {
    "label":0,
    "text":"work w headach that sit behind eye look comput screen isnt help anygtgtgt",
    "cleaned_text":"work w headach that sit behind eye look comput screen isnt help anygtgtgt",
    "normalized_text":"work w headach that sit behind eye look comput screen isnt help anygtgtgt",
    "tokens":[
      "work",
      "headach",
      "sit",
      "behind",
      "eye",
      "look",
      "comput",
      "screen",
      "isnt",
      "help",
      "anygtgtgt"
    ],
    "token_count":11,
    "processed_text":"work headach sit behind eye look comput screen isnt help anygtgtgt"
  },
  {
    "label":0,
    "text":"cours shitti day im gonna go kiyak",
    "cleaned_text":"cours shitti day im gonna go kiyak",
    "normalized_text":"cours shitti day im gonna go kiyak",
    "tokens":[
      "cour",
      "shitti",
      "day",
      "im",
      "gon",
      "na",
      "go",
      "kiyak"
    ],
    "token_count":8,
    "processed_text":"cour shitti day im gon na go kiyak"
  },
  {
    "label":4,
    "text":"bad idea",
    "cleaned_text":"bad idea",
    "normalized_text":"bad idea",
    "tokens":[
      "bad",
      "idea"
    ],
    "token_count":2,
    "processed_text":"bad idea"
  },
  {
    "label":0,
    "text":"yaay burn dinner",
    "cleaned_text":"yaay burn dinner",
    "normalized_text":"yaay burn dinner",
    "tokens":[
      "yaay",
      "burn",
      "dinner"
    ],
    "token_count":3,
    "processed_text":"yaay burn dinner"
  },
  {
    "label":0,
    "text":"realli upset live australia jona doesnt come till june",
    "cleaned_text":"realli upset live australia jona doesnt come till june",
    "normalized_text":"realli upset live australia jona doesnt come till june",
    "tokens":[
      "realli",
      "upset",
      "live",
      "australia",
      "jona",
      "doesnt",
      "come",
      "till",
      "june"
    ],
    "token_count":9,
    "processed_text":"realli upset live australia jona doesnt come till june"
  },
  {
    "label":0,
    "text":"realli dont wanna get bed",
    "cleaned_text":"realli dont wanna get bed",
    "normalized_text":"realli dont wanna get bed",
    "tokens":[
      "realli",
      "dont",
      "wan",
      "na",
      "get",
      "bed"
    ],
    "token_count":6,
    "processed_text":"realli dont wan na get bed"
  },
  {
    "label":0,
    "text":"wish freak registr work",
    "cleaned_text":"wish freak registr work",
    "normalized_text":"wish freak registr work",
    "tokens":[
      "wish",
      "freak",
      "registr",
      "work"
    ],
    "token_count":4,
    "processed_text":"wish freak registr work"
  },
  {
    "label":0,
    "text":"hi see lot soon goin thru ish right",
    "cleaned_text":"hi see lot soon goin thru ish right",
    "normalized_text":"hi see lot soon goin thru ish right",
    "tokens":[
      "hi",
      "see",
      "lot",
      "soon",
      "goin",
      "thru",
      "ish",
      "right"
    ],
    "token_count":8,
    "processed_text":"hi see lot soon goin thru ish right"
  },
  {
    "label":0,
    "text":"doctor ian",
    "cleaned_text":"doctor ian",
    "normalized_text":"doctor ian",
    "tokens":[
      "doctor",
      "ian"
    ],
    "token_count":2,
    "processed_text":"doctor ian"
  },
  {
    "label":4,
    "text":"sound amaz get better",
    "cleaned_text":"sound amaz get better",
    "normalized_text":"sound amaz get better",
    "tokens":[
      "sound",
      "amaz",
      "get",
      "better"
    ],
    "token_count":4,
    "processed_text":"sound amaz get better"
  },
  {
    "label":0,
    "text":"wish rain would go away",
    "cleaned_text":"wish rain would go away",
    "normalized_text":"wish rain would go away",
    "tokens":[
      "wish",
      "rain",
      "go",
      "away"
    ],
    "token_count":4,
    "processed_text":"wish rain go away"
  },
  {
    "label":0,
    "text":"us get work",
    "cleaned_text":"us get work",
    "normalized_text":"us get work",
    "tokens":[
      "us",
      "get",
      "work"
    ],
    "token_count":3,
    "processed_text":"us get work"
  },
  {
    "label":4,
    "text":"ill give u massag",
    "cleaned_text":"ill give u massag",
    "normalized_text":"ill give u massag",
    "tokens":[
      "ill",
      "give",
      "massag"
    ],
    "token_count":3,
    "processed_text":"ill give massag"
  },
  {
    "label":4,
    "text":"inde good help",
    "cleaned_text":"inde good help",
    "normalized_text":"inde good help",
    "tokens":[
      "ind",
      "good",
      "help"
    ],
    "token_count":3,
    "processed_text":"ind good help"
  },
  {
    "label":0,
    "text":"tri watch everyth english possibl g even cinema passau wont show film origin voic",
    "cleaned_text":"tri watch everyth english possibl g even cinema passau wont show film origin voic",
    "normalized_text":"tri watch everyth english possibl g even cinema passau wont show film origin voic",
    "tokens":[
      "tri",
      "watch",
      "everyth",
      "english",
      "possibl",
      "even",
      "cinema",
      "passau",
      "wont",
      "show",
      "film",
      "origin",
      "voic"
    ],
    "token_count":13,
    "processed_text":"tri watch everyth english possibl even cinema passau wont show film origin voic"
  },
  {
    "label":0,
    "text":"omgim upset must shitti person someth",
    "cleaned_text":"omgim upset must shitti person someth",
    "normalized_text":"omgim upset must shitti person someth",
    "tokens":[
      "omgim",
      "upset",
      "shitti",
      "person",
      "someth"
    ],
    "token_count":5,
    "processed_text":"omgim upset shitti person someth"
  },
  {
    "label":0,
    "text":"work work work",
    "cleaned_text":"work work work",
    "normalized_text":"work work work",
    "tokens":[
      "work",
      "work",
      "work"
    ],
    "token_count":3,
    "processed_text":"work work work"
  },
  {
    "label":4,
    "text":"today rememb smell popcorn brain freez vapor rub olden day",
    "cleaned_text":"today rememb smell popcorn brain freez vapor rub olden day",
    "normalized_text":"today rememb smell popcorn brain freez vapor rub olden day",
    "tokens":[
      "today",
      "rememb",
      "smell",
      "popcorn",
      "brain",
      "freez",
      "vapor",
      "rub",
      "olden",
      "day"
    ],
    "token_count":10,
    "processed_text":"today rememb smell popcorn brain freez vapor rub olden day"
  },
  {
    "label":0,
    "text":"yeah found bit seem plane atlant ocean",
    "cleaned_text":"yeah found bit seem plane atlant ocean",
    "normalized_text":"yeah found bit seem plane atlant ocean",
    "tokens":[
      "yeah",
      "found",
      "bit",
      "seem",
      "plane",
      "atlant",
      "ocean"
    ],
    "token_count":7,
    "processed_text":"yeah found bit seem plane atlant ocean"
  },
  {
    "label":4,
    "text":"lolz well princess voic would definit seal deal hope your feel better",
    "cleaned_text":"lolz well princess voic would definit seal deal hope your feel better",
    "normalized_text":"lolz well princess voic would definit seal deal hope your feel better",
    "tokens":[
      "lolz",
      "well",
      "princess",
      "voic",
      "definit",
      "seal",
      "deal",
      "hope",
      "feel",
      "better"
    ],
    "token_count":10,
    "processed_text":"lolz well princess voic definit seal deal hope feel better"
  },
  {
    "label":0,
    "text":"holi crap threw isnt anyth throw",
    "cleaned_text":"holi crap threw isnt anyth throw",
    "normalized_text":"holi crap threw isnt anyth throw",
    "tokens":[
      "holi",
      "crap",
      "threw",
      "isnt",
      "anyth",
      "throw"
    ],
    "token_count":6,
    "processed_text":"holi crap threw isnt anyth throw"
  },
  {
    "label":0,
    "text":"andi miss",
    "cleaned_text":"andi miss",
    "normalized_text":"andi miss",
    "tokens":[
      "andi",
      "miss"
    ],
    "token_count":2,
    "processed_text":"andi miss"
  },
  {
    "label":0,
    "text":"need brand oth cant tonightwhen mum get home w x",
    "cleaned_text":"need brand oth cant tonightwhen mum get home w x",
    "normalized_text":"need brand oth cant tonightwhen mum get home w x",
    "tokens":[
      "need",
      "brand",
      "oth",
      "cant",
      "tonightwhen",
      "mum",
      "get",
      "home"
    ],
    "token_count":8,
    "processed_text":"need brand oth cant tonightwhen mum get home"
  },
  {
    "label":4,
    "text":"feed greedi ray sonshin",
    "cleaned_text":"feed greedi ray sonshin",
    "normalized_text":"feed greedi ray sonshin",
    "tokens":[
      "feed",
      "greedi",
      "ray",
      "sonshin"
    ],
    "token_count":4,
    "processed_text":"feed greedi ray sonshin"
  },
  {
    "label":0,
    "text":"omg friend sweet young girl knew murder psycho month ago break heart",
    "cleaned_text":"omg friend sweet young girl knew murder psycho month ago break heart",
    "normalized_text":"omg friend sweet young girl knew murder psycho month ago break heart",
    "tokens":[
      "omg",
      "friend",
      "sweet",
      "young",
      "girl",
      "knew",
      "murder",
      "psycho",
      "month",
      "ago",
      "break",
      "heart"
    ],
    "token_count":12,
    "processed_text":"omg friend sweet young girl knew murder psycho month ago break heart"
  },
  {
    "label":0,
    "text":"awe your right slack",
    "cleaned_text":"awe your right slack",
    "normalized_text":"awe your right slack",
    "tokens":[
      "awe",
      "right",
      "slack"
    ],
    "token_count":3,
    "processed_text":"awe right slack"
  },
  {
    "label":4,
    "text":"tell didnt make day happier though lt",
    "cleaned_text":"tell didnt make day happier though lt",
    "normalized_text":"tell didnt make day happier though lt",
    "tokens":[
      "tell",
      "didnt",
      "make",
      "day",
      "happier",
      "though",
      "lt"
    ],
    "token_count":7,
    "processed_text":"tell didnt make day happier though lt"
  },
  {
    "label":0,
    "text":"tire look wholesal bead find want",
    "cleaned_text":"tire look wholesal bead find want",
    "normalized_text":"tire look wholesal bead find want",
    "tokens":[
      "tire",
      "look",
      "wholes",
      "bead",
      "find",
      "want"
    ],
    "token_count":6,
    "processed_text":"tire look wholes bead find want"
  },
  {
    "label":0,
    "text":"daili high got lot stuff done daili low mani tanner today",
    "cleaned_text":"daili high got lot stuff done daili low mani tanner today",
    "normalized_text":"daili high got lot stuff done daili low mani tanner today",
    "tokens":[
      "daili",
      "high",
      "got",
      "lot",
      "stuff",
      "done",
      "daili",
      "low",
      "mani",
      "tanner",
      "today"
    ],
    "token_count":11,
    "processed_text":"daili high got lot stuff done daili low mani tanner today"
  },
  {
    "label":4,
    "text":"pain time lunch nilli yay amp new underwear vs",
    "cleaned_text":"pain time lunch nilli yay amp new underwear vs",
    "normalized_text":"pain time lunch nilli yay amp new underwear vs",
    "tokens":[
      "pain",
      "time",
      "lunch",
      "nilli",
      "yay",
      "amp",
      "new",
      "underwear",
      "vs"
    ],
    "token_count":9,
    "processed_text":"pain time lunch nilli yay amp new underwear vs"
  },
  {
    "label":0,
    "text":"wish bfd",
    "cleaned_text":"wish bfd",
    "normalized_text":"wish bfd",
    "tokens":[
      "wish",
      "bfd"
    ],
    "token_count":2,
    "processed_text":"wish bfd"
  },
  {
    "label":4,
    "text":"write new song album work record album number tomorrow",
    "cleaned_text":"write new song album work record album number tomorrow",
    "normalized_text":"write new song album work record album number tomorrow",
    "tokens":[
      "write",
      "new",
      "song",
      "album",
      "work",
      "record",
      "album",
      "number",
      "tomorrow"
    ],
    "token_count":9,
    "processed_text":"write new song album work record album number tomorrow"
  },
  {
    "label":4,
    "text":"yeaaah mark love hoppusday",
    "cleaned_text":"yeaaah mark love hoppusday",
    "normalized_text":"yeaaah mark love hoppusday",
    "tokens":[
      "yeaaah",
      "mark",
      "love",
      "hoppusday"
    ],
    "token_count":4,
    "processed_text":"yeaaah mark love hoppusday"
  },
  {
    "label":4,
    "text":"havent busi late",
    "cleaned_text":"havent busi late",
    "normalized_text":"havent busi late",
    "tokens":[
      "havent",
      "busi",
      "late"
    ],
    "token_count":3,
    "processed_text":"havent busi late"
  },
  {
    "label":0,
    "text":"far behind schedul wed left camera hotel guess page wont pic",
    "cleaned_text":"far behind schedul wed left camera hotel guess page wont pic",
    "normalized_text":"far behind schedul wed left camera hotel guess page wont pic",
    "tokens":[
      "far",
      "behind",
      "schedul",
      "wed",
      "left",
      "camera",
      "hotel",
      "guess",
      "page",
      "wont",
      "pic"
    ],
    "token_count":11,
    "processed_text":"far behind schedul wed left camera hotel guess page wont pic"
  },
  {
    "label":0,
    "text":"think react come money realli need stop obsess save healthi",
    "cleaned_text":"think react come money realli need stop obsess save healthi",
    "normalized_text":"think react come money realli need stop obsess save healthi",
    "tokens":[
      "think",
      "react",
      "come",
      "money",
      "realli",
      "need",
      "stop",
      "obsess",
      "save",
      "healthi"
    ],
    "token_count":10,
    "processed_text":"think react come money realli need stop obsess save healthi"
  },
  {
    "label":0,
    "text":"way summer jam damn isnt gonna",
    "cleaned_text":"way summer jam damn isnt gonna",
    "normalized_text":"way summer jam damn isnt gonna",
    "tokens":[
      "way",
      "summer",
      "jam",
      "damn",
      "isnt",
      "gon",
      "na"
    ],
    "token_count":7,
    "processed_text":"way summer jam damn isnt gon na"
  },
  {
    "label":0,
    "text":"want go work lol davi fl",
    "cleaned_text":"want go work lol davi fl",
    "normalized_text":"want go work lol davi fl",
    "tokens":[
      "want",
      "go",
      "work",
      "lol",
      "davi",
      "fl"
    ],
    "token_count":6,
    "processed_text":"want go work lol davi fl"
  },
  {
    "label":0,
    "text":"hmph interest yeah whenev would drink coke teeth felt like strip squeaki",
    "cleaned_text":"hmph interest yeah whenev would drink coke teeth felt like strip squeaki",
    "normalized_text":"hmph interest yeah whenev would drink coke teeth felt like strip squeaki",
    "tokens":[
      "hmph",
      "interest",
      "yeah",
      "whenev",
      "drink",
      "coke",
      "teeth",
      "felt",
      "like",
      "strip",
      "squeaki"
    ],
    "token_count":11,
    "processed_text":"hmph interest yeah whenev drink coke teeth felt like strip squeaki"
  },
  {
    "label":4,
    "text":"anyon test new exe could pleas open task manag watch quotmemoryquot process tab engineex kthx",
    "cleaned_text":"anyon test new exe could pleas open task manag watch quotmemoryquot process tab engineex kthx",
    "normalized_text":"anyon test new exe could pleas open task manag watch quotmemoryquot process tab engineex kthx",
    "tokens":[
      "anyon",
      "test",
      "new",
      "exe",
      "plea",
      "open",
      "task",
      "manag",
      "watch",
      "quotmemoryquot",
      "process",
      "tab",
      "engineex",
      "kthx"
    ],
    "token_count":14,
    "processed_text":"anyon test new exe plea open task manag watch quotmemoryquot process tab engineex kthx"
  },
  {
    "label":4,
    "text":"listen fm static",
    "cleaned_text":"listen fm static",
    "normalized_text":"listen fm static",
    "tokens":[
      "listen",
      "fm",
      "static"
    ],
    "token_count":3,
    "processed_text":"listen fm static"
  },
  {
    "label":4,
    "text":"yep muzzl secret servic amp requir write content jame blackboard xday",
    "cleaned_text":"yep muzzl secret servic amp requir write content jame blackboard xday",
    "normalized_text":"yep muzzl secret servic amp requir write content jame blackboard xday",
    "tokens":[
      "yep",
      "muzzl",
      "secret",
      "servic",
      "amp",
      "requir",
      "write",
      "content",
      "jame",
      "blackboard",
      "xday"
    ],
    "token_count":11,
    "processed_text":"yep muzzl secret servic amp requir write content jame blackboard xday"
  },
  {
    "label":4,
    "text":"haha that smart def talk lot esp im nervou inevit say dumb thing",
    "cleaned_text":"haha that smart def talk lot esp im nervou inevit say dumb thing",
    "normalized_text":"haha that smart def talk lot esp im nervou inevit say dumb thing",
    "tokens":[
      "haha",
      "smart",
      "def",
      "talk",
      "lot",
      "esp",
      "im",
      "nervou",
      "inevit",
      "say",
      "dumb",
      "thing"
    ],
    "token_count":12,
    "processed_text":"haha smart def talk lot esp im nervou inevit say dumb thing"
  },
  {
    "label":4,
    "text":"keep say speak think time go mine",
    "cleaned_text":"keep say speak think time go mine",
    "normalized_text":"keep say speak think time go mine",
    "tokens":[
      "keep",
      "say",
      "speak",
      "think",
      "time",
      "go",
      "mine"
    ],
    "token_count":7,
    "processed_text":"keep say speak think time go mine"
  },
  {
    "label":4,
    "text":"woke weird dream kinda gross anywho im bore go oliv garden later mayb red lobster bday kay kay xoxo",
    "cleaned_text":"woke weird dream kinda gross anywho im bore go oliv garden later mayb red lobster bday kay kay xoxo",
    "normalized_text":"woke weird dream kinda gross anywho im bore go oliv garden later mayb red lobster bday kay kay xoxo",
    "tokens":[
      "woke",
      "weird",
      "dream",
      "kinda",
      "gross",
      "anywho",
      "im",
      "bore",
      "go",
      "oliv",
      "garden",
      "later",
      "mayb",
      "red",
      "lobster",
      "bday",
      "kay",
      "kay",
      "xoxo"
    ],
    "token_count":19,
    "processed_text":"woke weird dream kinda gross anywho im bore go oliv garden later mayb red lobster bday kay kay xoxo"
  },
  {
    "label":0,
    "text":"wake didnt sleep well feel mix emot today gonna start pack amp finish take home trig test excit",
    "cleaned_text":"wake didnt sleep well feel mix emot today gonna start pack amp finish take home trig test excit",
    "normalized_text":"wake didnt sleep well feel mix emot today gonna start pack amp finish take home trig test excit",
    "tokens":[
      "wake",
      "didnt",
      "sleep",
      "well",
      "feel",
      "mix",
      "emot",
      "today",
      "gon",
      "na",
      "start",
      "pack",
      "amp",
      "finish",
      "take",
      "home",
      "trig",
      "test",
      "excit"
    ],
    "token_count":19,
    "processed_text":"wake didnt sleep well feel mix emot today gon na start pack amp finish take home trig test excit"
  },
  {
    "label":0,
    "text":"whydidnt research excit school",
    "cleaned_text":"whydidnt research excit school",
    "normalized_text":"whydidnt research excit school",
    "tokens":[
      "whydidnt",
      "research",
      "excit",
      "school"
    ],
    "token_count":4,
    "processed_text":"whydidnt research excit school"
  },
  {
    "label":4,
    "text":"uh know wonder asian month may",
    "cleaned_text":"uh know wonder asian month may",
    "normalized_text":"uh know wonder asian month may",
    "tokens":[
      "uh",
      "know",
      "wonder",
      "asian",
      "month",
      "may"
    ],
    "token_count":6,
    "processed_text":"uh know wonder asian month may"
  },
  {
    "label":4,
    "text":"key advic b alway readi anyth good luck tell greeley hi plz",
    "cleaned_text":"key advic b alway readi anyth good luck tell greeley hi plz",
    "normalized_text":"key advic b alway readi anyth good luck tell greeley hi plz",
    "tokens":[
      "key",
      "advic",
      "alway",
      "readi",
      "anyth",
      "good",
      "luck",
      "tell",
      "greeley",
      "hi",
      "plz"
    ],
    "token_count":11,
    "processed_text":"key advic alway readi anyth good luck tell greeley hi plz"
  },
  {
    "label":0,
    "text":"cute guy bu last night come back frim wl got chatswood didnt get number devo",
    "cleaned_text":"cute guy bu last night come back frim wl got chatswood didnt get number devo",
    "normalized_text":"cute guy bu last night come back frim wl got chatswood didnt get number devo",
    "tokens":[
      "cute",
      "guy",
      "bu",
      "last",
      "night",
      "come",
      "back",
      "frim",
      "wl",
      "got",
      "chatswood",
      "didnt",
      "get",
      "number",
      "devo"
    ],
    "token_count":15,
    "processed_text":"cute guy bu last night come back frim wl got chatswood didnt get number devo"
  },
  {
    "label":0,
    "text":"battl scar left elbow",
    "cleaned_text":"battl scar left elbow",
    "normalized_text":"battl scar left elbow",
    "tokens":[
      "battl",
      "scar",
      "left",
      "elbow"
    ],
    "token_count":4,
    "processed_text":"battl scar left elbow"
  },
  {
    "label":4,
    "text":"laughin ass",
    "cleaned_text":"laughin ass",
    "normalized_text":"laughin ass",
    "tokens":[
      "laughin",
      "ass"
    ],
    "token_count":2,
    "processed_text":"laughin ass"
  },
  {
    "label":4,
    "text":"ye nag twitter haha thank lm",
    "cleaned_text":"ye nag twitter haha thank lm",
    "normalized_text":"ye nag twitter haha thank lm",
    "tokens":[
      "ye",
      "nag",
      "twitter",
      "haha",
      "thank",
      "lm"
    ],
    "token_count":6,
    "processed_text":"ye nag twitter haha thank lm"
  },
  {
    "label":4,
    "text":"dream reject life shitti im go work",
    "cleaned_text":"dream reject life shitti im go work",
    "normalized_text":"dream reject life shitti im go work",
    "tokens":[
      "dream",
      "reject",
      "life",
      "shitti",
      "im",
      "go",
      "work"
    ],
    "token_count":7,
    "processed_text":"dream reject life shitti im go work"
  },
  {
    "label":0,
    "text":"hope go tonight",
    "cleaned_text":"hope go tonight",
    "normalized_text":"hope go tonight",
    "tokens":[
      "hope",
      "go",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"hope go tonight"
  },
  {
    "label":4,
    "text":"great photoshoot today chri ryan roll stone keegan smith amp fam may open dave matthew band summer eek",
    "cleaned_text":"great photoshoot today chri ryan roll stone keegan smith amp fam may open dave matthew band summer eek",
    "normalized_text":"great photoshoot today chri ryan roll stone keegan smith amp fam may open dave matthew band summer eek",
    "tokens":[
      "great",
      "photoshoot",
      "today",
      "chri",
      "ryan",
      "roll",
      "stone",
      "keegan",
      "smith",
      "amp",
      "fam",
      "may",
      "open",
      "dave",
      "matthew",
      "band",
      "summer",
      "eek"
    ],
    "token_count":18,
    "processed_text":"great photoshoot today chri ryan roll stone keegan smith amp fam may open dave matthew band summer eek"
  },
  {
    "label":0,
    "text":"ill bring laptop work burn im wait lol",
    "cleaned_text":"ill bring laptop work burn im wait lol",
    "normalized_text":"ill bring laptop work burn im wait lol",
    "tokens":[
      "ill",
      "bring",
      "laptop",
      "work",
      "burn",
      "im",
      "wait",
      "lol"
    ],
    "token_count":8,
    "processed_text":"ill bring laptop work burn im wait lol"
  },
  {
    "label":4,
    "text":"decid stop follow norway place",
    "cleaned_text":"decid stop follow norway place",
    "normalized_text":"decid stop follow norway place",
    "tokens":[
      "decid",
      "stop",
      "follow",
      "norway",
      "place"
    ],
    "token_count":5,
    "processed_text":"decid stop follow norway place"
  },
  {
    "label":0,
    "text":"pagiii semuanyahav good day hari ini presentasihari terakhir ke kampu sblm ua",
    "cleaned_text":"pagiii semuanyahav good day hari ini presentasihari terakhir ke kampu sblm ua",
    "normalized_text":"pagiii semuanyahav good day hari ini presentasihari terakhir ke kampu sblm ua",
    "tokens":[
      "pagiii",
      "semuanyahav",
      "good",
      "day",
      "hari",
      "ini",
      "presentasihari",
      "terakhir",
      "ke",
      "kampu",
      "sblm",
      "ua"
    ],
    "token_count":12,
    "processed_text":"pagiii semuanyahav good day hari ini presentasihari terakhir ke kampu sblm ua"
  },
  {
    "label":4,
    "text":"im look desper housew",
    "cleaned_text":"im look desper housew",
    "normalized_text":"im look desper housew",
    "tokens":[
      "im",
      "look",
      "desper",
      "housew"
    ],
    "token_count":4,
    "processed_text":"im look desper housew"
  },
  {
    "label":4,
    "text":"make breakfast way get troubl christi janinelt",
    "cleaned_text":"make breakfast way get troubl christi janinelt",
    "normalized_text":"make breakfast way get troubl christi janinelt",
    "tokens":[
      "make",
      "breakfast",
      "way",
      "get",
      "troubl",
      "christi",
      "janinelt"
    ],
    "token_count":7,
    "processed_text":"make breakfast way get troubl christi janinelt"
  },
  {
    "label":0,
    "text":"way work feel total nake despit fact im fulli cover forgot ear",
    "cleaned_text":"way work feel total nake despit fact im fulli cover forgot ear",
    "normalized_text":"way work feel total nake despit fact im fulli cover forgot ear",
    "tokens":[
      "way",
      "work",
      "feel",
      "total",
      "nake",
      "despit",
      "fact",
      "im",
      "fulli",
      "cover",
      "forgot",
      "ear"
    ],
    "token_count":12,
    "processed_text":"way work feel total nake despit fact im fulli cover forgot ear"
  },
  {
    "label":0,
    "text":"hello world pour amp chilli amp dont think summer come",
    "cleaned_text":"hello world pour amp chilli amp dont think summer come",
    "normalized_text":"hello world pour amp chilli amp dont think summer come",
    "tokens":[
      "hello",
      "world",
      "pour",
      "amp",
      "chilli",
      "amp",
      "dont",
      "think",
      "summer",
      "come"
    ],
    "token_count":10,
    "processed_text":"hello world pour amp chilli amp dont think summer come"
  },
  {
    "label":0,
    "text":"starbuck cosima get drenc thunder storm",
    "cleaned_text":"starbuck cosima get drenc thunder storm",
    "normalized_text":"starbuck cosima get drenc thunder storm",
    "tokens":[
      "starbuck",
      "cosima",
      "get",
      "drenc",
      "thunder",
      "storm"
    ],
    "token_count":6,
    "processed_text":"starbuck cosima get drenc thunder storm"
  },
  {
    "label":0,
    "text":"cant use upload manag upload site file",
    "cleaned_text":"cant use upload manag upload site file",
    "normalized_text":"cant use upload manag upload site file",
    "tokens":[
      "cant",
      "use",
      "upload",
      "manag",
      "upload",
      "site",
      "file"
    ],
    "token_count":7,
    "processed_text":"cant use upload manag upload site file"
  },
  {
    "label":4,
    "text":"wait min u pretti much home til u jump pond wk go could find cheap flt thought haha",
    "cleaned_text":"wait min u pretti much home til u jump pond wk go could find cheap flt thought haha",
    "normalized_text":"wait min u pretti much home til u jump pond wk go could find cheap flt thought haha",
    "tokens":[
      "wait",
      "min",
      "pretti",
      "much",
      "home",
      "til",
      "jump",
      "pond",
      "wk",
      "go",
      "find",
      "cheap",
      "flt",
      "thought",
      "haha"
    ],
    "token_count":15,
    "processed_text":"wait min pretti much home til jump pond wk go find cheap flt thought haha"
  },
  {
    "label":4,
    "text":"cure own mani level think dont like cure might friend lol",
    "cleaned_text":"cure own mani level think dont like cure might friend lol",
    "normalized_text":"cure own mani level think dont like cure might friend lol",
    "tokens":[
      "cure",
      "mani",
      "level",
      "think",
      "dont",
      "like",
      "cure",
      "friend",
      "lol"
    ],
    "token_count":9,
    "processed_text":"cure mani level think dont like cure friend lol"
  },
  {
    "label":0,
    "text":"want know love cat ballou watch time grandpa die two year ago today",
    "cleaned_text":"want know love cat ballou watch time grandpa die two year ago today",
    "normalized_text":"want know love cat ballou watch time grandpa die two year ago today",
    "tokens":[
      "want",
      "know",
      "love",
      "cat",
      "ballou",
      "watch",
      "time",
      "grandpa",
      "die",
      "two",
      "year",
      "ago",
      "today"
    ],
    "token_count":13,
    "processed_text":"want know love cat ballou watch time grandpa die two year ago today"
  },
  {
    "label":0,
    "text":"ah blogtv die",
    "cleaned_text":"ah blogtv die",
    "normalized_text":"ah blogtv die",
    "tokens":[
      "ah",
      "blogtv",
      "die"
    ],
    "token_count":3,
    "processed_text":"ah blogtv die"
  },
  {
    "label":4,
    "text":"good run thank time hr min im happi see cours rather hilli amp hot amp im novic",
    "cleaned_text":"good run thank time hr min im happi see cours rather hilli amp hot amp im novic",
    "normalized_text":"good run thank time hr min im happi see cours rather hilli amp hot amp im novic",
    "tokens":[
      "good",
      "run",
      "thank",
      "time",
      "hr",
      "min",
      "im",
      "happi",
      "see",
      "cour",
      "rather",
      "hilli",
      "amp",
      "hot",
      "amp",
      "im",
      "novic"
    ],
    "token_count":17,
    "processed_text":"good run thank time hr min im happi see cour rather hilli amp hot amp im novic"
  },
  {
    "label":0,
    "text":"go rent movi cake eater movi store",
    "cleaned_text":"go rent movi cake eater movi store",
    "normalized_text":"go rent movi cake eater movi store",
    "tokens":[
      "go",
      "rent",
      "movi",
      "cake",
      "eater",
      "movi",
      "store"
    ],
    "token_count":7,
    "processed_text":"go rent movi cake eater movi store"
  },
  {
    "label":4,
    "text":"quota heart three time turn heart uncultiv dark destroyedquot",
    "cleaned_text":"quota heart three time turn heart uncultiv dark destroyedquot",
    "normalized_text":"quota heart three time turn heart uncultiv dark destroyedquot",
    "tokens":[
      "quota",
      "heart",
      "three",
      "time",
      "turn",
      "heart",
      "uncultiv",
      "dark",
      "destroyedquot"
    ],
    "token_count":9,
    "processed_text":"quota heart three time turn heart uncultiv dark destroyedquot"
  },
  {
    "label":4,
    "text":"hello sunday thank sunshin",
    "cleaned_text":"hello sunday thank sunshin",
    "normalized_text":"hello sunday thank sunshin",
    "tokens":[
      "hello",
      "sunday",
      "thank",
      "sunshin"
    ],
    "token_count":4,
    "processed_text":"hello sunday thank sunshin"
  },
  {
    "label":4,
    "text":"sit japanes speak exam wish luck",
    "cleaned_text":"sit japanes speak exam wish luck",
    "normalized_text":"sit japanes speak exam wish luck",
    "tokens":[
      "sit",
      "japan",
      "speak",
      "exam",
      "wish",
      "luck"
    ],
    "token_count":6,
    "processed_text":"sit japan speak exam wish luck"
  },
  {
    "label":0,
    "text":"anyway think im go bed school morn hope get better",
    "cleaned_text":"anyway think im go bed school morn hope get better",
    "normalized_text":"anyway think im go bed school morn hope get better",
    "tokens":[
      "anyway",
      "think",
      "im",
      "go",
      "bed",
      "school",
      "morn",
      "hope",
      "get",
      "better"
    ],
    "token_count":10,
    "processed_text":"anyway think im go bed school morn hope get better"
  },
  {
    "label":4,
    "text":"hour famin tonight",
    "cleaned_text":"hour famin tonight",
    "normalized_text":"hour famin tonight",
    "tokens":[
      "hour",
      "famin",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"hour famin tonight"
  },
  {
    "label":0,
    "text":"woke relli tire",
    "cleaned_text":"woke relli tire",
    "normalized_text":"woke relli tire",
    "tokens":[
      "woke",
      "relli",
      "tire"
    ],
    "token_count":3,
    "processed_text":"woke relli tire"
  },
  {
    "label":4,
    "text":"lmao im tri sooth wit sorbet",
    "cleaned_text":"lmao im tri sooth wit sorbet",
    "normalized_text":"lmao im tri sooth wit sorbet",
    "tokens":[
      "lmao",
      "im",
      "tri",
      "sooth",
      "wit",
      "sorbet"
    ],
    "token_count":6,
    "processed_text":"lmao im tri sooth wit sorbet"
  },
  {
    "label":0,
    "text":"didnt chia beer also didnt manag see u stagebi time arrivedy guy done",
    "cleaned_text":"didnt chia beer also didnt manag see u stagebi time arrivedy guy done",
    "normalized_text":"didnt chia beer also didnt manag see u stagebi time arrivedy guy done",
    "tokens":[
      "didnt",
      "chia",
      "beer",
      "also",
      "didnt",
      "manag",
      "see",
      "stagebi",
      "time",
      "arrivedi",
      "guy",
      "done"
    ],
    "token_count":12,
    "processed_text":"didnt chia beer also didnt manag see stagebi time arrivedi guy done"
  },
  {
    "label":4,
    "text":"shop new shirt gotta hang w besti day sweet start summer",
    "cleaned_text":"shop new shirt gotta hang w besti day sweet start summer",
    "normalized_text":"shop new shirt gotta hang w besti day sweet start summer",
    "tokens":[
      "shop",
      "new",
      "shirt",
      "got",
      "ta",
      "hang",
      "besti",
      "day",
      "sweet",
      "start",
      "summer"
    ],
    "token_count":11,
    "processed_text":"shop new shirt got ta hang besti day sweet start summer"
  },
  {
    "label":4,
    "text":"love good charlott",
    "cleaned_text":"love good charlott",
    "normalized_text":"love good charlott",
    "tokens":[
      "love",
      "good",
      "charlott"
    ],
    "token_count":3,
    "processed_text":"love good charlott"
  },
  {
    "label":4,
    "text":"run hour sleep amp red bull refus nap go make lazi sunday",
    "cleaned_text":"run hour sleep amp red bull refus nap go make lazi sunday",
    "normalized_text":"run hour sleep amp red bull refus nap go make lazi sunday",
    "tokens":[
      "run",
      "hour",
      "sleep",
      "amp",
      "red",
      "bull",
      "refu",
      "nap",
      "go",
      "make",
      "lazi",
      "sunday"
    ],
    "token_count":12,
    "processed_text":"run hour sleep amp red bull refu nap go make lazi sunday"
  },
  {
    "label":4,
    "text":"actual use standard speaker wire standard termin solder involv anyth",
    "cleaned_text":"actual use standard speaker wire standard termin solder involv anyth",
    "normalized_text":"actual use standard speaker wire standard termin solder involv anyth",
    "tokens":[
      "actual",
      "use",
      "standard",
      "speaker",
      "wire",
      "standard",
      "termin",
      "solder",
      "involv",
      "anyth"
    ],
    "token_count":10,
    "processed_text":"actual use standard speaker wire standard termin solder involv anyth"
  },
  {
    "label":0,
    "text":"yup take phone w week",
    "cleaned_text":"yup take phone w week",
    "normalized_text":"yup take phone w week",
    "tokens":[
      "yup",
      "take",
      "phone",
      "week"
    ],
    "token_count":4,
    "processed_text":"yup take phone week"
  },
  {
    "label":4,
    "text":"pm im still pj god love lazi non work day",
    "cleaned_text":"pm im still pj god love lazi non work day",
    "normalized_text":"pm im still pj god love lazi non work day",
    "tokens":[
      "pm",
      "im",
      "still",
      "pj",
      "god",
      "love",
      "lazi",
      "non",
      "work",
      "day"
    ],
    "token_count":10,
    "processed_text":"pm im still pj god love lazi non work day"
  },
  {
    "label":0,
    "text":"got feel murray match go set least",
    "cleaned_text":"got feel murray match go set least",
    "normalized_text":"got feel murray match go set least",
    "tokens":[
      "got",
      "feel",
      "murray",
      "match",
      "go",
      "set",
      "least"
    ],
    "token_count":7,
    "processed_text":"got feel murray match go set least"
  },
  {
    "label":0,
    "text":"like cloth annoy",
    "cleaned_text":"like cloth annoy",
    "normalized_text":"like cloth annoy",
    "tokens":[
      "like",
      "cloth",
      "annoy"
    ],
    "token_count":3,
    "processed_text":"like cloth annoy"
  },
  {
    "label":4,
    "text":"better vote least one jona brother pleas",
    "cleaned_text":"better vote least one jona brother pleas",
    "normalized_text":"better vote least one jona brother pleas",
    "tokens":[
      "better",
      "vote",
      "least",
      "one",
      "jona",
      "brother",
      "plea"
    ],
    "token_count":7,
    "processed_text":"better vote least one jona brother plea"
  },
  {
    "label":4,
    "text":"silverstein hoodi save day",
    "cleaned_text":"silverstein hoodi save day",
    "normalized_text":"silverstein hoodi save day",
    "tokens":[
      "silverstein",
      "hoodi",
      "save",
      "day"
    ],
    "token_count":4,
    "processed_text":"silverstein hoodi save day"
  },
  {
    "label":0,
    "text":"im love understand",
    "cleaned_text":"im love understand",
    "normalized_text":"im love understand",
    "tokens":[
      "im",
      "love",
      "understand"
    ],
    "token_count":3,
    "processed_text":"im love understand"
  },
  {
    "label":4,
    "text":"mumbai indian open inning ipl",
    "cleaned_text":"mumbai indian open inning ipl",
    "normalized_text":"mumbai indian open inning ipl",
    "tokens":[
      "mumbai",
      "indian",
      "open",
      "inning",
      "ipl"
    ],
    "token_count":5,
    "processed_text":"mumbai indian open inning ipl"
  },
  {
    "label":4,
    "text":"sound like perfect meal black pearl tweetbunchnz love profil background btw",
    "cleaned_text":"sound like perfect meal black pearl tweetbunchnz love profil background btw",
    "normalized_text":"sound like perfect meal black pearl tweetbunchnz love profil background btw",
    "tokens":[
      "sound",
      "like",
      "perfect",
      "meal",
      "black",
      "pearl",
      "tweetbunchnz",
      "love",
      "profil",
      "background",
      "btw"
    ],
    "token_count":11,
    "processed_text":"sound like perfect meal black pearl tweetbunchnz love profil background btw"
  },
  {
    "label":0,
    "text":"good morn fellow tweet allot spamer that ok ill get rid",
    "cleaned_text":"good morn fellow tweet allot spamer that ok ill get rid",
    "normalized_text":"good morn fellow tweet allot spamer that ok ill get rid",
    "tokens":[
      "good",
      "morn",
      "fellow",
      "tweet",
      "allot",
      "spamer",
      "ok",
      "ill",
      "get",
      "rid"
    ],
    "token_count":10,
    "processed_text":"good morn fellow tweet allot spamer ok ill get rid"
  },
  {
    "label":0,
    "text":"checkin amp bore",
    "cleaned_text":"checkin amp bore",
    "normalized_text":"checkin amp bore",
    "tokens":[
      "checkin",
      "amp",
      "bore"
    ],
    "token_count":3,
    "processed_text":"checkin amp bore"
  },
  {
    "label":0,
    "text":"left hair straighten dad car welcom frizztown",
    "cleaned_text":"left hair straighten dad car welcom frizztown",
    "normalized_text":"left hair straighten dad car welcom frizztown",
    "tokens":[
      "left",
      "hair",
      "straighten",
      "dad",
      "car",
      "welcom",
      "frizztown"
    ],
    "token_count":7,
    "processed_text":"left hair straighten dad car welcom frizztown"
  },
  {
    "label":4,
    "text":"omg pictur beauti bb",
    "cleaned_text":"omg pictur beauti bb",
    "normalized_text":"omg pictur beauti bb",
    "tokens":[
      "omg",
      "pictur",
      "beauti",
      "bb"
    ],
    "token_count":4,
    "processed_text":"omg pictur beauti bb"
  },
  {
    "label":0,
    "text":"miss sale urban today",
    "cleaned_text":"miss sale urban today",
    "normalized_text":"miss sale urban today",
    "tokens":[
      "miss",
      "sale",
      "urban",
      "today"
    ],
    "token_count":4,
    "processed_text":"miss sale urban today"
  },
  {
    "label":4,
    "text":"yea",
    "cleaned_text":"yea",
    "normalized_text":"yea",
    "tokens":[
      "yea"
    ],
    "token_count":1,
    "processed_text":"yea"
  },
  {
    "label":4,
    "text":"tralalalaa ye identifi musician murder win best consult time",
    "cleaned_text":"tralalalaa ye identifi musician murder win best consult time",
    "normalized_text":"tralalalaa ye identifi musician murder win best consult time",
    "tokens":[
      "tralalalaa",
      "ye",
      "identifi",
      "musician",
      "murder",
      "win",
      "best",
      "consult",
      "time"
    ],
    "token_count":9,
    "processed_text":"tralalalaa ye identifi musician murder win best consult time"
  },
  {
    "label":4,
    "text":"hey chennnn long time see",
    "cleaned_text":"hey chennnn long time see",
    "normalized_text":"hey chennnn long time see",
    "tokens":[
      "hey",
      "chennnn",
      "long",
      "time",
      "see"
    ],
    "token_count":5,
    "processed_text":"hey chennnn long time see"
  },
  {
    "label":4,
    "text":"kiddin need free wireless everi flight magazin",
    "cleaned_text":"kiddin need free wireless everi flight magazin",
    "normalized_text":"kiddin need free wireless everi flight magazin",
    "tokens":[
      "kiddin",
      "need",
      "free",
      "wireless",
      "everi",
      "flight",
      "magazin"
    ],
    "token_count":7,
    "processed_text":"kiddin need free wireless everi flight magazin"
  },
  {
    "label":0,
    "text":"rainingggg",
    "cleaned_text":"rainingggg",
    "normalized_text":"rainingggg",
    "tokens":[
      "rainingggg"
    ],
    "token_count":1,
    "processed_text":"rainingggg"
  },
  {
    "label":4,
    "text":"um ok lol u still havent earn yet impress babi luv u",
    "cleaned_text":"um ok lol u still havent earn yet impress babi luv u",
    "normalized_text":"um ok lol u still havent earn yet impress babi luv u",
    "tokens":[
      "um",
      "ok",
      "lol",
      "still",
      "havent",
      "earn",
      "yet",
      "impress",
      "babi",
      "luv"
    ],
    "token_count":10,
    "processed_text":"um ok lol still havent earn yet impress babi luv"
  },
  {
    "label":0,
    "text":"doesnt think enough websit devot big boy wear compress short amour tshirt start one",
    "cleaned_text":"doesnt think enough websit devot big boy wear compress short amour tshirt start one",
    "normalized_text":"doesnt think enough websit devot big boy wear compress short amour tshirt start one",
    "tokens":[
      "doesnt",
      "think",
      "enough",
      "websit",
      "devot",
      "big",
      "boy",
      "wear",
      "compress",
      "short",
      "amour",
      "tshirt",
      "start",
      "one"
    ],
    "token_count":14,
    "processed_text":"doesnt think enough websit devot big boy wear compress short amour tshirt start one"
  },
  {
    "label":4,
    "text":"momma make egggss",
    "cleaned_text":"momma make egggss",
    "normalized_text":"momma make egggss",
    "tokens":[
      "momma",
      "make",
      "egggss"
    ],
    "token_count":3,
    "processed_text":"momma make egggss"
  },
  {
    "label":4,
    "text":"thought didnt want think import",
    "cleaned_text":"thought didnt want think import",
    "normalized_text":"thought didnt want think import",
    "tokens":[
      "thought",
      "didnt",
      "want",
      "think",
      "import"
    ],
    "token_count":5,
    "processed_text":"thought didnt want think import"
  },
  {
    "label":4,
    "text":"bizarr im feel realli postiiv time think might guy said havent realli start yet well see",
    "cleaned_text":"bizarr im feel realli postiiv time think might guy said havent realli start yet well see",
    "normalized_text":"bizarr im feel realli postiiv time think might guy said havent realli start yet well see",
    "tokens":[
      "bizarr",
      "im",
      "feel",
      "realli",
      "postiiv",
      "time",
      "think",
      "guy",
      "said",
      "havent",
      "realli",
      "start",
      "yet",
      "well",
      "see"
    ],
    "token_count":15,
    "processed_text":"bizarr im feel realli postiiv time think guy said havent realli start yet well see"
  },
  {
    "label":0,
    "text":"spent time quarri examin granit ye would love see splendid countri",
    "cleaned_text":"spent time quarri examin granit ye would love see splendid countri",
    "normalized_text":"spent time quarri examin granit ye would love see splendid countri",
    "tokens":[
      "spent",
      "time",
      "quarri",
      "examin",
      "granit",
      "ye",
      "love",
      "see",
      "splendid",
      "countri"
    ],
    "token_count":10,
    "processed_text":"spent time quarri examin granit ye love see splendid countri"
  },
  {
    "label":0,
    "text":"front doubl two putt",
    "cleaned_text":"front doubl two putt",
    "normalized_text":"front doubl two putt",
    "tokens":[
      "front",
      "doubl",
      "two",
      "putt"
    ],
    "token_count":4,
    "processed_text":"front doubl two putt"
  },
  {
    "label":0,
    "text":"yeahh serious novemb well start mission im sad haha",
    "cleaned_text":"yeahh serious novemb well start mission im sad haha",
    "normalized_text":"yeahh serious novemb well start mission im sad haha",
    "tokens":[
      "yeahh",
      "seriou",
      "novemb",
      "well",
      "start",
      "mission",
      "im",
      "sad",
      "haha"
    ],
    "token_count":9,
    "processed_text":"yeahh seriou novemb well start mission im sad haha"
  },
  {
    "label":0,
    "text":"got thing hook uponli follow",
    "cleaned_text":"got thing hook uponli follow",
    "normalized_text":"got thing hook uponli follow",
    "tokens":[
      "got",
      "thing",
      "hook",
      "uponli",
      "follow"
    ],
    "token_count":5,
    "processed_text":"got thing hook uponli follow"
  },
  {
    "label":4,
    "text":"send fix problem",
    "cleaned_text":"send fix problem",
    "normalized_text":"send fix problem",
    "tokens":[
      "send",
      "fix",
      "problem"
    ],
    "token_count":3,
    "processed_text":"send fix problem"
  },
  {
    "label":4,
    "text":"start boot camp todayshould awesom",
    "cleaned_text":"start boot camp todayshould awesom",
    "normalized_text":"start boot camp todayshould awesom",
    "tokens":[
      "start",
      "boot",
      "camp",
      "todayshould",
      "awesom"
    ],
    "token_count":5,
    "processed_text":"start boot camp todayshould awesom"
  },
  {
    "label":4,
    "text":"thank far happen hope good day stay healthi",
    "cleaned_text":"thank far happen hope good day stay healthi",
    "normalized_text":"thank far happen hope good day stay healthi",
    "tokens":[
      "thank",
      "far",
      "happen",
      "hope",
      "good",
      "day",
      "stay",
      "healthi"
    ],
    "token_count":8,
    "processed_text":"thank far happen hope good day stay healthi"
  },
  {
    "label":0,
    "text":"he noth new realli anywher oh well he make movi im sure everyon enjoy x",
    "cleaned_text":"he noth new realli anywher oh well he make movi im sure everyon enjoy x",
    "normalized_text":"he noth new realli anywher oh well he make movi im sure everyon enjoy x",
    "tokens":[
      "noth",
      "new",
      "realli",
      "anywh",
      "oh",
      "well",
      "make",
      "movi",
      "im",
      "sure",
      "everyon",
      "enjoy"
    ],
    "token_count":12,
    "processed_text":"noth new realli anywh oh well make movi im sure everyon enjoy"
  },
  {
    "label":4,
    "text":"lovley thankyou",
    "cleaned_text":"lovley thankyou",
    "normalized_text":"lovley thankyou",
    "tokens":[
      "lovley",
      "thankyou"
    ],
    "token_count":2,
    "processed_text":"lovley thankyou"
  },
  {
    "label":0,
    "text":"retain hurt teeth havent worn like month owww",
    "cleaned_text":"retain hurt teeth havent worn like month owww",
    "normalized_text":"retain hurt teeth havent worn like month owww",
    "tokens":[
      "retain",
      "hurt",
      "teeth",
      "havent",
      "worn",
      "like",
      "month",
      "owww"
    ],
    "token_count":8,
    "processed_text":"retain hurt teeth havent worn like month owww"
  },
  {
    "label":4,
    "text":"love new logo typo great love subtl well place colour awesom",
    "cleaned_text":"love new logo typo great love subtl well place colour awesom",
    "normalized_text":"love new logo typo great love subtl well place colour awesom",
    "tokens":[
      "love",
      "new",
      "logo",
      "typo",
      "great",
      "love",
      "subtl",
      "well",
      "place",
      "colour",
      "awesom"
    ],
    "token_count":11,
    "processed_text":"love new logo typo great love subtl well place colour awesom"
  },
  {
    "label":0,
    "text":"alway look mess see cute ppl fml lol",
    "cleaned_text":"alway look mess see cute ppl fml lol",
    "normalized_text":"alway look mess see cute ppl fml lol",
    "tokens":[
      "alway",
      "look",
      "mess",
      "see",
      "cute",
      "ppl",
      "fml",
      "lol"
    ],
    "token_count":8,
    "processed_text":"alway look mess see cute ppl fml lol"
  },
  {
    "label":0,
    "text":"hello twitter miss home",
    "cleaned_text":"hello twitter miss home",
    "normalized_text":"hello twitter miss home",
    "tokens":[
      "hello",
      "twitter",
      "miss",
      "home"
    ],
    "token_count":4,
    "processed_text":"hello twitter miss home"
  },
  {
    "label":4,
    "text":"new version greenweez relas today developp project team may",
    "cleaned_text":"new version greenweez relas today developp project team may",
    "normalized_text":"new version greenweez relas today developp project team may",
    "tokens":[
      "new",
      "version",
      "greenweez",
      "rela",
      "today",
      "developp",
      "project",
      "team",
      "may"
    ],
    "token_count":9,
    "processed_text":"new version greenweez rela today developp project team may"
  },
  {
    "label":4,
    "text":"go bed",
    "cleaned_text":"go bed",
    "normalized_text":"go bed",
    "tokens":[
      "go",
      "bed"
    ],
    "token_count":2,
    "processed_text":"go bed"
  },
  {
    "label":0,
    "text":"gotta go work",
    "cleaned_text":"gotta go work",
    "normalized_text":"gotta go work",
    "tokens":[
      "got",
      "ta",
      "go",
      "work"
    ],
    "token_count":4,
    "processed_text":"got ta go work"
  },
  {
    "label":0,
    "text":"wish could go tonight",
    "cleaned_text":"wish could go tonight",
    "normalized_text":"wish could go tonight",
    "tokens":[
      "wish",
      "go",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"wish go tonight"
  },
  {
    "label":0,
    "text":"tri get ova skool day",
    "cleaned_text":"tri get ova skool day",
    "normalized_text":"tri get ova skool day",
    "tokens":[
      "tri",
      "get",
      "ova",
      "skool",
      "day"
    ],
    "token_count":5,
    "processed_text":"tri get ova skool day"
  },
  {
    "label":4,
    "text":"expect say quotnerd scientist scifi geeksquot oop",
    "cleaned_text":"expect say quotnerd scientist scifi geeksquot oop",
    "normalized_text":"expect say quotnerd scientist scifi geeksquot oop",
    "tokens":[
      "expect",
      "say",
      "quotnerd",
      "scientist",
      "scifi",
      "geeksquot",
      "oop"
    ],
    "token_count":7,
    "processed_text":"expect say quotnerd scientist scifi geeksquot oop"
  },
  {
    "label":0,
    "text":"fever take googletalk mobil fring wwwfringcomgoogletalk",
    "cleaned_text":"fever take googletalk mobil fring wwwfringcomgoogletalk",
    "normalized_text":"fever take googletalk mobil fring wwwfringcomgoogletalk",
    "tokens":[
      "fever",
      "take",
      "googletalk",
      "mobil",
      "fring"
    ],
    "token_count":5,
    "processed_text":"fever take googletalk mobil fring"
  },
  {
    "label":0,
    "text":"place bet sure thing lost bet",
    "cleaned_text":"place bet sure thing lost bet",
    "normalized_text":"place bet sure thing lost bet",
    "tokens":[
      "place",
      "bet",
      "sure",
      "thing",
      "lost",
      "bet"
    ],
    "token_count":6,
    "processed_text":"place bet sure thing lost bet"
  },
  {
    "label":0,
    "text":"wow rain la even worth wash ur car day",
    "cleaned_text":"wow rain la even worth wash ur car day",
    "normalized_text":"wow rain la even worth wash ur car day",
    "tokens":[
      "wow",
      "rain",
      "la",
      "even",
      "worth",
      "wash",
      "ur",
      "car",
      "day"
    ],
    "token_count":9,
    "processed_text":"wow rain la even worth wash ur car day"
  },
  {
    "label":4,
    "text":"great day love mommaamp spent dad besti took dinner momma day amp im feel nice",
    "cleaned_text":"great day love mommaamp spent dad besti took dinner momma day amp im feel nice",
    "normalized_text":"great day love mommaamp spent dad besti took dinner momma day amp im feel nice",
    "tokens":[
      "great",
      "day",
      "love",
      "mommaamp",
      "spent",
      "dad",
      "besti",
      "took",
      "dinner",
      "momma",
      "day",
      "amp",
      "im",
      "feel",
      "nice"
    ],
    "token_count":15,
    "processed_text":"great day love mommaamp spent dad besti took dinner momma day amp im feel nice"
  },
  {
    "label":0,
    "text":"good even last night pub meal countri side pub outsid aylesburi close one junction hr extra journey time back",
    "cleaned_text":"good even last night pub meal countri side pub outsid aylesburi close one junction hr extra journey time back",
    "normalized_text":"good even last night pub meal countri side pub outsid aylesburi close one junction hr extra journey time back",
    "tokens":[
      "good",
      "even",
      "last",
      "night",
      "pub",
      "meal",
      "countri",
      "side",
      "pub",
      "outsid",
      "aylesburi",
      "close",
      "one",
      "junction",
      "hr",
      "extra",
      "journey",
      "time",
      "back"
    ],
    "token_count":19,
    "processed_text":"good even last night pub meal countri side pub outsid aylesburi close one junction hr extra journey time back"
  },
  {
    "label":4,
    "text":"u like old time",
    "cleaned_text":"u like old time",
    "normalized_text":"u like old time",
    "tokens":[
      "like",
      "old",
      "time"
    ],
    "token_count":3,
    "processed_text":"like old time"
  },
  {
    "label":0,
    "text":"that thing im gone basic summer also ride",
    "cleaned_text":"that thing im gone basic summer also ride",
    "normalized_text":"that thing im gone basic summer also ride",
    "tokens":[
      "thing",
      "im",
      "gone",
      "basic",
      "summer",
      "also",
      "ride"
    ],
    "token_count":7,
    "processed_text":"thing im gone basic summer also ride"
  },
  {
    "label":0,
    "text":"lot web page doesnt seem work right cant get select text",
    "cleaned_text":"lot web page doesnt seem work right cant get select text",
    "normalized_text":"lot web page doesnt seem work right cant get select text",
    "tokens":[
      "lot",
      "web",
      "page",
      "doesnt",
      "seem",
      "work",
      "right",
      "cant",
      "get",
      "select",
      "text"
    ],
    "token_count":11,
    "processed_text":"lot web page doesnt seem work right cant get select text"
  },
  {
    "label":0,
    "text":"tri revis noth go hate sunday",
    "cleaned_text":"tri revis noth go hate sunday",
    "normalized_text":"tri revis noth go hate sunday",
    "tokens":[
      "tri",
      "revi",
      "noth",
      "go",
      "hate",
      "sunday"
    ],
    "token_count":6,
    "processed_text":"tri revi noth go hate sunday"
  },
  {
    "label":4,
    "text":"photo want free havaiana anybodi give heheh",
    "cleaned_text":"photo want free havaiana anybodi give heheh",
    "normalized_text":"photo want free havaiana anybodi give heheh",
    "tokens":[
      "photo",
      "want",
      "free",
      "havaiana",
      "anybodi",
      "give",
      "heheh"
    ],
    "token_count":7,
    "processed_text":"photo want free havaiana anybodi give heheh"
  },
  {
    "label":4,
    "text":"check myspaceadd wwwmyspacecomrazorbladelov",
    "cleaned_text":"check myspaceadd wwwmyspacecomrazorbladelov",
    "normalized_text":"check myspaceadd wwwmyspacecomrazorbladelov",
    "tokens":[
      "check",
      "myspaceadd"
    ],
    "token_count":2,
    "processed_text":"check myspaceadd"
  },
  {
    "label":4,
    "text":"lol jason got twitter hell suppos twit look techno world",
    "cleaned_text":"lol jason got twitter hell suppos twit look techno world",
    "normalized_text":"lol jason got twitter hell suppos twit look techno world",
    "tokens":[
      "lol",
      "jason",
      "got",
      "twitter",
      "hell",
      "suppo",
      "twit",
      "look",
      "techno",
      "world"
    ],
    "token_count":10,
    "processed_text":"lol jason got twitter hell suppo twit look techno world"
  },
  {
    "label":0,
    "text":"haha wish weather make lazi wanna go beach alreadi",
    "cleaned_text":"haha wish weather make lazi wanna go beach alreadi",
    "normalized_text":"haha wish weather make lazi wanna go beach alreadi",
    "tokens":[
      "haha",
      "wish",
      "weather",
      "make",
      "lazi",
      "wan",
      "na",
      "go",
      "beach",
      "alreadi"
    ],
    "token_count":10,
    "processed_text":"haha wish weather make lazi wan na go beach alreadi"
  },
  {
    "label":0,
    "text":"quotalway gonna hill battl sometim im gonna losequot",
    "cleaned_text":"quotalway gonna hill battl sometim im gonna losequot",
    "normalized_text":"quotalway gonna hill battl sometim im gonna losequot",
    "tokens":[
      "quotalway",
      "gon",
      "na",
      "hill",
      "battl",
      "sometim",
      "im",
      "gon",
      "na",
      "losequot"
    ],
    "token_count":10,
    "processed_text":"quotalway gon na hill battl sometim im gon na losequot"
  },
  {
    "label":0,
    "text":"power love hughi lewi news franki goe hollywood dont call sad",
    "cleaned_text":"power love hughi lewi news franki goe hollywood dont call sad",
    "normalized_text":"power love hughi lewi news franki goe hollywood dont call sad",
    "tokens":[
      "power",
      "love",
      "hughi",
      "lewi",
      "news",
      "franki",
      "goe",
      "hollywood",
      "dont",
      "call",
      "sad"
    ],
    "token_count":11,
    "processed_text":"power love hughi lewi news franki goe hollywood dont call sad"
  },
  {
    "label":4,
    "text":"wow theyr get obnoxi toward want notic cute goodnight j thank chat",
    "cleaned_text":"wow theyr get obnoxi toward want notic cute goodnight j thank chat",
    "normalized_text":"wow theyr get obnoxi toward want notic cute goodnight j thank chat",
    "tokens":[
      "wow",
      "theyr",
      "get",
      "obnoxi",
      "toward",
      "want",
      "notic",
      "cute",
      "goodnight",
      "thank",
      "chat"
    ],
    "token_count":11,
    "processed_text":"wow theyr get obnoxi toward want notic cute goodnight thank chat"
  },
  {
    "label":0,
    "text":"hayi miss perform buttsburi true know holli edward",
    "cleaned_text":"hayi miss perform buttsburi true know holli edward",
    "normalized_text":"hayi miss perform buttsburi true know holli edward",
    "tokens":[
      "hayi",
      "miss",
      "perform",
      "buttsburi",
      "true",
      "know",
      "holli",
      "edward"
    ],
    "token_count":8,
    "processed_text":"hayi miss perform buttsburi true know holli edward"
  },
  {
    "label":4,
    "text":"btw want make day weve got zoo fun stuff youll fun day",
    "cleaned_text":"btw want make day weve got zoo fun stuff youll fun day",
    "normalized_text":"btw want make day weve got zoo fun stuff youll fun day",
    "tokens":[
      "btw",
      "want",
      "make",
      "day",
      "weve",
      "got",
      "zoo",
      "fun",
      "stuff",
      "youll",
      "fun",
      "day"
    ],
    "token_count":12,
    "processed_text":"btw want make day weve got zoo fun stuff youll fun day"
  },
  {
    "label":4,
    "text":"like lot thing",
    "cleaned_text":"like lot thing",
    "normalized_text":"like lot thing",
    "tokens":[
      "like",
      "lot",
      "thing"
    ],
    "token_count":3,
    "processed_text":"like lot thing"
  },
  {
    "label":0,
    "text":"want new gg episod alreadi",
    "cleaned_text":"want new gg episod alreadi",
    "normalized_text":"want new gg episod alreadi",
    "tokens":[
      "want",
      "new",
      "gg",
      "episod",
      "alreadi"
    ],
    "token_count":5,
    "processed_text":"want new gg episod alreadi"
  },
  {
    "label":0,
    "text":"u didnt cum amp say hi thatz ok cuz got soak grr amp didnt go ride",
    "cleaned_text":"u didnt cum amp say hi thatz ok cuz got soak grr amp didnt go ride",
    "normalized_text":"u didnt cum amp say hi thatz ok cuz got soak grr amp didnt go ride",
    "tokens":[
      "didnt",
      "cum",
      "amp",
      "say",
      "hi",
      "thatz",
      "ok",
      "cuz",
      "got",
      "soak",
      "grr",
      "amp",
      "didnt",
      "go",
      "ride"
    ],
    "token_count":15,
    "processed_text":"didnt cum amp say hi thatz ok cuz got soak grr amp didnt go ride"
  },
  {
    "label":0,
    "text":"panera studi stat final final go",
    "cleaned_text":"panera studi stat final final go",
    "normalized_text":"panera studi stat final final go",
    "tokens":[
      "panera",
      "studi",
      "stat",
      "final",
      "final",
      "go"
    ],
    "token_count":6,
    "processed_text":"panera studi stat final final go"
  },
  {
    "label":0,
    "text":"sun nobodi time",
    "cleaned_text":"sun nobodi time",
    "normalized_text":"sun nobodi time",
    "tokens":[
      "sun",
      "nobodi",
      "time"
    ],
    "token_count":3,
    "processed_text":"sun nobodi time"
  },
  {
    "label":4,
    "text":"good phone call helena she whore take cardboard cutout formal sad",
    "cleaned_text":"good phone call helena she whore take cardboard cutout formal sad",
    "normalized_text":"good phone call helena she whore take cardboard cutout formal sad",
    "tokens":[
      "good",
      "phone",
      "call",
      "helena",
      "whore",
      "take",
      "cardboard",
      "cutout",
      "formal",
      "sad"
    ],
    "token_count":10,
    "processed_text":"good phone call helena whore take cardboard cutout formal sad"
  },
  {
    "label":4,
    "text":"wish could come see oslo oslo your play somewher els norway",
    "cleaned_text":"wish could come see oslo oslo your play somewher els norway",
    "normalized_text":"wish could come see oslo oslo your play somewher els norway",
    "tokens":[
      "wish",
      "come",
      "see",
      "oslo",
      "oslo",
      "play",
      "somewh",
      "el",
      "norway"
    ],
    "token_count":9,
    "processed_text":"wish come see oslo oslo play somewh el norway"
  },
  {
    "label":4,
    "text":"lot folk think im bc im lb im biblack shelti black face",
    "cleaned_text":"lot folk think im bc im lb im biblack shelti black face",
    "normalized_text":"lot folk think im bc im lb im biblack shelti black face",
    "tokens":[
      "lot",
      "folk",
      "think",
      "im",
      "bc",
      "im",
      "lb",
      "im",
      "biblack",
      "shelti",
      "black",
      "face"
    ],
    "token_count":12,
    "processed_text":"lot folk think im bc im lb im biblack shelti black face"
  },
  {
    "label":0,
    "text":"slept day away still hardcor studi monday bio quiz tuesday math exam ugh",
    "cleaned_text":"slept day away still hardcor studi monday bio quiz tuesday math exam ugh",
    "normalized_text":"slept day away still hardcor studi monday bio quiz tuesday math exam ugh",
    "tokens":[
      "slept",
      "day",
      "away",
      "still",
      "hardcor",
      "studi",
      "monday",
      "bio",
      "quiz",
      "tuesday",
      "math",
      "exam",
      "ugh"
    ],
    "token_count":13,
    "processed_text":"slept day away still hardcor studi monday bio quiz tuesday math exam ugh"
  },
  {
    "label":4,
    "text":"need catch gossip girl im watch episod",
    "cleaned_text":"need catch gossip girl im watch episod",
    "normalized_text":"need catch gossip girl im watch episod",
    "tokens":[
      "need",
      "catch",
      "gossip",
      "girl",
      "im",
      "watch",
      "episod"
    ],
    "token_count":7,
    "processed_text":"need catch gossip girl im watch episod"
  },
  {
    "label":4,
    "text":"clock strike pm sunday night break bad time",
    "cleaned_text":"clock strike pm sunday night break bad time",
    "normalized_text":"clock strike pm sunday night break bad time",
    "tokens":[
      "clock",
      "strike",
      "pm",
      "sunday",
      "night",
      "break",
      "bad",
      "time"
    ],
    "token_count":8,
    "processed_text":"clock strike pm sunday night break bad time"
  },
  {
    "label":0,
    "text":"oh know def didnt see end come tho shouldnt happen",
    "cleaned_text":"oh know def didnt see end come tho shouldnt happen",
    "normalized_text":"oh know def didnt see end come tho shouldnt happen",
    "tokens":[
      "oh",
      "know",
      "def",
      "didnt",
      "see",
      "end",
      "come",
      "tho",
      "shouldnt",
      "happen"
    ],
    "token_count":10,
    "processed_text":"oh know def didnt see end come tho shouldnt happen"
  },
  {
    "label":0,
    "text":"miss sushi much",
    "cleaned_text":"miss sushi much",
    "normalized_text":"miss sushi much",
    "tokens":[
      "miss",
      "sushi",
      "much"
    ],
    "token_count":3,
    "processed_text":"miss sushi much"
  },
  {
    "label":0,
    "text":"steve got siiick massiv headach coughin fever nooot funnn",
    "cleaned_text":"steve got siiick massiv headach coughin fever nooot funnn",
    "normalized_text":"steve got siiick massiv headach coughin fever nooot funnn",
    "tokens":[
      "steve",
      "got",
      "siiick",
      "massiv",
      "headach",
      "coughin",
      "fever",
      "nooot",
      "funnn"
    ],
    "token_count":9,
    "processed_text":"steve got siiick massiv headach coughin fever nooot funnn"
  },
  {
    "label":4,
    "text":"awak half hour writin thing get shower brother watchin friend",
    "cleaned_text":"awak half hour writin thing get shower brother watchin friend",
    "normalized_text":"awak half hour writin thing get shower brother watchin friend",
    "tokens":[
      "awak",
      "half",
      "hour",
      "writin",
      "thing",
      "get",
      "shower",
      "brother",
      "watchin",
      "friend"
    ],
    "token_count":10,
    "processed_text":"awak half hour writin thing get shower brother watchin friend"
  },
  {
    "label":0,
    "text":"need stop updat hurt upset actual let fool like ruin weekend hate say hate",
    "cleaned_text":"need stop updat hurt upset actual let fool like ruin weekend hate say hate",
    "normalized_text":"need stop updat hurt upset actual let fool like ruin weekend hate say hate",
    "tokens":[
      "need",
      "stop",
      "updat",
      "hurt",
      "upset",
      "actual",
      "let",
      "fool",
      "like",
      "ruin",
      "weekend",
      "hate",
      "say",
      "hate"
    ],
    "token_count":14,
    "processed_text":"need stop updat hurt upset actual let fool like ruin weekend hate say hate"
  },
  {
    "label":0,
    "text":"realli realli realli want ticket see taylor swift",
    "cleaned_text":"realli realli realli want ticket see taylor swift",
    "normalized_text":"realli realli realli want ticket see taylor swift",
    "tokens":[
      "realli",
      "realli",
      "realli",
      "want",
      "ticket",
      "see",
      "taylor",
      "swift"
    ],
    "token_count":8,
    "processed_text":"realli realli realli want ticket see taylor swift"
  },
  {
    "label":0,
    "text":"evil credit card compani god start build credit get stupid ding",
    "cleaned_text":"evil credit card compani god start build credit get stupid ding",
    "normalized_text":"evil credit card compani god start build credit get stupid ding",
    "tokens":[
      "evil",
      "credit",
      "card",
      "compani",
      "god",
      "start",
      "build",
      "credit",
      "get",
      "stupid",
      "ding"
    ],
    "token_count":11,
    "processed_text":"evil credit card compani god start build credit get stupid ding"
  },
  {
    "label":4,
    "text":"hoop weel replyyi pleeas ash",
    "cleaned_text":"hoop weel replyyi pleeas ash",
    "normalized_text":"hoop weel replyyi pleeas ash",
    "tokens":[
      "hoop",
      "weel",
      "replyyi",
      "pleea",
      "ash"
    ],
    "token_count":5,
    "processed_text":"hoop weel replyyi pleea ash"
  },
  {
    "label":4,
    "text":"saw newest build avid demod sundanc year crash someth fierc keep fcp within reach",
    "cleaned_text":"saw newest build avid demod sundanc year crash someth fierc keep fcp within reach",
    "normalized_text":"saw newest build avid demod sundanc year crash someth fierc keep fcp within reach",
    "tokens":[
      "saw",
      "newest",
      "build",
      "avid",
      "demod",
      "sundanc",
      "year",
      "crash",
      "someth",
      "fierc",
      "keep",
      "fcp",
      "within",
      "reach"
    ],
    "token_count":14,
    "processed_text":"saw newest build avid demod sundanc year crash someth fierc keep fcp within reach"
  },
  {
    "label":0,
    "text":"glade today last day see face mock reject affect",
    "cleaned_text":"glade today last day see face mock reject affect",
    "normalized_text":"glade today last day see face mock reject affect",
    "tokens":[
      "glade",
      "today",
      "last",
      "day",
      "see",
      "face",
      "mock",
      "reject",
      "affect"
    ],
    "token_count":9,
    "processed_text":"glade today last day see face mock reject affect"
  },
  {
    "label":4,
    "text":"could fit kym marsh refer im sure would happi",
    "cleaned_text":"could fit kym marsh refer im sure would happi",
    "normalized_text":"could fit kym marsh refer im sure would happi",
    "tokens":[
      "fit",
      "kym",
      "marsh",
      "refer",
      "im",
      "sure",
      "happi"
    ],
    "token_count":7,
    "processed_text":"fit kym marsh refer im sure happi"
  },
  {
    "label":4,
    "text":"wish friend",
    "cleaned_text":"wish friend",
    "normalized_text":"wish friend",
    "tokens":[
      "wish",
      "friend"
    ],
    "token_count":2,
    "processed_text":"wish friend"
  },
  {
    "label":4,
    "text":"lmfao yesterday fuck amaz",
    "cleaned_text":"lmfao yesterday fuck amaz",
    "normalized_text":"lmfao yesterday fuck amaz",
    "tokens":[
      "lmfao",
      "yesterday",
      "fuck",
      "amaz"
    ],
    "token_count":4,
    "processed_text":"lmfao yesterday fuck amaz"
  },
  {
    "label":0,
    "text":"go bed im especi tire sleepi somewhat problemat",
    "cleaned_text":"go bed im especi tire sleepi somewhat problemat",
    "normalized_text":"go bed im especi tire sleepi somewhat problemat",
    "tokens":[
      "go",
      "bed",
      "im",
      "especi",
      "tire",
      "sleepi",
      "somewhat",
      "problemat"
    ],
    "token_count":8,
    "processed_text":"go bed im especi tire sleepi somewhat problemat"
  },
  {
    "label":4,
    "text":"anyon interest profil locat couch",
    "cleaned_text":"anyon interest profil locat couch",
    "normalized_text":"anyon interest profil locat couch",
    "tokens":[
      "anyon",
      "interest",
      "profil",
      "locat",
      "couch"
    ],
    "token_count":5,
    "processed_text":"anyon interest profil locat couch"
  },
  {
    "label":0,
    "text":"joy tb backup drive fail time scrape togeth fund drobo",
    "cleaned_text":"joy tb backup drive fail time scrape togeth fund drobo",
    "normalized_text":"joy tb backup drive fail time scrape togeth fund drobo",
    "tokens":[
      "joy",
      "tb",
      "backup",
      "drive",
      "fail",
      "time",
      "scrape",
      "togeth",
      "fund",
      "drobo"
    ],
    "token_count":10,
    "processed_text":"joy tb backup drive fail time scrape togeth fund drobo"
  },
  {
    "label":0,
    "text":"im hungri ur help bout snack appl sauc ur talk shrimp pasta",
    "cleaned_text":"im hungri ur help bout snack appl sauc ur talk shrimp pasta",
    "normalized_text":"im hungri ur help bout snack appl sauc ur talk shrimp pasta",
    "tokens":[
      "im",
      "hungri",
      "ur",
      "help",
      "bout",
      "snack",
      "appl",
      "sauc",
      "ur",
      "talk",
      "shrimp",
      "pasta"
    ],
    "token_count":12,
    "processed_text":"im hungri ur help bout snack appl sauc ur talk shrimp pasta"
  },
  {
    "label":0,
    "text":"leesburg tomorrow see inlawsfunfunfun",
    "cleaned_text":"leesburg tomorrow see inlawsfunfunfun",
    "normalized_text":"leesburg tomorrow see inlawsfunfunfun",
    "tokens":[
      "leesburg",
      "tomorrow",
      "see",
      "inlawsfunfunfun"
    ],
    "token_count":4,
    "processed_text":"leesburg tomorrow see inlawsfunfunfun"
  },
  {
    "label":4,
    "text":"uff got home final ballet week feet hurt danc like hell",
    "cleaned_text":"uff got home final ballet week feet hurt danc like hell",
    "normalized_text":"uff got home final ballet week feet hurt danc like hell",
    "tokens":[
      "uff",
      "got",
      "home",
      "final",
      "ballet",
      "week",
      "feet",
      "hurt",
      "danc",
      "like",
      "hell"
    ],
    "token_count":11,
    "processed_text":"uff got home final ballet week feet hurt danc like hell"
  },
  {
    "label":0,
    "text":"unfortun point short hit record made kill screen",
    "cleaned_text":"unfortun point short hit record made kill screen",
    "normalized_text":"unfortun point short hit record made kill screen",
    "tokens":[
      "unfortun",
      "point",
      "short",
      "hit",
      "record",
      "made",
      "kill",
      "screen"
    ],
    "token_count":8,
    "processed_text":"unfortun point short hit record made kill screen"
  },
  {
    "label":0,
    "text":"cannot believ im stuck indoor iron love hot day",
    "cleaned_text":"cannot believ im stuck indoor iron love hot day",
    "normalized_text":"cannot believ im stuck indoor iron love hot day",
    "tokens":[
      "believ",
      "im",
      "stuck",
      "indoor",
      "iron",
      "love",
      "hot",
      "day"
    ],
    "token_count":8,
    "processed_text":"believ im stuck indoor iron love hot day"
  },
  {
    "label":4,
    "text":"way grandpa hang come back hope hang kert work",
    "cleaned_text":"way grandpa hang come back hope hang kert work",
    "normalized_text":"way grandpa hang come back hope hang kert work",
    "tokens":[
      "way",
      "grandpa",
      "hang",
      "come",
      "back",
      "hope",
      "hang",
      "kert",
      "work"
    ],
    "token_count":9,
    "processed_text":"way grandpa hang come back hope hang kert work"
  },
  {
    "label":4,
    "text":"temecula hot air balloon wine festiv beauti morn",
    "cleaned_text":"temecula hot air balloon wine festiv beauti morn",
    "normalized_text":"temecula hot air balloon wine festiv beauti morn",
    "tokens":[
      "temecula",
      "hot",
      "air",
      "balloon",
      "wine",
      "festiv",
      "beauti",
      "morn"
    ],
    "token_count":8,
    "processed_text":"temecula hot air balloon wine festiv beauti morn"
  },
  {
    "label":0,
    "text":"im feelin swag surfin songth regular one remixit remind miami",
    "cleaned_text":"im feelin swag surfin songth regular one remixit remind miami",
    "normalized_text":"im feelin swag surfin songth regular one remixit remind miami",
    "tokens":[
      "im",
      "feelin",
      "swag",
      "surfin",
      "songth",
      "regular",
      "one",
      "remixit",
      "remind",
      "miami"
    ],
    "token_count":10,
    "processed_text":"im feelin swag surfin songth regular one remixit remind miami"
  },
  {
    "label":4,
    "text":"summer",
    "cleaned_text":"summer",
    "normalized_text":"summer",
    "tokens":[
      "summer"
    ],
    "token_count":1,
    "processed_text":"summer"
  },
  {
    "label":0,
    "text":"umm got sunburn damn u medicin",
    "cleaned_text":"umm got sunburn damn u medicin",
    "normalized_text":"umm got sunburn damn u medicin",
    "tokens":[
      "umm",
      "got",
      "sunburn",
      "damn",
      "medicin"
    ],
    "token_count":5,
    "processed_text":"umm got sunburn damn medicin"
  },
  {
    "label":4,
    "text":"damn make proper websit one day help tiff",
    "cleaned_text":"damn make proper websit one day help tiff",
    "normalized_text":"damn make proper websit one day help tiff",
    "tokens":[
      "damn",
      "make",
      "proper",
      "websit",
      "one",
      "day",
      "help",
      "tiff"
    ],
    "token_count":8,
    "processed_text":"damn make proper websit one day help tiff"
  },
  {
    "label":4,
    "text":"sunni",
    "cleaned_text":"sunni",
    "normalized_text":"sunni",
    "tokens":[
      "sunni"
    ],
    "token_count":1,
    "processed_text":"sunni"
  },
  {
    "label":0,
    "text":"school philosophi doesnt seem like im learn hehe",
    "cleaned_text":"school philosophi doesnt seem like im learn hehe",
    "normalized_text":"school philosophi doesnt seem like im learn hehe",
    "tokens":[
      "school",
      "philosophi",
      "doesnt",
      "seem",
      "like",
      "im",
      "learn",
      "hehe"
    ],
    "token_count":8,
    "processed_text":"school philosophi doesnt seem like im learn hehe"
  },
  {
    "label":4,
    "text":"proud healthi weightloss fact she",
    "cleaned_text":"proud healthi weightloss fact she",
    "normalized_text":"proud healthi weightloss fact she",
    "tokens":[
      "proud",
      "healthi",
      "weightloss",
      "fact"
    ],
    "token_count":4,
    "processed_text":"proud healthi weightloss fact"
  },
  {
    "label":0,
    "text":"watch fli music vid note gonna tmrw shoot",
    "cleaned_text":"watch fli music vid note gonna tmrw shoot",
    "normalized_text":"watch fli music vid note gonna tmrw shoot",
    "tokens":[
      "watch",
      "fli",
      "music",
      "vid",
      "note",
      "gon",
      "na",
      "tmrw",
      "shoot"
    ],
    "token_count":9,
    "processed_text":"watch fli music vid note gon na tmrw shoot"
  },
  {
    "label":0,
    "text":"amaz day warm sunni im sky hd instal tv seem unimpress hdmi offer",
    "cleaned_text":"amaz day warm sunni im sky hd instal tv seem unimpress hdmi offer",
    "normalized_text":"amaz day warm sunni im sky hd instal tv seem unimpress hdmi offer",
    "tokens":[
      "amaz",
      "day",
      "warm",
      "sunni",
      "im",
      "sky",
      "hd",
      "instal",
      "tv",
      "seem",
      "unimpress",
      "hdmi",
      "offer"
    ],
    "token_count":13,
    "processed_text":"amaz day warm sunni im sky hd instal tv seem unimpress hdmi offer"
  },
  {
    "label":0,
    "text":"isnt happi team foggi sky ass hand us",
    "cleaned_text":"isnt happi team foggi sky ass hand us",
    "normalized_text":"isnt happi team foggi sky ass hand us",
    "tokens":[
      "isnt",
      "happi",
      "team",
      "foggi",
      "sky",
      "ass",
      "hand",
      "us"
    ],
    "token_count":8,
    "processed_text":"isnt happi team foggi sky ass hand us"
  },
  {
    "label":4,
    "text":"england beat cricket welli sport good mayb make new sport",
    "cleaned_text":"england beat cricket welli sport good mayb make new sport",
    "normalized_text":"england beat cricket welli sport good mayb make new sport",
    "tokens":[
      "england",
      "beat",
      "cricket",
      "welli",
      "sport",
      "good",
      "mayb",
      "make",
      "new",
      "sport"
    ],
    "token_count":10,
    "processed_text":"england beat cricket welli sport good mayb make new sport"
  },
  {
    "label":0,
    "text":"tvfox firefox plugin allow watch tv channel sever countri world regret avail linux",
    "cleaned_text":"tvfox firefox plugin allow watch tv channel sever countri world regret avail linux",
    "normalized_text":"tvfox firefox plugin allow watch tv channel sever countri world regret avail linux",
    "tokens":[
      "tvfox",
      "firefox",
      "plugin",
      "allow",
      "watch",
      "tv",
      "channel",
      "sever",
      "countri",
      "world",
      "regret",
      "avail",
      "linux"
    ],
    "token_count":13,
    "processed_text":"tvfox firefox plugin allow watch tv channel sever countri world regret avail linux"
  },
  {
    "label":0,
    "text":"pc",
    "cleaned_text":"pc",
    "normalized_text":"pc",
    "tokens":[
      "pc"
    ],
    "token_count":1,
    "processed_text":"pc"
  },
  {
    "label":4,
    "text":"great spirit due beauti day great citi plan shop anticip visitor later",
    "cleaned_text":"great spirit due beauti day great citi plan shop anticip visitor later",
    "normalized_text":"great spirit due beauti day great citi plan shop anticip visitor later",
    "tokens":[
      "great",
      "spirit",
      "due",
      "beauti",
      "day",
      "great",
      "citi",
      "plan",
      "shop",
      "anticip",
      "visitor",
      "later"
    ],
    "token_count":12,
    "processed_text":"great spirit due beauti day great citi plan shop anticip visitor later"
  },
  {
    "label":4,
    "text":"back work week long break",
    "cleaned_text":"back work week long break",
    "normalized_text":"back work week long break",
    "tokens":[
      "back",
      "work",
      "week",
      "long",
      "break"
    ],
    "token_count":5,
    "processed_text":"back work week long break"
  },
  {
    "label":0,
    "text":"ye burnt pop corn whole offic smell like",
    "cleaned_text":"ye burnt pop corn whole offic smell like",
    "normalized_text":"ye burnt pop corn whole offic smell like",
    "tokens":[
      "ye",
      "burnt",
      "pop",
      "corn",
      "whole",
      "offic",
      "smell",
      "like"
    ],
    "token_count":8,
    "processed_text":"ye burnt pop corn whole offic smell like"
  },
  {
    "label":0,
    "text":"um camera isnt work dont wanna tell hubbi either eek dont know happen",
    "cleaned_text":"um camera isnt work dont wanna tell hubbi either eek dont know happen",
    "normalized_text":"um camera isnt work dont wanna tell hubbi either eek dont know happen",
    "tokens":[
      "um",
      "camera",
      "isnt",
      "work",
      "dont",
      "wan",
      "na",
      "tell",
      "hubbi",
      "either",
      "eek",
      "dont",
      "know",
      "happen"
    ],
    "token_count":14,
    "processed_text":"um camera isnt work dont wan na tell hubbi either eek dont know happen"
  },
  {
    "label":4,
    "text":"ill tri thank",
    "cleaned_text":"ill tri thank",
    "normalized_text":"ill tri thank",
    "tokens":[
      "ill",
      "tri",
      "thank"
    ],
    "token_count":3,
    "processed_text":"ill tri thank"
  },
  {
    "label":4,
    "text":"get yer butt girl youll feel better",
    "cleaned_text":"get yer butt girl youll feel better",
    "normalized_text":"get yer butt girl youll feel better",
    "tokens":[
      "get",
      "yer",
      "butt",
      "girl",
      "youll",
      "feel",
      "better"
    ],
    "token_count":7,
    "processed_text":"get yer butt girl youll feel better"
  },
  {
    "label":4,
    "text":"summerwood frog pad kid expect play fountain get wet",
    "cleaned_text":"summerwood frog pad kid expect play fountain get wet",
    "normalized_text":"summerwood frog pad kid expect play fountain get wet",
    "tokens":[
      "summerwood",
      "frog",
      "pad",
      "kid",
      "expect",
      "play",
      "fountain",
      "get",
      "wet"
    ],
    "token_count":9,
    "processed_text":"summerwood frog pad kid expect play fountain get wet"
  },
  {
    "label":4,
    "text":"wish kavya happi nd birthday two year photo edit",
    "cleaned_text":"wish kavya happi nd birthday two year photo edit",
    "normalized_text":"wish kavya happi nd birthday two year photo edit",
    "tokens":[
      "wish",
      "kavya",
      "happi",
      "nd",
      "birthday",
      "two",
      "year",
      "photo",
      "edit"
    ],
    "token_count":9,
    "processed_text":"wish kavya happi nd birthday two year photo edit"
  },
  {
    "label":0,
    "text":"think might turn hungri caterpillar keep get hungri",
    "cleaned_text":"think might turn hungri caterpillar keep get hungri",
    "normalized_text":"think might turn hungri caterpillar keep get hungri",
    "tokens":[
      "think",
      "turn",
      "hungri",
      "caterpillar",
      "keep",
      "get",
      "hungri"
    ],
    "token_count":7,
    "processed_text":"think turn hungri caterpillar keep get hungri"
  },
  {
    "label":0,
    "text":"updat sdk cant deploy devic anymor what provisioningprofileallowedrequir hack doesnt work",
    "cleaned_text":"updat sdk cant deploy devic anymor what provisioningprofileallowedrequir hack doesnt work",
    "normalized_text":"updat sdk cant deploy devic anymor what provisioningprofileallowedrequir hack doesnt work",
    "tokens":[
      "updat",
      "sdk",
      "cant",
      "deploy",
      "devic",
      "anymor",
      "hack",
      "doesnt",
      "work"
    ],
    "token_count":9,
    "processed_text":"updat sdk cant deploy devic anymor hack doesnt work"
  },
  {
    "label":0,
    "text":"feel like cooki cream icecream aww dont",
    "cleaned_text":"feel like cooki cream icecream aww dont",
    "normalized_text":"feel like cooki cream icecream aww dont",
    "tokens":[
      "feel",
      "like",
      "cooki",
      "cream",
      "icecream",
      "aww",
      "dont"
    ],
    "token_count":7,
    "processed_text":"feel like cooki cream icecream aww dont"
  },
  {
    "label":4,
    "text":"prairi tale movi laura ingal wilder love watch kid alway made cri",
    "cleaned_text":"prairi tale movi laura ingal wilder love watch kid alway made cri",
    "normalized_text":"prairi tale movi laura ingal wilder love watch kid alway made cri",
    "tokens":[
      "prairi",
      "tale",
      "movi",
      "laura",
      "ingal",
      "wilder",
      "love",
      "watch",
      "kid",
      "alway",
      "made",
      "cri"
    ],
    "token_count":12,
    "processed_text":"prairi tale movi laura ingal wilder love watch kid alway made cri"
  },
  {
    "label":4,
    "text":"studi life away howev see light end tunnel day go",
    "cleaned_text":"studi life away howev see light end tunnel day go",
    "normalized_text":"studi life away howev see light end tunnel day go",
    "tokens":[
      "studi",
      "life",
      "away",
      "howev",
      "see",
      "light",
      "end",
      "tunnel",
      "day",
      "go"
    ],
    "token_count":10,
    "processed_text":"studi life away howev see light end tunnel day go"
  },
  {
    "label":4,
    "text":"omg one day oh guess chat vkim amp sterada im sucha l e r",
    "cleaned_text":"omg one day oh guess chat vkim amp sterada im sucha l e r",
    "normalized_text":"omg one day oh guess chat vkim amp sterada im sucha l e r",
    "tokens":[
      "omg",
      "one",
      "day",
      "oh",
      "guess",
      "chat",
      "vkim",
      "amp",
      "sterada",
      "im",
      "sucha"
    ],
    "token_count":11,
    "processed_text":"omg one day oh guess chat vkim amp sterada im sucha"
  },
  {
    "label":4,
    "text":"well im doctor ill go one",
    "cleaned_text":"well im doctor ill go one",
    "normalized_text":"well im doctor ill go one",
    "tokens":[
      "well",
      "im",
      "doctor",
      "ill",
      "go",
      "one"
    ],
    "token_count":6,
    "processed_text":"well im doctor ill go one"
  },
  {
    "label":4,
    "text":"cinema wait jona brother movi",
    "cleaned_text":"cinema wait jona brother movi",
    "normalized_text":"cinema wait jona brother movi",
    "tokens":[
      "cinema",
      "wait",
      "jona",
      "brother",
      "movi"
    ],
    "token_count":5,
    "processed_text":"cinema wait jona brother movi"
  },
  {
    "label":0,
    "text":"famili war begin cant shut divorc someth",
    "cleaned_text":"famili war begin cant shut divorc someth",
    "normalized_text":"famili war begin cant shut divorc someth",
    "tokens":[
      "famili",
      "war",
      "begin",
      "cant",
      "shut",
      "divorc",
      "someth"
    ],
    "token_count":7,
    "processed_text":"famili war begin cant shut divorc someth"
  },
  {
    "label":4,
    "text":"watch octob sky twa nice jake gyllenha pick good role",
    "cleaned_text":"watch octob sky twa nice jake gyllenha pick good role",
    "normalized_text":"watch octob sky twa nice jake gyllenha pick good role",
    "tokens":[
      "watch",
      "octob",
      "sky",
      "twa",
      "nice",
      "jake",
      "gyllenha",
      "pick",
      "good",
      "role"
    ],
    "token_count":10,
    "processed_text":"watch octob sky twa nice jake gyllenha pick good role"
  },
  {
    "label":0,
    "text":"well feel like failur littl bro want kill himeslf cuz like said he six",
    "cleaned_text":"well feel like failur littl bro want kill himeslf cuz like said he six",
    "normalized_text":"well feel like failur littl bro want kill himeslf cuz like said he six",
    "tokens":[
      "well",
      "feel",
      "like",
      "failur",
      "littl",
      "bro",
      "want",
      "kill",
      "himeslf",
      "cuz",
      "like",
      "said",
      "six"
    ],
    "token_count":13,
    "processed_text":"well feel like failur littl bro want kill himeslf cuz like said six"
  },
  {
    "label":4,
    "text":"love laugh drunk ppl",
    "cleaned_text":"love laugh drunk ppl",
    "normalized_text":"love laugh drunk ppl",
    "tokens":[
      "love",
      "laugh",
      "drunk",
      "ppl"
    ],
    "token_count":4,
    "processed_text":"love laugh drunk ppl"
  },
  {
    "label":0,
    "text":"cant finish paper dont know im tri say anymor",
    "cleaned_text":"cant finish paper dont know im tri say anymor",
    "normalized_text":"cant finish paper dont know im tri say anymor",
    "tokens":[
      "cant",
      "finish",
      "paper",
      "dont",
      "know",
      "im",
      "tri",
      "say",
      "anymor"
    ],
    "token_count":9,
    "processed_text":"cant finish paper dont know im tri say anymor"
  },
  {
    "label":0,
    "text":"woke earlyi think half still asleep",
    "cleaned_text":"woke earlyi think half still asleep",
    "normalized_text":"woke earlyi think half still asleep",
    "tokens":[
      "woke",
      "earlyi",
      "think",
      "half",
      "still",
      "asleep"
    ],
    "token_count":6,
    "processed_text":"woke earlyi think half still asleep"
  },
  {
    "label":0,
    "text":"serious though suck",
    "cleaned_text":"serious though suck",
    "normalized_text":"serious though suck",
    "tokens":[
      "seriou",
      "though",
      "suck"
    ],
    "token_count":3,
    "processed_text":"seriou though suck"
  },
  {
    "label":0,
    "text":"last day dc",
    "cleaned_text":"last day dc",
    "normalized_text":"last day dc",
    "tokens":[
      "last",
      "day",
      "dc"
    ],
    "token_count":3,
    "processed_text":"last day dc"
  },
  {
    "label":4,
    "text":"right im outta check dm great day",
    "cleaned_text":"right im outta check dm great day",
    "normalized_text":"right im outta check dm great day",
    "tokens":[
      "right",
      "im",
      "outta",
      "check",
      "dm",
      "great",
      "day"
    ],
    "token_count":7,
    "processed_text":"right im outta check dm great day"
  },
  {
    "label":0,
    "text":"mani hour left workday",
    "cleaned_text":"mani hour left workday",
    "normalized_text":"mani hour left workday",
    "tokens":[
      "mani",
      "hour",
      "left",
      "workday"
    ],
    "token_count":4,
    "processed_text":"mani hour left workday"
  },
  {
    "label":0,
    "text":"soo tire ugh pain",
    "cleaned_text":"soo tire ugh pain",
    "normalized_text":"soo tire ugh pain",
    "tokens":[
      "soo",
      "tire",
      "ugh",
      "pain"
    ],
    "token_count":4,
    "processed_text":"soo tire ugh pain"
  },
  {
    "label":0,
    "text":"run time",
    "cleaned_text":"run time",
    "normalized_text":"run time",
    "tokens":[
      "run",
      "time"
    ],
    "token_count":2,
    "processed_text":"run time"
  },
  {
    "label":0,
    "text":"unfortun may screw big run today self control sometim",
    "cleaned_text":"unfortun may screw big run today self control sometim",
    "normalized_text":"unfortun may screw big run today self control sometim",
    "tokens":[
      "unfortun",
      "may",
      "screw",
      "big",
      "run",
      "today",
      "self",
      "control",
      "sometim"
    ],
    "token_count":9,
    "processed_text":"unfortun may screw big run today self control sometim"
  },
  {
    "label":0,
    "text":"guess im jerk dont feel good",
    "cleaned_text":"guess im jerk dont feel good",
    "normalized_text":"guess im jerk dont feel good",
    "tokens":[
      "guess",
      "im",
      "jerk",
      "dont",
      "feel",
      "good"
    ],
    "token_count":6,
    "processed_text":"guess im jerk dont feel good"
  },
  {
    "label":0,
    "text":"woke headach",
    "cleaned_text":"woke headach",
    "normalized_text":"woke headach",
    "tokens":[
      "woke",
      "headach"
    ],
    "token_count":2,
    "processed_text":"woke headach"
  },
  {
    "label":4,
    "text":"go see grandpar today sinc visit",
    "cleaned_text":"go see grandpar today sinc visit",
    "normalized_text":"go see grandpar today sinc visit",
    "tokens":[
      "go",
      "see",
      "grandpar",
      "today",
      "sinc",
      "visit"
    ],
    "token_count":6,
    "processed_text":"go see grandpar today sinc visit"
  },
  {
    "label":4,
    "text":"hope better last",
    "cleaned_text":"hope better last",
    "normalized_text":"hope better last",
    "tokens":[
      "hope",
      "better",
      "last"
    ],
    "token_count":3,
    "processed_text":"hope better last"
  },
  {
    "label":0,
    "text":"im freak cant upload pictur",
    "cleaned_text":"im freak cant upload pictur",
    "normalized_text":"im freak cant upload pictur",
    "tokens":[
      "im",
      "freak",
      "cant",
      "upload",
      "pictur"
    ],
    "token_count":5,
    "processed_text":"im freak cant upload pictur"
  },
  {
    "label":0,
    "text":"ahhh live websit kill",
    "cleaned_text":"ahhh live websit kill",
    "normalized_text":"ahhh live websit kill",
    "tokens":[
      "ahhh",
      "live",
      "websit",
      "kill"
    ],
    "token_count":4,
    "processed_text":"ahhh live websit kill"
  },
  {
    "label":0,
    "text":"work tonight doesnt know sing cold",
    "cleaned_text":"work tonight doesnt know sing cold",
    "normalized_text":"work tonight doesnt know sing cold",
    "tokens":[
      "work",
      "tonight",
      "doesnt",
      "know",
      "sing",
      "cold"
    ],
    "token_count":6,
    "processed_text":"work tonight doesnt know sing cold"
  },
  {
    "label":4,
    "text":"tattoo parti place",
    "cleaned_text":"tattoo parti place",
    "normalized_text":"tattoo parti place",
    "tokens":[
      "tattoo",
      "parti",
      "place"
    ],
    "token_count":3,
    "processed_text":"tattoo parti place"
  },
  {
    "label":0,
    "text":"hurt back cosz bro thought donkey",
    "cleaned_text":"hurt back cosz bro thought donkey",
    "normalized_text":"hurt back cosz bro thought donkey",
    "tokens":[
      "hurt",
      "back",
      "cosz",
      "bro",
      "thought",
      "donkey"
    ],
    "token_count":6,
    "processed_text":"hurt back cosz bro thought donkey"
  },
  {
    "label":4,
    "text":"know goal might lot easier achiev friend go hint hint",
    "cleaned_text":"know goal might lot easier achiev friend go hint hint",
    "normalized_text":"know goal might lot easier achiev friend go hint hint",
    "tokens":[
      "know",
      "goal",
      "lot",
      "easier",
      "achiev",
      "friend",
      "go",
      "hint",
      "hint"
    ],
    "token_count":9,
    "processed_text":"know goal lot easier achiev friend go hint hint"
  },
  {
    "label":0,
    "text":"realiz burnt hair boo wind tri light cigarett",
    "cleaned_text":"realiz burnt hair boo wind tri light cigarett",
    "normalized_text":"realiz burnt hair boo wind tri light cigarett",
    "tokens":[
      "realiz",
      "burnt",
      "hair",
      "boo",
      "wind",
      "tri",
      "light",
      "cigarett"
    ],
    "token_count":8,
    "processed_text":"realiz burnt hair boo wind tri light cigarett"
  },
  {
    "label":0,
    "text":"everytim cant go home awesom lineup artist everytim",
    "cleaned_text":"everytim cant go home awesom lineup artist everytim",
    "normalized_text":"everytim cant go home awesom lineup artist everytim",
    "tokens":[
      "everytim",
      "cant",
      "go",
      "home",
      "awesom",
      "lineup",
      "artist",
      "everytim"
    ],
    "token_count":8,
    "processed_text":"everytim cant go home awesom lineup artist everytim"
  },
  {
    "label":4,
    "text":"aw cuti lucki xd",
    "cleaned_text":"aw cuti lucki xd",
    "normalized_text":"aw cuti lucki xd",
    "tokens":[
      "aw",
      "cuti",
      "lucki",
      "xd"
    ],
    "token_count":4,
    "processed_text":"aw cuti lucki xd"
  },
  {
    "label":4,
    "text":"realli bore decid liz live togeth go get cutest pet world haha",
    "cleaned_text":"realli bore decid liz live togeth go get cutest pet world haha",
    "normalized_text":"realli bore decid liz live togeth go get cutest pet world haha",
    "tokens":[
      "realli",
      "bore",
      "decid",
      "liz",
      "live",
      "togeth",
      "go",
      "get",
      "cutest",
      "pet",
      "world",
      "haha"
    ],
    "token_count":12,
    "processed_text":"realli bore decid liz live togeth go get cutest pet world haha"
  },
  {
    "label":0,
    "text":"swine flue test today neg mean treatment flue",
    "cleaned_text":"swine flue test today neg mean treatment flue",
    "normalized_text":"swine flue test today neg mean treatment flue",
    "tokens":[
      "swine",
      "flue",
      "test",
      "today",
      "neg",
      "mean",
      "treatment",
      "flue"
    ],
    "token_count":8,
    "processed_text":"swine flue test today neg mean treatment flue"
  },
  {
    "label":4,
    "text":"type dyeleepong click plu sign pleas thank",
    "cleaned_text":"type dyeleepong click plu sign pleas thank",
    "normalized_text":"type dyeleepong click plu sign pleas thank",
    "tokens":[
      "type",
      "dyeleepong",
      "click",
      "plu",
      "sign",
      "plea",
      "thank"
    ],
    "token_count":7,
    "processed_text":"type dyeleepong click plu sign plea thank"
  },
  {
    "label":4,
    "text":"love receiv packag post",
    "cleaned_text":"love receiv packag post",
    "normalized_text":"love receiv packag post",
    "tokens":[
      "love",
      "receiv",
      "packag",
      "post"
    ],
    "token_count":4,
    "processed_text":"love receiv packag post"
  },
  {
    "label":4,
    "text":"how go awesom morn",
    "cleaned_text":"how go awesom morn",
    "normalized_text":"how go awesom morn",
    "tokens":[
      "go",
      "awesom",
      "morn"
    ],
    "token_count":3,
    "processed_text":"go awesom morn"
  },
  {
    "label":4,
    "text":"chees toast cuppa",
    "cleaned_text":"chees toast cuppa",
    "normalized_text":"chees toast cuppa",
    "tokens":[
      "chee",
      "toast",
      "cuppa"
    ],
    "token_count":3,
    "processed_text":"chee toast cuppa"
  },
  {
    "label":4,
    "text":"lt holiday parad",
    "cleaned_text":"lt holiday parad",
    "normalized_text":"lt holiday parad",
    "tokens":[
      "lt",
      "holiday",
      "parad"
    ],
    "token_count":3,
    "processed_text":"lt holiday parad"
  },
  {
    "label":0,
    "text":"awak stupid hour lol",
    "cleaned_text":"awak stupid hour lol",
    "normalized_text":"awak stupid hour lol",
    "tokens":[
      "awak",
      "stupid",
      "hour",
      "lol"
    ],
    "token_count":4,
    "processed_text":"awak stupid hour lol"
  },
  {
    "label":0,
    "text":"lol hurt ampamp dont understand peopl babi know dont want smt depressin",
    "cleaned_text":"lol hurt ampamp dont understand peopl babi know dont want smt depressin",
    "normalized_text":"lol hurt ampamp dont understand peopl babi know dont want smt depressin",
    "tokens":[
      "lol",
      "hurt",
      "ampamp",
      "dont",
      "understand",
      "peopl",
      "babi",
      "know",
      "dont",
      "want",
      "smt",
      "depressin"
    ],
    "token_count":12,
    "processed_text":"lol hurt ampamp dont understand peopl babi know dont want smt depressin"
  },
  {
    "label":0,
    "text":"im realli sorri strong",
    "cleaned_text":"im realli sorri strong",
    "normalized_text":"im realli sorri strong",
    "tokens":[
      "im",
      "realli",
      "sorri",
      "strong"
    ],
    "token_count":4,
    "processed_text":"im realli sorri strong"
  },
  {
    "label":0,
    "text":"afraid dont eat much seafood",
    "cleaned_text":"afraid dont eat much seafood",
    "normalized_text":"afraid dont eat much seafood",
    "tokens":[
      "afraid",
      "dont",
      "eat",
      "much",
      "seafood"
    ],
    "token_count":5,
    "processed_text":"afraid dont eat much seafood"
  },
  {
    "label":0,
    "text":"get wisdom teeth minut",
    "cleaned_text":"get wisdom teeth minut",
    "normalized_text":"get wisdom teeth minut",
    "tokens":[
      "get",
      "wisdom",
      "teeth",
      "minut"
    ],
    "token_count":4,
    "processed_text":"get wisdom teeth minut"
  },
  {
    "label":0,
    "text":"land detroit right sigh demf",
    "cleaned_text":"land detroit right sigh demf",
    "normalized_text":"land detroit right sigh demf",
    "tokens":[
      "land",
      "detroit",
      "right",
      "sigh",
      "demf"
    ],
    "token_count":5,
    "processed_text":"land detroit right sigh demf"
  },
  {
    "label":4,
    "text":"watch new puppi littl nap",
    "cleaned_text":"watch new puppi littl nap",
    "normalized_text":"watch new puppi littl nap",
    "tokens":[
      "watch",
      "new",
      "puppi",
      "littl",
      "nap"
    ],
    "token_count":5,
    "processed_text":"watch new puppi littl nap"
  },
  {
    "label":0,
    "text":"home ny shoot went fabul sick bed boo",
    "cleaned_text":"home ny shoot went fabul sick bed boo",
    "normalized_text":"home ny shoot went fabul sick bed boo",
    "tokens":[
      "home",
      "ny",
      "shoot",
      "went",
      "fabul",
      "sick",
      "bed",
      "boo"
    ],
    "token_count":8,
    "processed_text":"home ny shoot went fabul sick bed boo"
  },
  {
    "label":4,
    "text":"caramel frappachino",
    "cleaned_text":"caramel frappachino",
    "normalized_text":"caramel frappachino",
    "tokens":[
      "caramel",
      "frappachino"
    ],
    "token_count":2,
    "processed_text":"caramel frappachino"
  },
  {
    "label":4,
    "text":"hope peopl read post",
    "cleaned_text":"hope peopl read post",
    "normalized_text":"hope peopl read post",
    "tokens":[
      "hope",
      "peopl",
      "read",
      "post"
    ],
    "token_count":4,
    "processed_text":"hope peopl read post"
  },
  {
    "label":0,
    "text":"ss project chines hol hmwk math hol hmwk",
    "cleaned_text":"ss project chines hol hmwk math hol hmwk",
    "normalized_text":"ss project chines hol hmwk math hol hmwk",
    "tokens":[
      "ss",
      "project",
      "chine",
      "hol",
      "hmwk",
      "math",
      "hol",
      "hmwk"
    ],
    "token_count":8,
    "processed_text":"ss project chine hol hmwk math hol hmwk"
  },
  {
    "label":4,
    "text":"sorri smartpunk preorder knew gk either way im get album",
    "cleaned_text":"sorri smartpunk preorder knew gk either way im get album",
    "normalized_text":"sorri smartpunk preorder knew gk either way im get album",
    "tokens":[
      "sorri",
      "smartpunk",
      "preorder",
      "knew",
      "gk",
      "either",
      "way",
      "im",
      "get",
      "album"
    ],
    "token_count":10,
    "processed_text":"sorri smartpunk preorder knew gk either way im get album"
  },
  {
    "label":4,
    "text":"tell someth new",
    "cleaned_text":"tell someth new",
    "normalized_text":"tell someth new",
    "tokens":[
      "tell",
      "someth",
      "new"
    ],
    "token_count":3,
    "processed_text":"tell someth new"
  },
  {
    "label":4,
    "text":"two fun tweet follow thank ladi",
    "cleaned_text":"two fun tweet follow thank ladi",
    "normalized_text":"two fun tweet follow thank ladi",
    "tokens":[
      "two",
      "fun",
      "tweet",
      "follow",
      "thank",
      "ladi"
    ],
    "token_count":6,
    "processed_text":"two fun tweet follow thank ladi"
  },
  {
    "label":0,
    "text":"guess need mobil spellcheck",
    "cleaned_text":"guess need mobil spellcheck",
    "normalized_text":"guess need mobil spellcheck",
    "tokens":[
      "guess",
      "need",
      "mobil",
      "spellcheck"
    ],
    "token_count":4,
    "processed_text":"guess need mobil spellcheck"
  },
  {
    "label":4,
    "text":"know im tri right",
    "cleaned_text":"know im tri right",
    "normalized_text":"know im tri right",
    "tokens":[
      "know",
      "im",
      "tri",
      "right"
    ],
    "token_count":4,
    "processed_text":"know im tri right"
  },
  {
    "label":0,
    "text":"im day master cleans pound lighter im hope lb still dwyl ticket yet",
    "cleaned_text":"im day master cleans pound lighter im hope lb still dwyl ticket yet",
    "normalized_text":"im day master cleans pound lighter im hope lb still dwyl ticket yet",
    "tokens":[
      "im",
      "day",
      "master",
      "clean",
      "pound",
      "lighter",
      "im",
      "hope",
      "lb",
      "still",
      "dwyl",
      "ticket",
      "yet"
    ],
    "token_count":13,
    "processed_text":"im day master clean pound lighter im hope lb still dwyl ticket yet"
  },
  {
    "label":0,
    "text":"argggg hate dentist dw appoint like th",
    "cleaned_text":"argggg hate dentist dw appoint like th",
    "normalized_text":"argggg hate dentist dw appoint like th",
    "tokens":[
      "argggg",
      "hate",
      "dentist",
      "dw",
      "appoint",
      "like",
      "th"
    ],
    "token_count":7,
    "processed_text":"argggg hate dentist dw appoint like th"
  },
  {
    "label":0,
    "text":"didnt sleep cant stand doesnt even notic unbear",
    "cleaned_text":"didnt sleep cant stand doesnt even notic unbear",
    "normalized_text":"didnt sleep cant stand doesnt even notic unbear",
    "tokens":[
      "didnt",
      "sleep",
      "cant",
      "stand",
      "doesnt",
      "even",
      "notic",
      "unbear"
    ],
    "token_count":8,
    "processed_text":"didnt sleep cant stand doesnt even notic unbear"
  },
  {
    "label":0,
    "text":"dont blame invit",
    "cleaned_text":"dont blame invit",
    "normalized_text":"dont blame invit",
    "tokens":[
      "dont",
      "blame",
      "invit"
    ],
    "token_count":3,
    "processed_text":"dont blame invit"
  },
  {
    "label":0,
    "text":"someon left messag yesterday sayinghi hannah miller give call back get dont know",
    "cleaned_text":"someon left messag yesterday sayinghi hannah miller give call back get dont know",
    "normalized_text":"someon left messag yesterday sayinghi hannah miller give call back get dont know",
    "tokens":[
      "someon",
      "left",
      "messag",
      "yesterday",
      "sayinghi",
      "hannah",
      "miller",
      "give",
      "call",
      "back",
      "get",
      "dont",
      "know"
    ],
    "token_count":13,
    "processed_text":"someon left messag yesterday sayinghi hannah miller give call back get dont know"
  },
  {
    "label":0,
    "text":"fuckmi eye like piss hole snow",
    "cleaned_text":"fuckmi eye like piss hole snow",
    "normalized_text":"fuckmi eye like piss hole snow",
    "tokens":[
      "fuckmi",
      "eye",
      "like",
      "piss",
      "hole",
      "snow"
    ],
    "token_count":6,
    "processed_text":"fuckmi eye like piss hole snow"
  },
  {
    "label":0,
    "text":"omg last night went yauatcha finish money",
    "cleaned_text":"omg last night went yauatcha finish money",
    "normalized_text":"omg last night went yauatcha finish money",
    "tokens":[
      "omg",
      "last",
      "night",
      "went",
      "yauatcha",
      "finish",
      "money"
    ],
    "token_count":7,
    "processed_text":"omg last night went yauatcha finish money"
  },
  {
    "label":4,
    "text":"amaz concert got realli good video pictur",
    "cleaned_text":"amaz concert got realli good video pictur",
    "normalized_text":"amaz concert got realli good video pictur",
    "tokens":[
      "amaz",
      "concert",
      "got",
      "realli",
      "good",
      "video",
      "pictur"
    ],
    "token_count":7,
    "processed_text":"amaz concert got realli good video pictur"
  },
  {
    "label":4,
    "text":"tell els right nownin tonight readi",
    "cleaned_text":"tell els right nownin tonight readi",
    "normalized_text":"tell els right nownin tonight readi",
    "tokens":[
      "tell",
      "el",
      "right",
      "nownin",
      "tonight",
      "readi"
    ],
    "token_count":6,
    "processed_text":"tell el right nownin tonight readi"
  },
  {
    "label":4,
    "text":"gt there much memor youll ever see",
    "cleaned_text":"gt there much memor youll ever see",
    "normalized_text":"gt there much memor youll ever see",
    "tokens":[
      "gt",
      "much",
      "memor",
      "youll",
      "ever",
      "see"
    ],
    "token_count":6,
    "processed_text":"gt much memor youll ever see"
  },
  {
    "label":0,
    "text":"tri avoid bb lol wont back til monday either miss guy xxx",
    "cleaned_text":"tri avoid bb lol wont back til monday either miss guy xxx",
    "normalized_text":"tri avoid bb lol wont back til monday either miss guy xxx",
    "tokens":[
      "tri",
      "avoid",
      "bb",
      "lol",
      "wont",
      "back",
      "til",
      "monday",
      "either",
      "miss",
      "guy",
      "xxx"
    ],
    "token_count":12,
    "processed_text":"tri avoid bb lol wont back til monday either miss guy xxx"
  },
  {
    "label":4,
    "text":"see like",
    "cleaned_text":"see like",
    "normalized_text":"see like",
    "tokens":[
      "see",
      "like"
    ],
    "token_count":2,
    "processed_text":"see like"
  },
  {
    "label":4,
    "text":"youuuu",
    "cleaned_text":"youuuu",
    "normalized_text":"youuuu",
    "tokens":[
      "youuuu"
    ],
    "token_count":1,
    "processed_text":"youuuu"
  },
  {
    "label":4,
    "text":"dinner pretti mom",
    "cleaned_text":"dinner pretti mom",
    "normalized_text":"dinner pretti mom",
    "tokens":[
      "dinner",
      "pretti",
      "mom"
    ],
    "token_count":3,
    "processed_text":"dinner pretti mom"
  },
  {
    "label":4,
    "text":"happi birthday eat someth good",
    "cleaned_text":"happi birthday eat someth good",
    "normalized_text":"happi birthday eat someth good",
    "tokens":[
      "happi",
      "birthday",
      "eat",
      "someth",
      "good"
    ],
    "token_count":5,
    "processed_text":"happi birthday eat someth good"
  },
  {
    "label":4,
    "text":"oh gosh awesom piec kri woot",
    "cleaned_text":"oh gosh awesom piec kri woot",
    "normalized_text":"oh gosh awesom piec kri woot",
    "tokens":[
      "oh",
      "gosh",
      "awesom",
      "piec",
      "kri",
      "woot"
    ],
    "token_count":6,
    "processed_text":"oh gosh awesom piec kri woot"
  },
  {
    "label":4,
    "text":"go camp weekend yippe",
    "cleaned_text":"go camp weekend yippe",
    "normalized_text":"go camp weekend yippe",
    "tokens":[
      "go",
      "camp",
      "weekend",
      "yipp"
    ],
    "token_count":4,
    "processed_text":"go camp weekend yipp"
  },
  {
    "label":4,
    "text":"got home swap meet gross gonna target look ipod touh case",
    "cleaned_text":"got home swap meet gross gonna target look ipod touh case",
    "normalized_text":"got home swap meet gross gonna target look ipod touh case",
    "tokens":[
      "got",
      "home",
      "swap",
      "meet",
      "gross",
      "gon",
      "na",
      "target",
      "look",
      "ipod",
      "touh",
      "case"
    ],
    "token_count":12,
    "processed_text":"got home swap meet gross gon na target look ipod touh case"
  },
  {
    "label":4,
    "text":"wth dont know song",
    "cleaned_text":"wth dont know song",
    "normalized_text":"wth dont know song",
    "tokens":[
      "wth",
      "dont",
      "know",
      "song"
    ],
    "token_count":4,
    "processed_text":"wth dont know song"
  },
  {
    "label":4,
    "text":"download album nowww",
    "cleaned_text":"download album nowww",
    "normalized_text":"download album nowww",
    "tokens":[
      "download",
      "album",
      "nowww"
    ],
    "token_count":3,
    "processed_text":"download album nowww"
  },
  {
    "label":4,
    "text":"nice pic big happi famili",
    "cleaned_text":"nice pic big happi famili",
    "normalized_text":"nice pic big happi famili",
    "tokens":[
      "nice",
      "pic",
      "big",
      "happi",
      "famili"
    ],
    "token_count":5,
    "processed_text":"nice pic big happi famili"
  },
  {
    "label":4,
    "text":"mayb hug need",
    "cleaned_text":"mayb hug need",
    "normalized_text":"mayb hug need",
    "tokens":[
      "mayb",
      "hug",
      "need"
    ],
    "token_count":3,
    "processed_text":"mayb hug need"
  },
  {
    "label":4,
    "text":"saw etalk reppin canadian ladi big hug much love vanciti girl lt",
    "cleaned_text":"saw etalk reppin canadian ladi big hug much love vanciti girl lt",
    "normalized_text":"saw etalk reppin canadian ladi big hug much love vanciti girl lt",
    "tokens":[
      "saw",
      "etalk",
      "reppin",
      "canadian",
      "ladi",
      "big",
      "hug",
      "much",
      "love",
      "vanciti",
      "girl",
      "lt"
    ],
    "token_count":12,
    "processed_text":"saw etalk reppin canadian ladi big hug much love vanciti girl lt"
  },
  {
    "label":4,
    "text":"sick chill tonight watch desper housew",
    "cleaned_text":"sick chill tonight watch desper housew",
    "normalized_text":"sick chill tonight watch desper housew",
    "tokens":[
      "sick",
      "chill",
      "tonight",
      "watch",
      "desper",
      "housew"
    ],
    "token_count":6,
    "processed_text":"sick chill tonight watch desper housew"
  },
  {
    "label":0,
    "text":"back home disney realli sad",
    "cleaned_text":"back home disney realli sad",
    "normalized_text":"back home disney realli sad",
    "tokens":[
      "back",
      "home",
      "disney",
      "realli",
      "sad"
    ],
    "token_count":5,
    "processed_text":"back home disney realli sad"
  },
  {
    "label":0,
    "text":"feel home evertoo bad gonna short live",
    "cleaned_text":"feel home evertoo bad gonna short live",
    "normalized_text":"feel home evertoo bad gonna short live",
    "tokens":[
      "feel",
      "home",
      "evertoo",
      "bad",
      "gon",
      "na",
      "short",
      "live"
    ],
    "token_count":8,
    "processed_text":"feel home evertoo bad gon na short live"
  },
  {
    "label":0,
    "text":"bu ride hell gt put singl doubl nearli alway full got bu back well stupid arriva",
    "cleaned_text":"bu ride hell gt put singl doubl nearli alway full got bu back well stupid arriva",
    "normalized_text":"bu ride hell gt put singl doubl nearli alway full got bu back well stupid arriva",
    "tokens":[
      "bu",
      "ride",
      "hell",
      "gt",
      "put",
      "singl",
      "doubl",
      "nearli",
      "alway",
      "full",
      "got",
      "bu",
      "back",
      "well",
      "stupid",
      "arriva"
    ],
    "token_count":16,
    "processed_text":"bu ride hell gt put singl doubl nearli alway full got bu back well stupid arriva"
  },
  {
    "label":4,
    "text":"talk mike via phone call aww miss",
    "cleaned_text":"talk mike via phone call aww miss",
    "normalized_text":"talk mike via phone call aww miss",
    "tokens":[
      "talk",
      "mike",
      "via",
      "phone",
      "call",
      "aww",
      "miss"
    ],
    "token_count":7,
    "processed_text":"talk mike via phone call aww miss"
  },
  {
    "label":4,
    "text":"thnx that good look ur myspac",
    "cleaned_text":"thnx that good look ur myspac",
    "normalized_text":"thnx that good look ur myspac",
    "tokens":[
      "thnx",
      "good",
      "look",
      "ur",
      "myspac"
    ],
    "token_count":5,
    "processed_text":"thnx good look ur myspac"
  },
  {
    "label":4,
    "text":"chef chees toasti chip",
    "cleaned_text":"chef chees toasti chip",
    "normalized_text":"chef chees toasti chip",
    "tokens":[
      "chef",
      "chee",
      "toasti",
      "chip"
    ],
    "token_count":4,
    "processed_text":"chef chee toasti chip"
  },
  {
    "label":0,
    "text":"psomg afflict there cure cc addict",
    "cleaned_text":"psomg afflict there cure cc addict",
    "normalized_text":"psomg afflict there cure cc addict",
    "tokens":[
      "psomg",
      "afflict",
      "cure",
      "cc",
      "addict"
    ],
    "token_count":5,
    "processed_text":"psomg afflict cure cc addict"
  },
  {
    "label":4,
    "text":"yawn morn starshin",
    "cleaned_text":"yawn morn starshin",
    "normalized_text":"yawn morn starshin",
    "tokens":[
      "yawn",
      "morn",
      "starshin"
    ],
    "token_count":3,
    "processed_text":"yawn morn starshin"
  },
  {
    "label":4,
    "text":"oop comput fart let tri",
    "cleaned_text":"oop comput fart let tri",
    "normalized_text":"oop comput fart let tri",
    "tokens":[
      "oop",
      "comput",
      "fart",
      "let",
      "tri"
    ],
    "token_count":5,
    "processed_text":"oop comput fart let tri"
  },
  {
    "label":0,
    "text":"cant bring tri new vegemit",
    "cleaned_text":"cant bring tri new vegemit",
    "normalized_text":"cant bring tri new vegemit",
    "tokens":[
      "cant",
      "bring",
      "tri",
      "new",
      "vegemit"
    ],
    "token_count":5,
    "processed_text":"cant bring tri new vegemit"
  },
  {
    "label":0,
    "text":"sequenc seri serious suck",
    "cleaned_text":"sequenc seri serious suck",
    "normalized_text":"sequenc seri serious suck",
    "tokens":[
      "sequenc",
      "seri",
      "seriou",
      "suck"
    ],
    "token_count":4,
    "processed_text":"sequenc seri seriou suck"
  },
  {
    "label":0,
    "text":"crap raini new york",
    "cleaned_text":"crap raini new york",
    "normalized_text":"crap raini new york",
    "tokens":[
      "crap",
      "raini",
      "new",
      "york"
    ],
    "token_count":4,
    "processed_text":"crap raini new york"
  },
  {
    "label":4,
    "text":"aint nowt ti true",
    "cleaned_text":"aint nowt ti true",
    "normalized_text":"aint nowt ti true",
    "tokens":[
      "aint",
      "nowt",
      "ti",
      "true"
    ],
    "token_count":4,
    "processed_text":"aint nowt ti true"
  },
  {
    "label":4,
    "text":"agre mayb theyr mao bing",
    "cleaned_text":"agre mayb theyr mao bing",
    "normalized_text":"agre mayb theyr mao bing",
    "tokens":[
      "agr",
      "mayb",
      "theyr",
      "mao",
      "bing"
    ],
    "token_count":5,
    "processed_text":"agr mayb theyr mao bing"
  },
  {
    "label":4,
    "text":"got back marketmarket bought gb memori card psp alreadi",
    "cleaned_text":"got back marketmarket bought gb memori card psp alreadi",
    "normalized_text":"got back marketmarket bought gb memori card psp alreadi",
    "tokens":[
      "got",
      "back",
      "marketmarket",
      "bought",
      "gb",
      "memori",
      "card",
      "psp",
      "alreadi"
    ],
    "token_count":9,
    "processed_text":"got back marketmarket bought gb memori card psp alreadi"
  },
  {
    "label":0,
    "text":"wala eh manila flight oper manila base fa balita ko nga anjan na mga balikbayan",
    "cleaned_text":"wala eh manila flight oper manila base fa balita ko nga anjan na mga balikbayan",
    "normalized_text":"wala eh manila flight oper manila base fa balita ko nga anjan na mga balikbayan",
    "tokens":[
      "wala",
      "eh",
      "manila",
      "flight",
      "oper",
      "manila",
      "base",
      "fa",
      "balita",
      "ko",
      "nga",
      "anjan",
      "na",
      "mga",
      "balikbayan"
    ],
    "token_count":15,
    "processed_text":"wala eh manila flight oper manila base fa balita ko nga anjan na mga balikbayan"
  },
  {
    "label":4,
    "text":"happi easter orthodox friend spend day famili",
    "cleaned_text":"happi easter orthodox friend spend day famili",
    "normalized_text":"happi easter orthodox friend spend day famili",
    "tokens":[
      "happi",
      "easter",
      "orthodox",
      "friend",
      "spend",
      "day",
      "famili"
    ],
    "token_count":7,
    "processed_text":"happi easter orthodox friend spend day famili"
  },
  {
    "label":0,
    "text":"oh verri cool dont think like movi cuz kiss someon minut dont like",
    "cleaned_text":"oh verri cool dont think like movi cuz kiss someon minut dont like",
    "normalized_text":"oh verri cool dont think like movi cuz kiss someon minut dont like",
    "tokens":[
      "oh",
      "verri",
      "cool",
      "dont",
      "think",
      "like",
      "movi",
      "cuz",
      "kiss",
      "someon",
      "minut",
      "dont",
      "like"
    ],
    "token_count":13,
    "processed_text":"oh verri cool dont think like movi cuz kiss someon minut dont like"
  },
  {
    "label":0,
    "text":"damn ive sleep like hour afternoon im still tire studi tho",
    "cleaned_text":"damn ive sleep like hour afternoon im still tire studi tho",
    "normalized_text":"damn ive sleep like hour afternoon im still tire studi tho",
    "tokens":[
      "damn",
      "ive",
      "sleep",
      "like",
      "hour",
      "afternoon",
      "im",
      "still",
      "tire",
      "studi",
      "tho"
    ],
    "token_count":11,
    "processed_text":"damn ive sleep like hour afternoon im still tire studi tho"
  },
  {
    "label":4,
    "text":"thank deeeaaarrr",
    "cleaned_text":"thank deeeaaarrr",
    "normalized_text":"thank deeeaaarrr",
    "tokens":[
      "thank",
      "deeeaaarrr"
    ],
    "token_count":2,
    "processed_text":"thank deeeaaarrr"
  },
  {
    "label":0,
    "text":"lol that surpris well realli creepi man",
    "cleaned_text":"lol that surpris well realli creepi man",
    "normalized_text":"lol that surpris well realli creepi man",
    "tokens":[
      "lol",
      "surpri",
      "well",
      "realli",
      "creepi",
      "man"
    ],
    "token_count":6,
    "processed_text":"lol surpri well realli creepi man"
  },
  {
    "label":0,
    "text":"come your come new zealand tour jordyn",
    "cleaned_text":"come your come new zealand tour jordyn",
    "normalized_text":"come your come new zealand tour jordyn",
    "tokens":[
      "come",
      "come",
      "new",
      "zealand",
      "tour",
      "jordyn"
    ],
    "token_count":6,
    "processed_text":"come come new zealand tour jordyn"
  },
  {
    "label":0,
    "text":"your gonna make go alllll way store lol find oneil googl thank lol",
    "cleaned_text":"your gonna make go alllll way store lol find oneil googl thank lol",
    "normalized_text":"your gonna make go alllll way store lol find oneil googl thank lol",
    "tokens":[
      "gon",
      "na",
      "make",
      "go",
      "alllll",
      "way",
      "store",
      "lol",
      "find",
      "oneil",
      "googl",
      "thank",
      "lol"
    ],
    "token_count":13,
    "processed_text":"gon na make go alllll way store lol find oneil googl thank lol"
  },
  {
    "label":4,
    "text":"realli love interview",
    "cleaned_text":"realli love interview",
    "normalized_text":"realli love interview",
    "tokens":[
      "realli",
      "love",
      "interview"
    ],
    "token_count":3,
    "processed_text":"realli love interview"
  },
  {
    "label":0,
    "text":"tri wake check email go work rest world husband still bed",
    "cleaned_text":"tri wake check email go work rest world husband still bed",
    "normalized_text":"tri wake check email go work rest world husband still bed",
    "tokens":[
      "tri",
      "wake",
      "check",
      "email",
      "go",
      "work",
      "rest",
      "world",
      "husband",
      "still",
      "bed"
    ],
    "token_count":11,
    "processed_text":"tri wake check email go work rest world husband still bed"
  },
  {
    "label":0,
    "text":"that dayum shame nig dont know babi lorenz daydream crushin sincw yr old",
    "cleaned_text":"that dayum shame nig dont know babi lorenz daydream crushin sincw yr old",
    "normalized_text":"that dayum shame nig dont know babi lorenz daydream crushin sincw yr old",
    "tokens":[
      "dayum",
      "shame",
      "nig",
      "dont",
      "know",
      "babi",
      "lorenz",
      "daydream",
      "crushin",
      "sincw",
      "yr",
      "old"
    ],
    "token_count":12,
    "processed_text":"dayum shame nig dont know babi lorenz daydream crushin sincw yr old"
  },
  {
    "label":4,
    "text":"joe jackson realli go",
    "cleaned_text":"joe jackson realli go",
    "normalized_text":"joe jackson realli go",
    "tokens":[
      "joe",
      "jackson",
      "realli",
      "go"
    ],
    "token_count":4,
    "processed_text":"joe jackson realli go"
  },
  {
    "label":4,
    "text":"admit dont admir spice girl also hannah dont shi tell us",
    "cleaned_text":"admit dont admir spice girl also hannah dont shi tell us",
    "normalized_text":"admit dont admir spice girl also hannah dont shi tell us",
    "tokens":[
      "admit",
      "dont",
      "admir",
      "spice",
      "girl",
      "also",
      "hannah",
      "dont",
      "shi",
      "tell",
      "us"
    ],
    "token_count":11,
    "processed_text":"admit dont admir spice girl also hannah dont shi tell us"
  },
  {
    "label":4,
    "text":"got picnic go tomoz listen hello goodby jona brother",
    "cleaned_text":"got picnic go tomoz listen hello goodby jona brother",
    "normalized_text":"got picnic go tomoz listen hello goodby jona brother",
    "tokens":[
      "got",
      "picnic",
      "go",
      "tomoz",
      "listen",
      "hello",
      "goodbi",
      "jona",
      "brother"
    ],
    "token_count":9,
    "processed_text":"got picnic go tomoz listen hello goodbi jona brother"
  },
  {
    "label":0,
    "text":"hmm gonna aaaaa idea",
    "cleaned_text":"hmm gonna aaaaa idea",
    "normalized_text":"hmm gonna aaaaa idea",
    "tokens":[
      "hmm",
      "gon",
      "na",
      "aaaaa",
      "idea"
    ],
    "token_count":5,
    "processed_text":"hmm gon na aaaaa idea"
  },
  {
    "label":4,
    "text":"im gonna watch star trek imax probabl weekend want join",
    "cleaned_text":"im gonna watch star trek imax probabl weekend want join",
    "normalized_text":"im gonna watch star trek imax probabl weekend want join",
    "tokens":[
      "im",
      "gon",
      "na",
      "watch",
      "star",
      "trek",
      "imax",
      "probabl",
      "weekend",
      "want",
      "join"
    ],
    "token_count":11,
    "processed_text":"im gon na watch star trek imax probabl weekend want join"
  },
  {
    "label":4,
    "text":"guest mvaaforumscom",
    "cleaned_text":"guest mvaaforumscom",
    "normalized_text":"guest mvaaforumscom",
    "tokens":[
      "guest",
      "mvaaforumscom"
    ],
    "token_count":2,
    "processed_text":"guest mvaaforumscom"
  },
  {
    "label":0,
    "text":"food smell great gonna bring anjeliqu poor thing probabl starv",
    "cleaned_text":"food smell great gonna bring anjeliqu poor thing probabl starv",
    "normalized_text":"food smell great gonna bring anjeliqu poor thing probabl starv",
    "tokens":[
      "food",
      "smell",
      "great",
      "gon",
      "na",
      "bring",
      "anjeliqu",
      "poor",
      "thing",
      "probabl",
      "starv"
    ],
    "token_count":11,
    "processed_text":"food smell great gon na bring anjeliqu poor thing probabl starv"
  },
  {
    "label":0,
    "text":"tan like five minut weather decid hate",
    "cleaned_text":"tan like five minut weather decid hate",
    "normalized_text":"tan like five minut weather decid hate",
    "tokens":[
      "tan",
      "like",
      "five",
      "minut",
      "weather",
      "decid",
      "hate"
    ],
    "token_count":7,
    "processed_text":"tan like five minut weather decid hate"
  },
  {
    "label":0,
    "text":"miss wont tie shoe",
    "cleaned_text":"miss wont tie shoe",
    "normalized_text":"miss wont tie shoe",
    "tokens":[
      "miss",
      "wont",
      "tie",
      "shoe"
    ],
    "token_count":4,
    "processed_text":"miss wont tie shoe"
  },
  {
    "label":4,
    "text":"yeah thought sunk hole cool",
    "cleaned_text":"yeah thought sunk hole cool",
    "normalized_text":"yeah thought sunk hole cool",
    "tokens":[
      "yeah",
      "thought",
      "sunk",
      "hole",
      "cool"
    ],
    "token_count":5,
    "processed_text":"yeah thought sunk hole cool"
  },
  {
    "label":4,
    "text":"yeah gay could flog anyon guitar hero im pro",
    "cleaned_text":"yeah gay could flog anyon guitar hero im pro",
    "normalized_text":"yeah gay could flog anyon guitar hero im pro",
    "tokens":[
      "yeah",
      "gay",
      "flog",
      "anyon",
      "guitar",
      "hero",
      "im",
      "pro"
    ],
    "token_count":8,
    "processed_text":"yeah gay flog anyon guitar hero im pro"
  },
  {
    "label":0,
    "text":"cant wait sleepi time stuck citi though",
    "cleaned_text":"cant wait sleepi time stuck citi though",
    "normalized_text":"cant wait sleepi time stuck citi though",
    "tokens":[
      "cant",
      "wait",
      "sleepi",
      "time",
      "stuck",
      "citi",
      "though"
    ],
    "token_count":7,
    "processed_text":"cant wait sleepi time stuck citi though"
  },
  {
    "label":0,
    "text":"need voic",
    "cleaned_text":"need voic",
    "normalized_text":"need voic",
    "tokens":[
      "need",
      "voic"
    ],
    "token_count":2,
    "processed_text":"need voic"
  },
  {
    "label":4,
    "text":"real issu mine lol let grow fk alreadi man luvsalway firstnot anymor",
    "cleaned_text":"real issu mine lol let grow fk alreadi man luvsalway firstnot anymor",
    "normalized_text":"real issu mine lol let grow fk alreadi man luvsalway firstnot anymor",
    "tokens":[
      "real",
      "issu",
      "mine",
      "lol",
      "let",
      "grow",
      "fk",
      "alreadi",
      "man",
      "luvsalway",
      "firstnot",
      "anymor"
    ],
    "token_count":12,
    "processed_text":"real issu mine lol let grow fk alreadi man luvsalway firstnot anymor"
  },
  {
    "label":0,
    "text":"sad come home earli even sadder circumst",
    "cleaned_text":"sad come home earli even sadder circumst",
    "normalized_text":"sad come home earli even sadder circumst",
    "tokens":[
      "sad",
      "come",
      "home",
      "earli",
      "even",
      "sadder",
      "circumst"
    ],
    "token_count":7,
    "processed_text":"sad come home earli even sadder circumst"
  },
  {
    "label":0,
    "text":"caught fast unto death roadblock way calcutta phew im aliv workplac hr delay",
    "cleaned_text":"caught fast unto death roadblock way calcutta phew im aliv workplac hr delay",
    "normalized_text":"caught fast unto death roadblock way calcutta phew im aliv workplac hr delay",
    "tokens":[
      "caught",
      "fast",
      "unto",
      "death",
      "roadblock",
      "way",
      "calcutta",
      "phew",
      "im",
      "aliv",
      "workplac",
      "hr",
      "delay"
    ],
    "token_count":13,
    "processed_text":"caught fast unto death roadblock way calcutta phew im aliv workplac hr delay"
  },
  {
    "label":4,
    "text":"listen see fantasia he alway mind",
    "cleaned_text":"listen see fantasia he alway mind",
    "normalized_text":"listen see fantasia he alway mind",
    "tokens":[
      "listen",
      "see",
      "fantasia",
      "alway",
      "mind"
    ],
    "token_count":5,
    "processed_text":"listen see fantasia alway mind"
  },
  {
    "label":4,
    "text":"hell yeah trism excel remind hexic xbox thank recommend",
    "cleaned_text":"hell yeah trism excel remind hexic xbox thank recommend",
    "normalized_text":"hell yeah trism excel remind hexic xbox thank recommend",
    "tokens":[
      "hell",
      "yeah",
      "trism",
      "excel",
      "remind",
      "hexic",
      "xbox",
      "thank",
      "recommend"
    ],
    "token_count":9,
    "processed_text":"hell yeah trism excel remind hexic xbox thank recommend"
  },
  {
    "label":4,
    "text":"glad hear here wish stressfre week",
    "cleaned_text":"glad hear here wish stressfre week",
    "normalized_text":"glad hear here wish stressfre week",
    "tokens":[
      "glad",
      "hear",
      "wish",
      "stressfr",
      "week"
    ],
    "token_count":5,
    "processed_text":"glad hear wish stressfr week"
  },
  {
    "label":0,
    "text":"final home trip houston dont like hate didnt get talk chri day",
    "cleaned_text":"final home trip houston dont like hate didnt get talk chri day",
    "normalized_text":"final home trip houston dont like hate didnt get talk chri day",
    "tokens":[
      "final",
      "home",
      "trip",
      "houston",
      "dont",
      "like",
      "hate",
      "didnt",
      "get",
      "talk",
      "chri",
      "day"
    ],
    "token_count":12,
    "processed_text":"final home trip houston dont like hate didnt get talk chri day"
  },
  {
    "label":4,
    "text":"sent mine feel free end contest sinc mine winner",
    "cleaned_text":"sent mine feel free end contest sinc mine winner",
    "normalized_text":"sent mine feel free end contest sinc mine winner",
    "tokens":[
      "sent",
      "mine",
      "feel",
      "free",
      "end",
      "contest",
      "sinc",
      "mine",
      "winner"
    ],
    "token_count":9,
    "processed_text":"sent mine feel free end contest sinc mine winner"
  },
  {
    "label":0,
    "text":"think flu",
    "cleaned_text":"think flu",
    "normalized_text":"think flu",
    "tokens":[
      "think",
      "flu"
    ],
    "token_count":2,
    "processed_text":"think flu"
  },
  {
    "label":4,
    "text":"panel nice far interest one chanc hear sure put tinyo perspect",
    "cleaned_text":"panel nice far interest one chanc hear sure put tinyo perspect",
    "normalized_text":"panel nice far interest one chanc hear sure put tinyo perspect",
    "tokens":[
      "panel",
      "nice",
      "far",
      "interest",
      "one",
      "chanc",
      "hear",
      "sure",
      "put",
      "tinyo",
      "perspect"
    ],
    "token_count":11,
    "processed_text":"panel nice far interest one chanc hear sure put tinyo perspect"
  },
  {
    "label":0,
    "text":"ah your awesom lt u that definit someth ive want dobut fix machin lol",
    "cleaned_text":"ah your awesom lt u that definit someth ive want dobut fix machin lol",
    "normalized_text":"ah your awesom lt u that definit someth ive want dobut fix machin lol",
    "tokens":[
      "ah",
      "awesom",
      "lt",
      "definit",
      "someth",
      "ive",
      "want",
      "dobut",
      "fix",
      "machin",
      "lol"
    ],
    "token_count":11,
    "processed_text":"ah awesom lt definit someth ive want dobut fix machin lol"
  },
  {
    "label":4,
    "text":"drive home irvin mari class amaaaaaaaz guess wat soar mayb shoul give bodi break nawwww",
    "cleaned_text":"drive home irvin mari class amaaaaaaaz guess wat soar mayb shoul give bodi break nawwww",
    "normalized_text":"drive home irvin mari class amaaaaaaaz guess wat soar mayb shoul give bodi break nawwww",
    "tokens":[
      "drive",
      "home",
      "irvin",
      "mari",
      "class",
      "amaaaaaaaz",
      "guess",
      "wat",
      "soar",
      "mayb",
      "shoul",
      "give",
      "bodi",
      "break",
      "nawwww"
    ],
    "token_count":15,
    "processed_text":"drive home irvin mari class amaaaaaaaz guess wat soar mayb shoul give bodi break nawwww"
  },
  {
    "label":0,
    "text":"rain beach look someth els st augustin",
    "cleaned_text":"rain beach look someth els st augustin",
    "normalized_text":"rain beach look someth els st augustin",
    "tokens":[
      "rain",
      "beach",
      "look",
      "someth",
      "el",
      "st",
      "augustin"
    ],
    "token_count":7,
    "processed_text":"rain beach look someth el st augustin"
  },
  {
    "label":4,
    "text":"happi mother day sharon",
    "cleaned_text":"happi mother day sharon",
    "normalized_text":"happi mother day sharon",
    "tokens":[
      "happi",
      "mother",
      "day",
      "sharon"
    ],
    "token_count":4,
    "processed_text":"happi mother day sharon"
  },
  {
    "label":4,
    "text":"thumb",
    "cleaned_text":"thumb",
    "normalized_text":"thumb",
    "tokens":[
      "thumb"
    ],
    "token_count":1,
    "processed_text":"thumb"
  },
  {
    "label":0,
    "text":"feelin better yesterday still feelin sick",
    "cleaned_text":"feelin better yesterday still feelin sick",
    "normalized_text":"feelin better yesterday still feelin sick",
    "tokens":[
      "feelin",
      "better",
      "yesterday",
      "still",
      "feelin",
      "sick"
    ],
    "token_count":6,
    "processed_text":"feelin better yesterday still feelin sick"
  },
  {
    "label":0,
    "text":"call get sens time mess",
    "cleaned_text":"call get sens time mess",
    "normalized_text":"call get sens time mess",
    "tokens":[
      "call",
      "get",
      "sen",
      "time",
      "mess"
    ],
    "token_count":5,
    "processed_text":"call get sen time mess"
  },
  {
    "label":4,
    "text":"hahaha your joke right btw that offens creepi anyth lol dont think ud tweet bout xd",
    "cleaned_text":"hahaha your joke right btw that offens creepi anyth lol dont think ud tweet bout xd",
    "normalized_text":"hahaha your joke right btw that offens creepi anyth lol dont think ud tweet bout xd",
    "tokens":[
      "hahaha",
      "joke",
      "right",
      "btw",
      "offen",
      "creepi",
      "anyth",
      "lol",
      "dont",
      "think",
      "ud",
      "tweet",
      "bout",
      "xd"
    ],
    "token_count":14,
    "processed_text":"hahaha joke right btw offen creepi anyth lol dont think ud tweet bout xd"
  },
  {
    "label":0,
    "text":"well guess quotar exhaustedquot appropri",
    "cleaned_text":"well guess quotar exhaustedquot appropri",
    "normalized_text":"well guess quotar exhaustedquot appropri",
    "tokens":[
      "well",
      "guess",
      "quotar",
      "exhaustedquot",
      "appropri"
    ],
    "token_count":5,
    "processed_text":"well guess quotar exhaustedquot appropri"
  },
  {
    "label":4,
    "text":"finish episod tonight hope episod tomorrow soo",
    "cleaned_text":"finish episod tonight hope episod tomorrow soo",
    "normalized_text":"finish episod tonight hope episod tomorrow soo",
    "tokens":[
      "finish",
      "episod",
      "tonight",
      "hope",
      "episod",
      "tomorrow",
      "soo"
    ],
    "token_count":7,
    "processed_text":"finish episod tonight hope episod tomorrow soo"
  },
  {
    "label":0,
    "text":"cut hair alway sad mean theyr grow everi day mine miss babi stage gr day",
    "cleaned_text":"cut hair alway sad mean theyr grow everi day mine miss babi stage gr day",
    "normalized_text":"cut hair alway sad mean theyr grow everi day mine miss babi stage gr day",
    "tokens":[
      "cut",
      "hair",
      "alway",
      "sad",
      "mean",
      "theyr",
      "grow",
      "everi",
      "day",
      "mine",
      "miss",
      "babi",
      "stage",
      "gr",
      "day"
    ],
    "token_count":15,
    "processed_text":"cut hair alway sad mean theyr grow everi day mine miss babi stage gr day"
  },
  {
    "label":0,
    "text":"im sorri miss went sleep",
    "cleaned_text":"im sorri miss went sleep",
    "normalized_text":"im sorri miss went sleep",
    "tokens":[
      "im",
      "sorri",
      "miss",
      "went",
      "sleep"
    ],
    "token_count":5,
    "processed_text":"im sorri miss went sleep"
  },
  {
    "label":4,
    "text":"got two stamp paddi coyn irish club list good go nate jeezi",
    "cleaned_text":"got two stamp paddi coyn irish club list good go nate jeezi",
    "normalized_text":"got two stamp paddi coyn irish club list good go nate jeezi",
    "tokens":[
      "got",
      "two",
      "stamp",
      "paddi",
      "coyn",
      "irish",
      "club",
      "list",
      "good",
      "go",
      "nate",
      "jeezi"
    ],
    "token_count":12,
    "processed_text":"got two stamp paddi coyn irish club list good go nate jeezi"
  },
  {
    "label":0,
    "text":"none post morn feed click name scroll back see",
    "cleaned_text":"none post morn feed click name scroll back see",
    "normalized_text":"none post morn feed click name scroll back see",
    "tokens":[
      "none",
      "post",
      "morn",
      "feed",
      "click",
      "name",
      "scroll",
      "back",
      "see"
    ],
    "token_count":9,
    "processed_text":"none post morn feed click name scroll back see"
  },
  {
    "label":4,
    "text":"altern quotw tri frog learnquot",
    "cleaned_text":"altern quotw tri frog learnquot",
    "normalized_text":"altern quotw tri frog learnquot",
    "tokens":[
      "altern",
      "quotw",
      "tri",
      "frog",
      "learnquot"
    ],
    "token_count":5,
    "processed_text":"altern quotw tri frog learnquot"
  },
  {
    "label":4,
    "text":"right nsb archiv done dust ape account date freeland winner sort haha real work",
    "cleaned_text":"right nsb archiv done dust ape account date freeland winner sort haha real work",
    "normalized_text":"right nsb archiv done dust ape account date freeland winner sort haha real work",
    "tokens":[
      "right",
      "nsb",
      "archiv",
      "done",
      "dust",
      "ape",
      "account",
      "date",
      "freeland",
      "winner",
      "sort",
      "haha",
      "real",
      "work"
    ],
    "token_count":14,
    "processed_text":"right nsb archiv done dust ape account date freeland winner sort haha real work"
  },
  {
    "label":0,
    "text":"kal penn hero realli leav hous",
    "cleaned_text":"kal penn hero realli leav hous",
    "normalized_text":"kal penn hero realli leav hous",
    "tokens":[
      "kal",
      "penn",
      "hero",
      "realli",
      "leav",
      "hou"
    ],
    "token_count":6,
    "processed_text":"kal penn hero realli leav hou"
  },
  {
    "label":0,
    "text":"final watch evita understand hype bake snickerdoodl burnt arm ive fanci perman scar",
    "cleaned_text":"final watch evita understand hype bake snickerdoodl burnt arm ive fanci perman scar",
    "normalized_text":"final watch evita understand hype bake snickerdoodl burnt arm ive fanci perman scar",
    "tokens":[
      "final",
      "watch",
      "evita",
      "understand",
      "hype",
      "bake",
      "snickerdoodl",
      "burnt",
      "arm",
      "ive",
      "fanci",
      "perman",
      "scar"
    ],
    "token_count":13,
    "processed_text":"final watch evita understand hype bake snickerdoodl burnt arm ive fanci perman scar"
  },
  {
    "label":4,
    "text":"watch scrub",
    "cleaned_text":"watch scrub",
    "normalized_text":"watch scrub",
    "tokens":[
      "watch",
      "scrub"
    ],
    "token_count":2,
    "processed_text":"watch scrub"
  },
  {
    "label":4,
    "text":"well that stupid idea",
    "cleaned_text":"well that stupid idea",
    "normalized_text":"well that stupid idea",
    "tokens":[
      "well",
      "stupid",
      "idea"
    ],
    "token_count":3,
    "processed_text":"well stupid idea"
  },
  {
    "label":0,
    "text":"head hurt walkin round skipton find monster everyth close other expens",
    "cleaned_text":"head hurt walkin round skipton find monster everyth close other expens",
    "normalized_text":"head hurt walkin round skipton find monster everyth close other expens",
    "tokens":[
      "head",
      "hurt",
      "walkin",
      "round",
      "skipton",
      "find",
      "monster",
      "everyth",
      "close",
      "expen"
    ],
    "token_count":10,
    "processed_text":"head hurt walkin round skipton find monster everyth close expen"
  },
  {
    "label":0,
    "text":"tri set mobil updat isnt work damn virgin mobil",
    "cleaned_text":"tri set mobil updat isnt work damn virgin mobil",
    "normalized_text":"tri set mobil updat isnt work damn virgin mobil",
    "tokens":[
      "tri",
      "set",
      "mobil",
      "updat",
      "isnt",
      "work",
      "damn",
      "virgin",
      "mobil"
    ],
    "token_count":9,
    "processed_text":"tri set mobil updat isnt work damn virgin mobil"
  },
  {
    "label":0,
    "text":"studi french exam tomorrow soooo bore",
    "cleaned_text":"studi french exam tomorrow soooo bore",
    "normalized_text":"studi french exam tomorrow soooo bore",
    "tokens":[
      "studi",
      "french",
      "exam",
      "tomorrow",
      "soooo",
      "bore"
    ],
    "token_count":6,
    "processed_text":"studi french exam tomorrow soooo bore"
  },
  {
    "label":0,
    "text":"bf hate im sad",
    "cleaned_text":"bf hate im sad",
    "normalized_text":"bf hate im sad",
    "tokens":[
      "bf",
      "hate",
      "im",
      "sad"
    ],
    "token_count":4,
    "processed_text":"bf hate im sad"
  },
  {
    "label":0,
    "text":"orthodontist place go bad gag reflex",
    "cleaned_text":"orthodontist place go bad gag reflex",
    "normalized_text":"orthodontist place go bad gag reflex",
    "tokens":[
      "orthodontist",
      "place",
      "go",
      "bad",
      "gag",
      "reflex"
    ],
    "token_count":6,
    "processed_text":"orthodontist place go bad gag reflex"
  },
  {
    "label":0,
    "text":"hard march april toughest time school final yet march mad nba playoff",
    "cleaned_text":"hard march april toughest time school final yet march mad nba playoff",
    "normalized_text":"hard march april toughest time school final yet march mad nba playoff",
    "tokens":[
      "hard",
      "march",
      "april",
      "toughest",
      "time",
      "school",
      "final",
      "yet",
      "march",
      "mad",
      "nba",
      "playoff"
    ],
    "token_count":12,
    "processed_text":"hard march april toughest time school final yet march mad nba playoff"
  },
  {
    "label":4,
    "text":"great know stori twist wife hear",
    "cleaned_text":"great know stori twist wife hear",
    "normalized_text":"great know stori twist wife hear",
    "tokens":[
      "great",
      "know",
      "stori",
      "twist",
      "wife",
      "hear"
    ],
    "token_count":6,
    "processed_text":"great know stori twist wife hear"
  },
  {
    "label":0,
    "text":"realli want watch belong music video youtub",
    "cleaned_text":"realli want watch belong music video youtub",
    "normalized_text":"realli want watch belong music video youtub",
    "tokens":[
      "realli",
      "want",
      "watch",
      "belong",
      "music",
      "video",
      "youtub"
    ],
    "token_count":7,
    "processed_text":"realli want watch belong music video youtub"
  },
  {
    "label":4,
    "text":"look hottest man around",
    "cleaned_text":"look hottest man around",
    "normalized_text":"look hottest man around",
    "tokens":[
      "look",
      "hottest",
      "man",
      "around"
    ],
    "token_count":4,
    "processed_text":"look hottest man around"
  },
  {
    "label":4,
    "text":"pain kickingg aha late nap time anyon think",
    "cleaned_text":"pain kickingg aha late nap time anyon think",
    "normalized_text":"pain kickingg aha late nap time anyon think",
    "tokens":[
      "pain",
      "kickingg",
      "aha",
      "late",
      "nap",
      "time",
      "anyon",
      "think"
    ],
    "token_count":8,
    "processed_text":"pain kickingg aha late nap time anyon think"
  },
  {
    "label":4,
    "text":"want new book read take advic",
    "cleaned_text":"want new book read take advic",
    "normalized_text":"want new book read take advic",
    "tokens":[
      "want",
      "new",
      "book",
      "read",
      "take",
      "advic"
    ],
    "token_count":6,
    "processed_text":"want new book read take advic"
  },
  {
    "label":4,
    "text":"met like four korean way home ha ha",
    "cleaned_text":"met like four korean way home ha ha",
    "normalized_text":"met like four korean way home ha ha",
    "tokens":[
      "met",
      "like",
      "four",
      "korean",
      "way",
      "home",
      "ha",
      "ha"
    ],
    "token_count":8,
    "processed_text":"met like four korean way home ha ha"
  },
  {
    "label":0,
    "text":"bunch fun prom go back school",
    "cleaned_text":"bunch fun prom go back school",
    "normalized_text":"bunch fun prom go back school",
    "tokens":[
      "bunch",
      "fun",
      "prom",
      "go",
      "back",
      "school"
    ],
    "token_count":6,
    "processed_text":"bunch fun prom go back school"
  },
  {
    "label":0,
    "text":"still confus",
    "cleaned_text":"still confus",
    "normalized_text":"still confus",
    "tokens":[
      "still",
      "confu"
    ],
    "token_count":2,
    "processed_text":"still confu"
  },
  {
    "label":0,
    "text":"student appar sunday psst dont answer theyll get messag eventu",
    "cleaned_text":"student appar sunday psst dont answer theyll get messag eventu",
    "normalized_text":"student appar sunday psst dont answer theyll get messag eventu",
    "tokens":[
      "student",
      "appar",
      "sunday",
      "psst",
      "dont",
      "answer",
      "theyll",
      "get",
      "messag",
      "eventu"
    ],
    "token_count":10,
    "processed_text":"student appar sunday psst dont answer theyll get messag eventu"
  },
  {
    "label":0,
    "text":"well didnt shit",
    "cleaned_text":"well didnt shit",
    "normalized_text":"well didnt shit",
    "tokens":[
      "well",
      "didnt",
      "shit"
    ],
    "token_count":3,
    "processed_text":"well didnt shit"
  },
  {
    "label":0,
    "text":"think scare littl mous exploratorium hassl snail tri make move faster",
    "cleaned_text":"think scare littl mous exploratorium hassl snail tri make move faster",
    "normalized_text":"think scare littl mous exploratorium hassl snail tri make move faster",
    "tokens":[
      "think",
      "scare",
      "littl",
      "mou",
      "exploratorium",
      "hassl",
      "snail",
      "tri",
      "make",
      "move",
      "faster"
    ],
    "token_count":11,
    "processed_text":"think scare littl mou exploratorium hassl snail tri make move faster"
  },
  {
    "label":4,
    "text":"brandi babi shower",
    "cleaned_text":"brandi babi shower",
    "normalized_text":"brandi babi shower",
    "tokens":[
      "brandi",
      "babi",
      "shower"
    ],
    "token_count":3,
    "processed_text":"brandi babi shower"
  },
  {
    "label":4,
    "text":"lol fine dont worri",
    "cleaned_text":"lol fine dont worri",
    "normalized_text":"lol fine dont worri",
    "tokens":[
      "lol",
      "fine",
      "dont",
      "worri"
    ],
    "token_count":4,
    "processed_text":"lol fine dont worri"
  },
  {
    "label":0,
    "text":"shouldnt tri go hour without painkil feel good morn",
    "cleaned_text":"shouldnt tri go hour without painkil feel good morn",
    "normalized_text":"shouldnt tri go hour without painkil feel good morn",
    "tokens":[
      "shouldnt",
      "tri",
      "go",
      "hour",
      "without",
      "painkil",
      "feel",
      "good",
      "morn"
    ],
    "token_count":9,
    "processed_text":"shouldnt tri go hour without painkil feel good morn"
  },
  {
    "label":4,
    "text":"yep mother day well famili ignor celebr everi day lol",
    "cleaned_text":"yep mother day well famili ignor celebr everi day lol",
    "normalized_text":"yep mother day well famili ignor celebr everi day lol",
    "tokens":[
      "yep",
      "mother",
      "day",
      "well",
      "famili",
      "ignor",
      "celebr",
      "everi",
      "day",
      "lol"
    ],
    "token_count":10,
    "processed_text":"yep mother day well famili ignor celebr everi day lol"
  },
  {
    "label":4,
    "text":"hi found pco tire doctor la vega area",
    "cleaned_text":"hi found pco tire doctor la vega area",
    "normalized_text":"hi found pco tire doctor la vega area",
    "tokens":[
      "hi",
      "found",
      "pco",
      "tire",
      "doctor",
      "la",
      "vega",
      "area"
    ],
    "token_count":8,
    "processed_text":"hi found pco tire doctor la vega area"
  },
  {
    "label":0,
    "text":"blink heat work full blastth side face near radiat feel like repeatedli smack",
    "cleaned_text":"blink heat work full blastth side face near radiat feel like repeatedli smack",
    "normalized_text":"blink heat work full blastth side face near radiat feel like repeatedli smack",
    "tokens":[
      "blink",
      "heat",
      "work",
      "full",
      "blastth",
      "side",
      "face",
      "near",
      "radiat",
      "feel",
      "like",
      "repeatedli",
      "smack"
    ],
    "token_count":13,
    "processed_text":"blink heat work full blastth side face near radiat feel like repeatedli smack"
  },
  {
    "label":0,
    "text":"nice today",
    "cleaned_text":"nice today",
    "normalized_text":"nice today",
    "tokens":[
      "nice",
      "today"
    ],
    "token_count":2,
    "processed_text":"nice today"
  },
  {
    "label":4,
    "text":"day sun row dublin still cant get top car day",
    "cleaned_text":"day sun row dublin still cant get top car day",
    "normalized_text":"day sun row dublin still cant get top car day",
    "tokens":[
      "day",
      "sun",
      "row",
      "dublin",
      "still",
      "cant",
      "get",
      "top",
      "car",
      "day"
    ],
    "token_count":10,
    "processed_text":"day sun row dublin still cant get top car day"
  },
  {
    "label":4,
    "text":"follow she realli awesom",
    "cleaned_text":"follow she realli awesom",
    "normalized_text":"follow she realli awesom",
    "tokens":[
      "follow",
      "realli",
      "awesom"
    ],
    "token_count":3,
    "processed_text":"follow realli awesom"
  },
  {
    "label":4,
    "text":"summer class start monday week freedom go decemb yet",
    "cleaned_text":"summer class start monday week freedom go decemb yet",
    "normalized_text":"summer class start monday week freedom go decemb yet",
    "tokens":[
      "summer",
      "class",
      "start",
      "monday",
      "week",
      "freedom",
      "go",
      "decemb",
      "yet"
    ],
    "token_count":9,
    "processed_text":"summer class start monday week freedom go decemb yet"
  },
  {
    "label":4,
    "text":"thanksbroken",
    "cleaned_text":"thanksbroken",
    "normalized_text":"thanksbroken",
    "tokens":[
      "thanksbroken"
    ],
    "token_count":1,
    "processed_text":"thanksbroken"
  },
  {
    "label":4,
    "text":"yup guess mean relax day today rat",
    "cleaned_text":"yup guess mean relax day today rat",
    "normalized_text":"yup guess mean relax day today rat",
    "tokens":[
      "yup",
      "guess",
      "mean",
      "relax",
      "day",
      "today",
      "rat"
    ],
    "token_count":7,
    "processed_text":"yup guess mean relax day today rat"
  },
  {
    "label":0,
    "text":"ill fluey",
    "cleaned_text":"ill fluey",
    "normalized_text":"ill fluey",
    "tokens":[
      "ill",
      "fluey"
    ],
    "token_count":2,
    "processed_text":"ill fluey"
  },
  {
    "label":4,
    "text":"feel great practicemi stamina danc increas",
    "cleaned_text":"feel great practicemi stamina danc increas",
    "normalized_text":"feel great practicemi stamina danc increas",
    "tokens":[
      "feel",
      "great",
      "practicemi",
      "stamina",
      "danc",
      "increa"
    ],
    "token_count":6,
    "processed_text":"feel great practicemi stamina danc increa"
  },
  {
    "label":4,
    "text":"anoth late nite w fella haha creepi call afternoon weird happi weekend everyon",
    "cleaned_text":"anoth late nite w fella haha creepi call afternoon weird happi weekend everyon",
    "normalized_text":"anoth late nite w fella haha creepi call afternoon weird happi weekend everyon",
    "tokens":[
      "anoth",
      "late",
      "nite",
      "fella",
      "haha",
      "creepi",
      "call",
      "afternoon",
      "weird",
      "happi",
      "weekend",
      "everyon"
    ],
    "token_count":12,
    "processed_text":"anoth late nite fella haha creepi call afternoon weird happi weekend everyon"
  },
  {
    "label":4,
    "text":"today start new day",
    "cleaned_text":"today start new day",
    "normalized_text":"today start new day",
    "tokens":[
      "today",
      "start",
      "new",
      "day"
    ],
    "token_count":4,
    "processed_text":"today start new day"
  },
  {
    "label":4,
    "text":"lmfao comment miley haha",
    "cleaned_text":"lmfao comment miley haha",
    "normalized_text":"lmfao comment miley haha",
    "tokens":[
      "lmfao",
      "comment",
      "miley",
      "haha"
    ],
    "token_count":4,
    "processed_text":"lmfao comment miley haha"
  },
  {
    "label":0,
    "text":"im sorri guy ive shoot lake week phone tragic accid went bottom havasu",
    "cleaned_text":"im sorri guy ive shoot lake week phone tragic accid went bottom havasu",
    "normalized_text":"im sorri guy ive shoot lake week phone tragic accid went bottom havasu",
    "tokens":[
      "im",
      "sorri",
      "guy",
      "ive",
      "shoot",
      "lake",
      "week",
      "phone",
      "tragic",
      "accid",
      "went",
      "bottom",
      "havasu"
    ],
    "token_count":13,
    "processed_text":"im sorri guy ive shoot lake week phone tragic accid went bottom havasu"
  },
  {
    "label":0,
    "text":"feel lik ur trynna make us feel guilti",
    "cleaned_text":"feel lik ur trynna make us feel guilti",
    "normalized_text":"feel lik ur trynna make us feel guilti",
    "tokens":[
      "feel",
      "lik",
      "ur",
      "trynna",
      "make",
      "us",
      "feel",
      "guilti"
    ],
    "token_count":8,
    "processed_text":"feel lik ur trynna make us feel guilti"
  },
  {
    "label":4,
    "text":"lol im good thank travel somewher nice hope",
    "cleaned_text":"lol im good thank travel somewher nice hope",
    "normalized_text":"lol im good thank travel somewher nice hope",
    "tokens":[
      "lol",
      "im",
      "good",
      "thank",
      "travel",
      "somewh",
      "nice",
      "hope"
    ],
    "token_count":8,
    "processed_text":"lol im good thank travel somewh nice hope"
  },
  {
    "label":0,
    "text":"bububut mac mac",
    "cleaned_text":"bububut mac mac",
    "normalized_text":"bububut mac mac",
    "tokens":[
      "bububut",
      "mac",
      "mac"
    ],
    "token_count":3,
    "processed_text":"bububut mac mac"
  },
  {
    "label":4,
    "text":"peopl train tweet issu",
    "cleaned_text":"peopl train tweet issu",
    "normalized_text":"peopl train tweet issu",
    "tokens":[
      "peopl",
      "train",
      "tweet",
      "issu"
    ],
    "token_count":4,
    "processed_text":"peopl train tweet issu"
  },
  {
    "label":0,
    "text":"bangin sore head burst last nite",
    "cleaned_text":"bangin sore head burst last nite",
    "normalized_text":"bangin sore head burst last nite",
    "tokens":[
      "bangin",
      "sore",
      "head",
      "burst",
      "last",
      "nite"
    ],
    "token_count":6,
    "processed_text":"bangin sore head burst last nite"
  },
  {
    "label":0,
    "text":"would point night smoke bowl cave bedroom listen awesom music opeth tonight",
    "cleaned_text":"would point night smoke bowl cave bedroom listen awesom music opeth tonight",
    "normalized_text":"would point night smoke bowl cave bedroom listen awesom music opeth tonight",
    "tokens":[
      "point",
      "night",
      "smoke",
      "bowl",
      "cave",
      "bedroom",
      "listen",
      "awesom",
      "music",
      "opeth",
      "tonight"
    ],
    "token_count":11,
    "processed_text":"point night smoke bowl cave bedroom listen awesom music opeth tonight"
  },
  {
    "label":4,
    "text":"big bang theori half hour",
    "cleaned_text":"big bang theori half hour",
    "normalized_text":"big bang theori half hour",
    "tokens":[
      "big",
      "bang",
      "theori",
      "half",
      "hour"
    ],
    "token_count":5,
    "processed_text":"big bang theori half hour"
  },
  {
    "label":0,
    "text":"trey cinespac rt",
    "cleaned_text":"trey cinespac rt",
    "normalized_text":"trey cinespac rt",
    "tokens":[
      "trey",
      "cinespac",
      "rt"
    ],
    "token_count":3,
    "processed_text":"trey cinespac rt"
  },
  {
    "label":4,
    "text":"guess son pick dinner vietnames beef noodl soup one favorit food back graduat school amp still",
    "cleaned_text":"guess son pick dinner vietnames beef noodl soup one favorit food back graduat school amp still",
    "normalized_text":"guess son pick dinner vietnames beef noodl soup one favorit food back graduat school amp still",
    "tokens":[
      "guess",
      "son",
      "pick",
      "dinner",
      "vietnam",
      "beef",
      "noodl",
      "soup",
      "one",
      "favorit",
      "food",
      "back",
      "graduat",
      "school",
      "amp",
      "still"
    ],
    "token_count":16,
    "processed_text":"guess son pick dinner vietnam beef noodl soup one favorit food back graduat school amp still"
  },
  {
    "label":4,
    "text":"ckckck parti sista",
    "cleaned_text":"ckckck parti sista",
    "normalized_text":"ckckck parti sista",
    "tokens":[
      "ckckck",
      "parti",
      "sista"
    ],
    "token_count":3,
    "processed_text":"ckckck parti sista"
  },
  {
    "label":0,
    "text":"watch report gun violenc cant believ everi rd home countri own one danger scari",
    "cleaned_text":"watch report gun violenc cant believ everi rd home countri own one danger scari",
    "normalized_text":"watch report gun violenc cant believ everi rd home countri own one danger scari",
    "tokens":[
      "watch",
      "report",
      "gun",
      "violenc",
      "cant",
      "believ",
      "everi",
      "rd",
      "home",
      "countri",
      "one",
      "danger",
      "scari"
    ],
    "token_count":13,
    "processed_text":"watch report gun violenc cant believ everi rd home countri one danger scari"
  },
  {
    "label":4,
    "text":"pleas",
    "cleaned_text":"pleas",
    "normalized_text":"pleas",
    "tokens":[
      "plea"
    ],
    "token_count":1,
    "processed_text":"plea"
  },
  {
    "label":0,
    "text":"good work that itoh trynna find new job",
    "cleaned_text":"good work that itoh trynna find new job",
    "normalized_text":"good work that itoh trynna find new job",
    "tokens":[
      "good",
      "work",
      "itoh",
      "trynna",
      "find",
      "new",
      "job"
    ],
    "token_count":7,
    "processed_text":"good work itoh trynna find new job"
  },
  {
    "label":4,
    "text":"finish season skin amp uglyboy",
    "cleaned_text":"finish season skin amp uglyboy",
    "normalized_text":"finish season skin amp uglyboy",
    "tokens":[
      "finish",
      "season",
      "skin",
      "amp",
      "uglyboy"
    ],
    "token_count":5,
    "processed_text":"finish season skin amp uglyboy"
  },
  {
    "label":4,
    "text":"great job claud thibault video crew product arboresc",
    "cleaned_text":"great job claud thibault video crew product arboresc",
    "normalized_text":"great job claud thibault video crew product arboresc",
    "tokens":[
      "great",
      "job",
      "claud",
      "thibault",
      "video",
      "crew",
      "product",
      "arboresc"
    ],
    "token_count":8,
    "processed_text":"great job claud thibault video crew product arboresc"
  },
  {
    "label":0,
    "text":"kristen cavalari ball show like way need catch finnish book work pm",
    "cleaned_text":"kristen cavalari ball show like way need catch finnish book work pm",
    "normalized_text":"kristen cavalari ball show like way need catch finnish book work pm",
    "tokens":[
      "kristen",
      "cavalari",
      "ball",
      "show",
      "like",
      "way",
      "need",
      "catch",
      "finnish",
      "book",
      "work",
      "pm"
    ],
    "token_count":12,
    "processed_text":"kristen cavalari ball show like way need catch finnish book work pm"
  },
  {
    "label":0,
    "text":"thng chc cng c tp c tun",
    "cleaned_text":"thng chc cng c tp c tun",
    "normalized_text":"thng chc cng c tp c tun",
    "tokens":[
      "thng",
      "chc",
      "cng",
      "tp",
      "tun"
    ],
    "token_count":5,
    "processed_text":"thng chc cng tp tun"
  },
  {
    "label":4,
    "text":"truli love go coffe shop soon lunch town mayb bbq tonight",
    "cleaned_text":"truli love go coffe shop soon lunch town mayb bbq tonight",
    "normalized_text":"truli love go coffe shop soon lunch town mayb bbq tonight",
    "tokens":[
      "truli",
      "love",
      "go",
      "coff",
      "shop",
      "soon",
      "lunch",
      "town",
      "mayb",
      "bbq",
      "tonight"
    ],
    "token_count":11,
    "processed_text":"truli love go coff shop soon lunch town mayb bbq tonight"
  },
  {
    "label":0,
    "text":"gettin band promo done readi tomorrow gonna busi one",
    "cleaned_text":"gettin band promo done readi tomorrow gonna busi one",
    "normalized_text":"gettin band promo done readi tomorrow gonna busi one",
    "tokens":[
      "gettin",
      "band",
      "promo",
      "done",
      "readi",
      "tomorrow",
      "gon",
      "na",
      "busi",
      "one"
    ],
    "token_count":10,
    "processed_text":"gettin band promo done readi tomorrow gon na busi one"
  },
  {
    "label":0,
    "text":"sad faceii want yu come",
    "cleaned_text":"sad faceii want yu come",
    "normalized_text":"sad faceii want yu come",
    "tokens":[
      "sad",
      "faceii",
      "want",
      "yu",
      "come"
    ],
    "token_count":5,
    "processed_text":"sad faceii want yu come"
  },
  {
    "label":4,
    "text":"net see peopl vote bjp econ agenda street tell vote co taught lesson",
    "cleaned_text":"net see peopl vote bjp econ agenda street tell vote co taught lesson",
    "normalized_text":"net see peopl vote bjp econ agenda street tell vote co taught lesson",
    "tokens":[
      "net",
      "see",
      "peopl",
      "vote",
      "bjp",
      "econ",
      "agenda",
      "street",
      "tell",
      "vote",
      "co",
      "taught",
      "lesson"
    ],
    "token_count":13,
    "processed_text":"net see peopl vote bjp econ agenda street tell vote co taught lesson"
  },
  {
    "label":4,
    "text":"true true",
    "cleaned_text":"true true",
    "normalized_text":"true true",
    "tokens":[
      "true",
      "true"
    ],
    "token_count":2,
    "processed_text":"true true"
  },
  {
    "label":0,
    "text":"yard sale work",
    "cleaned_text":"yard sale work",
    "normalized_text":"yard sale work",
    "tokens":[
      "yard",
      "sale",
      "work"
    ],
    "token_count":3,
    "processed_text":"yard sale work"
  },
  {
    "label":0,
    "text":"birthday fail knew someth meant rememb return hope good one",
    "cleaned_text":"birthday fail knew someth meant rememb return hope good one",
    "normalized_text":"birthday fail knew someth meant rememb return hope good one",
    "tokens":[
      "birthday",
      "fail",
      "knew",
      "someth",
      "meant",
      "rememb",
      "return",
      "hope",
      "good",
      "one"
    ],
    "token_count":10,
    "processed_text":"birthday fail knew someth meant rememb return hope good one"
  },
  {
    "label":4,
    "text":"sunday school teacher ben make join fb thing farm lmfao dork",
    "cleaned_text":"sunday school teacher ben make join fb thing farm lmfao dork",
    "normalized_text":"sunday school teacher ben make join fb thing farm lmfao dork",
    "tokens":[
      "sunday",
      "school",
      "teacher",
      "ben",
      "make",
      "join",
      "fb",
      "thing",
      "farm",
      "lmfao",
      "dork"
    ],
    "token_count":11,
    "processed_text":"sunday school teacher ben make join fb thing farm lmfao dork"
  },
  {
    "label":0,
    "text":"miss pricess",
    "cleaned_text":"miss pricess",
    "normalized_text":"miss pricess",
    "tokens":[
      "miss",
      "pricess"
    ],
    "token_count":2,
    "processed_text":"miss pricess"
  },
  {
    "label":0,
    "text":"miss old defult photo chang",
    "cleaned_text":"miss old defult photo chang",
    "normalized_text":"miss old defult photo chang",
    "tokens":[
      "miss",
      "old",
      "defult",
      "photo",
      "chang"
    ],
    "token_count":5,
    "processed_text":"miss old defult photo chang"
  },
  {
    "label":0,
    "text":"stomach feel like use punch bag",
    "cleaned_text":"stomach feel like use punch bag",
    "normalized_text":"stomach feel like use punch bag",
    "tokens":[
      "stomach",
      "feel",
      "like",
      "use",
      "punch",
      "bag"
    ],
    "token_count":6,
    "processed_text":"stomach feel like use punch bag"
  },
  {
    "label":4,
    "text":"asiamagiccom launch within next hour",
    "cleaned_text":"asiamagiccom launch within next hour",
    "normalized_text":"asiamagiccom launch within next hour",
    "tokens":[
      "asiamagiccom",
      "launch",
      "within",
      "next",
      "hour"
    ],
    "token_count":5,
    "processed_text":"asiamagiccom launch within next hour"
  },
  {
    "label":4,
    "text":"im quacker duck orang retweet time reciev noth",
    "cleaned_text":"im quacker duck orang retweet time reciev noth",
    "normalized_text":"im quacker duck orang retweet time reciev noth",
    "tokens":[
      "im",
      "quacker",
      "duck",
      "orang",
      "retweet",
      "time",
      "reciev",
      "noth"
    ],
    "token_count":8,
    "processed_text":"im quacker duck orang retweet time reciev noth"
  },
  {
    "label":0,
    "text":"mann got car accid im whip less right damn u know henni shit big homi",
    "cleaned_text":"mann got car accid im whip less right damn u know henni shit big homi",
    "normalized_text":"mann got car accid im whip less right damn u know henni shit big homi",
    "tokens":[
      "mann",
      "got",
      "car",
      "accid",
      "im",
      "whip",
      "less",
      "right",
      "damn",
      "know",
      "henni",
      "shit",
      "big",
      "homi"
    ],
    "token_count":14,
    "processed_text":"mann got car accid im whip less right damn know henni shit big homi"
  },
  {
    "label":4,
    "text":"grate wise mentor life your best",
    "cleaned_text":"grate wise mentor life your best",
    "normalized_text":"grate wise mentor life your best",
    "tokens":[
      "grate",
      "wise",
      "mentor",
      "life",
      "best"
    ],
    "token_count":5,
    "processed_text":"grate wise mentor life best"
  },
  {
    "label":4,
    "text":"hello kamusta na syempr dito nagkamustahan facebook",
    "cleaned_text":"hello kamusta na syempr dito nagkamustahan facebook",
    "normalized_text":"hello kamusta na syempr dito nagkamustahan facebook",
    "tokens":[
      "hello",
      "kamusta",
      "na",
      "syempr",
      "dito",
      "nagkamustahan",
      "facebook"
    ],
    "token_count":7,
    "processed_text":"hello kamusta na syempr dito nagkamustahan facebook"
  },
  {
    "label":4,
    "text":"oh read wrong tweet tehe",
    "cleaned_text":"oh read wrong tweet tehe",
    "normalized_text":"oh read wrong tweet tehe",
    "tokens":[
      "oh",
      "read",
      "wrong",
      "tweet",
      "tehe"
    ],
    "token_count":5,
    "processed_text":"oh read wrong tweet tehe"
  },
  {
    "label":0,
    "text":"know suck",
    "cleaned_text":"know suck",
    "normalized_text":"know suck",
    "tokens":[
      "know",
      "suck"
    ],
    "token_count":2,
    "processed_text":"know suck"
  },
  {
    "label":0,
    "text":"long day",
    "cleaned_text":"long day",
    "normalized_text":"long day",
    "tokens":[
      "long",
      "day"
    ],
    "token_count":2,
    "processed_text":"long day"
  },
  {
    "label":4,
    "text":"mmmmmmmm hash egg sausag love mommi cook",
    "cleaned_text":"mmmmmmmm hash egg sausag love mommi cook",
    "normalized_text":"mmmmmmmm hash egg sausag love mommi cook",
    "tokens":[
      "mmmmmmmm",
      "hash",
      "egg",
      "sausag",
      "love",
      "mommi",
      "cook"
    ],
    "token_count":7,
    "processed_text":"mmmmmmmm hash egg sausag love mommi cook"
  },
  {
    "label":0,
    "text":"award ceremoni today took home big bundl gonna miss friend",
    "cleaned_text":"award ceremoni today took home big bundl gonna miss friend",
    "normalized_text":"award ceremoni today took home big bundl gonna miss friend",
    "tokens":[
      "award",
      "ceremoni",
      "today",
      "took",
      "home",
      "big",
      "bundl",
      "gon",
      "na",
      "miss",
      "friend"
    ],
    "token_count":11,
    "processed_text":"award ceremoni today took home big bundl gon na miss friend"
  },
  {
    "label":0,
    "text":"dont wanna go home yet feel like",
    "cleaned_text":"dont wanna go home yet feel like",
    "normalized_text":"dont wanna go home yet feel like",
    "tokens":[
      "dont",
      "wan",
      "na",
      "go",
      "home",
      "yet",
      "feel",
      "like"
    ],
    "token_count":8,
    "processed_text":"dont wan na go home yet feel like"
  },
  {
    "label":4,
    "text":"photo cool place wow",
    "cleaned_text":"photo cool place wow",
    "normalized_text":"photo cool place wow",
    "tokens":[
      "photo",
      "cool",
      "place",
      "wow"
    ],
    "token_count":4,
    "processed_text":"photo cool place wow"
  },
  {
    "label":4,
    "text":"arrog though",
    "cleaned_text":"arrog though",
    "normalized_text":"arrog though",
    "tokens":[
      "arrog",
      "though"
    ],
    "token_count":2,
    "processed_text":"arrog though"
  },
  {
    "label":4,
    "text":"hungri made us egg hahah mmmmm goooood",
    "cleaned_text":"hungri made us egg hahah mmmmm goooood",
    "normalized_text":"hungri made us egg hahah mmmmm goooood",
    "tokens":[
      "hungri",
      "made",
      "us",
      "egg",
      "hahah",
      "mmmmm",
      "goooood"
    ],
    "token_count":7,
    "processed_text":"hungri made us egg hahah mmmmm goooood"
  },
  {
    "label":0,
    "text":"lt nice day pool shame ts rain cat amp dog",
    "cleaned_text":"lt nice day pool shame ts rain cat amp dog",
    "normalized_text":"lt nice day pool shame ts rain cat amp dog",
    "tokens":[
      "lt",
      "nice",
      "day",
      "pool",
      "shame",
      "ts",
      "rain",
      "cat",
      "amp",
      "dog"
    ],
    "token_count":10,
    "processed_text":"lt nice day pool shame ts rain cat amp dog"
  },
  {
    "label":0,
    "text":"amaz friday alway cut short cu gotta get drunk ass work earli tonit",
    "cleaned_text":"amaz friday alway cut short cu gotta get drunk ass work earli tonit",
    "normalized_text":"amaz friday alway cut short cu gotta get drunk ass work earli tonit",
    "tokens":[
      "amaz",
      "friday",
      "alway",
      "cut",
      "short",
      "cu",
      "got",
      "ta",
      "get",
      "drunk",
      "ass",
      "work",
      "earli",
      "tonit"
    ],
    "token_count":14,
    "processed_text":"amaz friday alway cut short cu got ta get drunk ass work earli tonit"
  },
  {
    "label":0,
    "text":"good day jer move today take well id hope",
    "cleaned_text":"good day jer move today take well id hope",
    "normalized_text":"good day jer move today take well id hope",
    "tokens":[
      "good",
      "day",
      "jer",
      "move",
      "today",
      "take",
      "well",
      "id",
      "hope"
    ],
    "token_count":9,
    "processed_text":"good day jer move today take well id hope"
  },
  {
    "label":0,
    "text":"th oct wii long wait",
    "cleaned_text":"th oct wii long wait",
    "normalized_text":"th oct wii long wait",
    "tokens":[
      "th",
      "oct",
      "wii",
      "long",
      "wait"
    ],
    "token_count":5,
    "processed_text":"th oct wii long wait"
  },
  {
    "label":0,
    "text":"aw im jealou miss home wish could home cg right watch spanish tv gparent",
    "cleaned_text":"aw im jealou miss home wish could home cg right watch spanish tv gparent",
    "normalized_text":"aw im jealou miss home wish could home cg right watch spanish tv gparent",
    "tokens":[
      "aw",
      "im",
      "jealou",
      "miss",
      "home",
      "wish",
      "home",
      "cg",
      "right",
      "watch",
      "spanish",
      "tv",
      "gparent"
    ],
    "token_count":13,
    "processed_text":"aw im jealou miss home wish home cg right watch spanish tv gparent"
  },
  {
    "label":0,
    "text":"close son",
    "cleaned_text":"close son",
    "normalized_text":"close son",
    "tokens":[
      "close",
      "son"
    ],
    "token_count":2,
    "processed_text":"close son"
  },
  {
    "label":4,
    "text":"yeah tweet random lyric",
    "cleaned_text":"yeah tweet random lyric",
    "normalized_text":"yeah tweet random lyric",
    "tokens":[
      "yeah",
      "tweet",
      "random",
      "lyric"
    ],
    "token_count":4,
    "processed_text":"yeah tweet random lyric"
  },
  {
    "label":4,
    "text":"good woman",
    "cleaned_text":"good woman",
    "normalized_text":"good woman",
    "tokens":[
      "good",
      "woman"
    ],
    "token_count":2,
    "processed_text":"good woman"
  },
  {
    "label":4,
    "text":"cigarett campari ice back porch damn fine weatherdoesnt get much better",
    "cleaned_text":"cigarett campari ice back porch damn fine weatherdoesnt get much better",
    "normalized_text":"cigarett campari ice back porch damn fine weatherdoesnt get much better",
    "tokens":[
      "cigarett",
      "campari",
      "ice",
      "back",
      "porch",
      "damn",
      "fine",
      "weatherdoesnt",
      "get",
      "much",
      "better"
    ],
    "token_count":11,
    "processed_text":"cigarett campari ice back porch damn fine weatherdoesnt get much better"
  },
  {
    "label":0,
    "text":"also rehears tomorrow toni mom record though phew",
    "cleaned_text":"also rehears tomorrow toni mom record though phew",
    "normalized_text":"also rehears tomorrow toni mom record though phew",
    "tokens":[
      "also",
      "rehear",
      "tomorrow",
      "toni",
      "mom",
      "record",
      "though",
      "phew"
    ],
    "token_count":8,
    "processed_text":"also rehear tomorrow toni mom record though phew"
  },
  {
    "label":0,
    "text":"yeah she awesom good sleeper eat like mad poor boob gone cup cant find bra fit",
    "cleaned_text":"yeah she awesom good sleeper eat like mad poor boob gone cup cant find bra fit",
    "normalized_text":"yeah she awesom good sleeper eat like mad poor boob gone cup cant find bra fit",
    "tokens":[
      "yeah",
      "awesom",
      "good",
      "sleeper",
      "eat",
      "like",
      "mad",
      "poor",
      "boob",
      "gone",
      "cup",
      "cant",
      "find",
      "bra",
      "fit"
    ],
    "token_count":15,
    "processed_text":"yeah awesom good sleeper eat like mad poor boob gone cup cant find bra fit"
  },
  {
    "label":4,
    "text":"man start use twitpic",
    "cleaned_text":"man start use twitpic",
    "normalized_text":"man start use twitpic",
    "tokens":[
      "man",
      "start",
      "use",
      "twitpic"
    ],
    "token_count":4,
    "processed_text":"man start use twitpic"
  },
  {
    "label":0,
    "text":"bad money",
    "cleaned_text":"bad money",
    "normalized_text":"bad money",
    "tokens":[
      "bad",
      "money"
    ],
    "token_count":2,
    "processed_text":"bad money"
  },
  {
    "label":0,
    "text":"sunni sunday",
    "cleaned_text":"sunni sunday",
    "normalized_text":"sunni sunday",
    "tokens":[
      "sunni",
      "sunday"
    ],
    "token_count":2,
    "processed_text":"sunni sunday"
  },
  {
    "label":4,
    "text":"write song better",
    "cleaned_text":"write song better",
    "normalized_text":"write song better",
    "tokens":[
      "write",
      "song",
      "better"
    ],
    "token_count":3,
    "processed_text":"write song better"
  },
  {
    "label":4,
    "text":"set quotconnect power onquot flag quotedit settingsquot",
    "cleaned_text":"set quotconnect power onquot flag quotedit settingsquot",
    "normalized_text":"set quotconnect power onquot flag quotedit settingsquot",
    "tokens":[
      "set",
      "quotconnect",
      "power",
      "onquot",
      "flag",
      "quotedit",
      "settingsquot"
    ],
    "token_count":7,
    "processed_text":"set quotconnect power onquot flag quotedit settingsquot"
  },
  {
    "label":4,
    "text":"given sortof greek oh well twitter thought cool ill fine gcse xx",
    "cleaned_text":"given sortof greek oh well twitter thought cool ill fine gcse xx",
    "normalized_text":"given sortof greek oh well twitter thought cool ill fine gcse xx",
    "tokens":[
      "given",
      "sortof",
      "greek",
      "oh",
      "well",
      "twitter",
      "thought",
      "cool",
      "ill",
      "fine",
      "gcse",
      "xx"
    ],
    "token_count":12,
    "processed_text":"given sortof greek oh well twitter thought cool ill fine gcse xx"
  },
  {
    "label":0,
    "text":"sooo hot",
    "cleaned_text":"sooo hot",
    "normalized_text":"sooo hot",
    "tokens":[
      "sooo",
      "hot"
    ],
    "token_count":2,
    "processed_text":"sooo hot"
  },
  {
    "label":4,
    "text":"would realli like toweli towel thank",
    "cleaned_text":"would realli like toweli towel thank",
    "normalized_text":"would realli like toweli towel thank",
    "tokens":[
      "realli",
      "like",
      "tow",
      "towel",
      "thank"
    ],
    "token_count":5,
    "processed_text":"realli like tow towel thank"
  },
  {
    "label":0,
    "text":"feel way heath post",
    "cleaned_text":"feel way heath post",
    "normalized_text":"feel way heath post",
    "tokens":[
      "feel",
      "way",
      "heath",
      "post"
    ],
    "token_count":4,
    "processed_text":"feel way heath post"
  },
  {
    "label":0,
    "text":"bad news",
    "cleaned_text":"bad news",
    "normalized_text":"bad news",
    "tokens":[
      "bad",
      "news"
    ],
    "token_count":2,
    "processed_text":"bad news"
  },
  {
    "label":4,
    "text":"worth tackl good luck",
    "cleaned_text":"worth tackl good luck",
    "normalized_text":"worth tackl good luck",
    "tokens":[
      "worth",
      "tackl",
      "good",
      "luck"
    ],
    "token_count":4,
    "processed_text":"worth tackl good luck"
  },
  {
    "label":4,
    "text":"wow chictopia allow tweet",
    "cleaned_text":"wow chictopia allow tweet",
    "normalized_text":"wow chictopia allow tweet",
    "tokens":[
      "wow",
      "chictopia",
      "allow",
      "tweet"
    ],
    "token_count":4,
    "processed_text":"wow chictopia allow tweet"
  },
  {
    "label":4,
    "text":"rockin robintweettweet tweet coffe errand interview class rehears pretti much success joy",
    "cleaned_text":"rockin robintweettweet tweet coffe errand interview class rehears pretti much success joy",
    "normalized_text":"rockin robintweettweet tweet coffe errand interview class rehears pretti much success joy",
    "tokens":[
      "rockin",
      "robintweettweet",
      "tweet",
      "coff",
      "errand",
      "interview",
      "class",
      "rehear",
      "pretti",
      "much",
      "success",
      "joy"
    ],
    "token_count":12,
    "processed_text":"rockin robintweettweet tweet coff errand interview class rehear pretti much success joy"
  },
  {
    "label":0,
    "text":"start regret decis buy itouch cuz play",
    "cleaned_text":"start regret decis buy itouch cuz play",
    "normalized_text":"start regret decis buy itouch cuz play",
    "tokens":[
      "start",
      "regret",
      "deci",
      "buy",
      "itouch",
      "cuz",
      "play"
    ],
    "token_count":7,
    "processed_text":"start regret deci buy itouch cuz play"
  },
  {
    "label":4,
    "text":"comput crash im back b twittaddict",
    "cleaned_text":"comput crash im back b twittaddict",
    "normalized_text":"comput crash im back b twittaddict",
    "tokens":[
      "comput",
      "crash",
      "im",
      "back",
      "twittaddict"
    ],
    "token_count":5,
    "processed_text":"comput crash im back twittaddict"
  },
  {
    "label":0,
    "text":"drop phone today scratch",
    "cleaned_text":"drop phone today scratch",
    "normalized_text":"drop phone today scratch",
    "tokens":[
      "drop",
      "phone",
      "today",
      "scratch"
    ],
    "token_count":4,
    "processed_text":"drop phone today scratch"
  },
  {
    "label":0,
    "text":"sorri im repli fast tweetdeck",
    "cleaned_text":"sorri im repli fast tweetdeck",
    "normalized_text":"sorri im repli fast tweetdeck",
    "tokens":[
      "sorri",
      "im",
      "repli",
      "fast",
      "tweetdeck"
    ],
    "token_count":5,
    "processed_text":"sorri im repli fast tweetdeck"
  },
  {
    "label":0,
    "text":"strand arlington ga card cash ssteeev hurri",
    "cleaned_text":"strand arlington ga card cash ssteeev hurri",
    "normalized_text":"strand arlington ga card cash ssteeev hurri",
    "tokens":[
      "strand",
      "arlington",
      "ga",
      "card",
      "cash",
      "ssteeev",
      "hurri"
    ],
    "token_count":7,
    "processed_text":"strand arlington ga card cash ssteeev hurri"
  },
  {
    "label":4,
    "text":"yessss thank come hang least bit",
    "cleaned_text":"yessss thank come hang least bit",
    "normalized_text":"yessss thank come hang least bit",
    "tokens":[
      "yessss",
      "thank",
      "come",
      "hang",
      "least",
      "bit"
    ],
    "token_count":6,
    "processed_text":"yessss thank come hang least bit"
  },
  {
    "label":0,
    "text":"work late tomorrow much fun",
    "cleaned_text":"work late tomorrow much fun",
    "normalized_text":"work late tomorrow much fun",
    "tokens":[
      "work",
      "late",
      "tomorrow",
      "much",
      "fun"
    ],
    "token_count":5,
    "processed_text":"work late tomorrow much fun"
  },
  {
    "label":4,
    "text":"yarn ummprob anyth love recommend weight pattern ill go grab link ya",
    "cleaned_text":"yarn ummprob anyth love recommend weight pattern ill go grab link ya",
    "normalized_text":"yarn ummprob anyth love recommend weight pattern ill go grab link ya",
    "tokens":[
      "yarn",
      "ummprob",
      "anyth",
      "love",
      "recommend",
      "weight",
      "pattern",
      "ill",
      "go",
      "grab",
      "link",
      "ya"
    ],
    "token_count":12,
    "processed_text":"yarn ummprob anyth love recommend weight pattern ill go grab link ya"
  },
  {
    "label":4,
    "text":"mind boggl lol",
    "cleaned_text":"mind boggl lol",
    "normalized_text":"mind boggl lol",
    "tokens":[
      "mind",
      "boggl",
      "lol"
    ],
    "token_count":3,
    "processed_text":"mind boggl lol"
  },
  {
    "label":4,
    "text":"eeek good luck",
    "cleaned_text":"eeek good luck",
    "normalized_text":"eeek good luck",
    "tokens":[
      "eeek",
      "good",
      "luck"
    ],
    "token_count":3,
    "processed_text":"eeek good luck"
  },
  {
    "label":4,
    "text":"im go hm movi pm",
    "cleaned_text":"im go hm movi pm",
    "normalized_text":"im go hm movi pm",
    "tokens":[
      "im",
      "go",
      "hm",
      "movi",
      "pm"
    ],
    "token_count":5,
    "processed_text":"im go hm movi pm"
  },
  {
    "label":0,
    "text":"eu nem te vi eu li twitter l ma nem te achei",
    "cleaned_text":"eu nem te vi eu li twitter l ma nem te achei",
    "normalized_text":"eu nem te vi eu li twitter l ma nem te achei",
    "tokens":[
      "eu",
      "nem",
      "te",
      "vi",
      "eu",
      "li",
      "twitter",
      "nem",
      "te",
      "achei"
    ],
    "token_count":10,
    "processed_text":"eu nem te vi eu li twitter nem te achei"
  },
  {
    "label":4,
    "text":"removedor acetona",
    "cleaned_text":"removedor acetona",
    "normalized_text":"removedor acetona",
    "tokens":[
      "removedor",
      "acetona"
    ],
    "token_count":2,
    "processed_text":"removedor acetona"
  },
  {
    "label":0,
    "text":"church choir alway sound bad",
    "cleaned_text":"church choir alway sound bad",
    "normalized_text":"church choir alway sound bad",
    "tokens":[
      "church",
      "choir",
      "alway",
      "sound",
      "bad"
    ],
    "token_count":5,
    "processed_text":"church choir alway sound bad"
  },
  {
    "label":0,
    "text":"electr hous internet right",
    "cleaned_text":"electr hous internet right",
    "normalized_text":"electr hous internet right",
    "tokens":[
      "electr",
      "hou",
      "internet",
      "right"
    ],
    "token_count":4,
    "processed_text":"electr hou internet right"
  },
  {
    "label":0,
    "text":"love chuck tweet help eas pain abl join week hope next chuckmemonday",
    "cleaned_text":"love chuck tweet help eas pain abl join week hope next chuckmemonday",
    "normalized_text":"love chuck tweet help eas pain abl join week hope next chuckmemonday",
    "tokens":[
      "love",
      "chuck",
      "tweet",
      "help",
      "ea",
      "pain",
      "abl",
      "join",
      "week",
      "hope",
      "next",
      "chuckmemonday"
    ],
    "token_count":12,
    "processed_text":"love chuck tweet help ea pain abl join week hope next chuckmemonday"
  },
  {
    "label":4,
    "text":"nevah stop music stop music voic head take kid",
    "cleaned_text":"nevah stop music stop music voic head take kid",
    "normalized_text":"nevah stop music stop music voic head take kid",
    "tokens":[
      "nevah",
      "stop",
      "music",
      "stop",
      "music",
      "voic",
      "head",
      "take",
      "kid"
    ],
    "token_count":9,
    "processed_text":"nevah stop music stop music voic head take kid"
  },
  {
    "label":4,
    "text":"ure weird also prefer cold dark that emo love life well meyeah im weird",
    "cleaned_text":"ure weird also prefer cold dark that emo love life well meyeah im weird",
    "normalized_text":"ure weird also prefer cold dark that emo love life well meyeah im weird",
    "tokens":[
      "ure",
      "weird",
      "also",
      "prefer",
      "cold",
      "dark",
      "emo",
      "love",
      "life",
      "well",
      "meyeah",
      "im",
      "weird"
    ],
    "token_count":13,
    "processed_text":"ure weird also prefer cold dark emo love life well meyeah im weird"
  },
  {
    "label":0,
    "text":"feel like creep without vehicl make wanna go anywher",
    "cleaned_text":"feel like creep without vehicl make wanna go anywher",
    "normalized_text":"feel like creep without vehicl make wanna go anywher",
    "tokens":[
      "feel",
      "like",
      "creep",
      "without",
      "vehicl",
      "make",
      "wan",
      "na",
      "go",
      "anywh"
    ],
    "token_count":10,
    "processed_text":"feel like creep without vehicl make wan na go anywh"
  },
  {
    "label":4,
    "text":"collect babi school",
    "cleaned_text":"collect babi school",
    "normalized_text":"collect babi school",
    "tokens":[
      "collect",
      "babi",
      "school"
    ],
    "token_count":3,
    "processed_text":"collect babi school"
  },
  {
    "label":0,
    "text":"person want one break amp mend time ili ben park lt",
    "cleaned_text":"person want one break amp mend time ili ben park lt",
    "normalized_text":"person want one break amp mend time ili ben park lt",
    "tokens":[
      "person",
      "want",
      "one",
      "break",
      "amp",
      "mend",
      "time",
      "ili",
      "ben",
      "park",
      "lt"
    ],
    "token_count":11,
    "processed_text":"person want one break amp mend time ili ben park lt"
  },
  {
    "label":0,
    "text":"got eye shadow blown",
    "cleaned_text":"got eye shadow blown",
    "normalized_text":"got eye shadow blown",
    "tokens":[
      "got",
      "eye",
      "shadow",
      "blown"
    ],
    "token_count":4,
    "processed_text":"got eye shadow blown"
  },
  {
    "label":4,
    "text":"monday run fast",
    "cleaned_text":"monday run fast",
    "normalized_text":"monday run fast",
    "tokens":[
      "monday",
      "run",
      "fast"
    ],
    "token_count":3,
    "processed_text":"monday run fast"
  },
  {
    "label":0,
    "text":"ugh use n word damn tryn ta get rid cum guyz damn",
    "cleaned_text":"ugh use n word damn tryn ta get rid cum guyz damn",
    "normalized_text":"ugh use n word damn tryn ta get rid cum guyz damn",
    "tokens":[
      "ugh",
      "use",
      "word",
      "damn",
      "tryn",
      "ta",
      "get",
      "rid",
      "cum",
      "guyz",
      "damn"
    ],
    "token_count":11,
    "processed_text":"ugh use word damn tryn ta get rid cum guyz damn"
  },
  {
    "label":4,
    "text":"ohh ok anywayswhat ur favorit song jona brother mine sosparanoidlov bug burnin",
    "cleaned_text":"ohh ok anywayswhat ur favorit song jona brother mine sosparanoidlov bug burnin",
    "normalized_text":"ohh ok anywayswhat ur favorit song jona brother mine sosparanoidlov bug burnin",
    "tokens":[
      "ohh",
      "ok",
      "anywayswhat",
      "ur",
      "favorit",
      "song",
      "jona",
      "brother",
      "mine",
      "sosparanoidlov",
      "bug",
      "burnin"
    ],
    "token_count":12,
    "processed_text":"ohh ok anywayswhat ur favorit song jona brother mine sosparanoidlov bug burnin"
  },
  {
    "label":4,
    "text":"there anyth wrong w nice policeman sir",
    "cleaned_text":"there anyth wrong w nice policeman sir",
    "normalized_text":"there anyth wrong w nice policeman sir",
    "tokens":[
      "anyth",
      "wrong",
      "nice",
      "policeman",
      "sir"
    ],
    "token_count":5,
    "processed_text":"anyth wrong nice policeman sir"
  },
  {
    "label":0,
    "text":"wish wa sin citi steve",
    "cleaned_text":"wish wa sin citi steve",
    "normalized_text":"wish wa sin citi steve",
    "tokens":[
      "wish",
      "wa",
      "sin",
      "citi",
      "steve"
    ],
    "token_count":5,
    "processed_text":"wish wa sin citi steve"
  },
  {
    "label":4,
    "text":"got home anoth amaz night",
    "cleaned_text":"got home anoth amaz night",
    "normalized_text":"got home anoth amaz night",
    "tokens":[
      "got",
      "home",
      "anoth",
      "amaz",
      "night"
    ],
    "token_count":5,
    "processed_text":"got home anoth amaz night"
  },
  {
    "label":0,
    "text":"im punish naughti behaviour yearth sun burn",
    "cleaned_text":"im punish naughti behaviour yearth sun burn",
    "normalized_text":"im punish naughti behaviour yearth sun burn",
    "tokens":[
      "im",
      "punish",
      "naughti",
      "behaviour",
      "yearth",
      "sun",
      "burn"
    ],
    "token_count":7,
    "processed_text":"im punish naughti behaviour yearth sun burn"
  },
  {
    "label":4,
    "text":"im waitn come n june shld back musta fell n luv lmao",
    "cleaned_text":"im waitn come n june shld back musta fell n luv lmao",
    "normalized_text":"im waitn come n june shld back musta fell n luv lmao",
    "tokens":[
      "im",
      "waitn",
      "come",
      "june",
      "shld",
      "back",
      "musta",
      "fell",
      "luv",
      "lmao"
    ],
    "token_count":10,
    "processed_text":"im waitn come june shld back musta fell luv lmao"
  },
  {
    "label":4,
    "text":"ahhhh coffe sip coffe thank hon",
    "cleaned_text":"ahhhh coffe sip coffe thank hon",
    "normalized_text":"ahhhh coffe sip coffe thank hon",
    "tokens":[
      "ahhhh",
      "coff",
      "sip",
      "coff",
      "thank",
      "hon"
    ],
    "token_count":6,
    "processed_text":"ahhhh coff sip coff thank hon"
  },
  {
    "label":4,
    "text":"besid would love explod collar mucho rez xp co know collor wouldnt make differ",
    "cleaned_text":"besid would love explod collar mucho rez xp co know collor wouldnt make differ",
    "normalized_text":"besid would love explod collar mucho rez xp co know collor wouldnt make differ",
    "tokens":[
      "besid",
      "love",
      "explod",
      "collar",
      "mucho",
      "rez",
      "xp",
      "co",
      "know",
      "collor",
      "wouldnt",
      "make",
      "differ"
    ],
    "token_count":13,
    "processed_text":"besid love explod collar mucho rez xp co know collor wouldnt make differ"
  },
  {
    "label":0,
    "text":"got exam delight studi rest",
    "cleaned_text":"got exam delight studi rest",
    "normalized_text":"got exam delight studi rest",
    "tokens":[
      "got",
      "exam",
      "delight",
      "studi",
      "rest"
    ],
    "token_count":5,
    "processed_text":"got exam delight studi rest"
  },
  {
    "label":4,
    "text":"home heart heart madison",
    "cleaned_text":"home heart heart madison",
    "normalized_text":"home heart heart madison",
    "tokens":[
      "home",
      "heart",
      "heart",
      "madison"
    ],
    "token_count":4,
    "processed_text":"home heart heart madison"
  },
  {
    "label":4,
    "text":"lunch eat lemon muffin top notch",
    "cleaned_text":"lunch eat lemon muffin top notch",
    "normalized_text":"lunch eat lemon muffin top notch",
    "tokens":[
      "lunch",
      "eat",
      "lemon",
      "muffin",
      "top",
      "notch"
    ],
    "token_count":6,
    "processed_text":"lunch eat lemon muffin top notch"
  },
  {
    "label":4,
    "text":"inquir girl mind want know get shirt",
    "cleaned_text":"inquir girl mind want know get shirt",
    "normalized_text":"inquir girl mind want know get shirt",
    "tokens":[
      "inquir",
      "girl",
      "mind",
      "want",
      "know",
      "get",
      "shirt"
    ],
    "token_count":7,
    "processed_text":"inquir girl mind want know get shirt"
  },
  {
    "label":0,
    "text":"tire leav",
    "cleaned_text":"tire leav",
    "normalized_text":"tire leav",
    "tokens":[
      "tire",
      "leav"
    ],
    "token_count":2,
    "processed_text":"tire leav"
  },
  {
    "label":4,
    "text":"havent seen like yr that tweet speak month",
    "cleaned_text":"havent seen like yr that tweet speak month",
    "normalized_text":"havent seen like yr that tweet speak month",
    "tokens":[
      "havent",
      "seen",
      "like",
      "yr",
      "tweet",
      "speak",
      "month"
    ],
    "token_count":7,
    "processed_text":"havent seen like yr tweet speak month"
  },
  {
    "label":0,
    "text":"ummm next friday believ hol save link fav",
    "cleaned_text":"ummm next friday believ hol save link fav",
    "normalized_text":"ummm next friday believ hol save link fav",
    "tokens":[
      "ummm",
      "next",
      "friday",
      "believ",
      "hol",
      "save",
      "link",
      "fav"
    ],
    "token_count":8,
    "processed_text":"ummm next friday believ hol save link fav"
  },
  {
    "label":0,
    "text":"danger lazzara bandit wayttli date year bummer cuz danger cuteibet bandit fug",
    "cleaned_text":"danger lazzara bandit wayttli date year bummer cuz danger cuteibet bandit fug",
    "normalized_text":"danger lazzara bandit wayttli date year bummer cuz danger cuteibet bandit fug",
    "tokens":[
      "danger",
      "lazzara",
      "bandit",
      "wayttli",
      "date",
      "year",
      "bummer",
      "cuz",
      "danger",
      "cuteibet",
      "bandit",
      "fug"
    ],
    "token_count":12,
    "processed_text":"danger lazzara bandit wayttli date year bummer cuz danger cuteibet bandit fug"
  },
  {
    "label":0,
    "text":"see note morn smelli guy crawl bart",
    "cleaned_text":"see note morn smelli guy crawl bart",
    "normalized_text":"see note morn smelli guy crawl bart",
    "tokens":[
      "see",
      "note",
      "morn",
      "smelli",
      "guy",
      "crawl",
      "bart"
    ],
    "token_count":7,
    "processed_text":"see note morn smelli guy crawl bart"
  },
  {
    "label":4,
    "text":"yeah funni",
    "cleaned_text":"yeah funni",
    "normalized_text":"yeah funni",
    "tokens":[
      "yeah",
      "funni"
    ],
    "token_count":2,
    "processed_text":"yeah funni"
  },
  {
    "label":0,
    "text":"dont like pic app uber twitter doesnt allow comment made",
    "cleaned_text":"dont like pic app uber twitter doesnt allow comment made",
    "normalized_text":"dont like pic app uber twitter doesnt allow comment made",
    "tokens":[
      "dont",
      "like",
      "pic",
      "app",
      "uber",
      "twitter",
      "doesnt",
      "allow",
      "comment",
      "made"
    ],
    "token_count":10,
    "processed_text":"dont like pic app uber twitter doesnt allow comment made"
  },
  {
    "label":4,
    "text":"omg marle much closer marcia alic ever worri",
    "cleaned_text":"omg marle much closer marcia alic ever worri",
    "normalized_text":"omg marle much closer marcia alic ever worri",
    "tokens":[
      "omg",
      "marl",
      "much",
      "closer",
      "marcia",
      "alic",
      "ever",
      "worri"
    ],
    "token_count":8,
    "processed_text":"omg marl much closer marcia alic ever worri"
  },
  {
    "label":4,
    "text":"aw love babe",
    "cleaned_text":"aw love babe",
    "normalized_text":"aw love babe",
    "tokens":[
      "aw",
      "love",
      "babe"
    ],
    "token_count":3,
    "processed_text":"aw love babe"
  },
  {
    "label":4,
    "text":"beef chili wholegrain rice nice",
    "cleaned_text":"beef chili wholegrain rice nice",
    "normalized_text":"beef chili wholegrain rice nice",
    "tokens":[
      "beef",
      "chili",
      "wholegrain",
      "rice",
      "nice"
    ],
    "token_count":5,
    "processed_text":"beef chili wholegrain rice nice"
  },
  {
    "label":0,
    "text":"peopl much smarter im depress heh",
    "cleaned_text":"peopl much smarter im depress heh",
    "normalized_text":"peopl much smarter im depress heh",
    "tokens":[
      "peopl",
      "much",
      "smarter",
      "im",
      "depress",
      "heh"
    ],
    "token_count":6,
    "processed_text":"peopl much smarter im depress heh"
  },
  {
    "label":0,
    "text":"ashley sit librari read im read certain book school librari doesnt",
    "cleaned_text":"ashley sit librari read im read certain book school librari doesnt",
    "normalized_text":"ashley sit librari read im read certain book school librari doesnt",
    "tokens":[
      "ashley",
      "sit",
      "librari",
      "read",
      "im",
      "read",
      "certain",
      "book",
      "school",
      "librari",
      "doesnt"
    ],
    "token_count":11,
    "processed_text":"ashley sit librari read im read certain book school librari doesnt"
  },
  {
    "label":0,
    "text":"straub gone time move bud light drink light beer sometim shit giggl",
    "cleaned_text":"straub gone time move bud light drink light beer sometim shit giggl",
    "normalized_text":"straub gone time move bud light drink light beer sometim shit giggl",
    "tokens":[
      "straub",
      "gone",
      "time",
      "move",
      "bud",
      "light",
      "drink",
      "light",
      "beer",
      "sometim",
      "shit",
      "giggl"
    ],
    "token_count":12,
    "processed_text":"straub gone time move bud light drink light beer sometim shit giggl"
  },
  {
    "label":4,
    "text":"thank video morn coffe yet video enough jar awak good stuff",
    "cleaned_text":"thank video morn coffe yet video enough jar awak good stuff",
    "normalized_text":"thank video morn coffe yet video enough jar awak good stuff",
    "tokens":[
      "thank",
      "video",
      "morn",
      "coff",
      "yet",
      "video",
      "enough",
      "jar",
      "awak",
      "good",
      "stuff"
    ],
    "token_count":11,
    "processed_text":"thank video morn coff yet video enough jar awak good stuff"
  },
  {
    "label":0,
    "text":"cant go show work give birthday greet next vid doe im needi lol",
    "cleaned_text":"cant go show work give birthday greet next vid doe im needi lol",
    "normalized_text":"cant go show work give birthday greet next vid doe im needi lol",
    "tokens":[
      "cant",
      "go",
      "show",
      "work",
      "give",
      "birthday",
      "greet",
      "next",
      "vid",
      "doe",
      "im",
      "needi",
      "lol"
    ],
    "token_count":13,
    "processed_text":"cant go show work give birthday greet next vid doe im needi lol"
  },
  {
    "label":4,
    "text":"seem like car dayonc wait son",
    "cleaned_text":"seem like car dayonc wait son",
    "normalized_text":"seem like car dayonc wait son",
    "tokens":[
      "seem",
      "like",
      "car",
      "dayonc",
      "wait",
      "son"
    ],
    "token_count":6,
    "processed_text":"seem like car dayonc wait son"
  },
  {
    "label":0,
    "text":"mel think jerri katrina pa",
    "cleaned_text":"mel think jerri katrina pa",
    "normalized_text":"mel think jerri katrina pa",
    "tokens":[
      "mel",
      "think",
      "jerri",
      "katrina",
      "pa"
    ],
    "token_count":5,
    "processed_text":"mel think jerri katrina pa"
  },
  {
    "label":0,
    "text":"heard today number pv job open real compani wait pv market turn around hire",
    "cleaned_text":"heard today number pv job open real compani wait pv market turn around hire",
    "normalized_text":"heard today number pv job open real compani wait pv market turn around hire",
    "tokens":[
      "heard",
      "today",
      "number",
      "pv",
      "job",
      "open",
      "real",
      "compani",
      "wait",
      "pv",
      "market",
      "turn",
      "around",
      "hire"
    ],
    "token_count":14,
    "processed_text":"heard today number pv job open real compani wait pv market turn around hire"
  },
  {
    "label":4,
    "text":"reason want come also asylum bco sister live birmingham could done",
    "cleaned_text":"reason want come also asylum bco sister live birmingham could done",
    "normalized_text":"reason want come also asylum bco sister live birmingham could done",
    "tokens":[
      "reason",
      "want",
      "come",
      "also",
      "asylum",
      "bco",
      "sister",
      "live",
      "birmingham",
      "done"
    ],
    "token_count":10,
    "processed_text":"reason want come also asylum bco sister live birmingham done"
  },
  {
    "label":4,
    "text":"eh thank ahha where missi",
    "cleaned_text":"eh thank ahha where missi",
    "normalized_text":"eh thank ahha where missi",
    "tokens":[
      "eh",
      "thank",
      "ahha",
      "missi"
    ],
    "token_count":4,
    "processed_text":"eh thank ahha missi"
  },
  {
    "label":0,
    "text":"listen dalla show last night guess spite love fest damn vike",
    "cleaned_text":"listen dalla show last night guess spite love fest damn vike",
    "normalized_text":"listen dalla show last night guess spite love fest damn vike",
    "tokens":[
      "listen",
      "dalla",
      "show",
      "last",
      "night",
      "guess",
      "spite",
      "love",
      "fest",
      "damn",
      "vike"
    ],
    "token_count":11,
    "processed_text":"listen dalla show last night guess spite love fest damn vike"
  },
  {
    "label":4,
    "text":"saw star trek last night love ithav seen skit yetil look onlin",
    "cleaned_text":"saw star trek last night love ithav seen skit yetil look onlin",
    "normalized_text":"saw star trek last night love ithav seen skit yetil look onlin",
    "tokens":[
      "saw",
      "star",
      "trek",
      "last",
      "night",
      "love",
      "ithav",
      "seen",
      "skit",
      "yetil",
      "look",
      "onlin"
    ],
    "token_count":12,
    "processed_text":"saw star trek last night love ithav seen skit yetil look onlin"
  },
  {
    "label":4,
    "text":"tri decid finish novel version last screenplay start next screenplay decis decis",
    "cleaned_text":"tri decid finish novel version last screenplay start next screenplay decis decis",
    "normalized_text":"tri decid finish novel version last screenplay start next screenplay decis decis",
    "tokens":[
      "tri",
      "decid",
      "finish",
      "novel",
      "version",
      "last",
      "screenplay",
      "start",
      "next",
      "screenplay",
      "deci",
      "deci"
    ],
    "token_count":12,
    "processed_text":"tri decid finish novel version last screenplay start next screenplay deci deci"
  },
  {
    "label":0,
    "text":"take lose someth love know much love miss big bag tesco coooki",
    "cleaned_text":"take lose someth love know much love miss big bag tesco coooki",
    "normalized_text":"take lose someth love know much love miss big bag tesco coooki",
    "tokens":[
      "take",
      "lose",
      "someth",
      "love",
      "know",
      "much",
      "love",
      "miss",
      "big",
      "bag",
      "tesco",
      "coooki"
    ],
    "token_count":12,
    "processed_text":"take lose someth love know much love miss big bag tesco coooki"
  },
  {
    "label":0,
    "text":"oh yeah know there nowher go",
    "cleaned_text":"oh yeah know there nowher go",
    "normalized_text":"oh yeah know there nowher go",
    "tokens":[
      "oh",
      "yeah",
      "know",
      "nowher",
      "go"
    ],
    "token_count":5,
    "processed_text":"oh yeah know nowher go"
  },
  {
    "label":4,
    "text":"lol im sorri sob ive tri studi late keepon forget updat",
    "cleaned_text":"lol im sorri sob ive tri studi late keepon forget updat",
    "normalized_text":"lol im sorri sob ive tri studi late keepon forget updat",
    "tokens":[
      "lol",
      "im",
      "sorri",
      "sob",
      "ive",
      "tri",
      "studi",
      "late",
      "keepon",
      "forget",
      "updat"
    ],
    "token_count":11,
    "processed_text":"lol im sorri sob ive tri studi late keepon forget updat"
  },
  {
    "label":4,
    "text":"lol would fricken sleep kitteh couch realli want go",
    "cleaned_text":"lol would fricken sleep kitteh couch realli want go",
    "normalized_text":"lol would fricken sleep kitteh couch realli want go",
    "tokens":[
      "lol",
      "fricken",
      "sleep",
      "kitteh",
      "couch",
      "realli",
      "want",
      "go"
    ],
    "token_count":8,
    "processed_text":"lol fricken sleep kitteh couch realli want go"
  },
  {
    "label":0,
    "text":"realli said someth could come friend sorri",
    "cleaned_text":"realli said someth could come friend sorri",
    "normalized_text":"realli said someth could come friend sorri",
    "tokens":[
      "realli",
      "said",
      "someth",
      "come",
      "friend",
      "sorri"
    ],
    "token_count":6,
    "processed_text":"realli said someth come friend sorri"
  },
  {
    "label":4,
    "text":"check tweetstat ms stat im happi",
    "cleaned_text":"check tweetstat ms stat im happi",
    "normalized_text":"check tweetstat ms stat im happi",
    "tokens":[
      "check",
      "tweetstat",
      "ms",
      "stat",
      "im",
      "happi"
    ],
    "token_count":6,
    "processed_text":"check tweetstat ms stat im happi"
  },
  {
    "label":4,
    "text":"hi littl si welcom twitter real laugh",
    "cleaned_text":"hi littl si welcom twitter real laugh",
    "normalized_text":"hi littl si welcom twitter real laugh",
    "tokens":[
      "hi",
      "littl",
      "si",
      "welcom",
      "twitter",
      "real",
      "laugh"
    ],
    "token_count":7,
    "processed_text":"hi littl si welcom twitter real laugh"
  },
  {
    "label":0,
    "text":"hi chanc ticket th bday octob bham went april got back car got park ticket boooo",
    "cleaned_text":"hi chanc ticket th bday octob bham went april got back car got park ticket boooo",
    "normalized_text":"hi chanc ticket th bday octob bham went april got back car got park ticket boooo",
    "tokens":[
      "hi",
      "chanc",
      "ticket",
      "th",
      "bday",
      "octob",
      "bham",
      "went",
      "april",
      "got",
      "back",
      "car",
      "got",
      "park",
      "ticket",
      "boooo"
    ],
    "token_count":16,
    "processed_text":"hi chanc ticket th bday octob bham went april got back car got park ticket boooo"
  },
  {
    "label":4,
    "text":"whoo iv time actual get bebo",
    "cleaned_text":"whoo iv time actual get bebo",
    "normalized_text":"whoo iv time actual get bebo",
    "tokens":[
      "whoo",
      "iv",
      "time",
      "actual",
      "get",
      "bebo"
    ],
    "token_count":6,
    "processed_text":"whoo iv time actual get bebo"
  },
  {
    "label":0,
    "text":"miss dawson creek",
    "cleaned_text":"miss dawson creek",
    "normalized_text":"miss dawson creek",
    "tokens":[
      "miss",
      "dawson",
      "creek"
    ],
    "token_count":3,
    "processed_text":"miss dawson creek"
  },
  {
    "label":0,
    "text":"missssssss oh much",
    "cleaned_text":"missssssss oh much",
    "normalized_text":"missssssss oh much",
    "tokens":[
      "missssssss",
      "oh",
      "much"
    ],
    "token_count":3,
    "processed_text":"missssssss oh much"
  },
  {
    "label":4,
    "text":"love twitter",
    "cleaned_text":"love twitter",
    "normalized_text":"love twitter",
    "tokens":[
      "love",
      "twitter"
    ],
    "token_count":2,
    "processed_text":"love twitter"
  },
  {
    "label":0,
    "text":"nigga wanna go homeaint home day",
    "cleaned_text":"nigga wanna go homeaint home day",
    "normalized_text":"nigga wanna go homeaint home day",
    "tokens":[
      "nigga",
      "wan",
      "na",
      "go",
      "homeaint",
      "home",
      "day"
    ],
    "token_count":7,
    "processed_text":"nigga wan na go homeaint home day"
  },
  {
    "label":4,
    "text":"rememb point anyth could neg connot context oh joy",
    "cleaned_text":"rememb point anyth could neg connot context oh joy",
    "normalized_text":"rememb point anyth could neg connot context oh joy",
    "tokens":[
      "rememb",
      "point",
      "anyth",
      "neg",
      "connot",
      "context",
      "oh",
      "joy"
    ],
    "token_count":8,
    "processed_text":"rememb point anyth neg connot context oh joy"
  },
  {
    "label":4,
    "text":"parabn criatura fofa ps anot",
    "cleaned_text":"parabn criatura fofa ps anot",
    "normalized_text":"parabn criatura fofa ps anot",
    "tokens":[
      "parabn",
      "criatura",
      "fofa",
      "ps",
      "anot"
    ],
    "token_count":5,
    "processed_text":"parabn criatura fofa ps anot"
  },
  {
    "label":4,
    "text":"machin haha fun",
    "cleaned_text":"machin haha fun",
    "normalized_text":"machin haha fun",
    "tokens":[
      "machin",
      "haha",
      "fun"
    ],
    "token_count":3,
    "processed_text":"machin haha fun"
  },
  {
    "label":0,
    "text":"suck peni",
    "cleaned_text":"suck peni",
    "normalized_text":"suck peni",
    "tokens":[
      "suck",
      "peni"
    ],
    "token_count":2,
    "processed_text":"suck peni"
  },
  {
    "label":0,
    "text":"get readi anoth day work",
    "cleaned_text":"get readi anoth day work",
    "normalized_text":"get readi anoth day work",
    "tokens":[
      "get",
      "readi",
      "anoth",
      "day",
      "work"
    ],
    "token_count":5,
    "processed_text":"get readi anoth day work"
  },
  {
    "label":0,
    "text":"stomach hurt shouldnt ate gyro sandwich last night",
    "cleaned_text":"stomach hurt shouldnt ate gyro sandwich last night",
    "normalized_text":"stomach hurt shouldnt ate gyro sandwich last night",
    "tokens":[
      "stomach",
      "hurt",
      "shouldnt",
      "ate",
      "gyro",
      "sandwich",
      "last",
      "night"
    ],
    "token_count":8,
    "processed_text":"stomach hurt shouldnt ate gyro sandwich last night"
  },
  {
    "label":4,
    "text":"stone fiyaaaah grill dinner yum onto irvin inc meet parti",
    "cleaned_text":"stone fiyaaaah grill dinner yum onto irvin inc meet parti",
    "normalized_text":"stone fiyaaaah grill dinner yum onto irvin inc meet parti",
    "tokens":[
      "stone",
      "fiyaaaah",
      "grill",
      "dinner",
      "yum",
      "onto",
      "irvin",
      "inc",
      "meet",
      "parti"
    ],
    "token_count":10,
    "processed_text":"stone fiyaaaah grill dinner yum onto irvin inc meet parti"
  },
  {
    "label":0,
    "text":"love like crazi time could know babi iloveyousincewewereinthestgradeeee",
    "cleaned_text":"love like crazi time could know babi iloveyousincewewereinthestgradeeee",
    "normalized_text":"love like crazi time could know babi iloveyousincewewereinthestgradeeee",
    "tokens":[
      "love",
      "like",
      "crazi",
      "time",
      "know",
      "babi"
    ],
    "token_count":6,
    "processed_text":"love like crazi time know babi"
  },
  {
    "label":4,
    "text":"might call nite",
    "cleaned_text":"might call nite",
    "normalized_text":"might call nite",
    "tokens":[
      "call",
      "nite"
    ],
    "token_count":2,
    "processed_text":"call nite"
  },
  {
    "label":4,
    "text":"watch tv wait parent get home",
    "cleaned_text":"watch tv wait parent get home",
    "normalized_text":"watch tv wait parent get home",
    "tokens":[
      "watch",
      "tv",
      "wait",
      "parent",
      "get",
      "home"
    ],
    "token_count":6,
    "processed_text":"watch tv wait parent get home"
  },
  {
    "label":0,
    "text":"phonee withh quan quan miss muchh",
    "cleaned_text":"phonee withh quan quan miss muchh",
    "normalized_text":"phonee withh quan quan miss muchh",
    "tokens":[
      "phone",
      "withh",
      "quan",
      "quan",
      "miss",
      "muchh"
    ],
    "token_count":6,
    "processed_text":"phone withh quan quan miss muchh"
  },
  {
    "label":4,
    "text":"le master state obviou",
    "cleaned_text":"le master state obviou",
    "normalized_text":"le master state obviou",
    "tokens":[
      "le",
      "master",
      "state",
      "obviou"
    ],
    "token_count":4,
    "processed_text":"le master state obviou"
  },
  {
    "label":4,
    "text":"youtub actuallya lot young talent",
    "cleaned_text":"youtub actuallya lot young talent",
    "normalized_text":"youtub actuallya lot young talent",
    "tokens":[
      "youtub",
      "actuallya",
      "lot",
      "young",
      "talent"
    ],
    "token_count":5,
    "processed_text":"youtub actuallya lot young talent"
  },
  {
    "label":4,
    "text":"yay busi good",
    "cleaned_text":"yay busi good",
    "normalized_text":"yay busi good",
    "tokens":[
      "yay",
      "busi",
      "good"
    ],
    "token_count":3,
    "processed_text":"yay busi good"
  },
  {
    "label":0,
    "text":"learn ill procedur train futur sinc tomorrow last day nekl",
    "cleaned_text":"learn ill procedur train futur sinc tomorrow last day nekl",
    "normalized_text":"learn ill procedur train futur sinc tomorrow last day nekl",
    "tokens":[
      "learn",
      "ill",
      "procedur",
      "train",
      "futur",
      "sinc",
      "tomorrow",
      "last",
      "day",
      "nekl"
    ],
    "token_count":10,
    "processed_text":"learn ill procedur train futur sinc tomorrow last day nekl"
  },
  {
    "label":4,
    "text":"love go church night realli play quotnot morn person sidequot",
    "cleaned_text":"love go church night realli play quotnot morn person sidequot",
    "normalized_text":"love go church night realli play quotnot morn person sidequot",
    "tokens":[
      "love",
      "go",
      "church",
      "night",
      "realli",
      "play",
      "quotnot",
      "morn",
      "person",
      "sidequot"
    ],
    "token_count":10,
    "processed_text":"love go church night realli play quotnot morn person sidequot"
  },
  {
    "label":4,
    "text":"realli think old bike punish dare buy new one",
    "cleaned_text":"realli think old bike punish dare buy new one",
    "normalized_text":"realli think old bike punish dare buy new one",
    "tokens":[
      "realli",
      "think",
      "old",
      "bike",
      "punish",
      "dare",
      "buy",
      "new",
      "one"
    ],
    "token_count":9,
    "processed_text":"realli think old bike punish dare buy new one"
  },
  {
    "label":4,
    "text":"knowh look hoth go run like kellan last week",
    "cleaned_text":"knowh look hoth go run like kellan last week",
    "normalized_text":"knowh look hoth go run like kellan last week",
    "tokens":[
      "knowh",
      "look",
      "hoth",
      "go",
      "run",
      "like",
      "kellan",
      "last",
      "week"
    ],
    "token_count":9,
    "processed_text":"knowh look hoth go run like kellan last week"
  },
  {
    "label":0,
    "text":"current shop want good free one doesnt exist",
    "cleaned_text":"current shop want good free one doesnt exist",
    "normalized_text":"current shop want good free one doesnt exist",
    "tokens":[
      "current",
      "shop",
      "want",
      "good",
      "free",
      "one",
      "doesnt",
      "exist"
    ],
    "token_count":8,
    "processed_text":"current shop want good free one doesnt exist"
  },
  {
    "label":0,
    "text":"wish could sleep home stuck work",
    "cleaned_text":"wish could sleep home stuck work",
    "normalized_text":"wish could sleep home stuck work",
    "tokens":[
      "wish",
      "sleep",
      "home",
      "stuck",
      "work"
    ],
    "token_count":5,
    "processed_text":"wish sleep home stuck work"
  },
  {
    "label":0,
    "text":"babi feelinn good",
    "cleaned_text":"babi feelinn good",
    "normalized_text":"babi feelinn good",
    "tokens":[
      "babi",
      "feelinn",
      "good"
    ],
    "token_count":3,
    "processed_text":"babi feelinn good"
  },
  {
    "label":0,
    "text":"home feel like equival pile pooopp malad txt jaiii taim beaucoup",
    "cleaned_text":"home feel like equival pile pooopp malad txt jaiii taim beaucoup",
    "normalized_text":"home feel like equival pile pooopp malad txt jaiii taim beaucoup",
    "tokens":[
      "home",
      "feel",
      "like",
      "equiv",
      "pile",
      "pooopp",
      "malad",
      "txt",
      "jaiii",
      "taim",
      "beaucoup"
    ],
    "token_count":11,
    "processed_text":"home feel like equiv pile pooopp malad txt jaiii taim beaucoup"
  },
  {
    "label":4,
    "text":"hella readi raider nfl season",
    "cleaned_text":"hella readi raider nfl season",
    "normalized_text":"hella readi raider nfl season",
    "tokens":[
      "hella",
      "readi",
      "raider",
      "nfl",
      "season"
    ],
    "token_count":5,
    "processed_text":"hella readi raider nfl season"
  },
  {
    "label":0,
    "text":"hate fact dat she becom closer",
    "cleaned_text":"hate fact dat she becom closer",
    "normalized_text":"hate fact dat she becom closer",
    "tokens":[
      "hate",
      "fact",
      "dat",
      "becom",
      "closer"
    ],
    "token_count":5,
    "processed_text":"hate fact dat becom closer"
  },
  {
    "label":0,
    "text":"sad see billi joel kati get divorc arti lang ban hbo sport funni",
    "cleaned_text":"sad see billi joel kati get divorc arti lang ban hbo sport funni",
    "normalized_text":"sad see billi joel kati get divorc arti lang ban hbo sport funni",
    "tokens":[
      "sad",
      "see",
      "billi",
      "joel",
      "kati",
      "get",
      "divorc",
      "arti",
      "lang",
      "ban",
      "hbo",
      "sport",
      "funni"
    ],
    "token_count":13,
    "processed_text":"sad see billi joel kati get divorc arti lang ban hbo sport funni"
  },
  {
    "label":4,
    "text":"beauti",
    "cleaned_text":"beauti",
    "normalized_text":"beauti",
    "tokens":[
      "beauti"
    ],
    "token_count":1,
    "processed_text":"beauti"
  },
  {
    "label":4,
    "text":"oh well agre disagre role thought perfect represent quotnetworkquot",
    "cleaned_text":"oh well agre disagre role thought perfect represent quotnetworkquot",
    "normalized_text":"oh well agre disagre role thought perfect represent quotnetworkquot",
    "tokens":[
      "oh",
      "well",
      "agr",
      "disagr",
      "role",
      "thought",
      "perfect",
      "repres",
      "quotnetworkquot"
    ],
    "token_count":9,
    "processed_text":"oh well agr disagr role thought perfect repres quotnetworkquot"
  },
  {
    "label":4,
    "text":"good night big day tomorrowstag rehears danc recit pm miss madison",
    "cleaned_text":"good night big day tomorrowstag rehears danc recit pm miss madison",
    "normalized_text":"good night big day tomorrowstag rehears danc recit pm miss madison",
    "tokens":[
      "good",
      "night",
      "big",
      "day",
      "tomorrowstag",
      "rehear",
      "danc",
      "recit",
      "pm",
      "miss",
      "madison"
    ],
    "token_count":11,
    "processed_text":"good night big day tomorrowstag rehear danc recit pm miss madison"
  },
  {
    "label":4,
    "text":"final home ampamp rain gloomi day guess ill ju sit home ampamp watch movi ampamp eat popcorn dayi",
    "cleaned_text":"final home ampamp rain gloomi day guess ill ju sit home ampamp watch movi ampamp eat popcorn dayi",
    "normalized_text":"final home ampamp rain gloomi day guess ill ju sit home ampamp watch movi ampamp eat popcorn dayi",
    "tokens":[
      "final",
      "home",
      "ampamp",
      "rain",
      "gloomi",
      "day",
      "guess",
      "ill",
      "ju",
      "sit",
      "home",
      "ampamp",
      "watch",
      "movi",
      "ampamp",
      "eat",
      "popcorn",
      "dayi"
    ],
    "token_count":18,
    "processed_text":"final home ampamp rain gloomi day guess ill ju sit home ampamp watch movi ampamp eat popcorn dayi"
  },
  {
    "label":0,
    "text":"lost two subscrib day ruin",
    "cleaned_text":"lost two subscrib day ruin",
    "normalized_text":"lost two subscrib day ruin",
    "tokens":[
      "lost",
      "two",
      "subscrib",
      "day",
      "ruin"
    ],
    "token_count":5,
    "processed_text":"lost two subscrib day ruin"
  },
  {
    "label":0,
    "text":"sunday jee day",
    "cleaned_text":"sunday jee day",
    "normalized_text":"sunday jee day",
    "tokens":[
      "sunday",
      "jee",
      "day"
    ],
    "token_count":3,
    "processed_text":"sunday jee day"
  },
  {
    "label":0,
    "text":"eat chocol bell fleur damn bought bigger block",
    "cleaned_text":"eat chocol bell fleur damn bought bigger block",
    "normalized_text":"eat chocol bell fleur damn bought bigger block",
    "tokens":[
      "eat",
      "chocol",
      "bell",
      "fleur",
      "damn",
      "bought",
      "bigger",
      "block"
    ],
    "token_count":8,
    "processed_text":"eat chocol bell fleur damn bought bigger block"
  },
  {
    "label":4,
    "text":"europ warm beer appar mayb tri",
    "cleaned_text":"europ warm beer appar mayb tri",
    "normalized_text":"europ warm beer appar mayb tri",
    "tokens":[
      "europ",
      "warm",
      "beer",
      "appar",
      "mayb",
      "tri"
    ],
    "token_count":6,
    "processed_text":"europ warm beer appar mayb tri"
  },
  {
    "label":0,
    "text":"accord man trader look like im work realli hard think progress final slow",
    "cleaned_text":"accord man trader look like im work realli hard think progress final slow",
    "normalized_text":"accord man trader look like im work realli hard think progress final slow",
    "tokens":[
      "accord",
      "man",
      "trader",
      "look",
      "like",
      "im",
      "work",
      "realli",
      "hard",
      "think",
      "progress",
      "final",
      "slow"
    ],
    "token_count":13,
    "processed_text":"accord man trader look like im work realli hard think progress final slow"
  },
  {
    "label":4,
    "text":"hi buck sarah",
    "cleaned_text":"hi buck sarah",
    "normalized_text":"hi buck sarah",
    "tokens":[
      "hi",
      "buck",
      "sarah"
    ],
    "token_count":3,
    "processed_text":"hi buck sarah"
  },
  {
    "label":0,
    "text":"u pictur nick jona sing daughtri cuz cant",
    "cleaned_text":"u pictur nick jona sing daughtri cuz cant",
    "normalized_text":"u pictur nick jona sing daughtri cuz cant",
    "tokens":[
      "pictur",
      "nick",
      "jona",
      "sing",
      "daughtri",
      "cuz",
      "cant"
    ],
    "token_count":7,
    "processed_text":"pictur nick jona sing daughtri cuz cant"
  },
  {
    "label":0,
    "text":"omg blackberri shadi cut like",
    "cleaned_text":"omg blackberri shadi cut like",
    "normalized_text":"omg blackberri shadi cut like",
    "tokens":[
      "omg",
      "blackberri",
      "shadi",
      "cut",
      "like"
    ],
    "token_count":5,
    "processed_text":"omg blackberri shadi cut like"
  },
  {
    "label":4,
    "text":"great game way go red wing stanley redw",
    "cleaned_text":"great game way go red wing stanley redw",
    "normalized_text":"great game way go red wing stanley redw",
    "tokens":[
      "great",
      "game",
      "way",
      "go",
      "red",
      "wing",
      "stanley",
      "redw"
    ],
    "token_count":8,
    "processed_text":"great game way go red wing stanley redw"
  },
  {
    "label":4,
    "text":"cheer followfriday",
    "cleaned_text":"cheer followfriday",
    "normalized_text":"cheer followfriday",
    "tokens":[
      "cheer",
      "followfriday"
    ],
    "token_count":2,
    "processed_text":"cheer followfriday"
  },
  {
    "label":4,
    "text":"love dream night alway wake happi cant wait see talk",
    "cleaned_text":"love dream night alway wake happi cant wait see talk",
    "normalized_text":"love dream night alway wake happi cant wait see talk",
    "tokens":[
      "love",
      "dream",
      "night",
      "alway",
      "wake",
      "happi",
      "cant",
      "wait",
      "see",
      "talk"
    ],
    "token_count":10,
    "processed_text":"love dream night alway wake happi cant wait see talk"
  },
  {
    "label":4,
    "text":"hehe thank colt",
    "cleaned_text":"hehe thank colt",
    "normalized_text":"hehe thank colt",
    "tokens":[
      "hehe",
      "thank",
      "colt"
    ],
    "token_count":3,
    "processed_text":"hehe thank colt"
  },
  {
    "label":0,
    "text":"shout thank strong friendship without id serious deep depress killself mood",
    "cleaned_text":"shout thank strong friendship without id serious deep depress killself mood",
    "normalized_text":"shout thank strong friendship without id serious deep depress killself mood",
    "tokens":[
      "shout",
      "thank",
      "strong",
      "friendship",
      "without",
      "id",
      "seriou",
      "deep",
      "depress",
      "killself",
      "mood"
    ],
    "token_count":11,
    "processed_text":"shout thank strong friendship without id seriou deep depress killself mood"
  },
  {
    "label":0,
    "text":"time never went sleep",
    "cleaned_text":"time never went sleep",
    "normalized_text":"time never went sleep",
    "tokens":[
      "time",
      "never",
      "went",
      "sleep"
    ],
    "token_count":4,
    "processed_text":"time never went sleep"
  },
  {
    "label":4,
    "text":"everyth amaz",
    "cleaned_text":"everyth amaz",
    "normalized_text":"everyth amaz",
    "tokens":[
      "everyth",
      "amaz"
    ],
    "token_count":2,
    "processed_text":"everyth amaz"
  },
  {
    "label":4,
    "text":"twitter client chanc escap combin power hope appreci",
    "cleaned_text":"twitter client chanc escap combin power hope appreci",
    "normalized_text":"twitter client chanc escap combin power hope appreci",
    "tokens":[
      "twitter",
      "client",
      "chanc",
      "escap",
      "combin",
      "power",
      "hope",
      "appreci"
    ],
    "token_count":8,
    "processed_text":"twitter client chanc escap combin power hope appreci"
  },
  {
    "label":0,
    "text":"go dentist bitafraid he go tell need get wisdom teeth pull",
    "cleaned_text":"go dentist bitafraid he go tell need get wisdom teeth pull",
    "normalized_text":"go dentist bitafraid he go tell need get wisdom teeth pull",
    "tokens":[
      "go",
      "dentist",
      "bitafraid",
      "go",
      "tell",
      "need",
      "get",
      "wisdom",
      "teeth",
      "pull"
    ],
    "token_count":10,
    "processed_text":"go dentist bitafraid go tell need get wisdom teeth pull"
  },
  {
    "label":4,
    "text":"yallah ill tri come inshaallah",
    "cleaned_text":"yallah ill tri come inshaallah",
    "normalized_text":"yallah ill tri come inshaallah",
    "tokens":[
      "yallah",
      "ill",
      "tri",
      "come",
      "inshaallah"
    ],
    "token_count":5,
    "processed_text":"yallah ill tri come inshaallah"
  },
  {
    "label":0,
    "text":"say bye virtual world got read read",
    "cleaned_text":"say bye virtual world got read read",
    "normalized_text":"say bye virtual world got read read",
    "tokens":[
      "say",
      "bye",
      "virtual",
      "world",
      "got",
      "read",
      "read"
    ],
    "token_count":7,
    "processed_text":"say bye virtual world got read read"
  },
  {
    "label":4,
    "text":"love pnk",
    "cleaned_text":"love pnk",
    "normalized_text":"love pnk",
    "tokens":[
      "love",
      "pnk"
    ],
    "token_count":2,
    "processed_text":"love pnk"
  },
  {
    "label":0,
    "text":"dont knoooooow",
    "cleaned_text":"dont knoooooow",
    "normalized_text":"dont knoooooow",
    "tokens":[
      "dont",
      "knoooooow"
    ],
    "token_count":2,
    "processed_text":"dont knoooooow"
  },
  {
    "label":4,
    "text":"hi ladi im feelin elov cyberhug",
    "cleaned_text":"hi ladi im feelin elov cyberhug",
    "normalized_text":"hi ladi im feelin elov cyberhug",
    "tokens":[
      "hi",
      "ladi",
      "im",
      "feelin",
      "elov",
      "cyberhug"
    ],
    "token_count":6,
    "processed_text":"hi ladi im feelin elov cyberhug"
  },
  {
    "label":0,
    "text":"noooo come",
    "cleaned_text":"noooo come",
    "normalized_text":"noooo come",
    "tokens":[
      "noooo",
      "come"
    ],
    "token_count":2,
    "processed_text":"noooo come"
  },
  {
    "label":0,
    "text":"realli need chang old routin",
    "cleaned_text":"realli need chang old routin",
    "normalized_text":"realli need chang old routin",
    "tokens":[
      "realli",
      "need",
      "chang",
      "old",
      "routin"
    ],
    "token_count":5,
    "processed_text":"realli need chang old routin"
  },
  {
    "label":0,
    "text":"miss hang hemlock hooligan",
    "cleaned_text":"miss hang hemlock hooligan",
    "normalized_text":"miss hang hemlock hooligan",
    "tokens":[
      "miss",
      "hang",
      "hemlock",
      "hooligan"
    ],
    "token_count":4,
    "processed_text":"miss hang hemlock hooligan"
  },
  {
    "label":0,
    "text":"rip one buffalo wild wing shirt cleaningi miss place",
    "cleaned_text":"rip one buffalo wild wing shirt cleaningi miss place",
    "normalized_text":"rip one buffalo wild wing shirt cleaningi miss place",
    "tokens":[
      "rip",
      "one",
      "buffalo",
      "wild",
      "wing",
      "shirt",
      "cleaningi",
      "miss",
      "place"
    ],
    "token_count":9,
    "processed_text":"rip one buffalo wild wing shirt cleaningi miss place"
  },
  {
    "label":0,
    "text":"knock feed show ughhhh show still cant see perform stupid thunder go away rain",
    "cleaned_text":"knock feed show ughhhh show still cant see perform stupid thunder go away rain",
    "normalized_text":"knock feed show ughhhh show still cant see perform stupid thunder go away rain",
    "tokens":[
      "knock",
      "feed",
      "show",
      "ughhhh",
      "show",
      "still",
      "cant",
      "see",
      "perform",
      "stupid",
      "thunder",
      "go",
      "away",
      "rain"
    ],
    "token_count":14,
    "processed_text":"knock feed show ughhhh show still cant see perform stupid thunder go away rain"
  },
  {
    "label":4,
    "text":"yup local market carri choctal jeni though your ever la check scoop",
    "cleaned_text":"yup local market carri choctal jeni though your ever la check scoop",
    "normalized_text":"yup local market carri choctal jeni though your ever la check scoop",
    "tokens":[
      "yup",
      "local",
      "market",
      "carri",
      "choctal",
      "jeni",
      "though",
      "ever",
      "la",
      "check",
      "scoop"
    ],
    "token_count":11,
    "processed_text":"yup local market carri choctal jeni though ever la check scoop"
  },
  {
    "label":4,
    "text":"love fact type darklightdav googl top result",
    "cleaned_text":"love fact type darklightdav googl top result",
    "normalized_text":"love fact type darklightdav googl top result",
    "tokens":[
      "love",
      "fact",
      "type",
      "darklightdav",
      "googl",
      "top",
      "result"
    ],
    "token_count":7,
    "processed_text":"love fact type darklightdav googl top result"
  },
  {
    "label":4,
    "text":"good morn tweep coffe need",
    "cleaned_text":"good morn tweep coffe need",
    "normalized_text":"good morn tweep coffe need",
    "tokens":[
      "good",
      "morn",
      "tweep",
      "coff",
      "need"
    ],
    "token_count":5,
    "processed_text":"good morn tweep coff need"
  },
  {
    "label":4,
    "text":"wow follow thank everyon",
    "cleaned_text":"wow follow thank everyon",
    "normalized_text":"wow follow thank everyon",
    "tokens":[
      "wow",
      "follow",
      "thank",
      "everyon"
    ],
    "token_count":4,
    "processed_text":"wow follow thank everyon"
  },
  {
    "label":0,
    "text":"shit signal",
    "cleaned_text":"shit signal",
    "normalized_text":"shit signal",
    "tokens":[
      "shit",
      "signal"
    ],
    "token_count":2,
    "processed_text":"shit signal"
  },
  {
    "label":0,
    "text":"ugh bella sore throat feel yucki",
    "cleaned_text":"ugh bella sore throat feel yucki",
    "normalized_text":"ugh bella sore throat feel yucki",
    "tokens":[
      "ugh",
      "bella",
      "sore",
      "throat",
      "feel",
      "yucki"
    ],
    "token_count":6,
    "processed_text":"ugh bella sore throat feel yucki"
  },
  {
    "label":0,
    "text":"best way start friday",
    "cleaned_text":"best way start friday",
    "normalized_text":"best way start friday",
    "tokens":[
      "best",
      "way",
      "start",
      "friday"
    ],
    "token_count":4,
    "processed_text":"best way start friday"
  },
  {
    "label":4,
    "text":"hurhurhur get use nowil tri haicaz sometim think weeksyou make den go beach",
    "cleaned_text":"hurhurhur get use nowil tri haicaz sometim think weeksyou make den go beach",
    "normalized_text":"hurhurhur get use nowil tri haicaz sometim think weeksyou make den go beach",
    "tokens":[
      "hurhurhur",
      "get",
      "use",
      "nowil",
      "tri",
      "haicaz",
      "sometim",
      "think",
      "weeksyou",
      "make",
      "den",
      "go",
      "beach"
    ],
    "token_count":13,
    "processed_text":"hurhurhur get use nowil tri haicaz sometim think weeksyou make den go beach"
  },
  {
    "label":4,
    "text":"dude let go",
    "cleaned_text":"dude let go",
    "normalized_text":"dude let go",
    "tokens":[
      "dude",
      "let",
      "go"
    ],
    "token_count":3,
    "processed_text":"dude let go"
  },
  {
    "label":4,
    "text":"happi th birthday guanp ah",
    "cleaned_text":"happi th birthday guanp ah",
    "normalized_text":"happi th birthday guanp ah",
    "tokens":[
      "happi",
      "th",
      "birthday",
      "guanp",
      "ah"
    ],
    "token_count":5,
    "processed_text":"happi th birthday guanp ah"
  },
  {
    "label":0,
    "text":"rachel ate much alli hous want puke run almost time supper room still mess",
    "cleaned_text":"rachel ate much alli hous want puke run almost time supper room still mess",
    "normalized_text":"rachel ate much alli hous want puke run almost time supper room still mess",
    "tokens":[
      "rachel",
      "ate",
      "much",
      "alli",
      "hou",
      "want",
      "puke",
      "run",
      "almost",
      "time",
      "supper",
      "room",
      "still",
      "mess"
    ],
    "token_count":14,
    "processed_text":"rachel ate much alli hou want puke run almost time supper room still mess"
  },
  {
    "label":0,
    "text":"forgot mom bday didnt call mother day",
    "cleaned_text":"forgot mom bday didnt call mother day",
    "normalized_text":"forgot mom bday didnt call mother day",
    "tokens":[
      "forgot",
      "mom",
      "bday",
      "didnt",
      "call",
      "mother",
      "day"
    ],
    "token_count":7,
    "processed_text":"forgot mom bday didnt call mother day"
  },
  {
    "label":4,
    "text":"dont forget daniel ate whole pizza mmmm good",
    "cleaned_text":"dont forget daniel ate whole pizza mmmm good",
    "normalized_text":"dont forget daniel ate whole pizza mmmm good",
    "tokens":[
      "dont",
      "forget",
      "daniel",
      "ate",
      "whole",
      "pizza",
      "mmmm",
      "good"
    ],
    "token_count":8,
    "processed_text":"dont forget daniel ate whole pizza mmmm good"
  },
  {
    "label":0,
    "text":"parent amp bro went mall without amp im hous alon wanna go shop",
    "cleaned_text":"parent amp bro went mall without amp im hous alon wanna go shop",
    "normalized_text":"parent amp bro went mall without amp im hous alon wanna go shop",
    "tokens":[
      "parent",
      "amp",
      "bro",
      "went",
      "mall",
      "without",
      "amp",
      "im",
      "hou",
      "alon",
      "wan",
      "na",
      "go",
      "shop"
    ],
    "token_count":14,
    "processed_text":"parent amp bro went mall without amp im hou alon wan na go shop"
  },
  {
    "label":0,
    "text":"need discuss hubbi night tonightjacq amp justin dubai probabl night though",
    "cleaned_text":"need discuss hubbi night tonightjacq amp justin dubai probabl night though",
    "normalized_text":"need discuss hubbi night tonightjacq amp justin dubai probabl night though",
    "tokens":[
      "need",
      "discuss",
      "hubbi",
      "night",
      "tonightjacq",
      "amp",
      "justin",
      "dubai",
      "probabl",
      "night",
      "though"
    ],
    "token_count":11,
    "processed_text":"need discuss hubbi night tonightjacq amp justin dubai probabl night though"
  },
  {
    "label":4,
    "text":"shoot updat today current",
    "cleaned_text":"shoot updat today current",
    "normalized_text":"shoot updat today current",
    "tokens":[
      "shoot",
      "updat",
      "today",
      "current"
    ],
    "token_count":4,
    "processed_text":"shoot updat today current"
  },
  {
    "label":4,
    "text":"quiet night bliss",
    "cleaned_text":"quiet night bliss",
    "normalized_text":"quiet night bliss",
    "tokens":[
      "quiet",
      "night",
      "bliss"
    ],
    "token_count":3,
    "processed_text":"quiet night bliss"
  },
  {
    "label":0,
    "text":"hey lil si u know im actin funni exam yesterday u know",
    "cleaned_text":"hey lil si u know im actin funni exam yesterday u know",
    "normalized_text":"hey lil si u know im actin funni exam yesterday u know",
    "tokens":[
      "hey",
      "lil",
      "si",
      "know",
      "im",
      "actin",
      "funni",
      "exam",
      "yesterday",
      "know"
    ],
    "token_count":10,
    "processed_text":"hey lil si know im actin funni exam yesterday know"
  },
  {
    "label":0,
    "text":"haha talagaa sayaang tsk miss lot",
    "cleaned_text":"haha talagaa sayaang tsk miss lot",
    "normalized_text":"haha talagaa sayaang tsk miss lot",
    "tokens":[
      "haha",
      "talagaa",
      "sayaang",
      "tsk",
      "miss",
      "lot"
    ],
    "token_count":6,
    "processed_text":"haha talagaa sayaang tsk miss lot"
  },
  {
    "label":0,
    "text":"oh wellanoth loss freo docker",
    "cleaned_text":"oh wellanoth loss freo docker",
    "normalized_text":"oh wellanoth loss freo docker",
    "tokens":[
      "oh",
      "wellanoth",
      "loss",
      "freo",
      "docker"
    ],
    "token_count":5,
    "processed_text":"oh wellanoth loss freo docker"
  },
  {
    "label":0,
    "text":"omg storm comingand im home alon",
    "cleaned_text":"omg storm comingand im home alon",
    "normalized_text":"omg storm comingand im home alon",
    "tokens":[
      "omg",
      "storm",
      "comingand",
      "im",
      "home",
      "alon"
    ],
    "token_count":6,
    "processed_text":"omg storm comingand im home alon"
  },
  {
    "label":0,
    "text":"bbc french plane lost atlant fear worst terribl",
    "cleaned_text":"bbc french plane lost atlant fear worst terribl",
    "normalized_text":"bbc french plane lost atlant fear worst terribl",
    "tokens":[
      "bbc",
      "french",
      "plane",
      "lost",
      "atlant",
      "fear",
      "worst",
      "terribl"
    ],
    "token_count":8,
    "processed_text":"bbc french plane lost atlant fear worst terribl"
  },
  {
    "label":4,
    "text":"anyon notic shirt bachelorett week awesom",
    "cleaned_text":"anyon notic shirt bachelorett week awesom",
    "normalized_text":"anyon notic shirt bachelorett week awesom",
    "tokens":[
      "anyon",
      "notic",
      "shirt",
      "bachelorett",
      "week",
      "awesom"
    ],
    "token_count":6,
    "processed_text":"anyon notic shirt bachelorett week awesom"
  },
  {
    "label":4,
    "text":"applic join besid twitter hmmmmm claudialeekehui die",
    "cleaned_text":"applic join besid twitter hmmmmm claudialeekehui die",
    "normalized_text":"applic join besid twitter hmmmmm claudialeekehui die",
    "tokens":[
      "applic",
      "join",
      "besid",
      "twitter",
      "hmmmmm",
      "claudialeekehui",
      "die"
    ],
    "token_count":7,
    "processed_text":"applic join besid twitter hmmmmm claudialeekehui die"
  },
  {
    "label":4,
    "text":"thank guy anoth wonder night",
    "cleaned_text":"thank guy anoth wonder night",
    "normalized_text":"thank guy anoth wonder night",
    "tokens":[
      "thank",
      "guy",
      "anoth",
      "wonder",
      "night"
    ],
    "token_count":5,
    "processed_text":"thank guy anoth wonder night"
  },
  {
    "label":0,
    "text":"bore want someth dooo",
    "cleaned_text":"bore want someth dooo",
    "normalized_text":"bore want someth dooo",
    "tokens":[
      "bore",
      "want",
      "someth",
      "dooo"
    ],
    "token_count":4,
    "processed_text":"bore want someth dooo"
  },
  {
    "label":4,
    "text":"realli nice day chat mum wander around campu",
    "cleaned_text":"realli nice day chat mum wander around campu",
    "normalized_text":"realli nice day chat mum wander around campu",
    "tokens":[
      "realli",
      "nice",
      "day",
      "chat",
      "mum",
      "wander",
      "around",
      "campu"
    ],
    "token_count":8,
    "processed_text":"realli nice day chat mum wander around campu"
  },
  {
    "label":4,
    "text":"shooot guy im happiest ive ever longggg time",
    "cleaned_text":"shooot guy im happiest ive ever longggg time",
    "normalized_text":"shooot guy im happiest ive ever longggg time",
    "tokens":[
      "shooot",
      "guy",
      "im",
      "happiest",
      "ive",
      "ever",
      "longggg",
      "time"
    ],
    "token_count":8,
    "processed_text":"shooot guy im happiest ive ever longggg time"
  },
  {
    "label":0,
    "text":"aww feel bad caus dunno nicol citi look she probabl worri srri hun",
    "cleaned_text":"aww feel bad caus dunno nicol citi look she probabl worri srri hun",
    "normalized_text":"aww feel bad caus dunno nicol citi look she probabl worri srri hun",
    "tokens":[
      "aww",
      "feel",
      "bad",
      "cau",
      "dunno",
      "nicol",
      "citi",
      "look",
      "probabl",
      "worri",
      "srri",
      "hun"
    ],
    "token_count":12,
    "processed_text":"aww feel bad cau dunno nicol citi look probabl worri srri hun"
  },
  {
    "label":4,
    "text":"bride war funni love",
    "cleaned_text":"bride war funni love",
    "normalized_text":"bride war funni love",
    "tokens":[
      "bride",
      "war",
      "funni",
      "love"
    ],
    "token_count":4,
    "processed_text":"bride war funni love"
  },
  {
    "label":0,
    "text":"oh beyond aw poor short",
    "cleaned_text":"oh beyond aw poor short",
    "normalized_text":"oh beyond aw poor short",
    "tokens":[
      "oh",
      "beyond",
      "aw",
      "poor",
      "short"
    ],
    "token_count":5,
    "processed_text":"oh beyond aw poor short"
  },
  {
    "label":0,
    "text":"need tan",
    "cleaned_text":"need tan",
    "normalized_text":"need tan",
    "tokens":[
      "need",
      "tan"
    ],
    "token_count":2,
    "processed_text":"need tan"
  },
  {
    "label":4,
    "text":"sat long day spend money cant afford sunbath gorgeou english sunshin zomg know",
    "cleaned_text":"sat long day spend money cant afford sunbath gorgeou english sunshin zomg know",
    "normalized_text":"sat long day spend money cant afford sunbath gorgeou english sunshin zomg know",
    "tokens":[
      "sat",
      "long",
      "day",
      "spend",
      "money",
      "cant",
      "afford",
      "sunbath",
      "gorgeou",
      "english",
      "sunshin",
      "zomg",
      "know"
    ],
    "token_count":13,
    "processed_text":"sat long day spend money cant afford sunbath gorgeou english sunshin zomg know"
  },
  {
    "label":4,
    "text":"that plan noth month get knock pay option pt work",
    "cleaned_text":"that plan noth month get knock pay option pt work",
    "normalized_text":"that plan noth month get knock pay option pt work",
    "tokens":[
      "plan",
      "noth",
      "month",
      "get",
      "knock",
      "pay",
      "option",
      "pt",
      "work"
    ],
    "token_count":9,
    "processed_text":"plan noth month get knock pay option pt work"
  },
  {
    "label":4,
    "text":"lolthank sigh love steeler nation",
    "cleaned_text":"lolthank sigh love steeler nation",
    "normalized_text":"lolthank sigh love steeler nation",
    "tokens":[
      "lolthank",
      "sigh",
      "love",
      "steeler",
      "nation"
    ],
    "token_count":5,
    "processed_text":"lolthank sigh love steeler nation"
  },
  {
    "label":0,
    "text":"got home im exhaust weekend think worst weekend life time school hour yay",
    "cleaned_text":"got home im exhaust weekend think worst weekend life time school hour yay",
    "normalized_text":"got home im exhaust weekend think worst weekend life time school hour yay",
    "tokens":[
      "got",
      "home",
      "im",
      "exhaust",
      "weekend",
      "think",
      "worst",
      "weekend",
      "life",
      "time",
      "school",
      "hour",
      "yay"
    ],
    "token_count":13,
    "processed_text":"got home im exhaust weekend think worst weekend life time school hour yay"
  },
  {
    "label":4,
    "text":"came play rain im need hot tea",
    "cleaned_text":"came play rain im need hot tea",
    "normalized_text":"came play rain im need hot tea",
    "tokens":[
      "came",
      "play",
      "rain",
      "im",
      "need",
      "hot",
      "tea"
    ],
    "token_count":7,
    "processed_text":"came play rain im need hot tea"
  },
  {
    "label":0,
    "text":"signific other desk type furious least twice day terribl ergonom",
    "cleaned_text":"signific other desk type furious least twice day terribl ergonom",
    "normalized_text":"signific other desk type furious least twice day terribl ergonom",
    "tokens":[
      "signif",
      "desk",
      "type",
      "furiou",
      "least",
      "twice",
      "day",
      "terribl",
      "ergonom"
    ],
    "token_count":9,
    "processed_text":"signif desk type furiou least twice day terribl ergonom"
  },
  {
    "label":0,
    "text":"friend tri convinc knew baltic state despit uncl travel",
    "cleaned_text":"friend tri convinc knew baltic state despit uncl travel",
    "normalized_text":"friend tri convinc knew baltic state despit uncl travel",
    "tokens":[
      "friend",
      "tri",
      "convinc",
      "knew",
      "baltic",
      "state",
      "despit",
      "uncl",
      "travel"
    ],
    "token_count":9,
    "processed_text":"friend tri convinc knew baltic state despit uncl travel"
  },
  {
    "label":4,
    "text":"forc unleash pc kotor",
    "cleaned_text":"forc unleash pc kotor",
    "normalized_text":"forc unleash pc kotor",
    "tokens":[
      "forc",
      "unleash",
      "pc",
      "kotor"
    ],
    "token_count":4,
    "processed_text":"forc unleash pc kotor"
  },
  {
    "label":4,
    "text":"stop im babi",
    "cleaned_text":"stop im babi",
    "normalized_text":"stop im babi",
    "tokens":[
      "stop",
      "im",
      "babi"
    ],
    "token_count":3,
    "processed_text":"stop im babi"
  },
  {
    "label":0,
    "text":"would love paint alasam broke",
    "cleaned_text":"would love paint alasam broke",
    "normalized_text":"would love paint alasam broke",
    "tokens":[
      "love",
      "paint",
      "alasam",
      "broke"
    ],
    "token_count":4,
    "processed_text":"love paint alasam broke"
  },
  {
    "label":4,
    "text":"nine day left school excit",
    "cleaned_text":"nine day left school excit",
    "normalized_text":"nine day left school excit",
    "tokens":[
      "nine",
      "day",
      "left",
      "school",
      "excit"
    ],
    "token_count":5,
    "processed_text":"nine day left school excit"
  },
  {
    "label":0,
    "text":"sorri hear boy sick want look phrase stamp cheap price",
    "cleaned_text":"sorri hear boy sick want look phrase stamp cheap price",
    "normalized_text":"sorri hear boy sick want look phrase stamp cheap price",
    "tokens":[
      "sorri",
      "hear",
      "boy",
      "sick",
      "want",
      "look",
      "phrase",
      "stamp",
      "cheap",
      "price"
    ],
    "token_count":10,
    "processed_text":"sorri hear boy sick want look phrase stamp cheap price"
  },
  {
    "label":0,
    "text":"cant whistl",
    "cleaned_text":"cant whistl",
    "normalized_text":"cant whistl",
    "tokens":[
      "cant",
      "whistl"
    ],
    "token_count":2,
    "processed_text":"cant whistl"
  },
  {
    "label":4,
    "text":"serious pleaseeeee",
    "cleaned_text":"serious pleaseeeee",
    "normalized_text":"serious pleaseeeee",
    "tokens":[
      "seriou",
      "pleaseeee"
    ],
    "token_count":2,
    "processed_text":"seriou pleaseeee"
  },
  {
    "label":0,
    "text":"sound like may need get new mac book",
    "cleaned_text":"sound like may need get new mac book",
    "normalized_text":"sound like may need get new mac book",
    "tokens":[
      "sound",
      "like",
      "may",
      "need",
      "get",
      "new",
      "mac",
      "book"
    ],
    "token_count":8,
    "processed_text":"sound like may need get new mac book"
  },
  {
    "label":0,
    "text":"wtf find fast wouldv never check wet seal actual close wet seal mall",
    "cleaned_text":"wtf find fast wouldv never check wet seal actual close wet seal mall",
    "normalized_text":"wtf find fast wouldv never check wet seal actual close wet seal mall",
    "tokens":[
      "wtf",
      "find",
      "fast",
      "wouldv",
      "never",
      "check",
      "wet",
      "seal",
      "actual",
      "close",
      "wet",
      "seal",
      "mall"
    ],
    "token_count":13,
    "processed_text":"wtf find fast wouldv never check wet seal actual close wet seal mall"
  },
  {
    "label":4,
    "text":"sing quoti gawk clock went walk want talk left shock quot",
    "cleaned_text":"sing quoti gawk clock went walk want talk left shock quot",
    "normalized_text":"sing quoti gawk clock went walk want talk left shock quot",
    "tokens":[
      "sing",
      "quoti",
      "gawk",
      "clock",
      "went",
      "walk",
      "want",
      "talk",
      "left",
      "shock",
      "quot"
    ],
    "token_count":11,
    "processed_text":"sing quoti gawk clock went walk want talk left shock quot"
  },
  {
    "label":0,
    "text":"shoot know tooi miss like minut",
    "cleaned_text":"shoot know tooi miss like minut",
    "normalized_text":"shoot know tooi miss like minut",
    "tokens":[
      "shoot",
      "know",
      "tooi",
      "miss",
      "like",
      "minut"
    ],
    "token_count":6,
    "processed_text":"shoot know tooi miss like minut"
  },
  {
    "label":0,
    "text":"get ridicul hour straighten sister hair cool",
    "cleaned_text":"get ridicul hour straighten sister hair cool",
    "normalized_text":"get ridicul hour straighten sister hair cool",
    "tokens":[
      "get",
      "ridicul",
      "hour",
      "straighten",
      "sister",
      "hair",
      "cool"
    ],
    "token_count":7,
    "processed_text":"get ridicul hour straighten sister hair cool"
  },
  {
    "label":0,
    "text":"distract snow june step kitti tail feel bad",
    "cleaned_text":"distract snow june step kitti tail feel bad",
    "normalized_text":"distract snow june step kitti tail feel bad",
    "tokens":[
      "distract",
      "snow",
      "june",
      "step",
      "kitti",
      "tail",
      "feel",
      "bad"
    ],
    "token_count":8,
    "processed_text":"distract snow june step kitti tail feel bad"
  },
  {
    "label":0,
    "text":"enrag damn tini bug fli around comput monitor wont leav alon think one even flew mouth ugh",
    "cleaned_text":"enrag damn tini bug fli around comput monitor wont leav alon think one even flew mouth ugh",
    "normalized_text":"enrag damn tini bug fli around comput monitor wont leav alon think one even flew mouth ugh",
    "tokens":[
      "enrag",
      "damn",
      "tini",
      "bug",
      "fli",
      "around",
      "comput",
      "monitor",
      "wont",
      "leav",
      "alon",
      "think",
      "one",
      "even",
      "flew",
      "mouth",
      "ugh"
    ],
    "token_count":17,
    "processed_text":"enrag damn tini bug fli around comput monitor wont leav alon think one even flew mouth ugh"
  },
  {
    "label":0,
    "text":"thought id let know pictur isnt showingg look like there technic difficulti",
    "cleaned_text":"thought id let know pictur isnt showingg look like there technic difficulti",
    "normalized_text":"thought id let know pictur isnt showingg look like there technic difficulti",
    "tokens":[
      "thought",
      "id",
      "let",
      "know",
      "pictur",
      "isnt",
      "showingg",
      "look",
      "like",
      "technic",
      "difficulti"
    ],
    "token_count":11,
    "processed_text":"thought id let know pictur isnt showingg look like technic difficulti"
  },
  {
    "label":4,
    "text":"wait cousin ghalda amalia marwan saputra haha",
    "cleaned_text":"wait cousin ghalda amalia marwan saputra haha",
    "normalized_text":"wait cousin ghalda amalia marwan saputra haha",
    "tokens":[
      "wait",
      "cousin",
      "ghalda",
      "amalia",
      "marwan",
      "saputra",
      "haha"
    ],
    "token_count":7,
    "processed_text":"wait cousin ghalda amalia marwan saputra haha"
  },
  {
    "label":0,
    "text":"quot dude last night phone dale dont want like want fuck sad isnt",
    "cleaned_text":"quot dude last night phone dale dont want like want fuck sad isnt",
    "normalized_text":"quot dude last night phone dale dont want like want fuck sad isnt",
    "tokens":[
      "quot",
      "dude",
      "last",
      "night",
      "phone",
      "dale",
      "dont",
      "want",
      "like",
      "want",
      "fuck",
      "sad",
      "isnt"
    ],
    "token_count":13,
    "processed_text":"quot dude last night phone dale dont want like want fuck sad isnt"
  },
  {
    "label":4,
    "text":"yeah shame aye come msn damo",
    "cleaned_text":"yeah shame aye come msn damo",
    "normalized_text":"yeah shame aye come msn damo",
    "tokens":[
      "yeah",
      "shame",
      "aye",
      "come",
      "msn",
      "damo"
    ],
    "token_count":6,
    "processed_text":"yeah shame aye come msn damo"
  },
  {
    "label":0,
    "text":"donut sound like magic right need money car",
    "cleaned_text":"donut sound like magic right need money car",
    "normalized_text":"donut sound like magic right need money car",
    "tokens":[
      "donut",
      "sound",
      "like",
      "magic",
      "right",
      "need",
      "money",
      "car"
    ],
    "token_count":8,
    "processed_text":"donut sound like magic right need money car"
  },
  {
    "label":4,
    "text":"often ye plu sit right next regularli bring puppi",
    "cleaned_text":"often ye plu sit right next regularli bring puppi",
    "normalized_text":"often ye plu sit right next regularli bring puppi",
    "tokens":[
      "often",
      "ye",
      "plu",
      "sit",
      "right",
      "next",
      "regularli",
      "bring",
      "puppi"
    ],
    "token_count":9,
    "processed_text":"often ye plu sit right next regularli bring puppi"
  },
  {
    "label":0,
    "text":"suck neighbour hospit wont come far preval",
    "cleaned_text":"suck neighbour hospit wont come far preval",
    "normalized_text":"suck neighbour hospit wont come far preval",
    "tokens":[
      "suck",
      "neighbour",
      "hospit",
      "wont",
      "come",
      "far",
      "preval"
    ],
    "token_count":7,
    "processed_text":"suck neighbour hospit wont come far preval"
  },
  {
    "label":0,
    "text":"home bed cant believ epic trip doneampov blast though",
    "cleaned_text":"home bed cant believ epic trip doneampov blast though",
    "normalized_text":"home bed cant believ epic trip doneampov blast though",
    "tokens":[
      "home",
      "bed",
      "cant",
      "believ",
      "epic",
      "trip",
      "doneampov",
      "blast",
      "though"
    ],
    "token_count":9,
    "processed_text":"home bed cant believ epic trip doneampov blast though"
  },
  {
    "label":0,
    "text":"ha tri last night wound crunchi rice cook longer mix crunchi soggi rice",
    "cleaned_text":"ha tri last night wound crunchi rice cook longer mix crunchi soggi rice",
    "normalized_text":"ha tri last night wound crunchi rice cook longer mix crunchi soggi rice",
    "tokens":[
      "ha",
      "tri",
      "last",
      "night",
      "wound",
      "crunchi",
      "rice",
      "cook",
      "longer",
      "mix",
      "crunchi",
      "soggi",
      "rice"
    ],
    "token_count":13,
    "processed_text":"ha tri last night wound crunchi rice cook longer mix crunchi soggi rice"
  },
  {
    "label":0,
    "text":"back berlin realli nice need time",
    "cleaned_text":"back berlin realli nice need time",
    "normalized_text":"back berlin realli nice need time",
    "tokens":[
      "back",
      "berlin",
      "realli",
      "nice",
      "need",
      "time"
    ],
    "token_count":6,
    "processed_text":"back berlin realli nice need time"
  },
  {
    "label":4,
    "text":"problem",
    "cleaned_text":"problem",
    "normalized_text":"problem",
    "tokens":[
      "problem"
    ],
    "token_count":1,
    "processed_text":"problem"
  },
  {
    "label":0,
    "text":"router upgrad firmwarei cant login anymor idea",
    "cleaned_text":"router upgrad firmwarei cant login anymor idea",
    "normalized_text":"router upgrad firmwarei cant login anymor idea",
    "tokens":[
      "router",
      "upgrad",
      "firmwarei",
      "cant",
      "login",
      "anymor",
      "idea"
    ],
    "token_count":7,
    "processed_text":"router upgrad firmwarei cant login anymor idea"
  },
  {
    "label":4,
    "text":"final list geometri religion english shelbi",
    "cleaned_text":"final list geometri religion english shelbi",
    "normalized_text":"final list geometri religion english shelbi",
    "tokens":[
      "final",
      "list",
      "geometri",
      "religion",
      "english",
      "shelbi"
    ],
    "token_count":6,
    "processed_text":"final list geometri religion english shelbi"
  },
  {
    "label":4,
    "text":"allow think bad bad heaven still want go heaven ye",
    "cleaned_text":"allow think bad bad heaven still want go heaven ye",
    "normalized_text":"allow think bad bad heaven still want go heaven ye",
    "tokens":[
      "allow",
      "think",
      "bad",
      "bad",
      "heaven",
      "still",
      "want",
      "go",
      "heaven",
      "ye"
    ],
    "token_count":10,
    "processed_text":"allow think bad bad heaven still want go heaven ye"
  },
  {
    "label":4,
    "text":"drink chili",
    "cleaned_text":"drink chili",
    "normalized_text":"drink chili",
    "tokens":[
      "drink",
      "chili"
    ],
    "token_count":2,
    "processed_text":"drink chili"
  },
  {
    "label":0,
    "text":"ooc canadian phone fail lock cannot sound drum rington unlock",
    "cleaned_text":"ooc canadian phone fail lock cannot sound drum rington unlock",
    "normalized_text":"ooc canadian phone fail lock cannot sound drum rington unlock",
    "tokens":[
      "ooc",
      "canadian",
      "phone",
      "fail",
      "lock",
      "sound",
      "drum",
      "rington",
      "unlock"
    ],
    "token_count":9,
    "processed_text":"ooc canadian phone fail lock sound drum rington unlock"
  },
  {
    "label":4,
    "text":"ty lolz beauti gonna die chines secur duck",
    "cleaned_text":"ty lolz beauti gonna die chines secur duck",
    "normalized_text":"ty lolz beauti gonna die chines secur duck",
    "tokens":[
      "ty",
      "lolz",
      "beauti",
      "gon",
      "na",
      "die",
      "chine",
      "secur",
      "duck"
    ],
    "token_count":9,
    "processed_text":"ty lolz beauti gon na die chine secur duck"
  },
  {
    "label":0,
    "text":"mom better expect mow lawn sunburn kill kthnx hope lulz girl tonight",
    "cleaned_text":"mom better expect mow lawn sunburn kill kthnx hope lulz girl tonight",
    "normalized_text":"mom better expect mow lawn sunburn kill kthnx hope lulz girl tonight",
    "tokens":[
      "mom",
      "better",
      "expect",
      "mow",
      "lawn",
      "sunburn",
      "kill",
      "kthnx",
      "hope",
      "lulz",
      "girl",
      "tonight"
    ],
    "token_count":12,
    "processed_text":"mom better expect mow lawn sunburn kill kthnx hope lulz girl tonight"
  },
  {
    "label":0,
    "text":"hip hurt bad im verg tear",
    "cleaned_text":"hip hurt bad im verg tear",
    "normalized_text":"hip hurt bad im verg tear",
    "tokens":[
      "hip",
      "hurt",
      "bad",
      "im",
      "verg",
      "tear"
    ],
    "token_count":6,
    "processed_text":"hip hurt bad im verg tear"
  },
  {
    "label":0,
    "text":"miss anarbor show forgot day today ughhh sick sleepless phoneless bffless yep life grand",
    "cleaned_text":"miss anarbor show forgot day today ughhh sick sleepless phoneless bffless yep life grand",
    "normalized_text":"miss anarbor show forgot day today ughhh sick sleepless phoneless bffless yep life grand",
    "tokens":[
      "miss",
      "anarbor",
      "show",
      "forgot",
      "day",
      "today",
      "ughhh",
      "sick",
      "sleepless",
      "phoneless",
      "bffless",
      "yep",
      "life",
      "grand"
    ],
    "token_count":14,
    "processed_text":"miss anarbor show forgot day today ughhh sick sleepless phoneless bffless yep life grand"
  },
  {
    "label":4,
    "text":"stay met fan sweet right",
    "cleaned_text":"stay met fan sweet right",
    "normalized_text":"stay met fan sweet right",
    "tokens":[
      "stay",
      "met",
      "fan",
      "sweet",
      "right"
    ],
    "token_count":5,
    "processed_text":"stay met fan sweet right"
  },
  {
    "label":0,
    "text":"ive almost spill someth phone time ive hour",
    "cleaned_text":"ive almost spill someth phone time ive hour",
    "normalized_text":"ive almost spill someth phone time ive hour",
    "tokens":[
      "ive",
      "almost",
      "spill",
      "someth",
      "phone",
      "time",
      "ive",
      "hour"
    ],
    "token_count":8,
    "processed_text":"ive almost spill someth phone time ive hour"
  },
  {
    "label":0,
    "text":"wont get tattoo tattoo jam anymor couldnt gut",
    "cleaned_text":"wont get tattoo tattoo jam anymor couldnt gut",
    "normalized_text":"wont get tattoo tattoo jam anymor couldnt gut",
    "tokens":[
      "wont",
      "get",
      "tattoo",
      "tattoo",
      "jam",
      "anymor",
      "couldnt",
      "gut"
    ],
    "token_count":8,
    "processed_text":"wont get tattoo tattoo jam anymor couldnt gut"
  },
  {
    "label":0,
    "text":"rickrol beth without sound",
    "cleaned_text":"rickrol beth without sound",
    "normalized_text":"rickrol beth without sound",
    "tokens":[
      "rickrol",
      "beth",
      "without",
      "sound"
    ],
    "token_count":4,
    "processed_text":"rickrol beth without sound"
  },
  {
    "label":4,
    "text":"ahaha well pancak r best especi awesom syrup lol",
    "cleaned_text":"ahaha well pancak r best especi awesom syrup lol",
    "normalized_text":"ahaha well pancak r best especi awesom syrup lol",
    "tokens":[
      "ahaha",
      "well",
      "pancak",
      "best",
      "especi",
      "awesom",
      "syrup",
      "lol"
    ],
    "token_count":8,
    "processed_text":"ahaha well pancak best especi awesom syrup lol"
  },
  {
    "label":0,
    "text":"hey boo havent talk day",
    "cleaned_text":"hey boo havent talk day",
    "normalized_text":"hey boo havent talk day",
    "tokens":[
      "hey",
      "boo",
      "havent",
      "talk",
      "day"
    ],
    "token_count":5,
    "processed_text":"hey boo havent talk day"
  },
  {
    "label":0,
    "text":"lol cours saw load southern fire engin arriv yesterday obvious sent retir men",
    "cleaned_text":"lol cours saw load southern fire engin arriv yesterday obvious sent retir men",
    "normalized_text":"lol cours saw load southern fire engin arriv yesterday obvious sent retir men",
    "tokens":[
      "lol",
      "cour",
      "saw",
      "load",
      "southern",
      "fire",
      "engin",
      "arriv",
      "yesterday",
      "obviou",
      "sent",
      "retir",
      "men"
    ],
    "token_count":13,
    "processed_text":"lol cour saw load southern fire engin arriv yesterday obviou sent retir men"
  },
  {
    "label":0,
    "text":"mann wors drug problem there cure rehab r screw",
    "cleaned_text":"mann wors drug problem there cure rehab r screw",
    "normalized_text":"mann wors drug problem there cure rehab r screw",
    "tokens":[
      "mann",
      "wor",
      "drug",
      "problem",
      "cure",
      "rehab",
      "screw"
    ],
    "token_count":7,
    "processed_text":"mann wor drug problem cure rehab screw"
  },
  {
    "label":4,
    "text":"kick ass drive lesson good hood",
    "cleaned_text":"kick ass drive lesson good hood",
    "normalized_text":"kick ass drive lesson good hood",
    "tokens":[
      "kick",
      "ass",
      "drive",
      "lesson",
      "good",
      "hood"
    ],
    "token_count":6,
    "processed_text":"kick ass drive lesson good hood"
  },
  {
    "label":0,
    "text":"yeah girl realli crazi poor rob",
    "cleaned_text":"yeah girl realli crazi poor rob",
    "normalized_text":"yeah girl realli crazi poor rob",
    "tokens":[
      "yeah",
      "girl",
      "realli",
      "crazi",
      "poor",
      "rob"
    ],
    "token_count":6,
    "processed_text":"yeah girl realli crazi poor rob"
  },
  {
    "label":0,
    "text":"mine piti counter never staf alway late collect arriv",
    "cleaned_text":"mine piti counter never staf alway late collect arriv",
    "normalized_text":"mine piti counter never staf alway late collect arriv",
    "tokens":[
      "mine",
      "piti",
      "counter",
      "never",
      "staf",
      "alway",
      "late",
      "collect",
      "arriv"
    ],
    "token_count":9,
    "processed_text":"mine piti counter never staf alway late collect arriv"
  },
  {
    "label":0,
    "text":"hope dont talk anybodi today least wait til take nap",
    "cleaned_text":"hope dont talk anybodi today least wait til take nap",
    "normalized_text":"hope dont talk anybodi today least wait til take nap",
    "tokens":[
      "hope",
      "dont",
      "talk",
      "anybodi",
      "today",
      "least",
      "wait",
      "til",
      "take",
      "nap"
    ],
    "token_count":10,
    "processed_text":"hope dont talk anybodi today least wait til take nap"
  },
  {
    "label":0,
    "text":"nicampjack hagn n luv ya bothti da love jacklol go bed damni dont wanna",
    "cleaned_text":"nicampjack hagn n luv ya bothti da love jacklol go bed damni dont wanna",
    "normalized_text":"nicampjack hagn n luv ya bothti da love jacklol go bed damni dont wanna",
    "tokens":[
      "nicampjack",
      "hagn",
      "luv",
      "ya",
      "bothti",
      "da",
      "love",
      "jacklol",
      "go",
      "bed",
      "damni",
      "dont",
      "wan",
      "na"
    ],
    "token_count":14,
    "processed_text":"nicampjack hagn luv ya bothti da love jacklol go bed damni dont wan na"
  },
  {
    "label":4,
    "text":"ami go bash credit card dont agre good hope everyth ok",
    "cleaned_text":"ami go bash credit card dont agre good hope everyth ok",
    "normalized_text":"ami go bash credit card dont agre good hope everyth ok",
    "tokens":[
      "ami",
      "go",
      "bash",
      "credit",
      "card",
      "dont",
      "agr",
      "good",
      "hope",
      "everyth",
      "ok"
    ],
    "token_count":11,
    "processed_text":"ami go bash credit card dont agr good hope everyth ok"
  },
  {
    "label":4,
    "text":"talk eh",
    "cleaned_text":"talk eh",
    "normalized_text":"talk eh",
    "tokens":[
      "talk",
      "eh"
    ],
    "token_count":2,
    "processed_text":"talk eh"
  },
  {
    "label":0,
    "text":"sick bed alreadi",
    "cleaned_text":"sick bed alreadi",
    "normalized_text":"sick bed alreadi",
    "tokens":[
      "sick",
      "bed",
      "alreadi"
    ],
    "token_count":3,
    "processed_text":"sick bed alreadi"
  },
  {
    "label":4,
    "text":"yessss chaser tonight",
    "cleaned_text":"yessss chaser tonight",
    "normalized_text":"yessss chaser tonight",
    "tokens":[
      "yessss",
      "chaser",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"yessss chaser tonight"
  },
  {
    "label":4,
    "text":"ok well sinc go bed end tweet convo ttyl hun",
    "cleaned_text":"ok well sinc go bed end tweet convo ttyl hun",
    "normalized_text":"ok well sinc go bed end tweet convo ttyl hun",
    "tokens":[
      "ok",
      "well",
      "sinc",
      "go",
      "bed",
      "end",
      "tweet",
      "convo",
      "ttyl",
      "hun"
    ],
    "token_count":10,
    "processed_text":"ok well sinc go bed end tweet convo ttyl hun"
  },
  {
    "label":4,
    "text":"hey happen anyway matter happen cheer okay jiayou ct",
    "cleaned_text":"hey happen anyway matter happen cheer okay jiayou ct",
    "normalized_text":"hey happen anyway matter happen cheer okay jiayou ct",
    "tokens":[
      "hey",
      "happen",
      "anyway",
      "matter",
      "happen",
      "cheer",
      "okay",
      "jiayou",
      "ct"
    ],
    "token_count":9,
    "processed_text":"hey happen anyway matter happen cheer okay jiayou ct"
  },
  {
    "label":4,
    "text":"vote u dear",
    "cleaned_text":"vote u dear",
    "normalized_text":"vote u dear",
    "tokens":[
      "vote",
      "dear"
    ],
    "token_count":2,
    "processed_text":"vote dear"
  },
  {
    "label":4,
    "text":"got sleep amp better lunch denni london",
    "cleaned_text":"got sleep amp better lunch denni london",
    "normalized_text":"got sleep amp better lunch denni london",
    "tokens":[
      "got",
      "sleep",
      "amp",
      "better",
      "lunch",
      "denni",
      "london"
    ],
    "token_count":7,
    "processed_text":"got sleep amp better lunch denni london"
  },
  {
    "label":0,
    "text":"ala im work tomorrow morn wont abl see",
    "cleaned_text":"ala im work tomorrow morn wont abl see",
    "normalized_text":"ala im work tomorrow morn wont abl see",
    "tokens":[
      "ala",
      "im",
      "work",
      "tomorrow",
      "morn",
      "wont",
      "abl",
      "see"
    ],
    "token_count":8,
    "processed_text":"ala im work tomorrow morn wont abl see"
  },
  {
    "label":4,
    "text":"cute littl creatur new roommat say hello allmal degufamili socuteitssickdotcom",
    "cleaned_text":"cute littl creatur new roommat say hello allmal degufamili socuteitssickdotcom",
    "normalized_text":"cute littl creatur new roommat say hello allmal degufamili socuteitssickdotcom",
    "tokens":[
      "cute",
      "littl",
      "creatur",
      "new",
      "roommat",
      "say",
      "hello",
      "allmal",
      "degufamili"
    ],
    "token_count":9,
    "processed_text":"cute littl creatur new roommat say hello allmal degufamili"
  },
  {
    "label":4,
    "text":"cityy today",
    "cleaned_text":"cityy today",
    "normalized_text":"cityy today",
    "tokens":[
      "cityy",
      "today"
    ],
    "token_count":2,
    "processed_text":"cityy today"
  },
  {
    "label":0,
    "text":"bad time last hous left dont know whether would get xxxx",
    "cleaned_text":"bad time last hous left dont know whether would get xxxx",
    "normalized_text":"bad time last hous left dont know whether would get xxxx",
    "tokens":[
      "bad",
      "time",
      "last",
      "hou",
      "left",
      "dont",
      "know",
      "whether",
      "get",
      "xxxx"
    ],
    "token_count":10,
    "processed_text":"bad time last hou left dont know whether get xxxx"
  },
  {
    "label":4,
    "text":"who grouchi",
    "cleaned_text":"who grouchi",
    "normalized_text":"who grouchi",
    "tokens":[
      "grouchi"
    ],
    "token_count":1,
    "processed_text":"grouchi"
  },
  {
    "label":4,
    "text":"let dress good reason get back",
    "cleaned_text":"let dress good reason get back",
    "normalized_text":"let dress good reason get back",
    "tokens":[
      "let",
      "dress",
      "good",
      "reason",
      "get",
      "back"
    ],
    "token_count":6,
    "processed_text":"let dress good reason get back"
  },
  {
    "label":4,
    "text":"omg meetoooo would love meet someon british texa accent babi lol",
    "cleaned_text":"omg meetoooo would love meet someon british texa accent babi lol",
    "normalized_text":"omg meetoooo would love meet someon british texa accent babi lol",
    "tokens":[
      "omg",
      "meetoooo",
      "love",
      "meet",
      "someon",
      "british",
      "texa",
      "accent",
      "babi",
      "lol"
    ],
    "token_count":10,
    "processed_text":"omg meetoooo love meet someon british texa accent babi lol"
  },
  {
    "label":4,
    "text":"sehen doch au",
    "cleaned_text":"sehen doch au",
    "normalized_text":"sehen doch au",
    "tokens":[
      "sehen",
      "doch",
      "au"
    ],
    "token_count":3,
    "processed_text":"sehen doch au"
  },
  {
    "label":0,
    "text":"realllllyyyyyy alreadi got set random drug test well see goe",
    "cleaned_text":"realllllyyyyyy alreadi got set random drug test well see goe",
    "normalized_text":"realllllyyyyyy alreadi got set random drug test well see goe",
    "tokens":[
      "realllllyyyyyy",
      "alreadi",
      "got",
      "set",
      "random",
      "drug",
      "test",
      "well",
      "see",
      "goe"
    ],
    "token_count":10,
    "processed_text":"realllllyyyyyy alreadi got set random drug test well see goe"
  },
  {
    "label":0,
    "text":"experienc technic challeng iphon",
    "cleaned_text":"experienc technic challeng iphon",
    "normalized_text":"experienc technic challeng iphon",
    "tokens":[
      "experienc",
      "technic",
      "challeng",
      "iphon"
    ],
    "token_count":4,
    "processed_text":"experienc technic challeng iphon"
  },
  {
    "label":4,
    "text":"hellllllooo raini skiesslt go back sleeepp go schooll",
    "cleaned_text":"hellllllooo raini skiesslt go back sleeepp go schooll",
    "normalized_text":"hellllllooo raini skiesslt go back sleeepp go schooll",
    "tokens":[
      "hellllllooo",
      "raini",
      "skiesslt",
      "go",
      "back",
      "sleeepp",
      "go",
      "schooll"
    ],
    "token_count":8,
    "processed_text":"hellllllooo raini skiesslt go back sleeepp go schooll"
  },
  {
    "label":4,
    "text":"real fun last night great meet peopl excit news startup come soon",
    "cleaned_text":"real fun last night great meet peopl excit news startup come soon",
    "normalized_text":"real fun last night great meet peopl excit news startup come soon",
    "tokens":[
      "real",
      "fun",
      "last",
      "night",
      "great",
      "meet",
      "peopl",
      "excit",
      "news",
      "startup",
      "come",
      "soon"
    ],
    "token_count":12,
    "processed_text":"real fun last night great meet peopl excit news startup come soon"
  },
  {
    "label":4,
    "text":"woke cook mood fabul meatloaf make return tabl tonight",
    "cleaned_text":"woke cook mood fabul meatloaf make return tabl tonight",
    "normalized_text":"woke cook mood fabul meatloaf make return tabl tonight",
    "tokens":[
      "woke",
      "cook",
      "mood",
      "fabul",
      "meatloaf",
      "make",
      "return",
      "tabl",
      "tonight"
    ],
    "token_count":9,
    "processed_text":"woke cook mood fabul meatloaf make return tabl tonight"
  },
  {
    "label":0,
    "text":"miss babi bad time",
    "cleaned_text":"miss babi bad time",
    "normalized_text":"miss babi bad time",
    "tokens":[
      "miss",
      "babi",
      "bad",
      "time"
    ],
    "token_count":4,
    "processed_text":"miss babi bad time"
  },
  {
    "label":4,
    "text":"write new song period make laugh cri ye huge dork",
    "cleaned_text":"write new song period make laugh cri ye huge dork",
    "normalized_text":"write new song period make laugh cri ye huge dork",
    "tokens":[
      "write",
      "new",
      "song",
      "period",
      "make",
      "laugh",
      "cri",
      "ye",
      "huge",
      "dork"
    ],
    "token_count":10,
    "processed_text":"write new song period make laugh cri ye huge dork"
  },
  {
    "label":0,
    "text":"need nap gonna happen go week night date night hub never happen",
    "cleaned_text":"need nap gonna happen go week night date night hub never happen",
    "normalized_text":"need nap gonna happen go week night date night hub never happen",
    "tokens":[
      "need",
      "nap",
      "gon",
      "na",
      "happen",
      "go",
      "week",
      "night",
      "date",
      "night",
      "hub",
      "never",
      "happen"
    ],
    "token_count":13,
    "processed_text":"need nap gon na happen go week night date night hub never happen"
  },
  {
    "label":0,
    "text":"cant take bean",
    "cleaned_text":"cant take bean",
    "normalized_text":"cant take bean",
    "tokens":[
      "cant",
      "take",
      "bean"
    ],
    "token_count":3,
    "processed_text":"cant take bean"
  },
  {
    "label":0,
    "text":"forecast chang think play rain friend",
    "cleaned_text":"forecast chang think play rain friend",
    "normalized_text":"forecast chang think play rain friend",
    "tokens":[
      "forecast",
      "chang",
      "think",
      "play",
      "rain",
      "friend"
    ],
    "token_count":6,
    "processed_text":"forecast chang think play rain friend"
  },
  {
    "label":0,
    "text":"sorri hear yr journalist frndam situat tho yr friend prob diff minemayv sent msg yr frnd",
    "cleaned_text":"sorri hear yr journalist frndam situat tho yr friend prob diff minemayv sent msg yr frnd",
    "normalized_text":"sorri hear yr journalist frndam situat tho yr friend prob diff minemayv sent msg yr frnd",
    "tokens":[
      "sorri",
      "hear",
      "yr",
      "journalist",
      "frndam",
      "situat",
      "tho",
      "yr",
      "friend",
      "prob",
      "diff",
      "minemayv",
      "sent",
      "msg",
      "yr",
      "frnd"
    ],
    "token_count":16,
    "processed_text":"sorri hear yr journalist frndam situat tho yr friend prob diff minemayv sent msg yr frnd"
  },
  {
    "label":0,
    "text":"hell aso order want pocket rocket cardigan",
    "cleaned_text":"hell aso order want pocket rocket cardigan",
    "normalized_text":"hell aso order want pocket rocket cardigan",
    "tokens":[
      "hell",
      "aso",
      "order",
      "want",
      "pocket",
      "rocket",
      "cardigan"
    ],
    "token_count":7,
    "processed_text":"hell aso order want pocket rocket cardigan"
  },
  {
    "label":4,
    "text":"oh yay think terp bad wait till your stuck room paint like tomorrow",
    "cleaned_text":"oh yay think terp bad wait till your stuck room paint like tomorrow",
    "normalized_text":"oh yay think terp bad wait till your stuck room paint like tomorrow",
    "tokens":[
      "oh",
      "yay",
      "think",
      "terp",
      "bad",
      "wait",
      "till",
      "stuck",
      "room",
      "paint",
      "like",
      "tomorrow"
    ],
    "token_count":12,
    "processed_text":"oh yay think terp bad wait till stuck room paint like tomorrow"
  },
  {
    "label":0,
    "text":"rain yet post",
    "cleaned_text":"rain yet post",
    "normalized_text":"rain yet post",
    "tokens":[
      "rain",
      "yet",
      "post"
    ],
    "token_count":3,
    "processed_text":"rain yet post"
  },
  {
    "label":4,
    "text":"free time u go newport that right ur perform nice place eat fun stuff",
    "cleaned_text":"free time u go newport that right ur perform nice place eat fun stuff",
    "normalized_text":"free time u go newport that right ur perform nice place eat fun stuff",
    "tokens":[
      "free",
      "time",
      "go",
      "newport",
      "right",
      "ur",
      "perform",
      "nice",
      "place",
      "eat",
      "fun",
      "stuff"
    ],
    "token_count":12,
    "processed_text":"free time go newport right ur perform nice place eat fun stuff"
  },
  {
    "label":0,
    "text":"cant twitter blackerri",
    "cleaned_text":"cant twitter blackerri",
    "normalized_text":"cant twitter blackerri",
    "tokens":[
      "cant",
      "twitter",
      "blackerri"
    ],
    "token_count":3,
    "processed_text":"cant twitter blackerri"
  },
  {
    "label":4,
    "text":"mean trust wikipedia fact instead fiction",
    "cleaned_text":"mean trust wikipedia fact instead fiction",
    "normalized_text":"mean trust wikipedia fact instead fiction",
    "tokens":[
      "mean",
      "trust",
      "wikipedia",
      "fact",
      "instead",
      "fiction"
    ],
    "token_count":6,
    "processed_text":"mean trust wikipedia fact instead fiction"
  },
  {
    "label":4,
    "text":"best time summer less cloth",
    "cleaned_text":"best time summer less cloth",
    "normalized_text":"best time summer less cloth",
    "tokens":[
      "best",
      "time",
      "summer",
      "less",
      "cloth"
    ],
    "token_count":5,
    "processed_text":"best time summer less cloth"
  },
  {
    "label":4,
    "text":"yesss back real world see meeee",
    "cleaned_text":"yesss back real world see meeee",
    "normalized_text":"yesss back real world see meeee",
    "tokens":[
      "yesss",
      "back",
      "real",
      "world",
      "see",
      "meeee"
    ],
    "token_count":6,
    "processed_text":"yesss back real world see meeee"
  },
  {
    "label":4,
    "text":"yeah isyou wanna play u gotta pay",
    "cleaned_text":"yeah isyou wanna play u gotta pay",
    "normalized_text":"yeah isyou wanna play u gotta pay",
    "tokens":[
      "yeah",
      "isyou",
      "wan",
      "na",
      "play",
      "got",
      "ta",
      "pay"
    ],
    "token_count":8,
    "processed_text":"yeah isyou wan na play got ta pay"
  },
  {
    "label":0,
    "text":"wish juli th lt",
    "cleaned_text":"wish juli th lt",
    "normalized_text":"wish juli th lt",
    "tokens":[
      "wish",
      "juli",
      "th",
      "lt"
    ],
    "token_count":4,
    "processed_text":"wish juli th lt"
  },
  {
    "label":4,
    "text":"here hope soggi stay away",
    "cleaned_text":"here hope soggi stay away",
    "normalized_text":"here hope soggi stay away",
    "tokens":[
      "hope",
      "soggi",
      "stay",
      "away"
    ],
    "token_count":4,
    "processed_text":"hope soggi stay away"
  },
  {
    "label":4,
    "text":"sooo excit new moon",
    "cleaned_text":"sooo excit new moon",
    "normalized_text":"sooo excit new moon",
    "tokens":[
      "sooo",
      "excit",
      "new",
      "moon"
    ],
    "token_count":4,
    "processed_text":"sooo excit new moon"
  },
  {
    "label":4,
    "text":"give someon twitter tutori",
    "cleaned_text":"give someon twitter tutori",
    "normalized_text":"give someon twitter tutori",
    "tokens":[
      "give",
      "someon",
      "twitter",
      "tutori"
    ],
    "token_count":4,
    "processed_text":"give someon twitter tutori"
  },
  {
    "label":0,
    "text":"give im go go sleep pain joy",
    "cleaned_text":"give im go go sleep pain joy",
    "normalized_text":"give im go go sleep pain joy",
    "tokens":[
      "give",
      "im",
      "go",
      "go",
      "sleep",
      "pain",
      "joy"
    ],
    "token_count":7,
    "processed_text":"give im go go sleep pain joy"
  },
  {
    "label":4,
    "text":"happi bday gerrard",
    "cleaned_text":"happi bday gerrard",
    "normalized_text":"happi bday gerrard",
    "tokens":[
      "happi",
      "bday",
      "gerrard"
    ],
    "token_count":3,
    "processed_text":"happi bday gerrard"
  },
  {
    "label":4,
    "text":"wife say need go vp semiannu yanke candl salenow along ride today",
    "cleaned_text":"wife say need go vp semiannu yanke candl salenow along ride today",
    "normalized_text":"wife say need go vp semiannu yanke candl salenow along ride today",
    "tokens":[
      "wife",
      "say",
      "need",
      "go",
      "vp",
      "semiannu",
      "yank",
      "candl",
      "salenow",
      "along",
      "ride",
      "today"
    ],
    "token_count":12,
    "processed_text":"wife say need go vp semiannu yank candl salenow along ride today"
  },
  {
    "label":0,
    "text":"complain sytycd far away realiz im way behind antm blog",
    "cleaned_text":"complain sytycd far away realiz im way behind antm blog",
    "normalized_text":"complain sytycd far away realiz im way behind antm blog",
    "tokens":[
      "complain",
      "sytycd",
      "far",
      "away",
      "realiz",
      "im",
      "way",
      "behind",
      "antm",
      "blog"
    ],
    "token_count":10,
    "processed_text":"complain sytycd far away realiz im way behind antm blog"
  },
  {
    "label":4,
    "text":"anybodi interest bhk hous vijaynagar plz dm bangalor",
    "cleaned_text":"anybodi interest bhk hous vijaynagar plz dm bangalor",
    "normalized_text":"anybodi interest bhk hous vijaynagar plz dm bangalor",
    "tokens":[
      "anybodi",
      "interest",
      "bhk",
      "hou",
      "vijaynagar",
      "plz",
      "dm",
      "bangalor"
    ],
    "token_count":8,
    "processed_text":"anybodi interest bhk hou vijaynagar plz dm bangalor"
  },
  {
    "label":0,
    "text":"hate your ill even fault your allow complain hangov",
    "cleaned_text":"hate your ill even fault your allow complain hangov",
    "normalized_text":"hate your ill even fault your allow complain hangov",
    "tokens":[
      "hate",
      "ill",
      "even",
      "fault",
      "allow",
      "complain",
      "hangov"
    ],
    "token_count":7,
    "processed_text":"hate ill even fault allow complain hangov"
  },
  {
    "label":4,
    "text":"long brazil america soon right",
    "cleaned_text":"long brazil america soon right",
    "normalized_text":"long brazil america soon right",
    "tokens":[
      "long",
      "brazil",
      "america",
      "soon",
      "right"
    ],
    "token_count":5,
    "processed_text":"long brazil america soon right"
  },
  {
    "label":0,
    "text":"dont get start slightli ocd number huge part obsess lol",
    "cleaned_text":"dont get start slightli ocd number huge part obsess lol",
    "normalized_text":"dont get start slightli ocd number huge part obsess lol",
    "tokens":[
      "dont",
      "get",
      "start",
      "slightli",
      "ocd",
      "number",
      "huge",
      "part",
      "obsess",
      "lol"
    ],
    "token_count":10,
    "processed_text":"dont get start slightli ocd number huge part obsess lol"
  },
  {
    "label":4,
    "text":"think make saw recent paint full color life alway",
    "cleaned_text":"think make saw recent paint full color life alway",
    "normalized_text":"think make saw recent paint full color life alway",
    "tokens":[
      "think",
      "make",
      "saw",
      "recent",
      "paint",
      "full",
      "color",
      "life",
      "alway"
    ],
    "token_count":9,
    "processed_text":"think make saw recent paint full color life alway"
  },
  {
    "label":4,
    "text":"ju smell favorit cologn random guy gave chill call unforgiv sean john",
    "cleaned_text":"ju smell favorit cologn random guy gave chill call unforgiv sean john",
    "normalized_text":"ju smell favorit cologn random guy gave chill call unforgiv sean john",
    "tokens":[
      "ju",
      "smell",
      "favorit",
      "cologn",
      "random",
      "guy",
      "gave",
      "chill",
      "call",
      "unforgiv",
      "sean",
      "john"
    ],
    "token_count":12,
    "processed_text":"ju smell favorit cologn random guy gave chill call unforgiv sean john"
  },
  {
    "label":0,
    "text":"closest get seahawk season tix floor ceil window work overlook qwest field",
    "cleaned_text":"closest get seahawk season tix floor ceil window work overlook qwest field",
    "normalized_text":"closest get seahawk season tix floor ceil window work overlook qwest field",
    "tokens":[
      "closest",
      "get",
      "seahawk",
      "season",
      "tix",
      "floor",
      "ceil",
      "window",
      "work",
      "overlook",
      "qwest",
      "field"
    ],
    "token_count":12,
    "processed_text":"closest get seahawk season tix floor ceil window work overlook qwest field"
  },
  {
    "label":0,
    "text":"wat bout old one",
    "cleaned_text":"wat bout old one",
    "normalized_text":"wat bout old one",
    "tokens":[
      "wat",
      "bout",
      "old",
      "one"
    ],
    "token_count":4,
    "processed_text":"wat bout old one"
  },
  {
    "label":0,
    "text":"chi ch stock cht ri v sm qu li khng say mi ch gt vy l ti ngu vietnam",
    "cleaned_text":"chi ch stock cht ri v sm qu li khng say mi ch gt vy l ti ngu vietnam",
    "normalized_text":"chi ch stock cht ri v sm qu li khng say mi ch gt vy l ti ngu vietnam",
    "tokens":[
      "chi",
      "ch",
      "stock",
      "cht",
      "ri",
      "sm",
      "qu",
      "li",
      "khng",
      "say",
      "mi",
      "ch",
      "gt",
      "vy",
      "ti",
      "ngu",
      "vietnam"
    ],
    "token_count":17,
    "processed_text":"chi ch stock cht ri sm qu li khng say mi ch gt vy ti ngu vietnam"
  },
  {
    "label":0,
    "text":"serious show prioriti iranelect",
    "cleaned_text":"serious show prioriti iranelect",
    "normalized_text":"serious show prioriti iranelect",
    "tokens":[
      "seriou",
      "show",
      "prioriti",
      "iranelect"
    ],
    "token_count":4,
    "processed_text":"seriou show prioriti iranelect"
  },
  {
    "label":0,
    "text":"muscl hate cramp badli mundan thing like toss hair put seatbelt",
    "cleaned_text":"muscl hate cramp badli mundan thing like toss hair put seatbelt",
    "normalized_text":"muscl hate cramp badli mundan thing like toss hair put seatbelt",
    "tokens":[
      "muscl",
      "hate",
      "cramp",
      "badli",
      "mundan",
      "thing",
      "like",
      "toss",
      "hair",
      "put",
      "seatbelt"
    ],
    "token_count":11,
    "processed_text":"muscl hate cramp badli mundan thing like toss hair put seatbelt"
  },
  {
    "label":0,
    "text":"via hey onlin market fail get free attract market train tweet soo",
    "cleaned_text":"via hey onlin market fail get free attract market train tweet soo",
    "normalized_text":"via hey onlin market fail get free attract market train tweet soo",
    "tokens":[
      "via",
      "hey",
      "onlin",
      "market",
      "fail",
      "get",
      "free",
      "attract",
      "market",
      "train",
      "tweet",
      "soo"
    ],
    "token_count":12,
    "processed_text":"via hey onlin market fail get free attract market train tweet soo"
  },
  {
    "label":0,
    "text":"im mad u drinkin wit",
    "cleaned_text":"im mad u drinkin wit",
    "normalized_text":"im mad u drinkin wit",
    "tokens":[
      "im",
      "mad",
      "drinkin",
      "wit"
    ],
    "token_count":4,
    "processed_text":"im mad drinkin wit"
  },
  {
    "label":0,
    "text":"last checknop im afraid im done im afraid",
    "cleaned_text":"last checknop im afraid im done im afraid",
    "normalized_text":"last checknop im afraid im done im afraid",
    "tokens":[
      "last",
      "checknop",
      "im",
      "afraid",
      "im",
      "done",
      "im",
      "afraid"
    ],
    "token_count":8,
    "processed_text":"last checknop im afraid im done im afraid"
  },
  {
    "label":0,
    "text":"ooh shit felt way pleas moment silenc nippl whew lord get ice",
    "cleaned_text":"ooh shit felt way pleas moment silenc nippl whew lord get ice",
    "normalized_text":"ooh shit felt way pleas moment silenc nippl whew lord get ice",
    "tokens":[
      "ooh",
      "shit",
      "felt",
      "way",
      "plea",
      "moment",
      "silenc",
      "nippl",
      "whew",
      "lord",
      "get",
      "ice"
    ],
    "token_count":12,
    "processed_text":"ooh shit felt way plea moment silenc nippl whew lord get ice"
  },
  {
    "label":0,
    "text":"tri fight damn cold",
    "cleaned_text":"tri fight damn cold",
    "normalized_text":"tri fight damn cold",
    "tokens":[
      "tri",
      "fight",
      "damn",
      "cold"
    ],
    "token_count":4,
    "processed_text":"tri fight damn cold"
  },
  {
    "label":4,
    "text":"thankyou im actual go send whenev finish",
    "cleaned_text":"thankyou im actual go send whenev finish",
    "normalized_text":"thankyou im actual go send whenev finish",
    "tokens":[
      "thankyou",
      "im",
      "actual",
      "go",
      "send",
      "whenev",
      "finish"
    ],
    "token_count":7,
    "processed_text":"thankyou im actual go send whenev finish"
  },
  {
    "label":0,
    "text":"gave blood today amp arm ach",
    "cleaned_text":"gave blood today amp arm ach",
    "normalized_text":"gave blood today amp arm ach",
    "tokens":[
      "gave",
      "blood",
      "today",
      "amp",
      "arm",
      "ach"
    ],
    "token_count":6,
    "processed_text":"gave blood today amp arm ach"
  },
  {
    "label":4,
    "text":"gooood morn play basketbal brother tube later woohoo",
    "cleaned_text":"gooood morn play basketbal brother tube later woohoo",
    "normalized_text":"gooood morn play basketbal brother tube later woohoo",
    "tokens":[
      "gooood",
      "morn",
      "play",
      "basketb",
      "brother",
      "tube",
      "later",
      "woohoo"
    ],
    "token_count":8,
    "processed_text":"gooood morn play basketb brother tube later woohoo"
  },
  {
    "label":0,
    "text":"watch haunt lol weather shit",
    "cleaned_text":"watch haunt lol weather shit",
    "normalized_text":"watch haunt lol weather shit",
    "tokens":[
      "watch",
      "haunt",
      "lol",
      "weather",
      "shit"
    ],
    "token_count":5,
    "processed_text":"watch haunt lol weather shit"
  },
  {
    "label":4,
    "text":"wow hot today glad pool nearbi",
    "cleaned_text":"wow hot today glad pool nearbi",
    "normalized_text":"wow hot today glad pool nearbi",
    "tokens":[
      "wow",
      "hot",
      "today",
      "glad",
      "pool",
      "nearbi"
    ],
    "token_count":6,
    "processed_text":"wow hot today glad pool nearbi"
  },
  {
    "label":0,
    "text":"sat busi bore",
    "cleaned_text":"sat busi bore",
    "normalized_text":"sat busi bore",
    "tokens":[
      "sat",
      "busi",
      "bore"
    ],
    "token_count":3,
    "processed_text":"sat busi bore"
  },
  {
    "label":0,
    "text":"makeov mate fan site address chang sorri pleas add new url love nicola xxx",
    "cleaned_text":"makeov mate fan site address chang sorri pleas add new url love nicola xxx",
    "normalized_text":"makeov mate fan site address chang sorri pleas add new url love nicola xxx",
    "tokens":[
      "makeov",
      "mate",
      "fan",
      "site",
      "address",
      "chang",
      "sorri",
      "plea",
      "add",
      "new",
      "url",
      "love",
      "nicola",
      "xxx"
    ],
    "token_count":14,
    "processed_text":"makeov mate fan site address chang sorri plea add new url love nicola xxx"
  },
  {
    "label":0,
    "text":"ciera netbook home im work",
    "cleaned_text":"ciera netbook home im work",
    "normalized_text":"ciera netbook home im work",
    "tokens":[
      "ciera",
      "netbook",
      "home",
      "im",
      "work"
    ],
    "token_count":5,
    "processed_text":"ciera netbook home im work"
  },
  {
    "label":0,
    "text":"awww yeah still fever mom came room amp turn fan quotsweat outquot die",
    "cleaned_text":"awww yeah still fever mom came room amp turn fan quotsweat outquot die",
    "normalized_text":"awww yeah still fever mom came room amp turn fan quotsweat outquot die",
    "tokens":[
      "awww",
      "yeah",
      "still",
      "fever",
      "mom",
      "came",
      "room",
      "amp",
      "turn",
      "fan",
      "quotsweat",
      "outquot",
      "die"
    ],
    "token_count":13,
    "processed_text":"awww yeah still fever mom came room amp turn fan quotsweat outquot die"
  },
  {
    "label":0,
    "text":"sorri cant post anoth video youtub right webcam isnt work right much nois background",
    "cleaned_text":"sorri cant post anoth video youtub right webcam isnt work right much nois background",
    "normalized_text":"sorri cant post anoth video youtub right webcam isnt work right much nois background",
    "tokens":[
      "sorri",
      "cant",
      "post",
      "anoth",
      "video",
      "youtub",
      "right",
      "webcam",
      "isnt",
      "work",
      "right",
      "much",
      "noi",
      "background"
    ],
    "token_count":14,
    "processed_text":"sorri cant post anoth video youtub right webcam isnt work right much noi background"
  },
  {
    "label":4,
    "text":"day natasja amp celebr year regist partnership similar marriag",
    "cleaned_text":"day natasja amp celebr year regist partnership similar marriag",
    "normalized_text":"day natasja amp celebr year regist partnership similar marriag",
    "tokens":[
      "day",
      "natasja",
      "amp",
      "celebr",
      "year",
      "regist",
      "partnership",
      "similar",
      "marriag"
    ],
    "token_count":9,
    "processed_text":"day natasja amp celebr year regist partnership similar marriag"
  },
  {
    "label":4,
    "text":"yeah bought ticket pearl jam august",
    "cleaned_text":"yeah bought ticket pearl jam august",
    "normalized_text":"yeah bought ticket pearl jam august",
    "tokens":[
      "yeah",
      "bought",
      "ticket",
      "pearl",
      "jam",
      "august"
    ],
    "token_count":6,
    "processed_text":"yeah bought ticket pearl jam august"
  },
  {
    "label":0,
    "text":"much go world sarah today",
    "cleaned_text":"much go world sarah today",
    "normalized_text":"much go world sarah today",
    "tokens":[
      "much",
      "go",
      "world",
      "sarah",
      "today"
    ],
    "token_count":5,
    "processed_text":"much go world sarah today"
  },
  {
    "label":0,
    "text":"umm come u didnt twitpic amc set todayim sad",
    "cleaned_text":"umm come u didnt twitpic amc set todayim sad",
    "normalized_text":"umm come u didnt twitpic amc set todayim sad",
    "tokens":[
      "umm",
      "come",
      "didnt",
      "twitpic",
      "amc",
      "set",
      "todayim",
      "sad"
    ],
    "token_count":8,
    "processed_text":"umm come didnt twitpic amc set todayim sad"
  },
  {
    "label":0,
    "text":"eu quero meu wayfar que roubaram cri",
    "cleaned_text":"eu quero meu wayfar que roubaram cri",
    "normalized_text":"eu quero meu wayfar que roubaram cri",
    "tokens":[
      "eu",
      "quero",
      "meu",
      "wayfar",
      "que",
      "roubaram",
      "cri"
    ],
    "token_count":7,
    "processed_text":"eu quero meu wayfar que roubaram cri"
  },
  {
    "label":4,
    "text":"love freedom share gospel th th grader school mention get pray everyday",
    "cleaned_text":"love freedom share gospel th th grader school mention get pray everyday",
    "normalized_text":"love freedom share gospel th th grader school mention get pray everyday",
    "tokens":[
      "love",
      "freedom",
      "share",
      "gospel",
      "th",
      "th",
      "grader",
      "school",
      "mention",
      "get",
      "pray",
      "everyday"
    ],
    "token_count":12,
    "processed_text":"love freedom share gospel th th grader school mention get pray everyday"
  },
  {
    "label":4,
    "text":"thx see day",
    "cleaned_text":"thx see day",
    "normalized_text":"thx see day",
    "tokens":[
      "thx",
      "see",
      "day"
    ],
    "token_count":3,
    "processed_text":"thx see day"
  },
  {
    "label":4,
    "text":"shout reason got twitter",
    "cleaned_text":"shout reason got twitter",
    "normalized_text":"shout reason got twitter",
    "tokens":[
      "shout",
      "reason",
      "got",
      "twitter"
    ],
    "token_count":4,
    "processed_text":"shout reason got twitter"
  },
  {
    "label":4,
    "text":"omg u simpli must post pic leia slave costum lol",
    "cleaned_text":"omg u simpli must post pic leia slave costum lol",
    "normalized_text":"omg u simpli must post pic leia slave costum lol",
    "tokens":[
      "omg",
      "simpli",
      "post",
      "pic",
      "leia",
      "slave",
      "costum",
      "lol"
    ],
    "token_count":8,
    "processed_text":"omg simpli post pic leia slave costum lol"
  },
  {
    "label":4,
    "text":"veget garden look good far",
    "cleaned_text":"veget garden look good far",
    "normalized_text":"veget garden look good far",
    "tokens":[
      "veget",
      "garden",
      "look",
      "good",
      "far"
    ],
    "token_count":5,
    "processed_text":"veget garden look good far"
  },
  {
    "label":0,
    "text":"actual probabl wont play ill get home pretti late tonight",
    "cleaned_text":"actual probabl wont play ill get home pretti late tonight",
    "normalized_text":"actual probabl wont play ill get home pretti late tonight",
    "tokens":[
      "actual",
      "probabl",
      "wont",
      "play",
      "ill",
      "get",
      "home",
      "pretti",
      "late",
      "tonight"
    ],
    "token_count":10,
    "processed_text":"actual probabl wont play ill get home pretti late tonight"
  },
  {
    "label":4,
    "text":"po could point sale well know",
    "cleaned_text":"po could point sale well know",
    "normalized_text":"po could point sale well know",
    "tokens":[
      "po",
      "point",
      "sale",
      "well",
      "know"
    ],
    "token_count":5,
    "processed_text":"po point sale well know"
  },
  {
    "label":0,
    "text":"gt depress",
    "cleaned_text":"gt depress",
    "normalized_text":"gt depress",
    "tokens":[
      "gt",
      "depress"
    ],
    "token_count":2,
    "processed_text":"gt depress"
  },
  {
    "label":4,
    "text":"would like introduc friend",
    "cleaned_text":"would like introduc friend",
    "normalized_text":"would like introduc friend",
    "tokens":[
      "like",
      "introduc",
      "friend"
    ],
    "token_count":3,
    "processed_text":"like introduc friend"
  },
  {
    "label":0,
    "text":"clean room messiest room world hour show hous",
    "cleaned_text":"clean room messiest room world hour show hous",
    "normalized_text":"clean room messiest room world hour show hous",
    "tokens":[
      "clean",
      "room",
      "messiest",
      "room",
      "world",
      "hour",
      "show",
      "hou"
    ],
    "token_count":8,
    "processed_text":"clean room messiest room world hour show hou"
  },
  {
    "label":0,
    "text":"sound awesom think im gonna stay home wknd",
    "cleaned_text":"sound awesom think im gonna stay home wknd",
    "normalized_text":"sound awesom think im gonna stay home wknd",
    "tokens":[
      "sound",
      "awesom",
      "think",
      "im",
      "gon",
      "na",
      "stay",
      "home",
      "wknd"
    ],
    "token_count":9,
    "processed_text":"sound awesom think im gon na stay home wknd"
  },
  {
    "label":4,
    "text":"serious saturday life fli texa next week make plan cali",
    "cleaned_text":"serious saturday life fli texa next week make plan cali",
    "normalized_text":"serious saturday life fli texa next week make plan cali",
    "tokens":[
      "seriou",
      "saturday",
      "life",
      "fli",
      "texa",
      "next",
      "week",
      "make",
      "plan",
      "cali"
    ],
    "token_count":10,
    "processed_text":"seriou saturday life fli texa next week make plan cali"
  },
  {
    "label":0,
    "text":"say goodby senior",
    "cleaned_text":"say goodby senior",
    "normalized_text":"say goodby senior",
    "tokens":[
      "say",
      "goodbi",
      "senior"
    ],
    "token_count":3,
    "processed_text":"say goodbi senior"
  },
  {
    "label":0,
    "text":"loooooooooooooooooooooooov u marri oh nou r alreadi marri",
    "cleaned_text":"loooooooooooooooooooooooov u marri oh nou r alreadi marri",
    "normalized_text":"loooooooooooooooooooooooov u marri oh nou r alreadi marri",
    "tokens":[
      "marri",
      "oh",
      "nou",
      "alreadi",
      "marri"
    ],
    "token_count":5,
    "processed_text":"marri oh nou alreadi marri"
  },
  {
    "label":4,
    "text":"life alcoholfre good boy go great sinc energi hit gym saturday",
    "cleaned_text":"life alcoholfre good boy go great sinc energi hit gym saturday",
    "normalized_text":"life alcoholfre good boy go great sinc energi hit gym saturday",
    "tokens":[
      "life",
      "alcoholfr",
      "good",
      "boy",
      "go",
      "great",
      "sinc",
      "energi",
      "hit",
      "gym",
      "saturday"
    ],
    "token_count":11,
    "processed_text":"life alcoholfr good boy go great sinc energi hit gym saturday"
  },
  {
    "label":4,
    "text":"trailer new moon awesom cant wait see movi",
    "cleaned_text":"trailer new moon awesom cant wait see movi",
    "normalized_text":"trailer new moon awesom cant wait see movi",
    "tokens":[
      "trailer",
      "new",
      "moon",
      "awesom",
      "cant",
      "wait",
      "see",
      "movi"
    ],
    "token_count":8,
    "processed_text":"trailer new moon awesom cant wait see movi"
  },
  {
    "label":4,
    "text":"life good hous inspect hous clean",
    "cleaned_text":"life good hous inspect hous clean",
    "normalized_text":"life good hous inspect hous clean",
    "tokens":[
      "life",
      "good",
      "hou",
      "inspect",
      "hou",
      "clean"
    ],
    "token_count":6,
    "processed_text":"life good hou inspect hou clean"
  },
  {
    "label":0,
    "text":"nope go suck",
    "cleaned_text":"nope go suck",
    "normalized_text":"nope go suck",
    "tokens":[
      "nope",
      "go",
      "suck"
    ],
    "token_count":3,
    "processed_text":"nope go suck"
  },
  {
    "label":0,
    "text":"well peopl go bye twitter peopl lt",
    "cleaned_text":"well peopl go bye twitter peopl lt",
    "normalized_text":"well peopl go bye twitter peopl lt",
    "tokens":[
      "well",
      "peopl",
      "go",
      "bye",
      "twitter",
      "peopl",
      "lt"
    ],
    "token_count":7,
    "processed_text":"well peopl go bye twitter peopl lt"
  },
  {
    "label":4,
    "text":"free food alcohol bowl good time",
    "cleaned_text":"free food alcohol bowl good time",
    "normalized_text":"free food alcohol bowl good time",
    "tokens":[
      "free",
      "food",
      "alcohol",
      "bowl",
      "good",
      "time"
    ],
    "token_count":6,
    "processed_text":"free food alcohol bowl good time"
  },
  {
    "label":4,
    "text":"chang account name everyon know",
    "cleaned_text":"chang account name everyon know",
    "normalized_text":"chang account name everyon know",
    "tokens":[
      "chang",
      "account",
      "name",
      "everyon",
      "know"
    ],
    "token_count":5,
    "processed_text":"chang account name everyon know"
  },
  {
    "label":4,
    "text":"lol hope pass final",
    "cleaned_text":"lol hope pass final",
    "normalized_text":"lol hope pass final",
    "tokens":[
      "lol",
      "hope",
      "pass",
      "final"
    ],
    "token_count":4,
    "processed_text":"lol hope pass final"
  },
  {
    "label":4,
    "text":"yay alien pretti much billion peopl wowsometast",
    "cleaned_text":"yay alien pretti much billion peopl wowsometast",
    "normalized_text":"yay alien pretti much billion peopl wowsometast",
    "tokens":[
      "yay",
      "alien",
      "pretti",
      "much",
      "billion",
      "peopl",
      "wowsometast"
    ],
    "token_count":7,
    "processed_text":"yay alien pretti much billion peopl wowsometast"
  },
  {
    "label":4,
    "text":"kany gonna produc someth call death autotun hypocrit say least",
    "cleaned_text":"kany gonna produc someth call death autotun hypocrit say least",
    "normalized_text":"kany gonna produc someth call death autotun hypocrit say least",
    "tokens":[
      "kani",
      "gon",
      "na",
      "produc",
      "someth",
      "call",
      "death",
      "autotun",
      "hypocrit",
      "say",
      "least"
    ],
    "token_count":11,
    "processed_text":"kani gon na produc someth call death autotun hypocrit say least"
  },
  {
    "label":4,
    "text":"pass human test yay bad come first best friend boyfriend bff ddnt hate bf itd fine sux",
    "cleaned_text":"pass human test yay bad come first best friend boyfriend bff ddnt hate bf itd fine sux",
    "normalized_text":"pass human test yay bad come first best friend boyfriend bff ddnt hate bf itd fine sux",
    "tokens":[
      "pass",
      "human",
      "test",
      "yay",
      "bad",
      "come",
      "first",
      "best",
      "friend",
      "boyfriend",
      "bff",
      "ddnt",
      "hate",
      "bf",
      "itd",
      "fine",
      "sux"
    ],
    "token_count":17,
    "processed_text":"pass human test yay bad come first best friend boyfriend bff ddnt hate bf itd fine sux"
  },
  {
    "label":0,
    "text":"true",
    "cleaned_text":"true",
    "normalized_text":"true",
    "tokens":[
      "true"
    ],
    "token_count":1,
    "processed_text":"true"
  },
  {
    "label":4,
    "text":"ok see wont eat rest month v nice extravag u",
    "cleaned_text":"ok see wont eat rest month v nice extravag u",
    "normalized_text":"ok see wont eat rest month v nice extravag u",
    "tokens":[
      "ok",
      "see",
      "wont",
      "eat",
      "rest",
      "month",
      "nice",
      "extravag"
    ],
    "token_count":8,
    "processed_text":"ok see wont eat rest month nice extravag"
  },
  {
    "label":0,
    "text":"guess way clean dust open",
    "cleaned_text":"guess way clean dust open",
    "normalized_text":"guess way clean dust open",
    "tokens":[
      "guess",
      "way",
      "clean",
      "dust",
      "open"
    ],
    "token_count":5,
    "processed_text":"guess way clean dust open"
  },
  {
    "label":4,
    "text":"morn everyon littl time heregotta tanworkoutget readi work make nite bout midnight",
    "cleaned_text":"morn everyon littl time heregotta tanworkoutget readi work make nite bout midnight",
    "normalized_text":"morn everyon littl time heregotta tanworkoutget readi work make nite bout midnight",
    "tokens":[
      "morn",
      "everyon",
      "littl",
      "time",
      "heregotta",
      "tanworkoutget",
      "readi",
      "work",
      "make",
      "nite",
      "bout",
      "midnight"
    ],
    "token_count":12,
    "processed_text":"morn everyon littl time heregotta tanworkoutget readi work make nite bout midnight"
  },
  {
    "label":4,
    "text":"hey first name meakin day feel like mine",
    "cleaned_text":"hey first name meakin day feel like mine",
    "normalized_text":"hey first name meakin day feel like mine",
    "tokens":[
      "hey",
      "first",
      "name",
      "meakin",
      "day",
      "feel",
      "like",
      "mine"
    ],
    "token_count":8,
    "processed_text":"hey first name meakin day feel like mine"
  },
  {
    "label":4,
    "text":"well wish quotgoood luckquot fun",
    "cleaned_text":"well wish quotgoood luckquot fun",
    "normalized_text":"well wish quotgoood luckquot fun",
    "tokens":[
      "well",
      "wish",
      "quotgoood",
      "luckquot",
      "fun"
    ],
    "token_count":5,
    "processed_text":"well wish quotgoood luckquot fun"
  },
  {
    "label":0,
    "text":"vb june nice alway august kid alway miss",
    "cleaned_text":"vb june nice alway august kid alway miss",
    "normalized_text":"vb june nice alway august kid alway miss",
    "tokens":[
      "vb",
      "june",
      "nice",
      "alway",
      "august",
      "kid",
      "alway",
      "miss"
    ],
    "token_count":8,
    "processed_text":"vb june nice alway august kid alway miss"
  },
  {
    "label":0,
    "text":"unfortun havent tweet today due meetingsbut hasnt talk late dont think luv",
    "cleaned_text":"unfortun havent tweet today due meetingsbut hasnt talk late dont think luv",
    "normalized_text":"unfortun havent tweet today due meetingsbut hasnt talk late dont think luv",
    "tokens":[
      "unfortun",
      "havent",
      "tweet",
      "today",
      "due",
      "meetingsbut",
      "hasnt",
      "talk",
      "late",
      "dont",
      "think",
      "luv"
    ],
    "token_count":12,
    "processed_text":"unfortun havent tweet today due meetingsbut hasnt talk late dont think luv"
  },
  {
    "label":4,
    "text":"moin au hamburg",
    "cleaned_text":"moin au hamburg",
    "normalized_text":"moin au hamburg",
    "tokens":[
      "moin",
      "au",
      "hamburg"
    ],
    "token_count":3,
    "processed_text":"moin au hamburg"
  },
  {
    "label":0,
    "text":"tellmee pleas",
    "cleaned_text":"tellmee pleas",
    "normalized_text":"tellmee pleas",
    "tokens":[
      "tellme",
      "plea"
    ],
    "token_count":2,
    "processed_text":"tellme plea"
  },
  {
    "label":0,
    "text":"ok neighbour share appreci ze german cinema experi subwoof your weakest link goodby",
    "cleaned_text":"ok neighbour share appreci ze german cinema experi subwoof your weakest link goodby",
    "normalized_text":"ok neighbour share appreci ze german cinema experi subwoof your weakest link goodby",
    "tokens":[
      "ok",
      "neighbour",
      "share",
      "appreci",
      "ze",
      "german",
      "cinema",
      "experi",
      "subwoof",
      "weakest",
      "link",
      "goodbi"
    ],
    "token_count":12,
    "processed_text":"ok neighbour share appreci ze german cinema experi subwoof weakest link goodbi"
  },
  {
    "label":0,
    "text":"shit call sick kid ask could hang tomorrow",
    "cleaned_text":"shit call sick kid ask could hang tomorrow",
    "normalized_text":"shit call sick kid ask could hang tomorrow",
    "tokens":[
      "shit",
      "call",
      "sick",
      "kid",
      "ask",
      "hang",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"shit call sick kid ask hang tomorrow"
  },
  {
    "label":4,
    "text":"hahaha yep note nice",
    "cleaned_text":"hahaha yep note nice",
    "normalized_text":"hahaha yep note nice",
    "tokens":[
      "hahaha",
      "yep",
      "note",
      "nice"
    ],
    "token_count":4,
    "processed_text":"hahaha yep note nice"
  },
  {
    "label":0,
    "text":"wasnt gonna watch shit thought said famili guy first",
    "cleaned_text":"wasnt gonna watch shit thought said famili guy first",
    "normalized_text":"wasnt gonna watch shit thought said famili guy first",
    "tokens":[
      "wasnt",
      "gon",
      "na",
      "watch",
      "shit",
      "thought",
      "said",
      "famili",
      "guy",
      "first"
    ],
    "token_count":10,
    "processed_text":"wasnt gon na watch shit thought said famili guy first"
  },
  {
    "label":4,
    "text":"home safe fun night",
    "cleaned_text":"home safe fun night",
    "normalized_text":"home safe fun night",
    "tokens":[
      "home",
      "safe",
      "fun",
      "night"
    ],
    "token_count":4,
    "processed_text":"home safe fun night"
  },
  {
    "label":0,
    "text":"sorri supplier carri either part",
    "cleaned_text":"sorri supplier carri either part",
    "normalized_text":"sorri supplier carri either part",
    "tokens":[
      "sorri",
      "supplier",
      "carri",
      "either",
      "part"
    ],
    "token_count":5,
    "processed_text":"sorri supplier carri either part"
  },
  {
    "label":4,
    "text":"that even quot here good one quoti think could fall madli bed youquot",
    "cleaned_text":"that even quot here good one quoti think could fall madli bed youquot",
    "normalized_text":"that even quot here good one quoti think could fall madli bed youquot",
    "tokens":[
      "even",
      "quot",
      "good",
      "one",
      "quoti",
      "think",
      "fall",
      "madli",
      "bed",
      "youquot"
    ],
    "token_count":10,
    "processed_text":"even quot good one quoti think fall madli bed youquot"
  },
  {
    "label":4,
    "text":"guy awesom creativ gear bar look mighti delici haha",
    "cleaned_text":"guy awesom creativ gear bar look mighti delici haha",
    "normalized_text":"guy awesom creativ gear bar look mighti delici haha",
    "tokens":[
      "guy",
      "awesom",
      "creativ",
      "gear",
      "bar",
      "look",
      "mighti",
      "delici",
      "haha"
    ],
    "token_count":9,
    "processed_text":"guy awesom creativ gear bar look mighti delici haha"
  },
  {
    "label":4,
    "text":"shoe mean figur level real level shoe tend reflect inner state wearer",
    "cleaned_text":"shoe mean figur level real level shoe tend reflect inner state wearer",
    "normalized_text":"shoe mean figur level real level shoe tend reflect inner state wearer",
    "tokens":[
      "shoe",
      "mean",
      "figur",
      "level",
      "real",
      "level",
      "shoe",
      "tend",
      "reflect",
      "inner",
      "state",
      "wearer"
    ],
    "token_count":12,
    "processed_text":"shoe mean figur level real level shoe tend reflect inner state wearer"
  },
  {
    "label":4,
    "text":"good great amp product day im workout take care grandma today amp offic",
    "cleaned_text":"good great amp product day im workout take care grandma today amp offic",
    "normalized_text":"good great amp product day im workout take care grandma today amp offic",
    "tokens":[
      "good",
      "great",
      "amp",
      "product",
      "day",
      "im",
      "workout",
      "take",
      "care",
      "grandma",
      "today",
      "amp",
      "offic"
    ],
    "token_count":13,
    "processed_text":"good great amp product day im workout take care grandma today amp offic"
  },
  {
    "label":0,
    "text":"well that confus thing even bloodi hell",
    "cleaned_text":"well that confus thing even bloodi hell",
    "normalized_text":"well that confus thing even bloodi hell",
    "tokens":[
      "well",
      "confu",
      "thing",
      "even",
      "bloodi",
      "hell"
    ],
    "token_count":6,
    "processed_text":"well confu thing even bloodi hell"
  },
  {
    "label":4,
    "text":"thank compliment amp remind gorgeou look like run return tomorrow",
    "cleaned_text":"thank compliment amp remind gorgeou look like run return tomorrow",
    "normalized_text":"thank compliment amp remind gorgeou look like run return tomorrow",
    "tokens":[
      "thank",
      "compliment",
      "amp",
      "remind",
      "gorgeou",
      "look",
      "like",
      "run",
      "return",
      "tomorrow"
    ],
    "token_count":10,
    "processed_text":"thank compliment amp remind gorgeou look like run return tomorrow"
  },
  {
    "label":4,
    "text":"let know need someon play spammi noob",
    "cleaned_text":"let know need someon play spammi noob",
    "normalized_text":"let know need someon play spammi noob",
    "tokens":[
      "let",
      "know",
      "need",
      "someon",
      "play",
      "spammi",
      "noob"
    ],
    "token_count":7,
    "processed_text":"let know need someon play spammi noob"
  },
  {
    "label":4,
    "text":"twitterfon awesom lt iphon rock",
    "cleaned_text":"twitterfon awesom lt iphon rock",
    "normalized_text":"twitterfon awesom lt iphon rock",
    "tokens":[
      "twitterfon",
      "awesom",
      "lt",
      "iphon",
      "rock"
    ],
    "token_count":5,
    "processed_text":"twitterfon awesom lt iphon rock"
  },
  {
    "label":0,
    "text":"dc wld blow w take shot headim chi bore",
    "cleaned_text":"dc wld blow w take shot headim chi bore",
    "normalized_text":"dc wld blow w take shot headim chi bore",
    "tokens":[
      "dc",
      "wld",
      "blow",
      "take",
      "shot",
      "headim",
      "chi",
      "bore"
    ],
    "token_count":8,
    "processed_text":"dc wld blow take shot headim chi bore"
  },
  {
    "label":0,
    "text":"thought good idea",
    "cleaned_text":"thought good idea",
    "normalized_text":"thought good idea",
    "tokens":[
      "thought",
      "good",
      "idea"
    ],
    "token_count":3,
    "processed_text":"thought good idea"
  },
  {
    "label":0,
    "text":"im hunger updat oh pleas tabi pleas your kill",
    "cleaned_text":"im hunger updat oh pleas tabi pleas your kill",
    "normalized_text":"im hunger updat oh pleas tabi pleas your kill",
    "tokens":[
      "im",
      "hunger",
      "updat",
      "oh",
      "plea",
      "tabi",
      "plea",
      "kill"
    ],
    "token_count":8,
    "processed_text":"im hunger updat oh plea tabi plea kill"
  },
  {
    "label":4,
    "text":"ha found contact page plugin one less thing custom build",
    "cleaned_text":"ha found contact page plugin one less thing custom build",
    "normalized_text":"ha found contact page plugin one less thing custom build",
    "tokens":[
      "ha",
      "found",
      "contact",
      "page",
      "plugin",
      "one",
      "less",
      "thing",
      "custom",
      "build"
    ],
    "token_count":10,
    "processed_text":"ha found contact page plugin one less thing custom build"
  },
  {
    "label":0,
    "text":"morn lang ako pwede errand sa hapon",
    "cleaned_text":"morn lang ako pwede errand sa hapon",
    "normalized_text":"morn lang ako pwede errand sa hapon",
    "tokens":[
      "morn",
      "lang",
      "ako",
      "pwede",
      "errand",
      "sa",
      "hapon"
    ],
    "token_count":7,
    "processed_text":"morn lang ako pwede errand sa hapon"
  },
  {
    "label":0,
    "text":"go pleas say hi ive done knee danc macarena im invalid mo pretti pleas mr song god",
    "cleaned_text":"go pleas say hi ive done knee danc macarena im invalid mo pretti pleas mr song god",
    "normalized_text":"go pleas say hi ive done knee danc macarena im invalid mo pretti pleas mr song god",
    "tokens":[
      "go",
      "plea",
      "say",
      "hi",
      "ive",
      "done",
      "knee",
      "danc",
      "macarena",
      "im",
      "invalid",
      "mo",
      "pretti",
      "plea",
      "mr",
      "song",
      "god"
    ],
    "token_count":17,
    "processed_text":"go plea say hi ive done knee danc macarena im invalid mo pretti plea mr song god"
  },
  {
    "label":4,
    "text":"got back churchhmmmcheck mailsgot email honeyheh",
    "cleaned_text":"got back churchhmmmcheck mailsgot email honeyheh",
    "normalized_text":"got back churchhmmmcheck mailsgot email honeyheh",
    "tokens":[
      "got",
      "back",
      "churchhmmmcheck",
      "mailsgot",
      "email",
      "honeyheh"
    ],
    "token_count":6,
    "processed_text":"got back churchhmmmcheck mailsgot email honeyheh"
  },
  {
    "label":0,
    "text":"oceana broke saddddddd birtheat phenomen album",
    "cleaned_text":"oceana broke saddddddd birtheat phenomen album",
    "normalized_text":"oceana broke saddddddd birtheat phenomen album",
    "tokens":[
      "oceana",
      "broke",
      "saddddddd",
      "birtheat",
      "phenomen",
      "album"
    ],
    "token_count":6,
    "processed_text":"oceana broke saddddddd birtheat phenomen album"
  },
  {
    "label":0,
    "text":"head ther cousin get marri tomorrow sooo need get earli im tech bed sleepin yet",
    "cleaned_text":"head ther cousin get marri tomorrow sooo need get earli im tech bed sleepin yet",
    "normalized_text":"head ther cousin get marri tomorrow sooo need get earli im tech bed sleepin yet",
    "tokens":[
      "head",
      "ther",
      "cousin",
      "get",
      "marri",
      "tomorrow",
      "sooo",
      "need",
      "get",
      "earli",
      "im",
      "tech",
      "bed",
      "sleepin",
      "yet"
    ],
    "token_count":15,
    "processed_text":"head ther cousin get marri tomorrow sooo need get earli im tech bed sleepin yet"
  },
  {
    "label":0,
    "text":"worri mamash heart attack today pleas send prayer good thought way",
    "cleaned_text":"worri mamash heart attack today pleas send prayer good thought way",
    "normalized_text":"worri mamash heart attack today pleas send prayer good thought way",
    "tokens":[
      "worri",
      "mamash",
      "heart",
      "attack",
      "today",
      "plea",
      "send",
      "prayer",
      "good",
      "thought",
      "way"
    ],
    "token_count":11,
    "processed_text":"worri mamash heart attack today plea send prayer good thought way"
  },
  {
    "label":0,
    "text":"previou twit banana stori best could come hour",
    "cleaned_text":"previou twit banana stori best could come hour",
    "normalized_text":"previou twit banana stori best could come hour",
    "tokens":[
      "previou",
      "twit",
      "banana",
      "stori",
      "best",
      "come",
      "hour"
    ],
    "token_count":7,
    "processed_text":"previou twit banana stori best come hour"
  },
  {
    "label":0,
    "text":"quotth hangoverquot existenti comedi demon intens said stop short classic",
    "cleaned_text":"quotth hangoverquot existenti comedi demon intens said stop short classic",
    "normalized_text":"quotth hangoverquot existenti comedi demon intens said stop short classic",
    "tokens":[
      "quotth",
      "hangoverquot",
      "existenti",
      "comedi",
      "demon",
      "inten",
      "said",
      "stop",
      "short",
      "classic"
    ],
    "token_count":10,
    "processed_text":"quotth hangoverquot existenti comedi demon inten said stop short classic"
  },
  {
    "label":0,
    "text":"one amaz guy ever reach utterli leagu",
    "cleaned_text":"one amaz guy ever reach utterli leagu",
    "normalized_text":"one amaz guy ever reach utterli leagu",
    "tokens":[
      "one",
      "amaz",
      "guy",
      "ever",
      "reach",
      "utterli",
      "leagu"
    ],
    "token_count":7,
    "processed_text":"one amaz guy ever reach utterli leagu"
  },
  {
    "label":4,
    "text":"democraci beast best option w chruchil net gestemd",
    "cleaned_text":"democraci beast best option w chruchil net gestemd",
    "normalized_text":"democraci beast best option w chruchil net gestemd",
    "tokens":[
      "democraci",
      "beast",
      "best",
      "option",
      "chruchil",
      "net",
      "gestemd"
    ],
    "token_count":7,
    "processed_text":"democraci beast best option chruchil net gestemd"
  },
  {
    "label":0,
    "text":"got tire wait im go tri bed",
    "cleaned_text":"got tire wait im go tri bed",
    "normalized_text":"got tire wait im go tri bed",
    "tokens":[
      "got",
      "tire",
      "wait",
      "im",
      "go",
      "tri",
      "bed"
    ],
    "token_count":7,
    "processed_text":"got tire wait im go tri bed"
  },
  {
    "label":0,
    "text":"theyr play headdeballtodekeep",
    "cleaned_text":"theyr play headdeballtodekeep",
    "normalized_text":"theyr play headdeballtodekeep",
    "tokens":[
      "theyr",
      "play"
    ],
    "token_count":2,
    "processed_text":"theyr play"
  },
  {
    "label":0,
    "text":"grow old triumph",
    "cleaned_text":"grow old triumph",
    "normalized_text":"grow old triumph",
    "tokens":[
      "grow",
      "old",
      "triumph"
    ],
    "token_count":3,
    "processed_text":"grow old triumph"
  },
  {
    "label":0,
    "text":"great boston legal alway one go sobsob dinner time indycar later wrap weekend sobsob",
    "cleaned_text":"great boston legal alway one go sobsob dinner time indycar later wrap weekend sobsob",
    "normalized_text":"great boston legal alway one go sobsob dinner time indycar later wrap weekend sobsob",
    "tokens":[
      "great",
      "boston",
      "legal",
      "alway",
      "one",
      "go",
      "sobsob",
      "dinner",
      "time",
      "indycar",
      "later",
      "wrap",
      "weekend",
      "sobsob"
    ],
    "token_count":14,
    "processed_text":"great boston legal alway one go sobsob dinner time indycar later wrap weekend sobsob"
  },
  {
    "label":4,
    "text":"right im much excit hp harri potter childhood sweetheart edward cullen sure understand haha",
    "cleaned_text":"right im much excit hp harri potter childhood sweetheart edward cullen sure understand haha",
    "normalized_text":"right im much excit hp harri potter childhood sweetheart edward cullen sure understand haha",
    "tokens":[
      "right",
      "im",
      "much",
      "excit",
      "hp",
      "harri",
      "potter",
      "childhood",
      "sweetheart",
      "edward",
      "cullen",
      "sure",
      "understand",
      "haha"
    ],
    "token_count":14,
    "processed_text":"right im much excit hp harri potter childhood sweetheart edward cullen sure understand haha"
  },
  {
    "label":4,
    "text":"blip dj huh cool guess leav music get readi work",
    "cleaned_text":"blip dj huh cool guess leav music get readi work",
    "normalized_text":"blip dj huh cool guess leav music get readi work",
    "tokens":[
      "blip",
      "dj",
      "huh",
      "cool",
      "guess",
      "leav",
      "music",
      "get",
      "readi",
      "work"
    ],
    "token_count":10,
    "processed_text":"blip dj huh cool guess leav music get readi work"
  },
  {
    "label":0,
    "text":"broken phone make sad panda sprint want way much new centro",
    "cleaned_text":"broken phone make sad panda sprint want way much new centro",
    "normalized_text":"broken phone make sad panda sprint want way much new centro",
    "tokens":[
      "broken",
      "phone",
      "make",
      "sad",
      "panda",
      "sprint",
      "want",
      "way",
      "much",
      "new",
      "centro"
    ],
    "token_count":11,
    "processed_text":"broken phone make sad panda sprint want way much new centro"
  },
  {
    "label":0,
    "text":"hope pineiro okay that noth major",
    "cleaned_text":"hope pineiro okay that noth major",
    "normalized_text":"hope pineiro okay that noth major",
    "tokens":[
      "hope",
      "pineiro",
      "okay",
      "noth",
      "major"
    ],
    "token_count":5,
    "processed_text":"hope pineiro okay noth major"
  },
  {
    "label":0,
    "text":"someone stole heaterr",
    "cleaned_text":"someone stole heaterr",
    "normalized_text":"someone stole heaterr",
    "tokens":[
      "someon",
      "stole",
      "heaterr"
    ],
    "token_count":3,
    "processed_text":"someon stole heaterr"
  },
  {
    "label":0,
    "text":"wish live",
    "cleaned_text":"wish live",
    "normalized_text":"wish live",
    "tokens":[
      "wish",
      "live"
    ],
    "token_count":2,
    "processed_text":"wish live"
  },
  {
    "label":4,
    "text":"im excit upcom trip washington state gonna blast take pictur laci wed",
    "cleaned_text":"im excit upcom trip washington state gonna blast take pictur laci wed",
    "normalized_text":"im excit upcom trip washington state gonna blast take pictur laci wed",
    "tokens":[
      "im",
      "excit",
      "upcom",
      "trip",
      "washington",
      "state",
      "gon",
      "na",
      "blast",
      "take",
      "pictur",
      "laci",
      "wed"
    ],
    "token_count":13,
    "processed_text":"im excit upcom trip washington state gon na blast take pictur laci wed"
  },
  {
    "label":4,
    "text":"cant wait",
    "cleaned_text":"cant wait",
    "normalized_text":"cant wait",
    "tokens":[
      "cant",
      "wait"
    ],
    "token_count":2,
    "processed_text":"cant wait"
  },
  {
    "label":4,
    "text":"woke hauent slept like foreuer",
    "cleaned_text":"woke hauent slept like foreuer",
    "normalized_text":"woke hauent slept like foreuer",
    "tokens":[
      "woke",
      "hauent",
      "slept",
      "like",
      "foreuer"
    ],
    "token_count":5,
    "processed_text":"woke hauent slept like foreuer"
  },
  {
    "label":4,
    "text":"clayton play prototyp pirinja",
    "cleaned_text":"clayton play prototyp pirinja",
    "normalized_text":"clayton play prototyp pirinja",
    "tokens":[
      "clayton",
      "play",
      "prototyp",
      "pirinja"
    ],
    "token_count":4,
    "processed_text":"clayton play prototyp pirinja"
  },
  {
    "label":0,
    "text":"pleas tell ubertwitt idk u got memo twitterberri mess ur phone",
    "cleaned_text":"pleas tell ubertwitt idk u got memo twitterberri mess ur phone",
    "normalized_text":"pleas tell ubertwitt idk u got memo twitterberri mess ur phone",
    "tokens":[
      "plea",
      "tell",
      "ubertwitt",
      "idk",
      "got",
      "memo",
      "twitterberri",
      "mess",
      "ur",
      "phone"
    ],
    "token_count":10,
    "processed_text":"plea tell ubertwitt idk got memo twitterberri mess ur phone"
  },
  {
    "label":0,
    "text":"take bath go pim fam bore saturday nigghhht",
    "cleaned_text":"take bath go pim fam bore saturday nigghhht",
    "normalized_text":"take bath go pim fam bore saturday nigghhht",
    "tokens":[
      "take",
      "bath",
      "go",
      "pim",
      "fam",
      "bore",
      "saturday",
      "nigghhht"
    ],
    "token_count":8,
    "processed_text":"take bath go pim fam bore saturday nigghhht"
  },
  {
    "label":0,
    "text":"miss see",
    "cleaned_text":"miss see",
    "normalized_text":"miss see",
    "tokens":[
      "miss",
      "see"
    ],
    "token_count":2,
    "processed_text":"miss see"
  },
  {
    "label":4,
    "text":"pleas fellow twitter",
    "cleaned_text":"pleas fellow twitter",
    "normalized_text":"pleas fellow twitter",
    "tokens":[
      "plea",
      "fellow",
      "twitter"
    ],
    "token_count":3,
    "processed_text":"plea fellow twitter"
  },
  {
    "label":0,
    "text":"pack stuff come back home probabl wont onlin much get back home fb",
    "cleaned_text":"pack stuff come back home probabl wont onlin much get back home fb",
    "normalized_text":"pack stuff come back home probabl wont onlin much get back home fb",
    "tokens":[
      "pack",
      "stuff",
      "come",
      "back",
      "home",
      "probabl",
      "wont",
      "onlin",
      "much",
      "get",
      "back",
      "home",
      "fb"
    ],
    "token_count":13,
    "processed_text":"pack stuff come back home probabl wont onlin much get back home fb"
  },
  {
    "label":4,
    "text":"goodnight",
    "cleaned_text":"goodnight",
    "normalized_text":"goodnight",
    "tokens":[
      "goodnight"
    ],
    "token_count":1,
    "processed_text":"goodnight"
  },
  {
    "label":0,
    "text":"sun shine make feel happier look heather goe home later",
    "cleaned_text":"sun shine make feel happier look heather goe home later",
    "normalized_text":"sun shine make feel happier look heather goe home later",
    "tokens":[
      "sun",
      "shine",
      "make",
      "feel",
      "happier",
      "look",
      "heather",
      "goe",
      "home",
      "later"
    ],
    "token_count":10,
    "processed_text":"sun shine make feel happier look heather goe home later"
  },
  {
    "label":4,
    "text":"lol brother fart nice ooer love sunshin much l good stuff",
    "cleaned_text":"lol brother fart nice ooer love sunshin much l good stuff",
    "normalized_text":"lol brother fart nice ooer love sunshin much l good stuff",
    "tokens":[
      "lol",
      "brother",
      "fart",
      "nice",
      "ooer",
      "love",
      "sunshin",
      "much",
      "good",
      "stuff"
    ],
    "token_count":10,
    "processed_text":"lol brother fart nice ooer love sunshin much good stuff"
  },
  {
    "label":0,
    "text":"tri get restsleepget well",
    "cleaned_text":"tri get restsleepget well",
    "normalized_text":"tri get restsleepget well",
    "tokens":[
      "tri",
      "get",
      "restsleepget",
      "well"
    ],
    "token_count":4,
    "processed_text":"tri get restsleepget well"
  },
  {
    "label":0,
    "text":"yea know im wrong sumtim core need counsel lol",
    "cleaned_text":"yea know im wrong sumtim core need counsel lol",
    "normalized_text":"yea know im wrong sumtim core need counsel lol",
    "tokens":[
      "yea",
      "know",
      "im",
      "wrong",
      "sumtim",
      "core",
      "need",
      "counsel",
      "lol"
    ],
    "token_count":9,
    "processed_text":"yea know im wrong sumtim core need counsel lol"
  },
  {
    "label":4,
    "text":"that im hear im home work hold went basement think r nap",
    "cleaned_text":"that im hear im home work hold went basement think r nap",
    "normalized_text":"that im hear im home work hold went basement think r nap",
    "tokens":[
      "im",
      "hear",
      "im",
      "home",
      "work",
      "hold",
      "went",
      "basement",
      "think",
      "nap"
    ],
    "token_count":10,
    "processed_text":"im hear im home work hold went basement think nap"
  },
  {
    "label":0,
    "text":"omg freak scare haha go one sad panda would criedscar alon bahaha kid",
    "cleaned_text":"omg freak scare haha go one sad panda would criedscar alon bahaha kid",
    "normalized_text":"omg freak scare haha go one sad panda would criedscar alon bahaha kid",
    "tokens":[
      "omg",
      "freak",
      "scare",
      "haha",
      "go",
      "one",
      "sad",
      "panda",
      "criedscar",
      "alon",
      "bahaha",
      "kid"
    ],
    "token_count":12,
    "processed_text":"omg freak scare haha go one sad panda criedscar alon bahaha kid"
  },
  {
    "label":0,
    "text":"fell shower almost broke leg",
    "cleaned_text":"fell shower almost broke leg",
    "normalized_text":"fell shower almost broke leg",
    "tokens":[
      "fell",
      "shower",
      "almost",
      "broke",
      "leg"
    ],
    "token_count":5,
    "processed_text":"fell shower almost broke leg"
  },
  {
    "label":0,
    "text":"ur trippin hahah went pf chang amp doto time see kati fli away sad",
    "cleaned_text":"ur trippin hahah went pf chang amp doto time see kati fli away sad",
    "normalized_text":"ur trippin hahah went pf chang amp doto time see kati fli away sad",
    "tokens":[
      "ur",
      "trippin",
      "hahah",
      "went",
      "pf",
      "chang",
      "amp",
      "doto",
      "time",
      "see",
      "kati",
      "fli",
      "away",
      "sad"
    ],
    "token_count":14,
    "processed_text":"ur trippin hahah went pf chang amp doto time see kati fli away sad"
  },
  {
    "label":0,
    "text":"hope sit look new screen pictur boooo",
    "cleaned_text":"hope sit look new screen pictur boooo",
    "normalized_text":"hope sit look new screen pictur boooo",
    "tokens":[
      "hope",
      "sit",
      "look",
      "new",
      "screen",
      "pictur",
      "boooo"
    ],
    "token_count":7,
    "processed_text":"hope sit look new screen pictur boooo"
  },
  {
    "label":4,
    "text":"heh ho",
    "cleaned_text":"heh ho",
    "normalized_text":"heh ho",
    "tokens":[
      "heh",
      "ho"
    ],
    "token_count":2,
    "processed_text":"heh ho"
  },
  {
    "label":4,
    "text":"call dr chenoweth",
    "cleaned_text":"call dr chenoweth",
    "normalized_text":"call dr chenoweth",
    "tokens":[
      "call",
      "dr",
      "chenoweth"
    ],
    "token_count":3,
    "processed_text":"call dr chenoweth"
  },
  {
    "label":4,
    "text":"oh know poor boy probabl horrif mix",
    "cleaned_text":"oh know poor boy probabl horrif mix",
    "normalized_text":"oh know poor boy probabl horrif mix",
    "tokens":[
      "oh",
      "know",
      "poor",
      "boy",
      "probabl",
      "horrif",
      "mix"
    ],
    "token_count":7,
    "processed_text":"oh know poor boy probabl horrif mix"
  },
  {
    "label":0,
    "text":"happi sunday sound like someon quotlawn moanquot insan intellig half would say back book",
    "cleaned_text":"happi sunday sound like someon quotlawn moanquot insan intellig half would say back book",
    "normalized_text":"happi sunday sound like someon quotlawn moanquot insan intellig half would say back book",
    "tokens":[
      "happi",
      "sunday",
      "sound",
      "like",
      "someon",
      "quotlawn",
      "moanquot",
      "insan",
      "intellig",
      "half",
      "say",
      "back",
      "book"
    ],
    "token_count":13,
    "processed_text":"happi sunday sound like someon quotlawn moanquot insan intellig half say back book"
  },
  {
    "label":0,
    "text":"didnt get email grainnei tweeter act bold child syndrom",
    "cleaned_text":"didnt get email grainnei tweeter act bold child syndrom",
    "normalized_text":"didnt get email grainnei tweeter act bold child syndrom",
    "tokens":[
      "didnt",
      "get",
      "email",
      "grainnei",
      "tweeter",
      "act",
      "bold",
      "child",
      "syndrom"
    ],
    "token_count":9,
    "processed_text":"didnt get email grainnei tweeter act bold child syndrom"
  },
  {
    "label":4,
    "text":"that realli wonder love friend great see peopl put social back social media",
    "cleaned_text":"that realli wonder love friend great see peopl put social back social media",
    "normalized_text":"that realli wonder love friend great see peopl put social back social media",
    "tokens":[
      "realli",
      "wonder",
      "love",
      "friend",
      "great",
      "see",
      "peopl",
      "put",
      "social",
      "back",
      "social",
      "media"
    ],
    "token_count":12,
    "processed_text":"realli wonder love friend great see peopl put social back social media"
  },
  {
    "label":0,
    "text":"sweat sometim hate south texa",
    "cleaned_text":"sweat sometim hate south texa",
    "normalized_text":"sweat sometim hate south texa",
    "tokens":[
      "sweat",
      "sometim",
      "hate",
      "south",
      "texa"
    ],
    "token_count":5,
    "processed_text":"sweat sometim hate south texa"
  },
  {
    "label":4,
    "text":"missin old colleg friend today love dear",
    "cleaned_text":"missin old colleg friend today love dear",
    "normalized_text":"missin old colleg friend today love dear",
    "tokens":[
      "missin",
      "old",
      "colleg",
      "friend",
      "today",
      "love",
      "dear"
    ],
    "token_count":7,
    "processed_text":"missin old colleg friend today love dear"
  },
  {
    "label":0,
    "text":"didnt get see wolverin full sob oh well mayb next time everyon miser cold day",
    "cleaned_text":"didnt get see wolverin full sob oh well mayb next time everyon miser cold day",
    "normalized_text":"didnt get see wolverin full sob oh well mayb next time everyon miser cold day",
    "tokens":[
      "didnt",
      "get",
      "see",
      "wolverin",
      "full",
      "sob",
      "oh",
      "well",
      "mayb",
      "next",
      "time",
      "everyon",
      "miser",
      "cold",
      "day"
    ],
    "token_count":15,
    "processed_text":"didnt get see wolverin full sob oh well mayb next time everyon miser cold day"
  },
  {
    "label":4,
    "text":"thank share tech support need everywher",
    "cleaned_text":"thank share tech support need everywher",
    "normalized_text":"thank share tech support need everywher",
    "tokens":[
      "thank",
      "share",
      "tech",
      "support",
      "need",
      "everywh"
    ],
    "token_count":6,
    "processed_text":"thank share tech support need everywh"
  },
  {
    "label":0,
    "text":"read tint window memphi would fun show take lil cousin",
    "cleaned_text":"read tint window memphi would fun show take lil cousin",
    "normalized_text":"read tint window memphi would fun show take lil cousin",
    "tokens":[
      "read",
      "tint",
      "window",
      "memphi",
      "fun",
      "show",
      "take",
      "lil",
      "cousin"
    ],
    "token_count":9,
    "processed_text":"read tint window memphi fun show take lil cousin"
  },
  {
    "label":4,
    "text":"ladd thank follow lol make feel import dont worri crush natur lol",
    "cleaned_text":"ladd thank follow lol make feel import dont worri crush natur lol",
    "normalized_text":"ladd thank follow lol make feel import dont worri crush natur lol",
    "tokens":[
      "ladd",
      "thank",
      "follow",
      "lol",
      "make",
      "feel",
      "import",
      "dont",
      "worri",
      "crush",
      "natur",
      "lol"
    ],
    "token_count":12,
    "processed_text":"ladd thank follow lol make feel import dont worri crush natur lol"
  },
  {
    "label":4,
    "text":"dy hair",
    "cleaned_text":"dy hair",
    "normalized_text":"dy hair",
    "tokens":[
      "dy",
      "hair"
    ],
    "token_count":2,
    "processed_text":"dy hair"
  },
  {
    "label":4,
    "text":"hah agre",
    "cleaned_text":"hah agre",
    "normalized_text":"hah agre",
    "tokens":[
      "hah",
      "agr"
    ],
    "token_count":2,
    "processed_text":"hah agr"
  },
  {
    "label":4,
    "text":"tulip fix im head flower market get huge bunch hydrangea basket hyacinth",
    "cleaned_text":"tulip fix im head flower market get huge bunch hydrangea basket hyacinth",
    "normalized_text":"tulip fix im head flower market get huge bunch hydrangea basket hyacinth",
    "tokens":[
      "tulip",
      "fix",
      "im",
      "head",
      "flower",
      "market",
      "get",
      "huge",
      "bunch",
      "hydrangea",
      "basket",
      "hyacinth"
    ],
    "token_count":12,
    "processed_text":"tulip fix im head flower market get huge bunch hydrangea basket hyacinth"
  },
  {
    "label":4,
    "text":"realli go work averag user",
    "cleaned_text":"realli go work averag user",
    "normalized_text":"realli go work averag user",
    "tokens":[
      "realli",
      "go",
      "work",
      "averag",
      "user"
    ],
    "token_count":5,
    "processed_text":"realli go work averag user"
  },
  {
    "label":4,
    "text":"silli girl like cute bag n wrinkl k hahaha dont like weep",
    "cleaned_text":"silli girl like cute bag n wrinkl k hahaha dont like weep",
    "normalized_text":"silli girl like cute bag n wrinkl k hahaha dont like weep",
    "tokens":[
      "silli",
      "girl",
      "like",
      "cute",
      "bag",
      "wrinkl",
      "hahaha",
      "dont",
      "like",
      "weep"
    ],
    "token_count":10,
    "processed_text":"silli girl like cute bag wrinkl hahaha dont like weep"
  },
  {
    "label":0,
    "text":"telephon",
    "cleaned_text":"telephon",
    "normalized_text":"telephon",
    "tokens":[
      "telephon"
    ],
    "token_count":1,
    "processed_text":"telephon"
  },
  {
    "label":0,
    "text":"oh well cant make cupcak dont enough egg",
    "cleaned_text":"oh well cant make cupcak dont enough egg",
    "normalized_text":"oh well cant make cupcak dont enough egg",
    "tokens":[
      "oh",
      "well",
      "cant",
      "make",
      "cupcak",
      "dont",
      "enough",
      "egg"
    ],
    "token_count":8,
    "processed_text":"oh well cant make cupcak dont enough egg"
  },
  {
    "label":4,
    "text":"im excit list event white board releas date song bff bday",
    "cleaned_text":"im excit list event white board releas date song bff bday",
    "normalized_text":"im excit list event white board releas date song bff bday",
    "tokens":[
      "im",
      "excit",
      "list",
      "event",
      "white",
      "board",
      "relea",
      "date",
      "song",
      "bff",
      "bday"
    ],
    "token_count":11,
    "processed_text":"im excit list event white board relea date song bff bday"
  },
  {
    "label":0,
    "text":"dad birthday tomor oh today today dad birthday hmmm havent bought gift yet",
    "cleaned_text":"dad birthday tomor oh today today dad birthday hmmm havent bought gift yet",
    "normalized_text":"dad birthday tomor oh today today dad birthday hmmm havent bought gift yet",
    "tokens":[
      "dad",
      "birthday",
      "tomor",
      "oh",
      "today",
      "today",
      "dad",
      "birthday",
      "hmmm",
      "havent",
      "bought",
      "gift",
      "yet"
    ],
    "token_count":13,
    "processed_text":"dad birthday tomor oh today today dad birthday hmmm havent bought gift yet"
  },
  {
    "label":0,
    "text":"didnt accept job day caus put quotgang bangquot resum appar that team work",
    "cleaned_text":"didnt accept job day caus put quotgang bangquot resum appar that team work",
    "normalized_text":"didnt accept job day caus put quotgang bangquot resum appar that team work",
    "tokens":[
      "didnt",
      "accept",
      "job",
      "day",
      "cau",
      "put",
      "quotgang",
      "bangquot",
      "resum",
      "appar",
      "team",
      "work"
    ],
    "token_count":12,
    "processed_text":"didnt accept job day cau put quotgang bangquot resum appar team work"
  },
  {
    "label":4,
    "text":"got back church good preacher",
    "cleaned_text":"got back church good preacher",
    "normalized_text":"got back church good preacher",
    "tokens":[
      "got",
      "back",
      "church",
      "good",
      "preacher"
    ],
    "token_count":5,
    "processed_text":"got back church good preacher"
  },
  {
    "label":0,
    "text":"havent eaten meat three day matter fact havent eaten much anyth chicken broth cracker",
    "cleaned_text":"havent eaten meat three day matter fact havent eaten much anyth chicken broth cracker",
    "normalized_text":"havent eaten meat three day matter fact havent eaten much anyth chicken broth cracker",
    "tokens":[
      "havent",
      "eaten",
      "meat",
      "three",
      "day",
      "matter",
      "fact",
      "havent",
      "eaten",
      "much",
      "anyth",
      "chicken",
      "broth",
      "cracker"
    ],
    "token_count":14,
    "processed_text":"havent eaten meat three day matter fact havent eaten much anyth chicken broth cracker"
  },
  {
    "label":4,
    "text":"how vega treatin ya",
    "cleaned_text":"how vega treatin ya",
    "normalized_text":"how vega treatin ya",
    "tokens":[
      "vega",
      "treatin",
      "ya"
    ],
    "token_count":3,
    "processed_text":"vega treatin ya"
  },
  {
    "label":4,
    "text":"excit xx",
    "cleaned_text":"excit xx",
    "normalized_text":"excit xx",
    "tokens":[
      "excit",
      "xx"
    ],
    "token_count":2,
    "processed_text":"excit xx"
  },
  {
    "label":4,
    "text":"dont think actual email address hope seen tweet",
    "cleaned_text":"dont think actual email address hope seen tweet",
    "normalized_text":"dont think actual email address hope seen tweet",
    "tokens":[
      "dont",
      "think",
      "actual",
      "email",
      "address",
      "hope",
      "seen",
      "tweet"
    ],
    "token_count":8,
    "processed_text":"dont think actual email address hope seen tweet"
  },
  {
    "label":4,
    "text":"gift suit didja attend",
    "cleaned_text":"gift suit didja attend",
    "normalized_text":"gift suit didja attend",
    "tokens":[
      "gift",
      "suit",
      "didja",
      "attend"
    ],
    "token_count":4,
    "processed_text":"gift suit didja attend"
  },
  {
    "label":0,
    "text":"sad bear today good",
    "cleaned_text":"sad bear today good",
    "normalized_text":"sad bear today good",
    "tokens":[
      "sad",
      "bear",
      "today",
      "good"
    ],
    "token_count":4,
    "processed_text":"sad bear today good"
  },
  {
    "label":0,
    "text":"ouch think hotel bed hurt take lot like bed cuz sleep anywher lower back achey",
    "cleaned_text":"ouch think hotel bed hurt take lot like bed cuz sleep anywher lower back achey",
    "normalized_text":"ouch think hotel bed hurt take lot like bed cuz sleep anywher lower back achey",
    "tokens":[
      "ouch",
      "think",
      "hotel",
      "bed",
      "hurt",
      "take",
      "lot",
      "like",
      "bed",
      "cuz",
      "sleep",
      "anywh",
      "lower",
      "back",
      "achey"
    ],
    "token_count":15,
    "processed_text":"ouch think hotel bed hurt take lot like bed cuz sleep anywh lower back achey"
  },
  {
    "label":0,
    "text":"poorli sick",
    "cleaned_text":"poorli sick",
    "normalized_text":"poorli sick",
    "tokens":[
      "poorli",
      "sick"
    ],
    "token_count":2,
    "processed_text":"poorli sick"
  },
  {
    "label":0,
    "text":"morn allsun shine im rotten mood",
    "cleaned_text":"morn allsun shine im rotten mood",
    "normalized_text":"morn allsun shine im rotten mood",
    "tokens":[
      "morn",
      "allsun",
      "shine",
      "im",
      "rotten",
      "mood"
    ],
    "token_count":6,
    "processed_text":"morn allsun shine im rotten mood"
  },
  {
    "label":0,
    "text":"want go",
    "cleaned_text":"want go",
    "normalized_text":"want go",
    "tokens":[
      "want",
      "go"
    ],
    "token_count":2,
    "processed_text":"want go"
  },
  {
    "label":4,
    "text":"thank end day work sweet note",
    "cleaned_text":"thank end day work sweet note",
    "normalized_text":"thank end day work sweet note",
    "tokens":[
      "thank",
      "end",
      "day",
      "work",
      "sweet",
      "note"
    ],
    "token_count":6,
    "processed_text":"thank end day work sweet note"
  },
  {
    "label":0,
    "text":"great day till choke water wail laugh car puke place wail highway",
    "cleaned_text":"great day till choke water wail laugh car puke place wail highway",
    "normalized_text":"great day till choke water wail laugh car puke place wail highway",
    "tokens":[
      "great",
      "day",
      "till",
      "choke",
      "water",
      "wail",
      "laugh",
      "car",
      "puke",
      "place",
      "wail",
      "highway"
    ],
    "token_count":12,
    "processed_text":"great day till choke water wail laugh car puke place wail highway"
  },
  {
    "label":4,
    "text":"follow guy",
    "cleaned_text":"follow guy",
    "normalized_text":"follow guy",
    "tokens":[
      "follow",
      "guy"
    ],
    "token_count":2,
    "processed_text":"follow guy"
  },
  {
    "label":0,
    "text":"thank hang tonight hope surviv tomorrow im kinda scare better call spartan back",
    "cleaned_text":"thank hang tonight hope surviv tomorrow im kinda scare better call spartan back",
    "normalized_text":"thank hang tonight hope surviv tomorrow im kinda scare better call spartan back",
    "tokens":[
      "thank",
      "hang",
      "tonight",
      "hope",
      "surviv",
      "tomorrow",
      "im",
      "kinda",
      "scare",
      "better",
      "call",
      "spartan",
      "back"
    ],
    "token_count":13,
    "processed_text":"thank hang tonight hope surviv tomorrow im kinda scare better call spartan back"
  },
  {
    "label":4,
    "text":"dear santa pleas pretti beach new board christma thankyou sign best behav person ever",
    "cleaned_text":"dear santa pleas pretti beach new board christma thankyou sign best behav person ever",
    "normalized_text":"dear santa pleas pretti beach new board christma thankyou sign best behav person ever",
    "tokens":[
      "dear",
      "santa",
      "plea",
      "pretti",
      "beach",
      "new",
      "board",
      "christma",
      "thankyou",
      "sign",
      "best",
      "behav",
      "person",
      "ever"
    ],
    "token_count":14,
    "processed_text":"dear santa plea pretti beach new board christma thankyou sign best behav person ever"
  },
  {
    "label":0,
    "text":"aww stinx u get get better drake",
    "cleaned_text":"aww stinx u get get better drake",
    "normalized_text":"aww stinx u get get better drake",
    "tokens":[
      "aww",
      "stinx",
      "get",
      "get",
      "better",
      "drake"
    ],
    "token_count":6,
    "processed_text":"aww stinx get get better drake"
  },
  {
    "label":0,
    "text":"death knight still",
    "cleaned_text":"death knight still",
    "normalized_text":"death knight still",
    "tokens":[
      "death",
      "knight",
      "still"
    ],
    "token_count":3,
    "processed_text":"death knight still"
  },
  {
    "label":0,
    "text":"wide awak even though total exhaust readi home bed",
    "cleaned_text":"wide awak even though total exhaust readi home bed",
    "normalized_text":"wide awak even though total exhaust readi home bed",
    "tokens":[
      "wide",
      "awak",
      "even",
      "though",
      "total",
      "exhaust",
      "readi",
      "home",
      "bed"
    ],
    "token_count":9,
    "processed_text":"wide awak even though total exhaust readi home bed"
  },
  {
    "label":4,
    "text":"back rosco get readi go riv later hoodrat year get see sissssssssssssssssi",
    "cleaned_text":"back rosco get readi go riv later hoodrat year get see sissssssssssssssssi",
    "normalized_text":"back rosco get readi go riv later hoodrat year get see sissssssssssssssssi",
    "tokens":[
      "back",
      "rosco",
      "get",
      "readi",
      "go",
      "riv",
      "later",
      "hoodrat",
      "year",
      "get",
      "see"
    ],
    "token_count":11,
    "processed_text":"back rosco get readi go riv later hoodrat year get see"
  },
  {
    "label":4,
    "text":"broke lt k websit rank better yet time spent websit doubl last month wahoo",
    "cleaned_text":"broke lt k websit rank better yet time spent websit doubl last month wahoo",
    "normalized_text":"broke lt k websit rank better yet time spent websit doubl last month wahoo",
    "tokens":[
      "broke",
      "lt",
      "websit",
      "rank",
      "better",
      "yet",
      "time",
      "spent",
      "websit",
      "doubl",
      "last",
      "month",
      "wahoo"
    ],
    "token_count":13,
    "processed_text":"broke lt websit rank better yet time spent websit doubl last month wahoo"
  },
  {
    "label":4,
    "text":"thank live discuss post bud",
    "cleaned_text":"thank live discuss post bud",
    "normalized_text":"thank live discuss post bud",
    "tokens":[
      "thank",
      "live",
      "discuss",
      "post",
      "bud"
    ],
    "token_count":5,
    "processed_text":"thank live discuss post bud"
  },
  {
    "label":0,
    "text":"oh subway sinc im order cant realli see salad deliv ill keep u post",
    "cleaned_text":"oh subway sinc im order cant realli see salad deliv ill keep u post",
    "normalized_text":"oh subway sinc im order cant realli see salad deliv ill keep u post",
    "tokens":[
      "oh",
      "subway",
      "sinc",
      "im",
      "order",
      "cant",
      "realli",
      "see",
      "salad",
      "deliv",
      "ill",
      "keep",
      "post"
    ],
    "token_count":13,
    "processed_text":"oh subway sinc im order cant realli see salad deliv ill keep post"
  },
  {
    "label":4,
    "text":"pleasur like spread joy hope good today",
    "cleaned_text":"pleasur like spread joy hope good today",
    "normalized_text":"pleasur like spread joy hope good today",
    "tokens":[
      "pleasur",
      "like",
      "spread",
      "joy",
      "hope",
      "good",
      "today"
    ],
    "token_count":7,
    "processed_text":"pleasur like spread joy hope good today"
  },
  {
    "label":4,
    "text":"back day present gr confer amp meet passion entrepreneur make sure deliv everi promis made",
    "cleaned_text":"back day present gr confer amp meet passion entrepreneur make sure deliv everi promis made",
    "normalized_text":"back day present gr confer amp meet passion entrepreneur make sure deliv everi promis made",
    "tokens":[
      "back",
      "day",
      "present",
      "gr",
      "confer",
      "amp",
      "meet",
      "passion",
      "entrepreneur",
      "make",
      "sure",
      "deliv",
      "everi",
      "promi",
      "made"
    ],
    "token_count":15,
    "processed_text":"back day present gr confer amp meet passion entrepreneur make sure deliv everi promi made"
  },
  {
    "label":4,
    "text":"rememb logo hope store stock soon want snag modern vintag",
    "cleaned_text":"rememb logo hope store stock soon want snag modern vintag",
    "normalized_text":"rememb logo hope store stock soon want snag modern vintag",
    "tokens":[
      "rememb",
      "logo",
      "hope",
      "store",
      "stock",
      "soon",
      "want",
      "snag",
      "modern",
      "vintag"
    ],
    "token_count":10,
    "processed_text":"rememb logo hope store stock soon want snag modern vintag"
  },
  {
    "label":4,
    "text":"eic allow enter heart dark lol wanna take congo river",
    "cleaned_text":"eic allow enter heart dark lol wanna take congo river",
    "normalized_text":"eic allow enter heart dark lol wanna take congo river",
    "tokens":[
      "eic",
      "allow",
      "enter",
      "heart",
      "dark",
      "lol",
      "wan",
      "na",
      "take",
      "congo",
      "river"
    ],
    "token_count":11,
    "processed_text":"eic allow enter heart dark lol wan na take congo river"
  },
  {
    "label":0,
    "text":"statu page said crap took hour log although dont worth",
    "cleaned_text":"statu page said crap took hour log although dont worth",
    "normalized_text":"statu page said crap took hour log although dont worth",
    "tokens":[
      "statu",
      "page",
      "said",
      "crap",
      "took",
      "hour",
      "log",
      "although",
      "dont",
      "worth"
    ],
    "token_count":10,
    "processed_text":"statu page said crap took hour log although dont worth"
  },
  {
    "label":0,
    "text":"lt mous ship third",
    "cleaned_text":"lt mous ship third",
    "normalized_text":"lt mous ship third",
    "tokens":[
      "lt",
      "mou",
      "ship",
      "third"
    ],
    "token_count":4,
    "processed_text":"lt mou ship third"
  },
  {
    "label":4,
    "text":"home bed dalla tx",
    "cleaned_text":"home bed dalla tx",
    "normalized_text":"home bed dalla tx",
    "tokens":[
      "home",
      "bed",
      "dalla",
      "tx"
    ],
    "token_count":4,
    "processed_text":"home bed dalla tx"
  },
  {
    "label":0,
    "text":"last day mustang ownerit sad give dealership tomorrow day til leav til show",
    "cleaned_text":"last day mustang ownerit sad give dealership tomorrow day til leav til show",
    "normalized_text":"last day mustang ownerit sad give dealership tomorrow day til leav til show",
    "tokens":[
      "last",
      "day",
      "mustang",
      "ownerit",
      "sad",
      "give",
      "dealership",
      "tomorrow",
      "day",
      "til",
      "leav",
      "til",
      "show"
    ],
    "token_count":13,
    "processed_text":"last day mustang ownerit sad give dealership tomorrow day til leav til show"
  },
  {
    "label":0,
    "text":"gonna leav like blt bacon",
    "cleaned_text":"gonna leav like blt bacon",
    "normalized_text":"gonna leav like blt bacon",
    "tokens":[
      "gon",
      "na",
      "leav",
      "like",
      "blt",
      "bacon"
    ],
    "token_count":6,
    "processed_text":"gon na leav like blt bacon"
  },
  {
    "label":4,
    "text":"omg quotsexyquot comment made laugh loud hyster also strang way compliment",
    "cleaned_text":"omg quotsexyquot comment made laugh loud hyster also strang way compliment",
    "normalized_text":"omg quotsexyquot comment made laugh loud hyster also strang way compliment",
    "tokens":[
      "omg",
      "quotsexyquot",
      "comment",
      "made",
      "laugh",
      "loud",
      "hyster",
      "also",
      "strang",
      "way",
      "compliment"
    ],
    "token_count":11,
    "processed_text":"omg quotsexyquot comment made laugh loud hyster also strang way compliment"
  },
  {
    "label":0,
    "text":"u around today geez cant believ monday",
    "cleaned_text":"u around today geez cant believ monday",
    "normalized_text":"u around today geez cant believ monday",
    "tokens":[
      "around",
      "today",
      "geez",
      "cant",
      "believ",
      "monday"
    ],
    "token_count":6,
    "processed_text":"around today geez cant believ monday"
  },
  {
    "label":4,
    "text":"im hungri feed",
    "cleaned_text":"im hungri feed",
    "normalized_text":"im hungri feed",
    "tokens":[
      "im",
      "hungri",
      "feed"
    ],
    "token_count":3,
    "processed_text":"im hungri feed"
  },
  {
    "label":4,
    "text":"invit nokia n blogger meet blore woohoo",
    "cleaned_text":"invit nokia n blogger meet blore woohoo",
    "normalized_text":"invit nokia n blogger meet blore woohoo",
    "tokens":[
      "invit",
      "nokia",
      "blogger",
      "meet",
      "blore",
      "woohoo"
    ],
    "token_count":6,
    "processed_text":"invit nokia blogger meet blore woohoo"
  },
  {
    "label":0,
    "text":"ahhhhhmist",
    "cleaned_text":"ahhhhhmist",
    "normalized_text":"ahhhhhmist",
    "tokens":[
      "ahhhhhmist"
    ],
    "token_count":1,
    "processed_text":"ahhhhhmist"
  },
  {
    "label":4,
    "text":"damn ya damn dont",
    "cleaned_text":"damn ya damn dont",
    "normalized_text":"damn ya damn dont",
    "tokens":[
      "damn",
      "ya",
      "damn",
      "dont"
    ],
    "token_count":4,
    "processed_text":"damn ya damn dont"
  },
  {
    "label":4,
    "text":"followfriday go follow might get th follow",
    "cleaned_text":"followfriday go follow might get th follow",
    "normalized_text":"followfriday go follow might get th follow",
    "tokens":[
      "followfriday",
      "go",
      "follow",
      "get",
      "th",
      "follow"
    ],
    "token_count":6,
    "processed_text":"followfriday go follow get th follow"
  },
  {
    "label":0,
    "text":"last night nashvil",
    "cleaned_text":"last night nashvil",
    "normalized_text":"last night nashvil",
    "tokens":[
      "last",
      "night",
      "nashvil"
    ],
    "token_count":3,
    "processed_text":"last night nashvil"
  },
  {
    "label":4,
    "text":"call bless miracl other happen time u pay attent identifi",
    "cleaned_text":"call bless miracl other happen time u pay attent identifi",
    "normalized_text":"call bless miracl other happen time u pay attent identifi",
    "tokens":[
      "call",
      "bless",
      "miracl",
      "happen",
      "time",
      "pay",
      "attent",
      "identifi"
    ],
    "token_count":8,
    "processed_text":"call bless miracl happen time pay attent identifi"
  },
  {
    "label":0,
    "text":"dont want go work today",
    "cleaned_text":"dont want go work today",
    "normalized_text":"dont want go work today",
    "tokens":[
      "dont",
      "want",
      "go",
      "work",
      "today"
    ],
    "token_count":5,
    "processed_text":"dont want go work today"
  },
  {
    "label":4,
    "text":"got home wasnt bumpin nite still good time good compani thnx guy",
    "cleaned_text":"got home wasnt bumpin nite still good time good compani thnx guy",
    "normalized_text":"got home wasnt bumpin nite still good time good compani thnx guy",
    "tokens":[
      "got",
      "home",
      "wasnt",
      "bumpin",
      "nite",
      "still",
      "good",
      "time",
      "good",
      "compani",
      "thnx",
      "guy"
    ],
    "token_count":12,
    "processed_text":"got home wasnt bumpin nite still good time good compani thnx guy"
  },
  {
    "label":0,
    "text":"dont see",
    "cleaned_text":"dont see",
    "normalized_text":"dont see",
    "tokens":[
      "dont",
      "see"
    ],
    "token_count":2,
    "processed_text":"dont see"
  },
  {
    "label":0,
    "text":"yea best stop punctur occur oh god bad punfail",
    "cleaned_text":"yea best stop punctur occur oh god bad punfail",
    "normalized_text":"yea best stop punctur occur oh god bad punfail",
    "tokens":[
      "yea",
      "best",
      "stop",
      "punctur",
      "occur",
      "oh",
      "god",
      "bad",
      "punfail"
    ],
    "token_count":9,
    "processed_text":"yea best stop punctur occur oh god bad punfail"
  },
  {
    "label":4,
    "text":"awww that thing",
    "cleaned_text":"awww that thing",
    "normalized_text":"awww that thing",
    "tokens":[
      "awww",
      "thing"
    ],
    "token_count":2,
    "processed_text":"awww thing"
  },
  {
    "label":0,
    "text":"homeopathi make incred angri sister realli kid",
    "cleaned_text":"homeopathi make incred angri sister realli kid",
    "normalized_text":"homeopathi make incred angri sister realli kid",
    "tokens":[
      "homeopathi",
      "make",
      "incr",
      "angri",
      "sister",
      "realli",
      "kid"
    ],
    "token_count":7,
    "processed_text":"homeopathi make incr angri sister realli kid"
  },
  {
    "label":0,
    "text":"delici dumpl",
    "cleaned_text":"delici dumpl",
    "normalized_text":"delici dumpl",
    "tokens":[
      "delici",
      "dumpl"
    ],
    "token_count":2,
    "processed_text":"delici dumpl"
  },
  {
    "label":0,
    "text":"sound delici get fresh seafood midwest suck",
    "cleaned_text":"sound delici get fresh seafood midwest suck",
    "normalized_text":"sound delici get fresh seafood midwest suck",
    "tokens":[
      "sound",
      "delici",
      "get",
      "fresh",
      "seafood",
      "midwest",
      "suck"
    ],
    "token_count":7,
    "processed_text":"sound delici get fresh seafood midwest suck"
  },
  {
    "label":0,
    "text":"got done lesson hard",
    "cleaned_text":"got done lesson hard",
    "normalized_text":"got done lesson hard",
    "tokens":[
      "got",
      "done",
      "lesson",
      "hard"
    ],
    "token_count":4,
    "processed_text":"got done lesson hard"
  },
  {
    "label":4,
    "text":"dog doesnt understand quotclean kitchenquot",
    "cleaned_text":"dog doesnt understand quotclean kitchenquot",
    "normalized_text":"dog doesnt understand quotclean kitchenquot",
    "tokens":[
      "dog",
      "doesnt",
      "understand",
      "quotclean",
      "kitchenquot"
    ],
    "token_count":5,
    "processed_text":"dog doesnt understand quotclean kitchenquot"
  },
  {
    "label":4,
    "text":"anybodi old tmobil phone dont want anymor plzzzz",
    "cleaned_text":"anybodi old tmobil phone dont want anymor plzzzz",
    "normalized_text":"anybodi old tmobil phone dont want anymor plzzzz",
    "tokens":[
      "anybodi",
      "old",
      "tmobil",
      "phone",
      "dont",
      "want",
      "anymor",
      "plzzzz"
    ],
    "token_count":8,
    "processed_text":"anybodi old tmobil phone dont want anymor plzzzz"
  },
  {
    "label":0,
    "text":"psycholog exam tomorrow havent revis like liter suck im go fail",
    "cleaned_text":"psycholog exam tomorrow havent revis like liter suck im go fail",
    "normalized_text":"psycholog exam tomorrow havent revis like liter suck im go fail",
    "tokens":[
      "psycholog",
      "exam",
      "tomorrow",
      "havent",
      "revi",
      "like",
      "liter",
      "suck",
      "im",
      "go",
      "fail"
    ],
    "token_count":11,
    "processed_text":"psycholog exam tomorrow havent revi like liter suck im go fail"
  },
  {
    "label":4,
    "text":"seo fun almost easi",
    "cleaned_text":"seo fun almost easi",
    "normalized_text":"seo fun almost easi",
    "tokens":[
      "seo",
      "fun",
      "almost",
      "easi"
    ],
    "token_count":4,
    "processed_text":"seo fun almost easi"
  },
  {
    "label":0,
    "text":"tiredbabysat till sleep timegnit yall",
    "cleaned_text":"tiredbabysat till sleep timegnit yall",
    "normalized_text":"tiredbabysat till sleep timegnit yall",
    "tokens":[
      "tiredbabysat",
      "till",
      "sleep",
      "timegnit",
      "yall"
    ],
    "token_count":5,
    "processed_text":"tiredbabysat till sleep timegnit yall"
  },
  {
    "label":4,
    "text":"experi life dont love",
    "cleaned_text":"experi life dont love",
    "normalized_text":"experi life dont love",
    "tokens":[
      "experi",
      "life",
      "dont",
      "love"
    ],
    "token_count":4,
    "processed_text":"experi life dont love"
  },
  {
    "label":0,
    "text":"nope isnt",
    "cleaned_text":"nope isnt",
    "normalized_text":"nope isnt",
    "tokens":[
      "nope",
      "isnt"
    ],
    "token_count":2,
    "processed_text":"nope isnt"
  },
  {
    "label":0,
    "text":"suck booo",
    "cleaned_text":"suck booo",
    "normalized_text":"suck booo",
    "tokens":[
      "suck",
      "booo"
    ],
    "token_count":2,
    "processed_text":"suck booo"
  },
  {
    "label":0,
    "text":"dont know gimp youll teach sometim",
    "cleaned_text":"dont know gimp youll teach sometim",
    "normalized_text":"dont know gimp youll teach sometim",
    "tokens":[
      "dont",
      "know",
      "gimp",
      "youll",
      "teach",
      "sometim"
    ],
    "token_count":6,
    "processed_text":"dont know gimp youll teach sometim"
  },
  {
    "label":0,
    "text":"school choic hey see besti",
    "cleaned_text":"school choic hey see besti",
    "normalized_text":"school choic hey see besti",
    "tokens":[
      "school",
      "choic",
      "hey",
      "see",
      "besti"
    ],
    "token_count":5,
    "processed_text":"school choic hey see besti"
  },
  {
    "label":4,
    "text":"sink toe sand wrightsvil beach nc love day",
    "cleaned_text":"sink toe sand wrightsvil beach nc love day",
    "normalized_text":"sink toe sand wrightsvil beach nc love day",
    "tokens":[
      "sink",
      "toe",
      "sand",
      "wrightsvil",
      "beach",
      "nc",
      "love",
      "day"
    ],
    "token_count":8,
    "processed_text":"sink toe sand wrightsvil beach nc love day"
  },
  {
    "label":4,
    "text":"thnk",
    "cleaned_text":"thnk",
    "normalized_text":"thnk",
    "tokens":[
      "thnk"
    ],
    "token_count":1,
    "processed_text":"thnk"
  },
  {
    "label":4,
    "text":"omg ye good see three make awesom great anim",
    "cleaned_text":"omg ye good see three make awesom great anim",
    "normalized_text":"omg ye good see three make awesom great anim",
    "tokens":[
      "omg",
      "ye",
      "good",
      "see",
      "three",
      "make",
      "awesom",
      "great",
      "anim"
    ],
    "token_count":9,
    "processed_text":"omg ye good see three make awesom great anim"
  },
  {
    "label":4,
    "text":"goe issu group plugin hard think would migrat sa stori plugin",
    "cleaned_text":"goe issu group plugin hard think would migrat sa stori plugin",
    "normalized_text":"goe issu group plugin hard think would migrat sa stori plugin",
    "tokens":[
      "goe",
      "issu",
      "group",
      "plugin",
      "hard",
      "think",
      "migrat",
      "sa",
      "stori",
      "plugin"
    ],
    "token_count":10,
    "processed_text":"goe issu group plugin hard think migrat sa stori plugin"
  },
  {
    "label":0,
    "text":"bb freddi voic annoy enjoy seri far though",
    "cleaned_text":"bb freddi voic annoy enjoy seri far though",
    "normalized_text":"bb freddi voic annoy enjoy seri far though",
    "tokens":[
      "bb",
      "freddi",
      "voic",
      "annoy",
      "enjoy",
      "seri",
      "far",
      "though"
    ],
    "token_count":8,
    "processed_text":"bb freddi voic annoy enjoy seri far though"
  },
  {
    "label":0,
    "text":"ate bradi mean u wont get anywher without whyd u pick himdi ask tast good",
    "cleaned_text":"ate bradi mean u wont get anywher without whyd u pick himdi ask tast good",
    "normalized_text":"ate bradi mean u wont get anywher without whyd u pick himdi ask tast good",
    "tokens":[
      "ate",
      "bradi",
      "mean",
      "wont",
      "get",
      "anywh",
      "without",
      "whyd",
      "pick",
      "himdi",
      "ask",
      "tast",
      "good"
    ],
    "token_count":13,
    "processed_text":"ate bradi mean wont get anywh without whyd pick himdi ask tast good"
  },
  {
    "label":0,
    "text":"grumpi one person said good morn back",
    "cleaned_text":"grumpi one person said good morn back",
    "normalized_text":"grumpi one person said good morn back",
    "tokens":[
      "grumpi",
      "one",
      "person",
      "said",
      "good",
      "morn",
      "back"
    ],
    "token_count":7,
    "processed_text":"grumpi one person said good morn back"
  },
  {
    "label":0,
    "text":"there fire build hope thing dont get burn",
    "cleaned_text":"there fire build hope thing dont get burn",
    "normalized_text":"there fire build hope thing dont get burn",
    "tokens":[
      "fire",
      "build",
      "hope",
      "thing",
      "dont",
      "get",
      "burn"
    ],
    "token_count":7,
    "processed_text":"fire build hope thing dont get burn"
  },
  {
    "label":0,
    "text":"dont feel good want go bed hw check mother job never done",
    "cleaned_text":"dont feel good want go bed hw check mother job never done",
    "normalized_text":"dont feel good want go bed hw check mother job never done",
    "tokens":[
      "dont",
      "feel",
      "good",
      "want",
      "go",
      "bed",
      "hw",
      "check",
      "mother",
      "job",
      "never",
      "done"
    ],
    "token_count":12,
    "processed_text":"dont feel good want go bed hw check mother job never done"
  },
  {
    "label":4,
    "text":"want bb g wifi bt amp possibl gp think bold good ive heard rumour newer model come",
    "cleaned_text":"want bb g wifi bt amp possibl gp think bold good ive heard rumour newer model come",
    "normalized_text":"want bb g wifi bt amp possibl gp think bold good ive heard rumour newer model come",
    "tokens":[
      "want",
      "bb",
      "wifi",
      "bt",
      "amp",
      "possibl",
      "gp",
      "think",
      "bold",
      "good",
      "ive",
      "heard",
      "rumour",
      "newer",
      "model",
      "come"
    ],
    "token_count":16,
    "processed_text":"want bb wifi bt amp possibl gp think bold good ive heard rumour newer model come"
  },
  {
    "label":4,
    "text":"dont like list peopl followfriday write nice littl tweet person pick",
    "cleaned_text":"dont like list peopl followfriday write nice littl tweet person pick",
    "normalized_text":"dont like list peopl followfriday write nice littl tweet person pick",
    "tokens":[
      "dont",
      "like",
      "list",
      "peopl",
      "followfriday",
      "write",
      "nice",
      "littl",
      "tweet",
      "person",
      "pick"
    ],
    "token_count":11,
    "processed_text":"dont like list peopl followfriday write nice littl tweet person pick"
  },
  {
    "label":0,
    "text":"lost cat kill two turtl fail real bad",
    "cleaned_text":"lost cat kill two turtl fail real bad",
    "normalized_text":"lost cat kill two turtl fail real bad",
    "tokens":[
      "lost",
      "cat",
      "kill",
      "two",
      "turtl",
      "fail",
      "real",
      "bad"
    ],
    "token_count":8,
    "processed_text":"lost cat kill two turtl fail real bad"
  },
  {
    "label":0,
    "text":"watch greek cant believ season end tonight bsb",
    "cleaned_text":"watch greek cant believ season end tonight bsb",
    "normalized_text":"watch greek cant believ season end tonight bsb",
    "tokens":[
      "watch",
      "greek",
      "cant",
      "believ",
      "season",
      "end",
      "tonight",
      "bsb"
    ],
    "token_count":8,
    "processed_text":"watch greek cant believ season end tonight bsb"
  },
  {
    "label":0,
    "text":"work drag roll half five",
    "cleaned_text":"work drag roll half five",
    "normalized_text":"work drag roll half five",
    "tokens":[
      "work",
      "drag",
      "roll",
      "half",
      "five"
    ],
    "token_count":5,
    "processed_text":"work drag roll half five"
  },
  {
    "label":4,
    "text":"woke cant believ tomorrow school weekend way short week day weekend",
    "cleaned_text":"woke cant believ tomorrow school weekend way short week day weekend",
    "normalized_text":"woke cant believ tomorrow school weekend way short week day weekend",
    "tokens":[
      "woke",
      "cant",
      "believ",
      "tomorrow",
      "school",
      "weekend",
      "way",
      "short",
      "week",
      "day",
      "weekend"
    ],
    "token_count":11,
    "processed_text":"woke cant believ tomorrow school weekend way short week day weekend"
  },
  {
    "label":4,
    "text":"check repliespeopl dont know followfridayd find cool",
    "cleaned_text":"check repliespeopl dont know followfridayd find cool",
    "normalized_text":"check repliespeopl dont know followfridayd find cool",
    "tokens":[
      "check",
      "repliespeopl",
      "dont",
      "know",
      "followfridayd",
      "find",
      "cool"
    ],
    "token_count":7,
    "processed_text":"check repliespeopl dont know followfridayd find cool"
  },
  {
    "label":0,
    "text":"ye god ye looooooooooooooooooooool need forum",
    "cleaned_text":"ye god ye looooooooooooooooooooool need forum",
    "normalized_text":"ye god ye looooooooooooooooooooool need forum",
    "tokens":[
      "ye",
      "god",
      "ye",
      "need",
      "forum"
    ],
    "token_count":5,
    "processed_text":"ye god ye need forum"
  },
  {
    "label":0,
    "text":"go pick batteri pack shame mm len hasnt come yet mth sinc order",
    "cleaned_text":"go pick batteri pack shame mm len hasnt come yet mth sinc order",
    "normalized_text":"go pick batteri pack shame mm len hasnt come yet mth sinc order",
    "tokens":[
      "go",
      "pick",
      "batteri",
      "pack",
      "shame",
      "mm",
      "len",
      "hasnt",
      "come",
      "yet",
      "mth",
      "sinc",
      "order"
    ],
    "token_count":13,
    "processed_text":"go pick batteri pack shame mm len hasnt come yet mth sinc order"
  },
  {
    "label":4,
    "text":"sigh would give beach somewher",
    "cleaned_text":"sigh would give beach somewher",
    "normalized_text":"sigh would give beach somewher",
    "tokens":[
      "sigh",
      "give",
      "beach",
      "somewh"
    ],
    "token_count":4,
    "processed_text":"sigh give beach somewh"
  },
  {
    "label":4,
    "text":"great show inde enjoy get better",
    "cleaned_text":"great show inde enjoy get better",
    "normalized_text":"great show inde enjoy get better",
    "tokens":[
      "great",
      "show",
      "ind",
      "enjoy",
      "get",
      "better"
    ],
    "token_count":6,
    "processed_text":"great show ind enjoy get better"
  },
  {
    "label":4,
    "text":"pokemon ftw curri pretti yummi hurri wrote name sand beach hour ago ill upload photo",
    "cleaned_text":"pokemon ftw curri pretti yummi hurri wrote name sand beach hour ago ill upload photo",
    "normalized_text":"pokemon ftw curri pretti yummi hurri wrote name sand beach hour ago ill upload photo",
    "tokens":[
      "pokemon",
      "ftw",
      "curri",
      "pretti",
      "yummi",
      "hurri",
      "wrote",
      "name",
      "sand",
      "beach",
      "hour",
      "ago",
      "ill",
      "upload",
      "photo"
    ],
    "token_count":15,
    "processed_text":"pokemon ftw curri pretti yummi hurri wrote name sand beach hour ago ill upload photo"
  },
  {
    "label":4,
    "text":"thank madison good came big soggi guess expect w deliveri",
    "cleaned_text":"thank madison good came big soggi guess expect w deliveri",
    "normalized_text":"thank madison good came big soggi guess expect w deliveri",
    "tokens":[
      "thank",
      "madison",
      "good",
      "came",
      "big",
      "soggi",
      "guess",
      "expect",
      "deliveri"
    ],
    "token_count":9,
    "processed_text":"thank madison good came big soggi guess expect deliveri"
  },
  {
    "label":0,
    "text":"hiya she txting still hosp unfortun got infect post op bleed she fed",
    "cleaned_text":"hiya she txting still hosp unfortun got infect post op bleed she fed",
    "normalized_text":"hiya she txting still hosp unfortun got infect post op bleed she fed",
    "tokens":[
      "hiya",
      "txting",
      "still",
      "hosp",
      "unfortun",
      "got",
      "infect",
      "post",
      "op",
      "bleed",
      "fed"
    ],
    "token_count":11,
    "processed_text":"hiya txting still hosp unfortun got infect post op bleed fed"
  },
  {
    "label":0,
    "text":"would love way get",
    "cleaned_text":"would love way get",
    "normalized_text":"would love way get",
    "tokens":[
      "love",
      "way",
      "get"
    ],
    "token_count":3,
    "processed_text":"love way get"
  },
  {
    "label":0,
    "text":"yeah gotta say pretti shock hear rob hope doesnt come jame",
    "cleaned_text":"yeah gotta say pretti shock hear rob hope doesnt come jame",
    "normalized_text":"yeah gotta say pretti shock hear rob hope doesnt come jame",
    "tokens":[
      "yeah",
      "got",
      "ta",
      "say",
      "pretti",
      "shock",
      "hear",
      "rob",
      "hope",
      "doesnt",
      "come",
      "jame"
    ],
    "token_count":12,
    "processed_text":"yeah got ta say pretti shock hear rob hope doesnt come jame"
  },
  {
    "label":0,
    "text":"item sell sinc guess pic n mix longer avail",
    "cleaned_text":"item sell sinc guess pic n mix longer avail",
    "normalized_text":"item sell sinc guess pic n mix longer avail",
    "tokens":[
      "item",
      "sell",
      "sinc",
      "guess",
      "pic",
      "mix",
      "longer",
      "avail"
    ],
    "token_count":8,
    "processed_text":"item sell sinc guess pic mix longer avail"
  },
  {
    "label":4,
    "text":"citi way mall",
    "cleaned_text":"citi way mall",
    "normalized_text":"citi way mall",
    "tokens":[
      "citi",
      "way",
      "mall"
    ],
    "token_count":3,
    "processed_text":"citi way mall"
  },
  {
    "label":4,
    "text":"thankyou help convinc skype todayy knowwhil fli lol",
    "cleaned_text":"thankyou help convinc skype todayy knowwhil fli lol",
    "normalized_text":"thankyou help convinc skype todayy knowwhil fli lol",
    "tokens":[
      "thankyou",
      "help",
      "convinc",
      "skype",
      "todayi",
      "knowwhil",
      "fli",
      "lol"
    ],
    "token_count":8,
    "processed_text":"thankyou help convinc skype todayi knowwhil fli lol"
  },
  {
    "label":0,
    "text":"got cold first day summer holiday",
    "cleaned_text":"got cold first day summer holiday",
    "normalized_text":"got cold first day summer holiday",
    "tokens":[
      "got",
      "cold",
      "first",
      "day",
      "summer",
      "holiday"
    ],
    "token_count":6,
    "processed_text":"got cold first day summer holiday"
  },
  {
    "label":4,
    "text":"nice articl check wpsyntax plugin let post syntax highlight php wordpress",
    "cleaned_text":"nice articl check wpsyntax plugin let post syntax highlight php wordpress",
    "normalized_text":"nice articl check wpsyntax plugin let post syntax highlight php wordpress",
    "tokens":[
      "nice",
      "articl",
      "check",
      "wpsyntax",
      "plugin",
      "let",
      "post",
      "syntax",
      "highlight",
      "php",
      "wordpress"
    ],
    "token_count":11,
    "processed_text":"nice articl check wpsyntax plugin let post syntax highlight php wordpress"
  },
  {
    "label":0,
    "text":"okay hate whoever decid teach math school hate whoever decid make compulsori",
    "cleaned_text":"okay hate whoever decid teach math school hate whoever decid make compulsori",
    "normalized_text":"okay hate whoever decid teach math school hate whoever decid make compulsori",
    "tokens":[
      "okay",
      "hate",
      "whoever",
      "decid",
      "teach",
      "math",
      "school",
      "hate",
      "whoever",
      "decid",
      "make",
      "compulsori"
    ],
    "token_count":12,
    "processed_text":"okay hate whoever decid teach math school hate whoever decid make compulsori"
  },
  {
    "label":0,
    "text":"wonder add heap follow categori check later date see even",
    "cleaned_text":"wonder add heap follow categori check later date see even",
    "normalized_text":"wonder add heap follow categori check later date see even",
    "tokens":[
      "wonder",
      "add",
      "heap",
      "follow",
      "categori",
      "check",
      "later",
      "date",
      "see",
      "even"
    ],
    "token_count":10,
    "processed_text":"wonder add heap follow categori check later date see even"
  },
  {
    "label":4,
    "text":"midst organis group activ win win win also like bailey amp icecream mysteri",
    "cleaned_text":"midst organis group activ win win win also like bailey amp icecream mysteri",
    "normalized_text":"midst organis group activ win win win also like bailey amp icecream mysteri",
    "tokens":[
      "midst",
      "organi",
      "group",
      "activ",
      "win",
      "win",
      "win",
      "also",
      "like",
      "bailey",
      "amp",
      "icecream",
      "mysteri"
    ],
    "token_count":13,
    "processed_text":"midst organi group activ win win win also like bailey amp icecream mysteri"
  },
  {
    "label":0,
    "text":"wanna get swag dnt wanna drink alon what fun",
    "cleaned_text":"wanna get swag dnt wanna drink alon what fun",
    "normalized_text":"wanna get swag dnt wanna drink alon what fun",
    "tokens":[
      "wan",
      "na",
      "get",
      "swag",
      "dnt",
      "wan",
      "na",
      "drink",
      "alon",
      "fun"
    ],
    "token_count":10,
    "processed_text":"wan na get swag dnt wan na drink alon fun"
  },
  {
    "label":0,
    "text":"want",
    "cleaned_text":"want",
    "normalized_text":"want",
    "tokens":[
      "want"
    ],
    "token_count":1,
    "processed_text":"want"
  },
  {
    "label":0,
    "text":"tri watch celt u realli hate basketbal lol sorri still luv u",
    "cleaned_text":"tri watch celt u realli hate basketbal lol sorri still luv u",
    "normalized_text":"tri watch celt u realli hate basketbal lol sorri still luv u",
    "tokens":[
      "tri",
      "watch",
      "celt",
      "realli",
      "hate",
      "basketb",
      "lol",
      "sorri",
      "still",
      "luv"
    ],
    "token_count":10,
    "processed_text":"tri watch celt realli hate basketb lol sorri still luv"
  },
  {
    "label":4,
    "text":"dana last day school grad mon eng present librari fine pay amp detent serv thendon",
    "cleaned_text":"dana last day school grad mon eng present librari fine pay amp detent serv thendon",
    "normalized_text":"dana last day school grad mon eng present librari fine pay amp detent serv thendon",
    "tokens":[
      "dana",
      "last",
      "day",
      "school",
      "grad",
      "mon",
      "eng",
      "present",
      "librari",
      "fine",
      "pay",
      "amp",
      "detent",
      "serv",
      "thendon"
    ],
    "token_count":15,
    "processed_text":"dana last day school grad mon eng present librari fine pay amp detent serv thendon"
  },
  {
    "label":4,
    "text":"life center around trash day everi mon eveclean bird cage clean frig carri recycl trash",
    "cleaned_text":"life center around trash day everi mon eveclean bird cage clean frig carri recycl trash",
    "normalized_text":"life center around trash day everi mon eveclean bird cage clean frig carri recycl trash",
    "tokens":[
      "life",
      "center",
      "around",
      "trash",
      "day",
      "everi",
      "mon",
      "eveclean",
      "bird",
      "cage",
      "clean",
      "frig",
      "carri",
      "recycl",
      "trash"
    ],
    "token_count":15,
    "processed_text":"life center around trash day everi mon eveclean bird cage clean frig carri recycl trash"
  },
  {
    "label":4,
    "text":"one excit tmr hahaha",
    "cleaned_text":"one excit tmr hahaha",
    "normalized_text":"one excit tmr hahaha",
    "tokens":[
      "one",
      "excit",
      "tmr",
      "hahaha"
    ],
    "token_count":4,
    "processed_text":"one excit tmr hahaha"
  },
  {
    "label":4,
    "text":"trust mani blog handl right",
    "cleaned_text":"trust mani blog handl right",
    "normalized_text":"trust mani blog handl right",
    "tokens":[
      "trust",
      "mani",
      "blog",
      "handl",
      "right"
    ],
    "token_count":5,
    "processed_text":"trust mani blog handl right"
  },
  {
    "label":4,
    "text":"good night everi one catch tomorrow befor go work yippe work cant wait need someth",
    "cleaned_text":"good night everi one catch tomorrow befor go work yippe work cant wait need someth",
    "normalized_text":"good night everi one catch tomorrow befor go work yippe work cant wait need someth",
    "tokens":[
      "good",
      "night",
      "everi",
      "one",
      "catch",
      "tomorrow",
      "befor",
      "go",
      "work",
      "yipp",
      "work",
      "cant",
      "wait",
      "need",
      "someth"
    ],
    "token_count":15,
    "processed_text":"good night everi one catch tomorrow befor go work yipp work cant wait need someth"
  },
  {
    "label":0,
    "text":"lighthous cooki today decor humid littl bugger never dri",
    "cleaned_text":"lighthous cooki today decor humid littl bugger never dri",
    "normalized_text":"lighthous cooki today decor humid littl bugger never dri",
    "tokens":[
      "lighthou",
      "cooki",
      "today",
      "decor",
      "humid",
      "littl",
      "bugger",
      "never",
      "dri"
    ],
    "token_count":9,
    "processed_text":"lighthou cooki today decor humid littl bugger never dri"
  },
  {
    "label":4,
    "text":"ok ok quotdeep breathsquot im calm",
    "cleaned_text":"ok ok quotdeep breathsquot im calm",
    "normalized_text":"ok ok quotdeep breathsquot im calm",
    "tokens":[
      "ok",
      "ok",
      "quotdeep",
      "breathsquot",
      "im",
      "calm"
    ],
    "token_count":6,
    "processed_text":"ok ok quotdeep breathsquot im calm"
  },
  {
    "label":0,
    "text":"got phone call hospit appar dad heart attack last night move he well",
    "cleaned_text":"got phone call hospit appar dad heart attack last night move he well",
    "normalized_text":"got phone call hospit appar dad heart attack last night move he well",
    "tokens":[
      "got",
      "phone",
      "call",
      "hospit",
      "appar",
      "dad",
      "heart",
      "attack",
      "last",
      "night",
      "move",
      "well"
    ],
    "token_count":12,
    "processed_text":"got phone call hospit appar dad heart attack last night move well"
  },
  {
    "label":0,
    "text":"thank updat everyon must busi actual talk bother us peopl",
    "cleaned_text":"thank updat everyon must busi actual talk bother us peopl",
    "normalized_text":"thank updat everyon must busi actual talk bother us peopl",
    "tokens":[
      "thank",
      "updat",
      "everyon",
      "busi",
      "actual",
      "talk",
      "bother",
      "us",
      "peopl"
    ],
    "token_count":9,
    "processed_text":"thank updat everyon busi actual talk bother us peopl"
  },
  {
    "label":0,
    "text":"jailbreak lol bore cours",
    "cleaned_text":"jailbreak lol bore cours",
    "normalized_text":"jailbreak lol bore cours",
    "tokens":[
      "jailbreak",
      "lol",
      "bore",
      "cour"
    ],
    "token_count":4,
    "processed_text":"jailbreak lol bore cour"
  },
  {
    "label":0,
    "text":"oh wait better small img",
    "cleaned_text":"oh wait better small img",
    "normalized_text":"oh wait better small img",
    "tokens":[
      "oh",
      "wait",
      "better",
      "small",
      "img"
    ],
    "token_count":5,
    "processed_text":"oh wait better small img"
  },
  {
    "label":4,
    "text":"woke need sleep",
    "cleaned_text":"woke need sleep",
    "normalized_text":"woke need sleep",
    "tokens":[
      "woke",
      "need",
      "sleep"
    ],
    "token_count":3,
    "processed_text":"woke need sleep"
  },
  {
    "label":0,
    "text":"reping lol click ye reliz pic though real haha im playin",
    "cleaned_text":"reping lol click ye reliz pic though real haha im playin",
    "normalized_text":"reping lol click ye reliz pic though real haha im playin",
    "tokens":[
      "repe",
      "lol",
      "click",
      "ye",
      "reliz",
      "pic",
      "though",
      "real",
      "haha",
      "im",
      "playin"
    ],
    "token_count":11,
    "processed_text":"repe lol click ye reliz pic though real haha im playin"
  },
  {
    "label":0,
    "text":"dont feel good wish could take nap much",
    "cleaned_text":"dont feel good wish could take nap much",
    "normalized_text":"dont feel good wish could take nap much",
    "tokens":[
      "dont",
      "feel",
      "good",
      "wish",
      "take",
      "nap",
      "much"
    ],
    "token_count":7,
    "processed_text":"dont feel good wish take nap much"
  },
  {
    "label":0,
    "text":"confus twitter realli work",
    "cleaned_text":"confus twitter realli work",
    "normalized_text":"confus twitter realli work",
    "tokens":[
      "confu",
      "twitter",
      "realli",
      "work"
    ],
    "token_count":4,
    "processed_text":"confu twitter realli work"
  },
  {
    "label":0,
    "text":"three asham come parti miss",
    "cleaned_text":"three asham come parti miss",
    "normalized_text":"three asham come parti miss",
    "tokens":[
      "three",
      "asham",
      "come",
      "parti",
      "miss"
    ],
    "token_count":5,
    "processed_text":"three asham come parti miss"
  },
  {
    "label":4,
    "text":"there someth bowi morn",
    "cleaned_text":"there someth bowi morn",
    "normalized_text":"there someth bowi morn",
    "tokens":[
      "someth",
      "bowi",
      "morn"
    ],
    "token_count":3,
    "processed_text":"someth bowi morn"
  },
  {
    "label":4,
    "text":"eat watch bloodi valentin",
    "cleaned_text":"eat watch bloodi valentin",
    "normalized_text":"eat watch bloodi valentin",
    "tokens":[
      "eat",
      "watch",
      "bloodi",
      "valentin"
    ],
    "token_count":4,
    "processed_text":"eat watch bloodi valentin"
  },
  {
    "label":4,
    "text":"ohh icic sound good go food right winter alreadi summer didnt even start yet",
    "cleaned_text":"ohh icic sound good go food right winter alreadi summer didnt even start yet",
    "normalized_text":"ohh icic sound good go food right winter alreadi summer didnt even start yet",
    "tokens":[
      "ohh",
      "icic",
      "sound",
      "good",
      "go",
      "food",
      "right",
      "winter",
      "alreadi",
      "summer",
      "didnt",
      "even",
      "start",
      "yet"
    ],
    "token_count":14,
    "processed_text":"ohh icic sound good go food right winter alreadi summer didnt even start yet"
  },
  {
    "label":4,
    "text":"contthat sooo unbeliev hilariousr laugh time hat u man u",
    "cleaned_text":"contthat sooo unbeliev hilariousr laugh time hat u man u",
    "normalized_text":"contthat sooo unbeliev hilariousr laugh time hat u man u",
    "tokens":[
      "contthat",
      "sooo",
      "unbeliev",
      "hilariousr",
      "laugh",
      "time",
      "hat",
      "man"
    ],
    "token_count":8,
    "processed_text":"contthat sooo unbeliev hilariousr laugh time hat man"
  },
  {
    "label":4,
    "text":"everybodi save good money cash around",
    "cleaned_text":"everybodi save good money cash around",
    "normalized_text":"everybodi save good money cash around",
    "tokens":[
      "everybodi",
      "save",
      "good",
      "money",
      "cash",
      "around"
    ],
    "token_count":6,
    "processed_text":"everybodi save good money cash around"
  },
  {
    "label":0,
    "text":"yeah serious soderl guy anyway",
    "cleaned_text":"yeah serious soderl guy anyway",
    "normalized_text":"yeah serious soderl guy anyway",
    "tokens":[
      "yeah",
      "seriou",
      "soderl",
      "guy",
      "anyway"
    ],
    "token_count":5,
    "processed_text":"yeah seriou soderl guy anyway"
  },
  {
    "label":0,
    "text":"yeah bad itll song gonna sing im grate anythin amp itll st time see band wo leroi",
    "cleaned_text":"yeah bad itll song gonna sing im grate anythin amp itll st time see band wo leroi",
    "normalized_text":"yeah bad itll song gonna sing im grate anythin amp itll st time see band wo leroi",
    "tokens":[
      "yeah",
      "bad",
      "itll",
      "song",
      "gon",
      "na",
      "sing",
      "im",
      "grate",
      "anythin",
      "amp",
      "itll",
      "st",
      "time",
      "see",
      "band",
      "wo",
      "leroi"
    ],
    "token_count":18,
    "processed_text":"yeah bad itll song gon na sing im grate anythin amp itll st time see band wo leroi"
  },
  {
    "label":0,
    "text":"injur knee race life worri wont ok lb amp importantli team rounder rd pass ibuprofen",
    "cleaned_text":"injur knee race life worri wont ok lb amp importantli team rounder rd pass ibuprofen",
    "normalized_text":"injur knee race life worri wont ok lb amp importantli team rounder rd pass ibuprofen",
    "tokens":[
      "injur",
      "knee",
      "race",
      "life",
      "worri",
      "wont",
      "ok",
      "lb",
      "amp",
      "importantli",
      "team",
      "rounder",
      "rd",
      "pass",
      "ibuprofen"
    ],
    "token_count":15,
    "processed_text":"injur knee race life worri wont ok lb amp importantli team rounder rd pass ibuprofen"
  },
  {
    "label":4,
    "text":"warm thank u followfriday luv hope u great weekend",
    "cleaned_text":"warm thank u followfriday luv hope u great weekend",
    "normalized_text":"warm thank u followfriday luv hope u great weekend",
    "tokens":[
      "warm",
      "thank",
      "followfriday",
      "luv",
      "hope",
      "great",
      "weekend"
    ],
    "token_count":7,
    "processed_text":"warm thank followfriday luv hope great weekend"
  },
  {
    "label":0,
    "text":"ignor readingle tweet didnt think could go",
    "cleaned_text":"ignor readingle tweet didnt think could go",
    "normalized_text":"ignor readingle tweet didnt think could go",
    "tokens":[
      "ignor",
      "readingl",
      "tweet",
      "didnt",
      "think",
      "go"
    ],
    "token_count":6,
    "processed_text":"ignor readingl tweet didnt think go"
  },
  {
    "label":0,
    "text":"add aunt got follow",
    "cleaned_text":"add aunt got follow",
    "normalized_text":"add aunt got follow",
    "tokens":[
      "add",
      "aunt",
      "got",
      "follow"
    ],
    "token_count":4,
    "processed_text":"add aunt got follow"
  },
  {
    "label":4,
    "text":"good morn germani today go open air friend",
    "cleaned_text":"good morn germani today go open air friend",
    "normalized_text":"good morn germani today go open air friend",
    "tokens":[
      "good",
      "morn",
      "germani",
      "today",
      "go",
      "open",
      "air",
      "friend"
    ],
    "token_count":8,
    "processed_text":"good morn germani today go open air friend"
  },
  {
    "label":0,
    "text":"lamb im sorri hear dissapointmentshey u yahoo messeng sn talk anytim",
    "cleaned_text":"lamb im sorri hear dissapointmentshey u yahoo messeng sn talk anytim",
    "normalized_text":"lamb im sorri hear dissapointmentshey u yahoo messeng sn talk anytim",
    "tokens":[
      "lamb",
      "im",
      "sorri",
      "hear",
      "yahoo",
      "messeng",
      "sn",
      "talk",
      "anytim"
    ],
    "token_count":9,
    "processed_text":"lamb im sorri hear yahoo messeng sn talk anytim"
  },
  {
    "label":4,
    "text":"read definit overr hope your great weekend",
    "cleaned_text":"read definit overr hope your great weekend",
    "normalized_text":"read definit overr hope your great weekend",
    "tokens":[
      "read",
      "definit",
      "overr",
      "hope",
      "great",
      "weekend"
    ],
    "token_count":6,
    "processed_text":"read definit overr hope great weekend"
  },
  {
    "label":4,
    "text":"morn unreal god good awesom cant wait tonight",
    "cleaned_text":"morn unreal god good awesom cant wait tonight",
    "normalized_text":"morn unreal god good awesom cant wait tonight",
    "tokens":[
      "morn",
      "unreal",
      "god",
      "good",
      "awesom",
      "cant",
      "wait",
      "tonight"
    ],
    "token_count":8,
    "processed_text":"morn unreal god good awesom cant wait tonight"
  },
  {
    "label":0,
    "text":"back fishless",
    "cleaned_text":"back fishless",
    "normalized_text":"back fishless",
    "tokens":[
      "back",
      "fishless"
    ],
    "token_count":2,
    "processed_text":"back fishless"
  },
  {
    "label":4,
    "text":"what hair ranita least im boy haha",
    "cleaned_text":"what hair ranita least im boy haha",
    "normalized_text":"what hair ranita least im boy haha",
    "tokens":[
      "hair",
      "ranita",
      "least",
      "im",
      "boy",
      "haha"
    ],
    "token_count":6,
    "processed_text":"hair ranita least im boy haha"
  },
  {
    "label":0,
    "text":"dont like peopl take bunch quizz facebook fill homepag cant believ tomorrow monday quot",
    "cleaned_text":"dont like peopl take bunch quizz facebook fill homepag cant believ tomorrow monday quot",
    "normalized_text":"dont like peopl take bunch quizz facebook fill homepag cant believ tomorrow monday quot",
    "tokens":[
      "dont",
      "like",
      "peopl",
      "take",
      "bunch",
      "quizz",
      "facebook",
      "fill",
      "homepag",
      "cant",
      "believ",
      "tomorrow",
      "monday",
      "quot"
    ],
    "token_count":14,
    "processed_text":"dont like peopl take bunch quizz facebook fill homepag cant believ tomorrow monday quot"
  },
  {
    "label":0,
    "text":"practic joke go wrong paul hous mate swap passport found checkin desk lol",
    "cleaned_text":"practic joke go wrong paul hous mate swap passport found checkin desk lol",
    "normalized_text":"practic joke go wrong paul hous mate swap passport found checkin desk lol",
    "tokens":[
      "practic",
      "joke",
      "go",
      "wrong",
      "paul",
      "hou",
      "mate",
      "swap",
      "passport",
      "found",
      "checkin",
      "desk",
      "lol"
    ],
    "token_count":13,
    "processed_text":"practic joke go wrong paul hou mate swap passport found checkin desk lol"
  },
  {
    "label":0,
    "text":"white dog sit near feet haha look dead",
    "cleaned_text":"white dog sit near feet haha look dead",
    "normalized_text":"white dog sit near feet haha look dead",
    "tokens":[
      "white",
      "dog",
      "sit",
      "near",
      "feet",
      "haha",
      "look",
      "dead"
    ],
    "token_count":8,
    "processed_text":"white dog sit near feet haha look dead"
  },
  {
    "label":0,
    "text":"send link wanna watch time miss last time send link",
    "cleaned_text":"send link wanna watch time miss last time send link",
    "normalized_text":"send link wanna watch time miss last time send link",
    "tokens":[
      "send",
      "link",
      "wan",
      "na",
      "watch",
      "time",
      "miss",
      "last",
      "time",
      "send",
      "link"
    ],
    "token_count":11,
    "processed_text":"send link wan na watch time miss last time send link"
  },
  {
    "label":0,
    "text":"correct exhaust realli wish could stay bed",
    "cleaned_text":"correct exhaust realli wish could stay bed",
    "normalized_text":"correct exhaust realli wish could stay bed",
    "tokens":[
      "correct",
      "exhaust",
      "realli",
      "wish",
      "stay",
      "bed"
    ],
    "token_count":6,
    "processed_text":"correct exhaust realli wish stay bed"
  },
  {
    "label":4,
    "text":"chillin home much wii bowl make arm sore",
    "cleaned_text":"chillin home much wii bowl make arm sore",
    "normalized_text":"chillin home much wii bowl make arm sore",
    "tokens":[
      "chillin",
      "home",
      "much",
      "wii",
      "bowl",
      "make",
      "arm",
      "sore"
    ],
    "token_count":8,
    "processed_text":"chillin home much wii bowl make arm sore"
  },
  {
    "label":0,
    "text":"im jarrod tri order pizza money",
    "cleaned_text":"im jarrod tri order pizza money",
    "normalized_text":"im jarrod tri order pizza money",
    "tokens":[
      "im",
      "jarrod",
      "tri",
      "order",
      "pizza",
      "money"
    ],
    "token_count":6,
    "processed_text":"im jarrod tri order pizza money"
  },
  {
    "label":4,
    "text":"smart idea loll fav tw show gonna bee hard lol neighbour",
    "cleaned_text":"smart idea loll fav tw show gonna bee hard lol neighbour",
    "normalized_text":"smart idea loll fav tw show gonna bee hard lol neighbour",
    "tokens":[
      "smart",
      "idea",
      "loll",
      "fav",
      "tw",
      "show",
      "gon",
      "na",
      "bee",
      "hard",
      "lol",
      "neighbour"
    ],
    "token_count":12,
    "processed_text":"smart idea loll fav tw show gon na bee hard lol neighbour"
  },
  {
    "label":0,
    "text":"wish could work home",
    "cleaned_text":"wish could work home",
    "normalized_text":"wish could work home",
    "tokens":[
      "wish",
      "work",
      "home"
    ],
    "token_count":3,
    "processed_text":"wish work home"
  },
  {
    "label":4,
    "text":"cold rock white chocol cooki cream ice cream mango ice cream macadamia nut win",
    "cleaned_text":"cold rock white chocol cooki cream ice cream mango ice cream macadamia nut win",
    "normalized_text":"cold rock white chocol cooki cream ice cream mango ice cream macadamia nut win",
    "tokens":[
      "cold",
      "rock",
      "white",
      "chocol",
      "cooki",
      "cream",
      "ice",
      "cream",
      "mango",
      "ice",
      "cream",
      "macadamia",
      "nut",
      "win"
    ],
    "token_count":14,
    "processed_text":"cold rock white chocol cooki cream ice cream mango ice cream macadamia nut win"
  },
  {
    "label":4,
    "text":"mmmm perhap could bring along choccoat coffe bean",
    "cleaned_text":"mmmm perhap could bring along choccoat coffe bean",
    "normalized_text":"mmmm perhap could bring along choccoat coffe bean",
    "tokens":[
      "mmmm",
      "perhap",
      "bring",
      "along",
      "choccoat",
      "coff",
      "bean"
    ],
    "token_count":7,
    "processed_text":"mmmm perhap bring along choccoat coff bean"
  },
  {
    "label":4,
    "text":"hi didnt see last friend good ill absolut see ive seen hana yori dango love lt",
    "cleaned_text":"hi didnt see last friend good ill absolut see ive seen hana yori dango love lt",
    "normalized_text":"hi didnt see last friend good ill absolut see ive seen hana yori dango love lt",
    "tokens":[
      "hi",
      "didnt",
      "see",
      "last",
      "friend",
      "good",
      "ill",
      "absolut",
      "see",
      "ive",
      "seen",
      "hana",
      "yori",
      "dango",
      "love",
      "lt"
    ],
    "token_count":16,
    "processed_text":"hi didnt see last friend good ill absolut see ive seen hana yori dango love lt"
  },
  {
    "label":0,
    "text":"still rain still bore still home ugh mayb tomorrow clear",
    "cleaned_text":"still rain still bore still home ugh mayb tomorrow clear",
    "normalized_text":"still rain still bore still home ugh mayb tomorrow clear",
    "tokens":[
      "still",
      "rain",
      "still",
      "bore",
      "still",
      "home",
      "ugh",
      "mayb",
      "tomorrow",
      "clear"
    ],
    "token_count":10,
    "processed_text":"still rain still bore still home ugh mayb tomorrow clear"
  },
  {
    "label":4,
    "text":"okay okay ive decid go vegetarian let see long last",
    "cleaned_text":"okay okay ive decid go vegetarian let see long last",
    "normalized_text":"okay okay ive decid go vegetarian let see long last",
    "tokens":[
      "okay",
      "okay",
      "ive",
      "decid",
      "go",
      "vegetarian",
      "let",
      "see",
      "long",
      "last"
    ],
    "token_count":10,
    "processed_text":"okay okay ive decid go vegetarian let see long last"
  },
  {
    "label":0,
    "text":"itsz rainin n parad fr damn tx asz weather",
    "cleaned_text":"itsz rainin n parad fr damn tx asz weather",
    "normalized_text":"itsz rainin n parad fr damn tx asz weather",
    "tokens":[
      "itsz",
      "rainin",
      "parad",
      "fr",
      "damn",
      "tx",
      "asz",
      "weather"
    ],
    "token_count":8,
    "processed_text":"itsz rainin parad fr damn tx asz weather"
  },
  {
    "label":0,
    "text":"finish eat im happi aaaaaaaaaand studi",
    "cleaned_text":"finish eat im happi aaaaaaaaaand studi",
    "normalized_text":"finish eat im happi aaaaaaaaaand studi",
    "tokens":[
      "finish",
      "eat",
      "im",
      "happi",
      "aaaaaaaaaand",
      "studi"
    ],
    "token_count":6,
    "processed_text":"finish eat im happi aaaaaaaaaand studi"
  },
  {
    "label":0,
    "text":"cricket nan birthday still sick happi x",
    "cleaned_text":"cricket nan birthday still sick happi x",
    "normalized_text":"cricket nan birthday still sick happi x",
    "tokens":[
      "cricket",
      "nan",
      "birthday",
      "still",
      "sick",
      "happi"
    ],
    "token_count":6,
    "processed_text":"cricket nan birthday still sick happi"
  },
  {
    "label":0,
    "text":"frake kitteh wake caus want go c idiot back blanki amp pokemonz p",
    "cleaned_text":"frake kitteh wake caus want go c idiot back blanki amp pokemonz p",
    "normalized_text":"frake kitteh wake caus want go c idiot back blanki amp pokemonz p",
    "tokens":[
      "frake",
      "kitteh",
      "wake",
      "cau",
      "want",
      "go",
      "idiot",
      "back",
      "blanki",
      "amp",
      "pokemonz"
    ],
    "token_count":11,
    "processed_text":"frake kitteh wake cau want go idiot back blanki amp pokemonz"
  },
  {
    "label":4,
    "text":"luv u guy bye",
    "cleaned_text":"luv u guy bye",
    "normalized_text":"luv u guy bye",
    "tokens":[
      "luv",
      "guy",
      "bye"
    ],
    "token_count":3,
    "processed_text":"luv guy bye"
  },
  {
    "label":0,
    "text":"okay soo wnna gt ver bs cnt cuz yu mean tht much friend sigh ps hush lol",
    "cleaned_text":"okay soo wnna gt ver bs cnt cuz yu mean tht much friend sigh ps hush lol",
    "normalized_text":"okay soo wnna gt ver bs cnt cuz yu mean tht much friend sigh ps hush lol",
    "tokens":[
      "okay",
      "soo",
      "wnna",
      "gt",
      "ver",
      "bs",
      "cnt",
      "cuz",
      "yu",
      "mean",
      "tht",
      "much",
      "friend",
      "sigh",
      "ps",
      "hush",
      "lol"
    ],
    "token_count":17,
    "processed_text":"okay soo wnna gt ver bs cnt cuz yu mean tht much friend sigh ps hush lol"
  },
  {
    "label":4,
    "text":"thnx guy gtlt back",
    "cleaned_text":"thnx guy gtlt back",
    "normalized_text":"thnx guy gtlt back",
    "tokens":[
      "thnx",
      "guy",
      "gtlt",
      "back"
    ],
    "token_count":4,
    "processed_text":"thnx guy gtlt back"
  },
  {
    "label":4,
    "text":"wish could see guy perform song love movi think ill watch tonight",
    "cleaned_text":"wish could see guy perform song love movi think ill watch tonight",
    "normalized_text":"wish could see guy perform song love movi think ill watch tonight",
    "tokens":[
      "wish",
      "see",
      "guy",
      "perform",
      "song",
      "love",
      "movi",
      "think",
      "ill",
      "watch",
      "tonight"
    ],
    "token_count":11,
    "processed_text":"wish see guy perform song love movi think ill watch tonight"
  },
  {
    "label":4,
    "text":"lose realli never im social media whore everyon els",
    "cleaned_text":"lose realli never im social media whore everyon els",
    "normalized_text":"lose realli never im social media whore everyon els",
    "tokens":[
      "lose",
      "realli",
      "never",
      "im",
      "social",
      "media",
      "whore",
      "everyon",
      "el"
    ],
    "token_count":9,
    "processed_text":"lose realli never im social media whore everyon el"
  },
  {
    "label":4,
    "text":"read say nice thing jewelleri make malarki yay happi first custom",
    "cleaned_text":"read say nice thing jewelleri make malarki yay happi first custom",
    "normalized_text":"read say nice thing jewelleri make malarki yay happi first custom",
    "tokens":[
      "read",
      "say",
      "nice",
      "thing",
      "jewelleri",
      "make",
      "malarki",
      "yay",
      "happi",
      "first",
      "custom"
    ],
    "token_count":11,
    "processed_text":"read say nice thing jewelleri make malarki yay happi first custom"
  },
  {
    "label":4,
    "text":"wink offer look awesom that fun email get monday",
    "cleaned_text":"wink offer look awesom that fun email get monday",
    "normalized_text":"wink offer look awesom that fun email get monday",
    "tokens":[
      "wink",
      "offer",
      "look",
      "awesom",
      "fun",
      "email",
      "get",
      "monday"
    ],
    "token_count":8,
    "processed_text":"wink offer look awesom fun email get monday"
  },
  {
    "label":4,
    "text":"move couchamp matress new pad cajun crab irvin amp final sleep babyy",
    "cleaned_text":"move couchamp matress new pad cajun crab irvin amp final sleep babyy",
    "normalized_text":"move couchamp matress new pad cajun crab irvin amp final sleep babyy",
    "tokens":[
      "move",
      "couchamp",
      "matress",
      "new",
      "pad",
      "cajun",
      "crab",
      "irvin",
      "amp",
      "final",
      "sleep",
      "babyy"
    ],
    "token_count":12,
    "processed_text":"move couchamp matress new pad cajun crab irvin amp final sleep babyy"
  },
  {
    "label":0,
    "text":"lay bed think talk mood",
    "cleaned_text":"lay bed think talk mood",
    "normalized_text":"lay bed think talk mood",
    "tokens":[
      "lay",
      "bed",
      "think",
      "talk",
      "mood"
    ],
    "token_count":5,
    "processed_text":"lay bed think talk mood"
  },
  {
    "label":4,
    "text":"think got old pink beetl",
    "cleaned_text":"think got old pink beetl",
    "normalized_text":"think got old pink beetl",
    "tokens":[
      "think",
      "got",
      "old",
      "pink",
      "beetl"
    ],
    "token_count":5,
    "processed_text":"think got old pink beetl"
  },
  {
    "label":0,
    "text":"miss terribl",
    "cleaned_text":"miss terribl",
    "normalized_text":"miss terribl",
    "tokens":[
      "miss",
      "terribl"
    ],
    "token_count":2,
    "processed_text":"miss terribl"
  },
  {
    "label":0,
    "text":"famili children kid returnjosh northcarolina parentalright injustic",
    "cleaned_text":"famili children kid returnjosh northcarolina parentalright injustic",
    "normalized_text":"famili children kid returnjosh northcarolina parentalright injustic",
    "tokens":[
      "famili",
      "children",
      "kid",
      "returnjosh",
      "northcarolina",
      "parentalright",
      "injust"
    ],
    "token_count":7,
    "processed_text":"famili children kid returnjosh northcarolina parentalright injust"
  },
  {
    "label":4,
    "text":"also think kirk said spock star trek movi still seem work twitter theori",
    "cleaned_text":"also think kirk said spock star trek movi still seem work twitter theori",
    "normalized_text":"also think kirk said spock star trek movi still seem work twitter theori",
    "tokens":[
      "also",
      "think",
      "kirk",
      "said",
      "spock",
      "star",
      "trek",
      "movi",
      "still",
      "seem",
      "work",
      "twitter",
      "theori"
    ],
    "token_count":13,
    "processed_text":"also think kirk said spock star trek movi still seem work twitter theori"
  },
  {
    "label":4,
    "text":"plan parti jessica even though octob",
    "cleaned_text":"plan parti jessica even though octob",
    "normalized_text":"plan parti jessica even though octob",
    "tokens":[
      "plan",
      "parti",
      "jessica",
      "even",
      "though",
      "octob"
    ],
    "token_count":6,
    "processed_text":"plan parti jessica even though octob"
  },
  {
    "label":4,
    "text":"ent look band play fresher week septemb fresher event decid excit time",
    "cleaned_text":"ent look band play fresher week septemb fresher event decid excit time",
    "normalized_text":"ent look band play fresher week septemb fresher event decid excit time",
    "tokens":[
      "ent",
      "look",
      "band",
      "play",
      "fresher",
      "week",
      "septemb",
      "fresher",
      "event",
      "decid",
      "excit",
      "time"
    ],
    "token_count":12,
    "processed_text":"ent look band play fresher week septemb fresher event decid excit time"
  },
  {
    "label":4,
    "text":"gmorn everyon",
    "cleaned_text":"gmorn everyon",
    "normalized_text":"gmorn everyon",
    "tokens":[
      "gmorn",
      "everyon"
    ],
    "token_count":2,
    "processed_text":"gmorn everyon"
  },
  {
    "label":4,
    "text":"loverli time",
    "cleaned_text":"loverli time",
    "normalized_text":"loverli time",
    "tokens":[
      "loverli",
      "time"
    ],
    "token_count":2,
    "processed_text":"loverli time"
  },
  {
    "label":0,
    "text":"math exam done miss elisa",
    "cleaned_text":"math exam done miss elisa",
    "normalized_text":"math exam done miss elisa",
    "tokens":[
      "math",
      "exam",
      "done",
      "miss",
      "elisa"
    ],
    "token_count":5,
    "processed_text":"math exam done miss elisa"
  },
  {
    "label":4,
    "text":"ive stay late im hungri see cereal futur couch pj oooh keep get better",
    "cleaned_text":"ive stay late im hungri see cereal futur couch pj oooh keep get better",
    "normalized_text":"ive stay late im hungri see cereal futur couch pj oooh keep get better",
    "tokens":[
      "ive",
      "stay",
      "late",
      "im",
      "hungri",
      "see",
      "cereal",
      "futur",
      "couch",
      "pj",
      "oooh",
      "keep",
      "get",
      "better"
    ],
    "token_count":14,
    "processed_text":"ive stay late im hungri see cereal futur couch pj oooh keep get better"
  },
  {
    "label":0,
    "text":"meant someth clever use lyric town fail send done",
    "cleaned_text":"meant someth clever use lyric town fail send done",
    "normalized_text":"meant someth clever use lyric town fail send done",
    "tokens":[
      "meant",
      "someth",
      "clever",
      "use",
      "lyric",
      "town",
      "fail",
      "send",
      "done"
    ],
    "token_count":9,
    "processed_text":"meant someth clever use lyric town fail send done"
  },
  {
    "label":0,
    "text":"fone wit work pralum lol hadnt spoken one day felt like year",
    "cleaned_text":"fone wit work pralum lol hadnt spoken one day felt like year",
    "normalized_text":"fone wit work pralum lol hadnt spoken one day felt like year",
    "tokens":[
      "fone",
      "wit",
      "work",
      "pralum",
      "lol",
      "hadnt",
      "spoken",
      "one",
      "day",
      "felt",
      "like",
      "year"
    ],
    "token_count":12,
    "processed_text":"fone wit work pralum lol hadnt spoken one day felt like year"
  },
  {
    "label":4,
    "text":"think abstract refer deep structur might use reconfigur citi",
    "cleaned_text":"think abstract refer deep structur might use reconfigur citi",
    "normalized_text":"think abstract refer deep structur might use reconfigur citi",
    "tokens":[
      "think",
      "abstract",
      "refer",
      "deep",
      "structur",
      "use",
      "reconfigur",
      "citi"
    ],
    "token_count":8,
    "processed_text":"think abstract refer deep structur use reconfigur citi"
  },
  {
    "label":0,
    "text":"plllleeeassee vote mtv movi award omgosh nervou wreck ahhh need time",
    "cleaned_text":"plllleeeassee vote mtv movi award omgosh nervou wreck ahhh need time",
    "normalized_text":"plllleeeassee vote mtv movi award omgosh nervou wreck ahhh need time",
    "tokens":[
      "plllleeeasse",
      "vote",
      "mtv",
      "movi",
      "award",
      "omgosh",
      "nervou",
      "wreck",
      "ahhh",
      "need",
      "time"
    ],
    "token_count":11,
    "processed_text":"plllleeeasse vote mtv movi award omgosh nervou wreck ahhh need time"
  },
  {
    "label":0,
    "text":"wowza got upmi alarm clock fail wake today",
    "cleaned_text":"wowza got upmi alarm clock fail wake today",
    "normalized_text":"wowza got upmi alarm clock fail wake today",
    "tokens":[
      "wowza",
      "got",
      "upmi",
      "alarm",
      "clock",
      "fail",
      "wake",
      "today"
    ],
    "token_count":8,
    "processed_text":"wowza got upmi alarm clock fail wake today"
  },
  {
    "label":4,
    "text":"happi birthday meeeeeeeeeeee open card present",
    "cleaned_text":"happi birthday meeeeeeeeeeee open card present",
    "normalized_text":"happi birthday meeeeeeeeeeee open card present",
    "tokens":[
      "happi",
      "birthday",
      "meeeeeeeeeeee",
      "open",
      "card",
      "present"
    ],
    "token_count":6,
    "processed_text":"happi birthday meeeeeeeeeeee open card present"
  },
  {
    "label":4,
    "text":"good morn twitter land im crave vitamin water earli oh",
    "cleaned_text":"good morn twitter land im crave vitamin water earli oh",
    "normalized_text":"good morn twitter land im crave vitamin water earli oh",
    "tokens":[
      "good",
      "morn",
      "twitter",
      "land",
      "im",
      "crave",
      "vitamin",
      "water",
      "earli",
      "oh"
    ],
    "token_count":10,
    "processed_text":"good morn twitter land im crave vitamin water earli oh"
  },
  {
    "label":0,
    "text":"miss wife take test tucson tomorrow wont see sunday night pray well",
    "cleaned_text":"miss wife take test tucson tomorrow wont see sunday night pray well",
    "normalized_text":"miss wife take test tucson tomorrow wont see sunday night pray well",
    "tokens":[
      "miss",
      "wife",
      "take",
      "test",
      "tucson",
      "tomorrow",
      "wont",
      "see",
      "sunday",
      "night",
      "pray",
      "well"
    ],
    "token_count":12,
    "processed_text":"miss wife take test tucson tomorrow wont see sunday night pray well"
  },
  {
    "label":4,
    "text":"watch movi work site stuff morn appoint woohoo",
    "cleaned_text":"watch movi work site stuff morn appoint woohoo",
    "normalized_text":"watch movi work site stuff morn appoint woohoo",
    "tokens":[
      "watch",
      "movi",
      "work",
      "site",
      "stuff",
      "morn",
      "appoint",
      "woohoo"
    ],
    "token_count":8,
    "processed_text":"watch movi work site stuff morn appoint woohoo"
  },
  {
    "label":0,
    "text":"bore workkkkkkkk",
    "cleaned_text":"bore workkkkkkkk",
    "normalized_text":"bore workkkkkkkk",
    "tokens":[
      "bore",
      "workkkkkkkk"
    ],
    "token_count":2,
    "processed_text":"bore workkkkkkkk"
  },
  {
    "label":0,
    "text":"enjoy last day summer",
    "cleaned_text":"enjoy last day summer",
    "normalized_text":"enjoy last day summer",
    "tokens":[
      "enjoy",
      "last",
      "day",
      "summer"
    ],
    "token_count":4,
    "processed_text":"enjoy last day summer"
  },
  {
    "label":4,
    "text":"start year watch german bigbroth like",
    "cleaned_text":"start year watch german bigbroth like",
    "normalized_text":"start year watch german bigbroth like",
    "tokens":[
      "start",
      "year",
      "watch",
      "german",
      "bigbroth",
      "like"
    ],
    "token_count":6,
    "processed_text":"start year watch german bigbroth like"
  },
  {
    "label":0,
    "text":"poor boy sick",
    "cleaned_text":"poor boy sick",
    "normalized_text":"poor boy sick",
    "tokens":[
      "poor",
      "boy",
      "sick"
    ],
    "token_count":3,
    "processed_text":"poor boy sick"
  },
  {
    "label":4,
    "text":"miley cyru mitchel musso duet welcom hollywood rarther nice",
    "cleaned_text":"miley cyru mitchel musso duet welcom hollywood rarther nice",
    "normalized_text":"miley cyru mitchel musso duet welcom hollywood rarther nice",
    "tokens":[
      "miley",
      "cyru",
      "mitchel",
      "musso",
      "duet",
      "welcom",
      "hollywood",
      "rarther",
      "nice"
    ],
    "token_count":9,
    "processed_text":"miley cyru mitchel musso duet welcom hollywood rarther nice"
  },
  {
    "label":0,
    "text":"wanna see",
    "cleaned_text":"wanna see",
    "normalized_text":"wanna see",
    "tokens":[
      "wan",
      "na",
      "see"
    ],
    "token_count":3,
    "processed_text":"wan na see"
  },
  {
    "label":4,
    "text":"peoni mother day great husband",
    "cleaned_text":"peoni mother day great husband",
    "normalized_text":"peoni mother day great husband",
    "tokens":[
      "peoni",
      "mother",
      "day",
      "great",
      "husband"
    ],
    "token_count":5,
    "processed_text":"peoni mother day great husband"
  },
  {
    "label":4,
    "text":"yeah order month one day im heavi movi watcher",
    "cleaned_text":"yeah order month one day im heavi movi watcher",
    "normalized_text":"yeah order month one day im heavi movi watcher",
    "tokens":[
      "yeah",
      "order",
      "month",
      "one",
      "day",
      "im",
      "heavi",
      "movi",
      "watcher"
    ],
    "token_count":9,
    "processed_text":"yeah order month one day im heavi movi watcher"
  },
  {
    "label":0,
    "text":"sayang nga hay mahal pa naman ang memori card ngayon anoth expens na naman ito lol",
    "cleaned_text":"sayang nga hay mahal pa naman ang memori card ngayon anoth expens na naman ito lol",
    "normalized_text":"sayang nga hay mahal pa naman ang memori card ngayon anoth expens na naman ito lol",
    "tokens":[
      "sayang",
      "nga",
      "hay",
      "mahal",
      "pa",
      "naman",
      "ang",
      "memori",
      "card",
      "ngayon",
      "anoth",
      "expen",
      "na",
      "naman",
      "ito",
      "lol"
    ],
    "token_count":16,
    "processed_text":"sayang nga hay mahal pa naman ang memori card ngayon anoth expen na naman ito lol"
  },
  {
    "label":0,
    "text":"shower iron work",
    "cleaned_text":"shower iron work",
    "normalized_text":"shower iron work",
    "tokens":[
      "shower",
      "iron",
      "work"
    ],
    "token_count":3,
    "processed_text":"shower iron work"
  },
  {
    "label":4,
    "text":"yum great guyand great dish",
    "cleaned_text":"yum great guyand great dish",
    "normalized_text":"yum great guyand great dish",
    "tokens":[
      "yum",
      "great",
      "guyand",
      "great",
      "dish"
    ],
    "token_count":5,
    "processed_text":"yum great guyand great dish"
  },
  {
    "label":0,
    "text":"finish evid law exam think ace one question fail",
    "cleaned_text":"finish evid law exam think ace one question fail",
    "normalized_text":"finish evid law exam think ace one question fail",
    "tokens":[
      "finish",
      "evid",
      "law",
      "exam",
      "think",
      "ace",
      "one",
      "question",
      "fail"
    ],
    "token_count":9,
    "processed_text":"finish evid law exam think ace one question fail"
  },
  {
    "label":0,
    "text":"next thesi paperwhi god invent idli",
    "cleaned_text":"next thesi paperwhi god invent idli",
    "normalized_text":"next thesi paperwhi god invent idli",
    "tokens":[
      "next",
      "thesi",
      "paperwhi",
      "god",
      "invent",
      "idli"
    ],
    "token_count":6,
    "processed_text":"next thesi paperwhi god invent idli"
  },
  {
    "label":0,
    "text":"pictur work",
    "cleaned_text":"pictur work",
    "normalized_text":"pictur work",
    "tokens":[
      "pictur",
      "work"
    ],
    "token_count":2,
    "processed_text":"pictur work"
  },
  {
    "label":0,
    "text":"must sent spain humid loud outsid wait net connect go",
    "cleaned_text":"must sent spain humid loud outsid wait net connect go",
    "normalized_text":"must sent spain humid loud outsid wait net connect go",
    "tokens":[
      "sent",
      "spain",
      "humid",
      "loud",
      "outsid",
      "wait",
      "net",
      "connect",
      "go"
    ],
    "token_count":9,
    "processed_text":"sent spain humid loud outsid wait net connect go"
  },
  {
    "label":0,
    "text":"omg neitherrrr headach bodi ach chill fever",
    "cleaned_text":"omg neitherrrr headach bodi ach chill fever",
    "normalized_text":"omg neitherrrr headach bodi ach chill fever",
    "tokens":[
      "omg",
      "neitherrrr",
      "headach",
      "bodi",
      "ach",
      "chill",
      "fever"
    ],
    "token_count":7,
    "processed_text":"omg neitherrrr headach bodi ach chill fever"
  },
  {
    "label":4,
    "text":"clean car pack work",
    "cleaned_text":"clean car pack work",
    "normalized_text":"clean car pack work",
    "tokens":[
      "clean",
      "car",
      "pack",
      "work"
    ],
    "token_count":4,
    "processed_text":"clean car pack work"
  },
  {
    "label":0,
    "text":"dont know im awak right half sleep im offici go sleep wake",
    "cleaned_text":"dont know im awak right half sleep im offici go sleep wake",
    "normalized_text":"dont know im awak right half sleep im offici go sleep wake",
    "tokens":[
      "dont",
      "know",
      "im",
      "awak",
      "right",
      "half",
      "sleep",
      "im",
      "offici",
      "go",
      "sleep",
      "wake"
    ],
    "token_count":12,
    "processed_text":"dont know im awak right half sleep im offici go sleep wake"
  },
  {
    "label":4,
    "text":"lol must idea gonna todaygtlol",
    "cleaned_text":"lol must idea gonna todaygtlol",
    "normalized_text":"lol must idea gonna todaygtlol",
    "tokens":[
      "lol",
      "idea",
      "gon",
      "na",
      "todaygtlol"
    ],
    "token_count":5,
    "processed_text":"lol idea gon na todaygtlol"
  },
  {
    "label":0,
    "text":"wish could ive studi week fr next week seminar ill give call least",
    "cleaned_text":"wish could ive studi week fr next week seminar ill give call least",
    "normalized_text":"wish could ive studi week fr next week seminar ill give call least",
    "tokens":[
      "wish",
      "ive",
      "studi",
      "week",
      "fr",
      "next",
      "week",
      "seminar",
      "ill",
      "give",
      "call",
      "least"
    ],
    "token_count":12,
    "processed_text":"wish ive studi week fr next week seminar ill give call least"
  },
  {
    "label":4,
    "text":"ye time diplomaci someth need practic",
    "cleaned_text":"ye time diplomaci someth need practic",
    "normalized_text":"ye time diplomaci someth need practic",
    "tokens":[
      "ye",
      "time",
      "diplomaci",
      "someth",
      "need",
      "practic"
    ],
    "token_count":6,
    "processed_text":"ye time diplomaci someth need practic"
  },
  {
    "label":4,
    "text":"anoth physic theatr perform thank follow",
    "cleaned_text":"anoth physic theatr perform thank follow",
    "normalized_text":"anoth physic theatr perform thank follow",
    "tokens":[
      "anoth",
      "physic",
      "theatr",
      "perform",
      "thank",
      "follow"
    ],
    "token_count":6,
    "processed_text":"anoth physic theatr perform thank follow"
  },
  {
    "label":4,
    "text":"night cycl tire day haha least im go tomorrow",
    "cleaned_text":"night cycl tire day haha least im go tomorrow",
    "normalized_text":"night cycl tire day haha least im go tomorrow",
    "tokens":[
      "night",
      "cycl",
      "tire",
      "day",
      "haha",
      "least",
      "im",
      "go",
      "tomorrow"
    ],
    "token_count":9,
    "processed_text":"night cycl tire day haha least im go tomorrow"
  },
  {
    "label":0,
    "text":"price right time go run",
    "cleaned_text":"price right time go run",
    "normalized_text":"price right time go run",
    "tokens":[
      "price",
      "right",
      "time",
      "go",
      "run"
    ],
    "token_count":5,
    "processed_text":"price right time go run"
  },
  {
    "label":0,
    "text":"woke rain abl anyth list",
    "cleaned_text":"woke rain abl anyth list",
    "normalized_text":"woke rain abl anyth list",
    "tokens":[
      "woke",
      "rain",
      "abl",
      "anyth",
      "list"
    ],
    "token_count":5,
    "processed_text":"woke rain abl anyth list"
  },
  {
    "label":0,
    "text":"oh tom arent repli anyon today",
    "cleaned_text":"oh tom arent repli anyon today",
    "normalized_text":"oh tom arent repli anyon today",
    "tokens":[
      "oh",
      "tom",
      "arent",
      "repli",
      "anyon",
      "today"
    ],
    "token_count":6,
    "processed_text":"oh tom arent repli anyon today"
  },
  {
    "label":4,
    "text":"good birthday far much food far mani cocktail feel liiiittl rough today thank messag",
    "cleaned_text":"good birthday far much food far mani cocktail feel liiiittl rough today thank messag",
    "normalized_text":"good birthday far much food far mani cocktail feel liiiittl rough today thank messag",
    "tokens":[
      "good",
      "birthday",
      "far",
      "much",
      "food",
      "far",
      "mani",
      "cocktail",
      "feel",
      "liiiittl",
      "rough",
      "today",
      "thank",
      "messag"
    ],
    "token_count":14,
    "processed_text":"good birthday far much food far mani cocktail feel liiiittl rough today thank messag"
  },
  {
    "label":0,
    "text":"sleep help ill hour thank love",
    "cleaned_text":"sleep help ill hour thank love",
    "normalized_text":"sleep help ill hour thank love",
    "tokens":[
      "sleep",
      "help",
      "ill",
      "hour",
      "thank",
      "love"
    ],
    "token_count":6,
    "processed_text":"sleep help ill hour thank love"
  },
  {
    "label":0,
    "text":"omg hasnt post yet",
    "cleaned_text":"omg hasnt post yet",
    "normalized_text":"omg hasnt post yet",
    "tokens":[
      "omg",
      "hasnt",
      "post",
      "yet"
    ],
    "token_count":4,
    "processed_text":"omg hasnt post yet"
  },
  {
    "label":4,
    "text":"must reststart new project friend go huge wish us lucklif love sportshav good night",
    "cleaned_text":"must reststart new project friend go huge wish us lucklif love sportshav good night",
    "normalized_text":"must reststart new project friend go huge wish us lucklif love sportshav good night",
    "tokens":[
      "reststart",
      "new",
      "project",
      "friend",
      "go",
      "huge",
      "wish",
      "us",
      "lucklif",
      "love",
      "sportshav",
      "good",
      "night"
    ],
    "token_count":13,
    "processed_text":"reststart new project friend go huge wish us lucklif love sportshav good night"
  },
  {
    "label":4,
    "text":"even outsid oh btw x bf appar invit dont know think would cool hang",
    "cleaned_text":"even outsid oh btw x bf appar invit dont know think would cool hang",
    "normalized_text":"even outsid oh btw x bf appar invit dont know think would cool hang",
    "tokens":[
      "even",
      "outsid",
      "oh",
      "btw",
      "bf",
      "appar",
      "invit",
      "dont",
      "know",
      "think",
      "cool",
      "hang"
    ],
    "token_count":12,
    "processed_text":"even outsid oh btw bf appar invit dont know think cool hang"
  },
  {
    "label":0,
    "text":"yep still havent forgiven kill max haha",
    "cleaned_text":"yep still havent forgiven kill max haha",
    "normalized_text":"yep still havent forgiven kill max haha",
    "tokens":[
      "yep",
      "still",
      "havent",
      "forgiven",
      "kill",
      "max",
      "haha"
    ],
    "token_count":7,
    "processed_text":"yep still havent forgiven kill max haha"
  },
  {
    "label":0,
    "text":"what lack male dancer manchest preston flood cant get singl one",
    "cleaned_text":"what lack male dancer manchest preston flood cant get singl one",
    "normalized_text":"what lack male dancer manchest preston flood cant get singl one",
    "tokens":[
      "lack",
      "male",
      "dancer",
      "manchest",
      "preston",
      "flood",
      "cant",
      "get",
      "singl",
      "one"
    ],
    "token_count":10,
    "processed_text":"lack male dancer manchest preston flood cant get singl one"
  },
  {
    "label":4,
    "text":"go movi tonight dad yay",
    "cleaned_text":"go movi tonight dad yay",
    "normalized_text":"go movi tonight dad yay",
    "tokens":[
      "go",
      "movi",
      "tonight",
      "dad",
      "yay"
    ],
    "token_count":5,
    "processed_text":"go movi tonight dad yay"
  },
  {
    "label":0,
    "text":"depress soo depress need someon talk",
    "cleaned_text":"depress soo depress need someon talk",
    "normalized_text":"depress soo depress need someon talk",
    "tokens":[
      "depress",
      "soo",
      "depress",
      "need",
      "someon",
      "talk"
    ],
    "token_count":6,
    "processed_text":"depress soo depress need someon talk"
  },
  {
    "label":0,
    "text":"neither im get bit worri",
    "cleaned_text":"neither im get bit worri",
    "normalized_text":"neither im get bit worri",
    "tokens":[
      "neither",
      "im",
      "get",
      "bit",
      "worri"
    ],
    "token_count":5,
    "processed_text":"neither im get bit worri"
  },
  {
    "label":0,
    "text":"instal servic web server setup account finish crash",
    "cleaned_text":"instal servic web server setup account finish crash",
    "normalized_text":"instal servic web server setup account finish crash",
    "tokens":[
      "instal",
      "servic",
      "web",
      "server",
      "setup",
      "account",
      "finish",
      "crash"
    ],
    "token_count":8,
    "processed_text":"instal servic web server setup account finish crash"
  },
  {
    "label":4,
    "text":"top great weekend motorcycl ride gatineau park",
    "cleaned_text":"top great weekend motorcycl ride gatineau park",
    "normalized_text":"top great weekend motorcycl ride gatineau park",
    "tokens":[
      "top",
      "great",
      "weekend",
      "motorcycl",
      "ride",
      "gatineau",
      "park"
    ],
    "token_count":7,
    "processed_text":"top great weekend motorcycl ride gatineau park"
  },
  {
    "label":4,
    "text":"safe trip back home great see friday night dont stranger ok",
    "cleaned_text":"safe trip back home great see friday night dont stranger ok",
    "normalized_text":"safe trip back home great see friday night dont stranger ok",
    "tokens":[
      "safe",
      "trip",
      "back",
      "home",
      "great",
      "see",
      "friday",
      "night",
      "dont",
      "stranger",
      "ok"
    ],
    "token_count":11,
    "processed_text":"safe trip back home great see friday night dont stranger ok"
  },
  {
    "label":4,
    "text":"go sleepp exam morn goodnight xo",
    "cleaned_text":"go sleepp exam morn goodnight xo",
    "normalized_text":"go sleepp exam morn goodnight xo",
    "tokens":[
      "go",
      "sleepp",
      "exam",
      "morn",
      "goodnight",
      "xo"
    ],
    "token_count":6,
    "processed_text":"go sleepp exam morn goodnight xo"
  },
  {
    "label":4,
    "text":"road dalla oh hello",
    "cleaned_text":"road dalla oh hello",
    "normalized_text":"road dalla oh hello",
    "tokens":[
      "road",
      "dalla",
      "oh",
      "hello"
    ],
    "token_count":4,
    "processed_text":"road dalla oh hello"
  },
  {
    "label":0,
    "text":"feel well blah",
    "cleaned_text":"feel well blah",
    "normalized_text":"feel well blah",
    "tokens":[
      "feel",
      "well",
      "blah"
    ],
    "token_count":3,
    "processed_text":"feel well blah"
  },
  {
    "label":0,
    "text":"go work hour",
    "cleaned_text":"go work hour",
    "normalized_text":"go work hour",
    "tokens":[
      "go",
      "work",
      "hour"
    ],
    "token_count":3,
    "processed_text":"go work hour"
  },
  {
    "label":0,
    "text":"start sleep later get closer school time that alway happen least",
    "cleaned_text":"start sleep later get closer school time that alway happen least",
    "normalized_text":"start sleep later get closer school time that alway happen least",
    "tokens":[
      "start",
      "sleep",
      "later",
      "get",
      "closer",
      "school",
      "time",
      "alway",
      "happen",
      "least"
    ],
    "token_count":10,
    "processed_text":"start sleep later get closer school time alway happen least"
  },
  {
    "label":4,
    "text":"final ibm zo parallel sysplex oper scenario publish friday",
    "cleaned_text":"final ibm zo parallel sysplex oper scenario publish friday",
    "normalized_text":"final ibm zo parallel sysplex oper scenario publish friday",
    "tokens":[
      "final",
      "ibm",
      "zo",
      "parallel",
      "sysplex",
      "oper",
      "scenario",
      "publish",
      "friday"
    ],
    "token_count":9,
    "processed_text":"final ibm zo parallel sysplex oper scenario publish friday"
  },
  {
    "label":4,
    "text":"good mornoon",
    "cleaned_text":"good mornoon",
    "normalized_text":"good mornoon",
    "tokens":[
      "good",
      "mornoon"
    ],
    "token_count":2,
    "processed_text":"good mornoon"
  },
  {
    "label":0,
    "text":"ahn school pleas keep everyon safe",
    "cleaned_text":"ahn school pleas keep everyon safe",
    "normalized_text":"ahn school pleas keep everyon safe",
    "tokens":[
      "ahn",
      "school",
      "plea",
      "keep",
      "everyon",
      "safe"
    ],
    "token_count":6,
    "processed_text":"ahn school plea keep everyon safe"
  },
  {
    "label":4,
    "text":"walk chelsea home",
    "cleaned_text":"walk chelsea home",
    "normalized_text":"walk chelsea home",
    "tokens":[
      "walk",
      "chelsea",
      "home"
    ],
    "token_count":3,
    "processed_text":"walk chelsea home"
  },
  {
    "label":0,
    "text":"back still stiff milk tea",
    "cleaned_text":"back still stiff milk tea",
    "normalized_text":"back still stiff milk tea",
    "tokens":[
      "back",
      "still",
      "stiff",
      "milk",
      "tea"
    ],
    "token_count":5,
    "processed_text":"back still stiff milk tea"
  },
  {
    "label":0,
    "text":"decid tell switch pic good idea amp cant get pic lol",
    "cleaned_text":"decid tell switch pic good idea amp cant get pic lol",
    "normalized_text":"decid tell switch pic good idea amp cant get pic lol",
    "tokens":[
      "decid",
      "tell",
      "switch",
      "pic",
      "good",
      "idea",
      "amp",
      "cant",
      "get",
      "pic",
      "lol"
    ],
    "token_count":11,
    "processed_text":"decid tell switch pic good idea amp cant get pic lol"
  },
  {
    "label":0,
    "text":"omgomgomgomg cedric sexyfac die",
    "cleaned_text":"omgomgomgomg cedric sexyfac die",
    "normalized_text":"omgomgomgomg cedric sexyfac die",
    "tokens":[
      "omgomgomgomg",
      "cedric",
      "sexyfac",
      "die"
    ],
    "token_count":4,
    "processed_text":"omgomgomgomg cedric sexyfac die"
  },
  {
    "label":4,
    "text":"kayi final goin bed haha sweet drean",
    "cleaned_text":"kayi final goin bed haha sweet drean",
    "normalized_text":"kayi final goin bed haha sweet drean",
    "tokens":[
      "kayi",
      "final",
      "goin",
      "bed",
      "haha",
      "sweet",
      "drean"
    ],
    "token_count":7,
    "processed_text":"kayi final goin bed haha sweet drean"
  },
  {
    "label":4,
    "text":"alright work",
    "cleaned_text":"alright work",
    "normalized_text":"alright work",
    "tokens":[
      "alright",
      "work"
    ],
    "token_count":2,
    "processed_text":"alright work"
  },
  {
    "label":4,
    "text":"oooh rob pattinson hot guitarist well hot go premier sequel methink",
    "cleaned_text":"oooh rob pattinson hot guitarist well hot go premier sequel methink",
    "normalized_text":"oooh rob pattinson hot guitarist well hot go premier sequel methink",
    "tokens":[
      "oooh",
      "rob",
      "pattinson",
      "hot",
      "guitarist",
      "well",
      "hot",
      "go",
      "premier",
      "sequel",
      "methink"
    ],
    "token_count":11,
    "processed_text":"oooh rob pattinson hot guitarist well hot go premier sequel methink"
  },
  {
    "label":0,
    "text":"someday somehow gonna make alright right father day sunday n",
    "cleaned_text":"someday somehow gonna make alright right father day sunday n",
    "normalized_text":"someday somehow gonna make alright right father day sunday n",
    "tokens":[
      "someday",
      "somehow",
      "gon",
      "na",
      "make",
      "alright",
      "right",
      "father",
      "day",
      "sunday"
    ],
    "token_count":10,
    "processed_text":"someday somehow gon na make alright right father day sunday"
  },
  {
    "label":4,
    "text":"book good lol cant wait see moiv love us taylor hehehehom knockout night night",
    "cleaned_text":"book good lol cant wait see moiv love us taylor hehehehom knockout night night",
    "normalized_text":"book good lol cant wait see moiv love us taylor hehehehom knockout night night",
    "tokens":[
      "book",
      "good",
      "lol",
      "cant",
      "wait",
      "see",
      "moiv",
      "love",
      "us",
      "taylor",
      "hehehehom",
      "knockout",
      "night",
      "night"
    ],
    "token_count":14,
    "processed_text":"book good lol cant wait see moiv love us taylor hehehehom knockout night night"
  },
  {
    "label":0,
    "text":"noooo love come da criizzzzib chang pretti girl diaper pleeeeeeas",
    "cleaned_text":"noooo love come da criizzzzib chang pretti girl diaper pleeeeeeas",
    "normalized_text":"noooo love come da criizzzzib chang pretti girl diaper pleeeeeeas",
    "tokens":[
      "noooo",
      "love",
      "come",
      "da",
      "criizzzzib",
      "chang",
      "pretti",
      "girl",
      "diaper",
      "pleeeeeea"
    ],
    "token_count":10,
    "processed_text":"noooo love come da criizzzzib chang pretti girl diaper pleeeeeea"
  },
  {
    "label":4,
    "text":"hiya enjoy day thendid get bbq weather crappi",
    "cleaned_text":"hiya enjoy day thendid get bbq weather crappi",
    "normalized_text":"hiya enjoy day thendid get bbq weather crappi",
    "tokens":[
      "hiya",
      "enjoy",
      "day",
      "thendid",
      "get",
      "bbq",
      "weather",
      "crappi"
    ],
    "token_count":8,
    "processed_text":"hiya enjoy day thendid get bbq weather crappi"
  },
  {
    "label":4,
    "text":"want write someth special write tweet",
    "cleaned_text":"want write someth special write tweet",
    "normalized_text":"want write someth special write tweet",
    "tokens":[
      "want",
      "write",
      "someth",
      "special",
      "write",
      "tweet"
    ],
    "token_count":6,
    "processed_text":"want write someth special write tweet"
  },
  {
    "label":4,
    "text":"thank mention quotschafskltequot sorri umlaut",
    "cleaned_text":"thank mention quotschafskltequot sorri umlaut",
    "normalized_text":"thank mention quotschafskltequot sorri umlaut",
    "tokens":[
      "thank",
      "mention",
      "sorri",
      "umlaut"
    ],
    "token_count":4,
    "processed_text":"thank mention sorri umlaut"
  },
  {
    "label":0,
    "text":"hot flat im stick leather sofa",
    "cleaned_text":"hot flat im stick leather sofa",
    "normalized_text":"hot flat im stick leather sofa",
    "tokens":[
      "hot",
      "flat",
      "im",
      "stick",
      "leather",
      "sofa"
    ],
    "token_count":6,
    "processed_text":"hot flat im stick leather sofa"
  },
  {
    "label":0,
    "text":"discov nasti kink view ie",
    "cleaned_text":"discov nasti kink view ie",
    "normalized_text":"discov nasti kink view ie",
    "tokens":[
      "discov",
      "nasti",
      "kink",
      "view",
      "ie"
    ],
    "token_count":5,
    "processed_text":"discov nasti kink view ie"
  },
  {
    "label":4,
    "text":"got back dinner lari",
    "cleaned_text":"got back dinner lari",
    "normalized_text":"got back dinner lari",
    "tokens":[
      "got",
      "back",
      "dinner",
      "lari"
    ],
    "token_count":4,
    "processed_text":"got back dinner lari"
  },
  {
    "label":4,
    "text":"come london pleas howabout tomorrow",
    "cleaned_text":"come london pleas howabout tomorrow",
    "normalized_text":"come london pleas howabout tomorrow",
    "tokens":[
      "come",
      "london",
      "plea",
      "howabout",
      "tomorrow"
    ],
    "token_count":5,
    "processed_text":"come london plea howabout tomorrow"
  },
  {
    "label":4,
    "text":"atleast got someth look forward inrusdx",
    "cleaned_text":"atleast got someth look forward inrusdx",
    "normalized_text":"atleast got someth look forward inrusdx",
    "tokens":[
      "atleast",
      "got",
      "someth",
      "look",
      "forward",
      "inrusdx"
    ],
    "token_count":6,
    "processed_text":"atleast got someth look forward inrusdx"
  },
  {
    "label":4,
    "text":"hahahaha well morn start slow def made lot better",
    "cleaned_text":"hahahaha well morn start slow def made lot better",
    "normalized_text":"hahahaha well morn start slow def made lot better",
    "tokens":[
      "hahahaha",
      "well",
      "morn",
      "start",
      "slow",
      "def",
      "made",
      "lot",
      "better"
    ],
    "token_count":9,
    "processed_text":"hahahaha well morn start slow def made lot better"
  },
  {
    "label":4,
    "text":"furnitur regular hous stuff wouldnt part camera stuff attach issu plu need",
    "cleaned_text":"furnitur regular hous stuff wouldnt part camera stuff attach issu plu need",
    "normalized_text":"furnitur regular hous stuff wouldnt part camera stuff attach issu plu need",
    "tokens":[
      "furnitur",
      "regular",
      "hou",
      "stuff",
      "wouldnt",
      "part",
      "camera",
      "stuff",
      "attach",
      "issu",
      "plu",
      "need"
    ],
    "token_count":12,
    "processed_text":"furnitur regular hou stuff wouldnt part camera stuff attach issu plu need"
  },
  {
    "label":4,
    "text":"day big day fridg home pack hahahaha woo",
    "cleaned_text":"day big day fridg home pack hahahaha woo",
    "normalized_text":"day big day fridg home pack hahahaha woo",
    "tokens":[
      "day",
      "big",
      "day",
      "fridg",
      "home",
      "pack",
      "hahahaha",
      "woo"
    ],
    "token_count":8,
    "processed_text":"day big day fridg home pack hahahaha woo"
  },
  {
    "label":4,
    "text":"want new psp sooooo cool",
    "cleaned_text":"want new psp sooooo cool",
    "normalized_text":"want new psp sooooo cool",
    "tokens":[
      "want",
      "new",
      "psp",
      "sooooo",
      "cool"
    ],
    "token_count":5,
    "processed_text":"want new psp sooooo cool"
  },
  {
    "label":4,
    "text":"almost mani viewer live eagl cam ustream",
    "cleaned_text":"almost mani viewer live eagl cam ustream",
    "normalized_text":"almost mani viewer live eagl cam ustream",
    "tokens":[
      "almost",
      "mani",
      "viewer",
      "live",
      "eagl",
      "cam",
      "ustream"
    ],
    "token_count":7,
    "processed_text":"almost mani viewer live eagl cam ustream"
  },
  {
    "label":4,
    "text":"hooray think ill spend time someon actual want spend time",
    "cleaned_text":"hooray think ill spend time someon actual want spend time",
    "normalized_text":"hooray think ill spend time someon actual want spend time",
    "tokens":[
      "hooray",
      "think",
      "ill",
      "spend",
      "time",
      "someon",
      "actual",
      "want",
      "spend",
      "time"
    ],
    "token_count":10,
    "processed_text":"hooray think ill spend time someon actual want spend time"
  },
  {
    "label":0,
    "text":"went pick strawberri yesterday ripe lot still green rot wno hope ripen",
    "cleaned_text":"went pick strawberri yesterday ripe lot still green rot wno hope ripen",
    "normalized_text":"went pick strawberri yesterday ripe lot still green rot wno hope ripen",
    "tokens":[
      "went",
      "pick",
      "strawberri",
      "yesterday",
      "ripe",
      "lot",
      "still",
      "green",
      "rot",
      "wno",
      "hope",
      "ripen"
    ],
    "token_count":12,
    "processed_text":"went pick strawberri yesterday ripe lot still green rot wno hope ripen"
  },
  {
    "label":4,
    "text":"fair enough propag seamless tho set everyth rd parti provid switch nameserv",
    "cleaned_text":"fair enough propag seamless tho set everyth rd parti provid switch nameserv",
    "normalized_text":"fair enough propag seamless tho set everyth rd parti provid switch nameserv",
    "tokens":[
      "fair",
      "enough",
      "propag",
      "seamless",
      "tho",
      "set",
      "everyth",
      "rd",
      "parti",
      "provid",
      "switch",
      "nameserv"
    ],
    "token_count":12,
    "processed_text":"fair enough propag seamless tho set everyth rd parti provid switch nameserv"
  },
  {
    "label":4,
    "text":"sun victoria park",
    "cleaned_text":"sun victoria park",
    "normalized_text":"sun victoria park",
    "tokens":[
      "sun",
      "victoria",
      "park"
    ],
    "token_count":3,
    "processed_text":"sun victoria park"
  },
  {
    "label":4,
    "text":"warn hors drawn carriag kind hick countri",
    "cleaned_text":"warn hors drawn carriag kind hick countri",
    "normalized_text":"warn hors drawn carriag kind hick countri",
    "tokens":[
      "warn",
      "hor",
      "drawn",
      "carriag",
      "kind",
      "hick",
      "countri"
    ],
    "token_count":7,
    "processed_text":"warn hor drawn carriag kind hick countri"
  },
  {
    "label":0,
    "text":"miss face bonnaroo",
    "cleaned_text":"miss face bonnaroo",
    "normalized_text":"miss face bonnaroo",
    "tokens":[
      "miss",
      "face",
      "bonnaroo"
    ],
    "token_count":3,
    "processed_text":"miss face bonnaroo"
  },
  {
    "label":0,
    "text":"ncaa men colleg world seri asu done texa lsu",
    "cleaned_text":"ncaa men colleg world seri asu done texa lsu",
    "normalized_text":"ncaa men colleg world seri asu done texa lsu",
    "tokens":[
      "ncaa",
      "men",
      "colleg",
      "world",
      "seri",
      "asu",
      "done",
      "texa",
      "lsu"
    ],
    "token_count":9,
    "processed_text":"ncaa men colleg world seri asu done texa lsu"
  },
  {
    "label":4,
    "text":"yay got shoutout xoxo",
    "cleaned_text":"yay got shoutout xoxo",
    "normalized_text":"yay got shoutout xoxo",
    "tokens":[
      "yay",
      "got",
      "shoutout",
      "xoxo"
    ],
    "token_count":4,
    "processed_text":"yay got shoutout xoxo"
  },
  {
    "label":0,
    "text":"ill uni tho wont abl go current friend point gig",
    "cleaned_text":"ill uni tho wont abl go current friend point gig",
    "normalized_text":"ill uni tho wont abl go current friend point gig",
    "tokens":[
      "ill",
      "uni",
      "tho",
      "wont",
      "abl",
      "go",
      "current",
      "friend",
      "point",
      "gig"
    ],
    "token_count":10,
    "processed_text":"ill uni tho wont abl go current friend point gig"
  },
  {
    "label":0,
    "text":"omg irvin triffi resign im lose friend ogdi get back might know anyon anymor",
    "cleaned_text":"omg irvin triffi resign im lose friend ogdi get back might know anyon anymor",
    "normalized_text":"omg irvin triffi resign im lose friend ogdi get back might know anyon anymor",
    "tokens":[
      "omg",
      "irvin",
      "triffi",
      "resign",
      "im",
      "lose",
      "friend",
      "ogdi",
      "get",
      "back",
      "know",
      "anyon",
      "anymor"
    ],
    "token_count":13,
    "processed_text":"omg irvin triffi resign im lose friend ogdi get back know anyon anymor"
  },
  {
    "label":4,
    "text":"worri man im alreadi follow everyth happen reason good ill rebuild list scratch",
    "cleaned_text":"worri man im alreadi follow everyth happen reason good ill rebuild list scratch",
    "normalized_text":"worri man im alreadi follow everyth happen reason good ill rebuild list scratch",
    "tokens":[
      "worri",
      "man",
      "im",
      "alreadi",
      "follow",
      "everyth",
      "happen",
      "reason",
      "good",
      "ill",
      "rebuild",
      "list",
      "scratch"
    ],
    "token_count":13,
    "processed_text":"worri man im alreadi follow everyth happen reason good ill rebuild list scratch"
  },
  {
    "label":4,
    "text":"la laker go back nba final babi wt wt",
    "cleaned_text":"la laker go back nba final babi wt wt",
    "normalized_text":"la laker go back nba final babi wt wt",
    "tokens":[
      "la",
      "laker",
      "go",
      "back",
      "nba",
      "final",
      "babi",
      "wt",
      "wt"
    ],
    "token_count":9,
    "processed_text":"la laker go back nba final babi wt wt"
  },
  {
    "label":0,
    "text":"chapter",
    "cleaned_text":"chapter",
    "normalized_text":"chapter",
    "tokens":[
      "chapter"
    ],
    "token_count":1,
    "processed_text":"chapter"
  },
  {
    "label":4,
    "text":"call mum pop head bed see morn yall carla okay xp see tomorrow",
    "cleaned_text":"call mum pop head bed see morn yall carla okay xp see tomorrow",
    "normalized_text":"call mum pop head bed see morn yall carla okay xp see tomorrow",
    "tokens":[
      "call",
      "mum",
      "pop",
      "head",
      "bed",
      "see",
      "morn",
      "yall",
      "carla",
      "okay",
      "xp",
      "see",
      "tomorrow"
    ],
    "token_count":13,
    "processed_text":"call mum pop head bed see morn yall carla okay xp see tomorrow"
  },
  {
    "label":4,
    "text":"want",
    "cleaned_text":"want",
    "normalized_text":"want",
    "tokens":[
      "want"
    ],
    "token_count":1,
    "processed_text":"want"
  },
  {
    "label":0,
    "text":"hot anyth",
    "cleaned_text":"hot anyth",
    "normalized_text":"hot anyth",
    "tokens":[
      "hot",
      "anyth"
    ],
    "token_count":2,
    "processed_text":"hot anyth"
  },
  {
    "label":4,
    "text":"must wake bh anyway chanc read hope interrupt",
    "cleaned_text":"must wake bh anyway chanc read hope interrupt",
    "normalized_text":"must wake bh anyway chanc read hope interrupt",
    "tokens":[
      "wake",
      "bh",
      "anyway",
      "chanc",
      "read",
      "hope",
      "interrupt"
    ],
    "token_count":7,
    "processed_text":"wake bh anyway chanc read hope interrupt"
  },
  {
    "label":0,
    "text":"final made money last night spend thing need dont want buy",
    "cleaned_text":"final made money last night spend thing need dont want buy",
    "normalized_text":"final made money last night spend thing need dont want buy",
    "tokens":[
      "final",
      "made",
      "money",
      "last",
      "night",
      "spend",
      "thing",
      "need",
      "dont",
      "want",
      "buy"
    ],
    "token_count":11,
    "processed_text":"final made money last night spend thing need dont want buy"
  },
  {
    "label":4,
    "text":"nm week old today",
    "cleaned_text":"nm week old today",
    "normalized_text":"nm week old today",
    "tokens":[
      "nm",
      "week",
      "old",
      "today"
    ],
    "token_count":4,
    "processed_text":"nm week old today"
  },
  {
    "label":4,
    "text":"one moreeeee day",
    "cleaned_text":"one moreeeee day",
    "normalized_text":"one moreeeee day",
    "tokens":[
      "one",
      "moreeee",
      "day"
    ],
    "token_count":3,
    "processed_text":"one moreeee day"
  },
  {
    "label":0,
    "text":"tri figur tweet mobil",
    "cleaned_text":"tri figur tweet mobil",
    "normalized_text":"tri figur tweet mobil",
    "tokens":[
      "tri",
      "figur",
      "tweet",
      "mobil"
    ],
    "token_count":4,
    "processed_text":"tri figur tweet mobil"
  },
  {
    "label":0,
    "text":"cant stop heart call",
    "cleaned_text":"cant stop heart call",
    "normalized_text":"cant stop heart call",
    "tokens":[
      "cant",
      "stop",
      "heart",
      "call"
    ],
    "token_count":4,
    "processed_text":"cant stop heart call"
  },
  {
    "label":4,
    "text":"mean someon mayb get littl troubl",
    "cleaned_text":"mean someon mayb get littl troubl",
    "normalized_text":"mean someon mayb get littl troubl",
    "tokens":[
      "mean",
      "someon",
      "mayb",
      "get",
      "littl",
      "troubl"
    ],
    "token_count":6,
    "processed_text":"mean someon mayb get littl troubl"
  },
  {
    "label":0,
    "text":"hate alot hurt hurt even",
    "cleaned_text":"hate alot hurt hurt even",
    "normalized_text":"hate alot hurt hurt even",
    "tokens":[
      "hate",
      "alot",
      "hurt",
      "hurt",
      "even"
    ],
    "token_count":5,
    "processed_text":"hate alot hurt hurt even"
  },
  {
    "label":0,
    "text":"come come wherev u",
    "cleaned_text":"come come wherev u",
    "normalized_text":"come come wherev u",
    "tokens":[
      "come",
      "come",
      "wherev"
    ],
    "token_count":3,
    "processed_text":"come come wherev"
  },
  {
    "label":0,
    "text":"librari rori got st nd nd noth littl red cap fail",
    "cleaned_text":"librari rori got st nd nd noth littl red cap fail",
    "normalized_text":"librari rori got st nd nd noth littl red cap fail",
    "tokens":[
      "librari",
      "rori",
      "got",
      "st",
      "nd",
      "nd",
      "noth",
      "littl",
      "red",
      "cap",
      "fail"
    ],
    "token_count":11,
    "processed_text":"librari rori got st nd nd noth littl red cap fail"
  },
  {
    "label":0,
    "text":"ask later get good side",
    "cleaned_text":"ask later get good side",
    "normalized_text":"ask later get good side",
    "tokens":[
      "ask",
      "later",
      "get",
      "good",
      "side"
    ],
    "token_count":5,
    "processed_text":"ask later get good side"
  },
  {
    "label":4,
    "text":"here shocker keyword get traffic twitter amp articl directori googl",
    "cleaned_text":"here shocker keyword get traffic twitter amp articl directori googl",
    "normalized_text":"here shocker keyword get traffic twitter amp articl directori googl",
    "tokens":[
      "shocker",
      "keyword",
      "get",
      "traffic",
      "twitter",
      "amp",
      "articl",
      "directori",
      "googl"
    ],
    "token_count":9,
    "processed_text":"shocker keyword get traffic twitter amp articl directori googl"
  },
  {
    "label":4,
    "text":"brainstorm design love",
    "cleaned_text":"brainstorm design love",
    "normalized_text":"brainstorm design love",
    "tokens":[
      "brainstorm",
      "design",
      "love"
    ],
    "token_count":3,
    "processed_text":"brainstorm design love"
  },
  {
    "label":0,
    "text":"thank suck sick",
    "cleaned_text":"thank suck sick",
    "normalized_text":"thank suck sick",
    "tokens":[
      "thank",
      "suck",
      "sick"
    ],
    "token_count":3,
    "processed_text":"thank suck sick"
  },
  {
    "label":4,
    "text":"ritz",
    "cleaned_text":"ritz",
    "normalized_text":"ritz",
    "tokens":[
      "ritz"
    ],
    "token_count":1,
    "processed_text":"ritz"
  },
  {
    "label":0,
    "text":"broken work phone vm direct altern number today",
    "cleaned_text":"broken work phone vm direct altern number today",
    "normalized_text":"broken work phone vm direct altern number today",
    "tokens":[
      "broken",
      "work",
      "phone",
      "vm",
      "direct",
      "altern",
      "number",
      "today"
    ],
    "token_count":8,
    "processed_text":"broken work phone vm direct altern number today"
  },
  {
    "label":0,
    "text":"wish wasnt work could play",
    "cleaned_text":"wish wasnt work could play",
    "normalized_text":"wish wasnt work could play",
    "tokens":[
      "wish",
      "wasnt",
      "work",
      "play"
    ],
    "token_count":4,
    "processed_text":"wish wasnt work play"
  },
  {
    "label":0,
    "text":"ta chick gander ever get offic",
    "cleaned_text":"ta chick gander ever get offic",
    "normalized_text":"ta chick gander ever get offic",
    "tokens":[
      "ta",
      "chick",
      "gander",
      "ever",
      "get",
      "offic"
    ],
    "token_count":6,
    "processed_text":"ta chick gander ever get offic"
  },
  {
    "label":0,
    "text":"sniffl late",
    "cleaned_text":"sniffl late",
    "normalized_text":"sniffl late",
    "tokens":[
      "sniffl",
      "late"
    ],
    "token_count":2,
    "processed_text":"sniffl late"
  },
  {
    "label":4,
    "text":"ak tonight ladi friend",
    "cleaned_text":"ak tonight ladi friend",
    "normalized_text":"ak tonight ladi friend",
    "tokens":[
      "ak",
      "tonight",
      "ladi",
      "friend"
    ],
    "token_count":4,
    "processed_text":"ak tonight ladi friend"
  },
  {
    "label":0,
    "text":"when next act accounc month bit harsh",
    "cleaned_text":"when next act accounc month bit harsh",
    "normalized_text":"when next act accounc month bit harsh",
    "tokens":[
      "next",
      "act",
      "accounc",
      "month",
      "bit",
      "harsh"
    ],
    "token_count":6,
    "processed_text":"next act accounc month bit harsh"
  },
  {
    "label":0,
    "text":"im winer offic get cross browser test job mani browser virtual app",
    "cleaned_text":"im winer offic get cross browser test job mani browser virtual app",
    "normalized_text":"im winer offic get cross browser test job mani browser virtual app",
    "tokens":[
      "im",
      "winer",
      "offic",
      "get",
      "cross",
      "browser",
      "test",
      "job",
      "mani",
      "browser",
      "virtual",
      "app"
    ],
    "token_count":12,
    "processed_text":"im winer offic get cross browser test job mani browser virtual app"
  },
  {
    "label":0,
    "text":"night time month ago odyessi pnk came stage id anyth go back",
    "cleaned_text":"night time month ago odyessi pnk came stage id anyth go back",
    "normalized_text":"night time month ago odyessi pnk came stage id anyth go back",
    "tokens":[
      "night",
      "time",
      "month",
      "ago",
      "odyessi",
      "pnk",
      "came",
      "stage",
      "id",
      "anyth",
      "go",
      "back"
    ],
    "token_count":12,
    "processed_text":"night time month ago odyessi pnk came stage id anyth go back"
  },
  {
    "label":0,
    "text":"ye im mix emot",
    "cleaned_text":"ye im mix emot",
    "normalized_text":"ye im mix emot",
    "tokens":[
      "ye",
      "im",
      "mix",
      "emot"
    ],
    "token_count":4,
    "processed_text":"ye im mix emot"
  },
  {
    "label":0,
    "text":"want galaxi",
    "cleaned_text":"want galaxi",
    "normalized_text":"want galaxi",
    "tokens":[
      "want",
      "galaxi"
    ],
    "token_count":2,
    "processed_text":"want galaxi"
  },
  {
    "label":0,
    "text":"smoke hookah made throat hurt",
    "cleaned_text":"smoke hookah made throat hurt",
    "normalized_text":"smoke hookah made throat hurt",
    "tokens":[
      "smoke",
      "hookah",
      "made",
      "throat",
      "hurt"
    ],
    "token_count":5,
    "processed_text":"smoke hookah made throat hurt"
  },
  {
    "label":4,
    "text":"suuuuuurrr quotdisinfectantquot wink wink",
    "cleaned_text":"suuuuuurrr quotdisinfectantquot wink wink",
    "normalized_text":"suuuuuurrr quotdisinfectantquot wink wink",
    "tokens":[
      "suuuuuurrr",
      "wink",
      "wink"
    ],
    "token_count":3,
    "processed_text":"suuuuuurrr wink wink"
  },
  {
    "label":0,
    "text":"st day month havent spoken arab learn help michel thoma recommend wwwmichelthomascouk",
    "cleaned_text":"st day month havent spoken arab learn help michel thoma recommend wwwmichelthomascouk",
    "normalized_text":"st day month havent spoken arab learn help michel thoma recommend wwwmichelthomascouk",
    "tokens":[
      "st",
      "day",
      "month",
      "havent",
      "spoken",
      "arab",
      "learn",
      "help",
      "michel",
      "thoma",
      "recommend"
    ],
    "token_count":11,
    "processed_text":"st day month havent spoken arab learn help michel thoma recommend"
  },
  {
    "label":4,
    "text":"photo presley cute",
    "cleaned_text":"photo presley cute",
    "normalized_text":"photo presley cute",
    "tokens":[
      "photo",
      "presley",
      "cute"
    ],
    "token_count":3,
    "processed_text":"photo presley cute"
  },
  {
    "label":0,
    "text":"uhh umm clear throat miss",
    "cleaned_text":"uhh umm clear throat miss",
    "normalized_text":"uhh umm clear throat miss",
    "tokens":[
      "uhh",
      "umm",
      "clear",
      "throat",
      "miss"
    ],
    "token_count":5,
    "processed_text":"uhh umm clear throat miss"
  },
  {
    "label":4,
    "text":"run late today got distract suit life zack codi im make pasta tuna im work xxx",
    "cleaned_text":"run late today got distract suit life zack codi im make pasta tuna im work xxx",
    "normalized_text":"run late today got distract suit life zack codi im make pasta tuna im work xxx",
    "tokens":[
      "run",
      "late",
      "today",
      "got",
      "distract",
      "suit",
      "life",
      "zack",
      "codi",
      "im",
      "make",
      "pasta",
      "tuna",
      "im",
      "work",
      "xxx"
    ],
    "token_count":16,
    "processed_text":"run late today got distract suit life zack codi im make pasta tuna im work xxx"
  },
  {
    "label":4,
    "text":"cold fun stori though",
    "cleaned_text":"cold fun stori though",
    "normalized_text":"cold fun stori though",
    "tokens":[
      "cold",
      "fun",
      "stori",
      "though"
    ],
    "token_count":4,
    "processed_text":"cold fun stori though"
  },
  {
    "label":4,
    "text":"hope fantast birthday chri guy sing tonight hehe",
    "cleaned_text":"hope fantast birthday chri guy sing tonight hehe",
    "normalized_text":"hope fantast birthday chri guy sing tonight hehe",
    "tokens":[
      "hope",
      "fantast",
      "birthday",
      "chri",
      "guy",
      "sing",
      "tonight",
      "hehe"
    ],
    "token_count":8,
    "processed_text":"hope fantast birthday chri guy sing tonight hehe"
  },
  {
    "label":0,
    "text":"wahhh still sick first day os school todayi dont class tho sick go addbooo",
    "cleaned_text":"wahhh still sick first day os school todayi dont class tho sick go addbooo",
    "normalized_text":"wahhh still sick first day os school todayi dont class tho sick go addbooo",
    "tokens":[
      "wahhh",
      "still",
      "sick",
      "first",
      "day",
      "os",
      "school",
      "todayi",
      "dont",
      "class",
      "tho",
      "sick",
      "go",
      "addbooo"
    ],
    "token_count":14,
    "processed_text":"wahhh still sick first day os school todayi dont class tho sick go addbooo"
  },
  {
    "label":0,
    "text":"thank hun didnt think would hard",
    "cleaned_text":"thank hun didnt think would hard",
    "normalized_text":"thank hun didnt think would hard",
    "tokens":[
      "thank",
      "hun",
      "didnt",
      "think",
      "hard"
    ],
    "token_count":5,
    "processed_text":"thank hun didnt think hard"
  },
  {
    "label":4,
    "text":"awww cute fluffi hugback night",
    "cleaned_text":"awww cute fluffi hugback night",
    "normalized_text":"awww cute fluffi hugback night",
    "tokens":[
      "awww",
      "cute",
      "fluffi",
      "hugback",
      "night"
    ],
    "token_count":5,
    "processed_text":"awww cute fluffi hugback night"
  },
  {
    "label":0,
    "text":"guy great work shame made fool fli thing",
    "cleaned_text":"guy great work shame made fool fli thing",
    "normalized_text":"guy great work shame made fool fli thing",
    "tokens":[
      "guy",
      "great",
      "work",
      "shame",
      "made",
      "fool",
      "fli",
      "thing"
    ],
    "token_count":8,
    "processed_text":"guy great work shame made fool fli thing"
  },
  {
    "label":0,
    "text":"your spank next weekend",
    "cleaned_text":"your spank next weekend",
    "normalized_text":"your spank next weekend",
    "tokens":[
      "spank",
      "next",
      "weekend"
    ],
    "token_count":3,
    "processed_text":"spank next weekend"
  },
  {
    "label":0,
    "text":"home relax got lot laundri though",
    "cleaned_text":"home relax got lot laundri though",
    "normalized_text":"home relax got lot laundri though",
    "tokens":[
      "home",
      "relax",
      "got",
      "lot",
      "laundri",
      "though"
    ],
    "token_count":6,
    "processed_text":"home relax got lot laundri though"
  },
  {
    "label":0,
    "text":"hello warm preston hand stick keyboard cant move well",
    "cleaned_text":"hello warm preston hand stick keyboard cant move well",
    "normalized_text":"hello warm preston hand stick keyboard cant move well",
    "tokens":[
      "hello",
      "warm",
      "preston",
      "hand",
      "stick",
      "keyboard",
      "cant",
      "move",
      "well"
    ],
    "token_count":9,
    "processed_text":"hello warm preston hand stick keyboard cant move well"
  },
  {
    "label":0,
    "text":"bum spend ridicul gorgeou day lab work especi sinc get like day sun year",
    "cleaned_text":"bum spend ridicul gorgeou day lab work especi sinc get like day sun year",
    "normalized_text":"bum spend ridicul gorgeou day lab work especi sinc get like day sun year",
    "tokens":[
      "bum",
      "spend",
      "ridicul",
      "gorgeou",
      "day",
      "lab",
      "work",
      "especi",
      "sinc",
      "get",
      "like",
      "day",
      "sun",
      "year"
    ],
    "token_count":14,
    "processed_text":"bum spend ridicul gorgeou day lab work especi sinc get like day sun year"
  },
  {
    "label":4,
    "text":"pleasur good respons frm fam that present mom happi th",
    "cleaned_text":"pleasur good respons frm fam that present mom happi th",
    "normalized_text":"pleasur good respons frm fam that present mom happi th",
    "tokens":[
      "pleasur",
      "good",
      "respon",
      "frm",
      "fam",
      "present",
      "mom",
      "happi",
      "th"
    ],
    "token_count":9,
    "processed_text":"pleasur good respon frm fam present mom happi th"
  },
  {
    "label":0,
    "text":"send messag greg time answer ok still love amp goog luck move eri",
    "cleaned_text":"send messag greg time answer ok still love amp goog luck move eri",
    "normalized_text":"send messag greg time answer ok still love amp goog luck move eri",
    "tokens":[
      "send",
      "messag",
      "greg",
      "time",
      "answer",
      "ok",
      "still",
      "love",
      "amp",
      "goog",
      "luck",
      "move",
      "eri"
    ],
    "token_count":13,
    "processed_text":"send messag greg time answer ok still love amp goog luck move eri"
  },
  {
    "label":0,
    "text":"wait",
    "cleaned_text":"wait",
    "normalized_text":"wait",
    "tokens":[
      "wait"
    ],
    "token_count":1,
    "processed_text":"wait"
  },
  {
    "label":4,
    "text":"total find site custom make hat make wed offici coolest peopl ever",
    "cleaned_text":"total find site custom make hat make wed offici coolest peopl ever",
    "normalized_text":"total find site custom make hat make wed offici coolest peopl ever",
    "tokens":[
      "total",
      "find",
      "site",
      "custom",
      "make",
      "hat",
      "make",
      "wed",
      "offici",
      "coolest",
      "peopl",
      "ever"
    ],
    "token_count":12,
    "processed_text":"total find site custom make hat make wed offici coolest peopl ever"
  },
  {
    "label":0,
    "text":"poor iz big fella talent",
    "cleaned_text":"poor iz big fella talent",
    "normalized_text":"poor iz big fella talent",
    "tokens":[
      "poor",
      "iz",
      "big",
      "fella",
      "talent"
    ],
    "token_count":5,
    "processed_text":"poor iz big fella talent"
  },
  {
    "label":0,
    "text":"im dont file miss person report time slept late last night foolia",
    "cleaned_text":"im dont file miss person report time slept late last night foolia",
    "normalized_text":"im dont file miss person report time slept late last night foolia",
    "tokens":[
      "im",
      "dont",
      "file",
      "miss",
      "person",
      "report",
      "time",
      "slept",
      "late",
      "last",
      "night",
      "foolia"
    ],
    "token_count":12,
    "processed_text":"im dont file miss person report time slept late last night foolia"
  },
  {
    "label":0,
    "text":"ahhhh need see pieeee letter tooo sorri havnt post",
    "cleaned_text":"ahhhh need see pieeee letter tooo sorri havnt post",
    "normalized_text":"ahhhh need see pieeee letter tooo sorri havnt post",
    "tokens":[
      "ahhhh",
      "need",
      "see",
      "pieeee",
      "letter",
      "tooo",
      "sorri",
      "havnt",
      "post"
    ],
    "token_count":9,
    "processed_text":"ahhhh need see pieeee letter tooo sorri havnt post"
  },
  {
    "label":0,
    "text":"im jealou girl",
    "cleaned_text":"im jealou girl",
    "normalized_text":"im jealou girl",
    "tokens":[
      "im",
      "jealou",
      "girl"
    ],
    "token_count":3,
    "processed_text":"im jealou girl"
  },
  {
    "label":4,
    "text":"ahhh miss g essay sister apt hang talki emolin",
    "cleaned_text":"ahhh miss g essay sister apt hang talki emolin",
    "normalized_text":"ahhh miss g essay sister apt hang talki emolin",
    "tokens":[
      "ahhh",
      "miss",
      "essay",
      "sister",
      "apt",
      "hang",
      "talki",
      "emolin"
    ],
    "token_count":8,
    "processed_text":"ahhh miss essay sister apt hang talki emolin"
  },
  {
    "label":0,
    "text":"got home work hour",
    "cleaned_text":"got home work hour",
    "normalized_text":"got home work hour",
    "tokens":[
      "got",
      "home",
      "work",
      "hour"
    ],
    "token_count":4,
    "processed_text":"got home work hour"
  },
  {
    "label":4,
    "text":"get brought week bonz forum tri reach mani peopl",
    "cleaned_text":"get brought week bonz forum tri reach mani peopl",
    "normalized_text":"get brought week bonz forum tri reach mani peopl",
    "tokens":[
      "get",
      "brought",
      "week",
      "bonz",
      "forum",
      "tri",
      "reach",
      "mani",
      "peopl"
    ],
    "token_count":9,
    "processed_text":"get brought week bonz forum tri reach mani peopl"
  },
  {
    "label":4,
    "text":"that exactli need right your best ever",
    "cleaned_text":"that exactli need right your best ever",
    "normalized_text":"that exactli need right your best ever",
    "tokens":[
      "exactli",
      "need",
      "right",
      "best",
      "ever"
    ],
    "token_count":5,
    "processed_text":"exactli need right best ever"
  },
  {
    "label":0,
    "text":"tummi ach",
    "cleaned_text":"tummi ach",
    "normalized_text":"tummi ach",
    "tokens":[
      "tummi",
      "ach"
    ],
    "token_count":2,
    "processed_text":"tummi ach"
  },
  {
    "label":4,
    "text":"whore want follow",
    "cleaned_text":"whore want follow",
    "normalized_text":"whore want follow",
    "tokens":[
      "whore",
      "want",
      "follow"
    ],
    "token_count":3,
    "processed_text":"whore want follow"
  },
  {
    "label":0,
    "text":"head lunch wit tha girlsyay gotta get b leav tomorrow",
    "cleaned_text":"head lunch wit tha girlsyay gotta get b leav tomorrow",
    "normalized_text":"head lunch wit tha girlsyay gotta get b leav tomorrow",
    "tokens":[
      "head",
      "lunch",
      "wit",
      "tha",
      "girlsyay",
      "got",
      "ta",
      "get",
      "leav",
      "tomorrow"
    ],
    "token_count":10,
    "processed_text":"head lunch wit tha girlsyay got ta get leav tomorrow"
  },
  {
    "label":0,
    "text":"know good describ symptom express friend face chang piti dismay",
    "cleaned_text":"know good describ symptom express friend face chang piti dismay",
    "normalized_text":"know good describ symptom express friend face chang piti dismay",
    "tokens":[
      "know",
      "good",
      "describ",
      "symptom",
      "express",
      "friend",
      "face",
      "chang",
      "piti",
      "dismay"
    ],
    "token_count":10,
    "processed_text":"know good describ symptom express friend face chang piti dismay"
  },
  {
    "label":0,
    "text":"doesnt know make head hurt",
    "cleaned_text":"doesnt know make head hurt",
    "normalized_text":"doesnt know make head hurt",
    "tokens":[
      "doesnt",
      "know",
      "make",
      "head",
      "hurt"
    ],
    "token_count":5,
    "processed_text":"doesnt know make head hurt"
  },
  {
    "label":4,
    "text":"ah hope let stay",
    "cleaned_text":"ah hope let stay",
    "normalized_text":"ah hope let stay",
    "tokens":[
      "ah",
      "hope",
      "let",
      "stay"
    ],
    "token_count":4,
    "processed_text":"ah hope let stay"
  },
  {
    "label":4,
    "text":"thank inspir love new album",
    "cleaned_text":"thank inspir love new album",
    "normalized_text":"thank inspir love new album",
    "tokens":[
      "thank",
      "inspir",
      "love",
      "new",
      "album"
    ],
    "token_count":5,
    "processed_text":"thank inspir love new album"
  },
  {
    "label":0,
    "text":"bout",
    "cleaned_text":"bout",
    "normalized_text":"bout",
    "tokens":[
      "bout"
    ],
    "token_count":1,
    "processed_text":"bout"
  },
  {
    "label":4,
    "text":"cant wait movieeeeeeee",
    "cleaned_text":"cant wait movieeeeeeee",
    "normalized_text":"cant wait movieeeeeeee",
    "tokens":[
      "cant",
      "wait",
      "movieeeeeee"
    ],
    "token_count":3,
    "processed_text":"cant wait movieeeeeee"
  },
  {
    "label":0,
    "text":"okay ill say miss corp fb",
    "cleaned_text":"okay ill say miss corp fb",
    "normalized_text":"okay ill say miss corp fb",
    "tokens":[
      "okay",
      "ill",
      "say",
      "miss",
      "corp",
      "fb"
    ],
    "token_count":6,
    "processed_text":"okay ill say miss corp fb"
  },
  {
    "label":4,
    "text":"today wasnt fun someon put smile face",
    "cleaned_text":"today wasnt fun someon put smile face",
    "normalized_text":"today wasnt fun someon put smile face",
    "tokens":[
      "today",
      "wasnt",
      "fun",
      "someon",
      "put",
      "smile",
      "face"
    ],
    "token_count":7,
    "processed_text":"today wasnt fun someon put smile face"
  },
  {
    "label":0,
    "text":"big bruddah one nemor",
    "cleaned_text":"big bruddah one nemor",
    "normalized_text":"big bruddah one nemor",
    "tokens":[
      "big",
      "bruddah",
      "one",
      "nemor"
    ],
    "token_count":4,
    "processed_text":"big bruddah one nemor"
  },
  {
    "label":0,
    "text":"dang dont nobodi wanna c movi w ya lol",
    "cleaned_text":"dang dont nobodi wanna c movi w ya lol",
    "normalized_text":"dang dont nobodi wanna c movi w ya lol",
    "tokens":[
      "dang",
      "dont",
      "nobodi",
      "wan",
      "na",
      "movi",
      "ya",
      "lol"
    ],
    "token_count":8,
    "processed_text":"dang dont nobodi wan na movi ya lol"
  },
  {
    "label":4,
    "text":"ill tell friend lea follow",
    "cleaned_text":"ill tell friend lea follow",
    "normalized_text":"ill tell friend lea follow",
    "tokens":[
      "ill",
      "tell",
      "friend",
      "lea",
      "follow"
    ],
    "token_count":5,
    "processed_text":"ill tell friend lea follow"
  },
  {
    "label":0,
    "text":"rush tan son nhat airport hcm go back brunei",
    "cleaned_text":"rush tan son nhat airport hcm go back brunei",
    "normalized_text":"rush tan son nhat airport hcm go back brunei",
    "tokens":[
      "rush",
      "tan",
      "son",
      "nhat",
      "airport",
      "hcm",
      "go",
      "back",
      "brunei"
    ],
    "token_count":9,
    "processed_text":"rush tan son nhat airport hcm go back brunei"
  },
  {
    "label":4,
    "text":"yeah want bottl sanit good tast like choc orang",
    "cleaned_text":"yeah want bottl sanit good tast like choc orang",
    "normalized_text":"yeah want bottl sanit good tast like choc orang",
    "tokens":[
      "yeah",
      "want",
      "bottl",
      "sanit",
      "good",
      "tast",
      "like",
      "choc",
      "orang"
    ],
    "token_count":9,
    "processed_text":"yeah want bottl sanit good tast like choc orang"
  },
  {
    "label":0,
    "text":"wont let vote",
    "cleaned_text":"wont let vote",
    "normalized_text":"wont let vote",
    "tokens":[
      "wont",
      "let",
      "vote"
    ],
    "token_count":3,
    "processed_text":"wont let vote"
  },
  {
    "label":0,
    "text":"tour lol live way tennesse p jk would",
    "cleaned_text":"tour lol live way tennesse p jk would",
    "normalized_text":"tour lol live way tennesse p jk would",
    "tokens":[
      "tour",
      "lol",
      "live",
      "way",
      "tenness",
      "jk"
    ],
    "token_count":6,
    "processed_text":"tour lol live way tenness jk"
  },
  {
    "label":4,
    "text":"dont wanna live dont wanna breath unless feel next love song",
    "cleaned_text":"dont wanna live dont wanna breath unless feel next love song",
    "normalized_text":"dont wanna live dont wanna breath unless feel next love song",
    "tokens":[
      "dont",
      "wan",
      "na",
      "live",
      "dont",
      "wan",
      "na",
      "breath",
      "unless",
      "feel",
      "next",
      "love",
      "song"
    ],
    "token_count":13,
    "processed_text":"dont wan na live dont wan na breath unless feel next love song"
  },
  {
    "label":0,
    "text":"loooool ya drama fun anoth ep gg p ahh miss show",
    "cleaned_text":"loooool ya drama fun anoth ep gg p ahh miss show",
    "normalized_text":"loooool ya drama fun anoth ep gg p ahh miss show",
    "tokens":[
      "loooool",
      "ya",
      "drama",
      "fun",
      "anoth",
      "ep",
      "gg",
      "ahh",
      "miss",
      "show"
    ],
    "token_count":10,
    "processed_text":"loooool ya drama fun anoth ep gg ahh miss show"
  },
  {
    "label":0,
    "text":"suck im miami go rain like week",
    "cleaned_text":"suck im miami go rain like week",
    "normalized_text":"suck im miami go rain like week",
    "tokens":[
      "suck",
      "im",
      "miami",
      "go",
      "rain",
      "like",
      "week"
    ],
    "token_count":7,
    "processed_text":"suck im miami go rain like week"
  },
  {
    "label":0,
    "text":"oh got feedback qualifi",
    "cleaned_text":"oh got feedback qualifi",
    "normalized_text":"oh got feedback qualifi",
    "tokens":[
      "oh",
      "got",
      "feedback",
      "qualifi"
    ],
    "token_count":4,
    "processed_text":"oh got feedback qualifi"
  },
  {
    "label":0,
    "text":"feel jip todayindescrib beauti day est spend day sew room",
    "cleaned_text":"feel jip todayindescrib beauti day est spend day sew room",
    "normalized_text":"feel jip todayindescrib beauti day est spend day sew room",
    "tokens":[
      "feel",
      "jip",
      "todayindescrib",
      "beauti",
      "day",
      "est",
      "spend",
      "day",
      "sew",
      "room"
    ],
    "token_count":10,
    "processed_text":"feel jip todayindescrib beauti day est spend day sew room"
  },
  {
    "label":0,
    "text":"hmm raini sunni raini sunni bathgat galaday may get wet",
    "cleaned_text":"hmm raini sunni raini sunni bathgat galaday may get wet",
    "normalized_text":"hmm raini sunni raini sunni bathgat galaday may get wet",
    "tokens":[
      "hmm",
      "raini",
      "sunni",
      "raini",
      "sunni",
      "bathgat",
      "galaday",
      "may",
      "get",
      "wet"
    ],
    "token_count":10,
    "processed_text":"hmm raini sunni raini sunni bathgat galaday may get wet"
  },
  {
    "label":4,
    "text":"enjoy first day summer",
    "cleaned_text":"enjoy first day summer",
    "normalized_text":"enjoy first day summer",
    "tokens":[
      "enjoy",
      "first",
      "day",
      "summer"
    ],
    "token_count":4,
    "processed_text":"enjoy first day summer"
  },
  {
    "label":0,
    "text":"need help someon giv advis",
    "cleaned_text":"need help someon giv advis",
    "normalized_text":"need help someon giv advis",
    "tokens":[
      "need",
      "help",
      "someon",
      "giv",
      "advi"
    ],
    "token_count":5,
    "processed_text":"need help someon giv advi"
  },
  {
    "label":0,
    "text":"oh lord wont let sleep stupid cramp ill never get sleep play sim till cramp gonesigh",
    "cleaned_text":"oh lord wont let sleep stupid cramp ill never get sleep play sim till cramp gonesigh",
    "normalized_text":"oh lord wont let sleep stupid cramp ill never get sleep play sim till cramp gonesigh",
    "tokens":[
      "oh",
      "lord",
      "wont",
      "let",
      "sleep",
      "stupid",
      "cramp",
      "ill",
      "never",
      "get",
      "sleep",
      "play",
      "sim",
      "till",
      "cramp",
      "gonesigh"
    ],
    "token_count":16,
    "processed_text":"oh lord wont let sleep stupid cramp ill never get sleep play sim till cramp gonesigh"
  },
  {
    "label":4,
    "text":"better last time buck brown underneath amp blond top expens blond im save money",
    "cleaned_text":"better last time buck brown underneath amp blond top expens blond im save money",
    "normalized_text":"better last time buck brown underneath amp blond top expens blond im save money",
    "tokens":[
      "better",
      "last",
      "time",
      "buck",
      "brown",
      "underneath",
      "amp",
      "blond",
      "top",
      "expen",
      "blond",
      "im",
      "save",
      "money"
    ],
    "token_count":14,
    "processed_text":"better last time buck brown underneath amp blond top expen blond im save money"
  },
  {
    "label":4,
    "text":"im see wolverin",
    "cleaned_text":"im see wolverin",
    "normalized_text":"im see wolverin",
    "tokens":[
      "im",
      "see",
      "wolverin"
    ],
    "token_count":3,
    "processed_text":"im see wolverin"
  },
  {
    "label":4,
    "text":"gwenyth parltrow pretti drink beer haha that awesom ps im almost sure spelt name wrong sorrrrryy haha",
    "cleaned_text":"gwenyth parltrow pretti drink beer haha that awesom ps im almost sure spelt name wrong sorrrrryy haha",
    "normalized_text":"gwenyth parltrow pretti drink beer haha that awesom ps im almost sure spelt name wrong sorrrrryy haha",
    "tokens":[
      "gwenyth",
      "parltrow",
      "pretti",
      "drink",
      "beer",
      "haha",
      "awesom",
      "ps",
      "im",
      "almost",
      "sure",
      "spelt",
      "name",
      "wrong",
      "sorrrrryy",
      "haha"
    ],
    "token_count":16,
    "processed_text":"gwenyth parltrow pretti drink beer haha awesom ps im almost sure spelt name wrong sorrrrryy haha"
  },
  {
    "label":0,
    "text":"without",
    "cleaned_text":"without",
    "normalized_text":"without",
    "tokens":[
      "without"
    ],
    "token_count":1,
    "processed_text":"without"
  },
  {
    "label":4,
    "text":"doesnt take much",
    "cleaned_text":"doesnt take much",
    "normalized_text":"doesnt take much",
    "tokens":[
      "doesnt",
      "take",
      "much"
    ],
    "token_count":3,
    "processed_text":"doesnt take much"
  },
  {
    "label":4,
    "text":"welcom glad could help",
    "cleaned_text":"welcom glad could help",
    "normalized_text":"welcom glad could help",
    "tokens":[
      "welcom",
      "glad",
      "help"
    ],
    "token_count":3,
    "processed_text":"welcom glad help"
  },
  {
    "label":4,
    "text":"morn sunshin nice day demi",
    "cleaned_text":"morn sunshin nice day demi",
    "normalized_text":"morn sunshin nice day demi",
    "tokens":[
      "morn",
      "sunshin",
      "nice",
      "day",
      "demi"
    ],
    "token_count":5,
    "processed_text":"morn sunshin nice day demi"
  },
  {
    "label":0,
    "text":"dad call slip bathroom woke day later blood place",
    "cleaned_text":"dad call slip bathroom woke day later blood place",
    "normalized_text":"dad call slip bathroom woke day later blood place",
    "tokens":[
      "dad",
      "call",
      "slip",
      "bathroom",
      "woke",
      "day",
      "later",
      "blood",
      "place"
    ],
    "token_count":9,
    "processed_text":"dad call slip bathroom woke day later blood place"
  },
  {
    "label":0,
    "text":"bore lone",
    "cleaned_text":"bore lone",
    "normalized_text":"bore lone",
    "tokens":[
      "bore",
      "lone"
    ],
    "token_count":2,
    "processed_text":"bore lone"
  },
  {
    "label":4,
    "text":"relax home fabul husband ador",
    "cleaned_text":"relax home fabul husband ador",
    "normalized_text":"relax home fabul husband ador",
    "tokens":[
      "relax",
      "home",
      "fabul",
      "husband",
      "ador"
    ],
    "token_count":5,
    "processed_text":"relax home fabul husband ador"
  },
  {
    "label":0,
    "text":"bodi isnt use breakfast cereal milk morn rock world might hit gym today",
    "cleaned_text":"bodi isnt use breakfast cereal milk morn rock world might hit gym today",
    "normalized_text":"bodi isnt use breakfast cereal milk morn rock world might hit gym today",
    "tokens":[
      "bodi",
      "isnt",
      "use",
      "breakfast",
      "cereal",
      "milk",
      "morn",
      "rock",
      "world",
      "hit",
      "gym",
      "today"
    ],
    "token_count":12,
    "processed_text":"bodi isnt use breakfast cereal milk morn rock world hit gym today"
  },
  {
    "label":4,
    "text":"well werent marri id give larg hug jame scott could",
    "cleaned_text":"well werent marri id give larg hug jame scott could",
    "normalized_text":"well werent marri id give larg hug jame scott could",
    "tokens":[
      "well",
      "werent",
      "marri",
      "id",
      "give",
      "larg",
      "hug",
      "jame",
      "scott"
    ],
    "token_count":9,
    "processed_text":"well werent marri id give larg hug jame scott"
  },
  {
    "label":0,
    "text":"stink comput sorri didnt get better news",
    "cleaned_text":"stink comput sorri didnt get better news",
    "normalized_text":"stink comput sorri didnt get better news",
    "tokens":[
      "stink",
      "comput",
      "sorri",
      "didnt",
      "get",
      "better",
      "news"
    ],
    "token_count":7,
    "processed_text":"stink comput sorri didnt get better news"
  },
  {
    "label":0,
    "text":"iphon os far aggress wifi power manag sometim take upto sec come standbi",
    "cleaned_text":"iphon os far aggress wifi power manag sometim take upto sec come standbi",
    "normalized_text":"iphon os far aggress wifi power manag sometim take upto sec come standbi",
    "tokens":[
      "iphon",
      "os",
      "far",
      "aggress",
      "wifi",
      "power",
      "manag",
      "sometim",
      "take",
      "upto",
      "sec",
      "come",
      "standbi"
    ],
    "token_count":13,
    "processed_text":"iphon os far aggress wifi power manag sometim take upto sec come standbi"
  },
  {
    "label":0,
    "text":"hand pain sloww twitter night tonight",
    "cleaned_text":"hand pain sloww twitter night tonight",
    "normalized_text":"hand pain sloww twitter night tonight",
    "tokens":[
      "hand",
      "pain",
      "sloww",
      "twitter",
      "night",
      "tonight"
    ],
    "token_count":6,
    "processed_text":"hand pain sloww twitter night tonight"
  },
  {
    "label":4,
    "text":"electro good long make",
    "cleaned_text":"electro good long make",
    "normalized_text":"electro good long make",
    "tokens":[
      "electro",
      "good",
      "long",
      "make"
    ],
    "token_count":4,
    "processed_text":"electro good long make"
  },
  {
    "label":0,
    "text":"headfon broke got ta get rid gum",
    "cleaned_text":"headfon broke got ta get rid gum",
    "normalized_text":"headfon broke got ta get rid gum",
    "tokens":[
      "headfon",
      "broke",
      "got",
      "ta",
      "get",
      "rid",
      "gum"
    ],
    "token_count":7,
    "processed_text":"headfon broke got ta get rid gum"
  },
  {
    "label":0,
    "text":"even keep much shit music ipod",
    "cleaned_text":"even keep much shit music ipod",
    "normalized_text":"even keep much shit music ipod",
    "tokens":[
      "even",
      "keep",
      "much",
      "shit",
      "music",
      "ipod"
    ],
    "token_count":6,
    "processed_text":"even keep much shit music ipod"
  },
  {
    "label":0,
    "text":"haha yep miss besti bud savannah jet ski jb day agod",
    "cleaned_text":"haha yep miss besti bud savannah jet ski jb day agod",
    "normalized_text":"haha yep miss besti bud savannah jet ski jb day agod",
    "tokens":[
      "haha",
      "yep",
      "miss",
      "besti",
      "bud",
      "savannah",
      "jet",
      "ski",
      "jb",
      "day",
      "agod"
    ],
    "token_count":11,
    "processed_text":"haha yep miss besti bud savannah jet ski jb day agod"
  },
  {
    "label":4,
    "text":"quotbig dick everyon itquot spam messag header sure delight fan big dick gender",
    "cleaned_text":"quotbig dick everyon itquot spam messag header sure delight fan big dick gender",
    "normalized_text":"quotbig dick everyon itquot spam messag header sure delight fan big dick gender",
    "tokens":[
      "quotbig",
      "dick",
      "everyon",
      "itquot",
      "spam",
      "messag",
      "header",
      "sure",
      "delight",
      "fan",
      "big",
      "dick",
      "gender"
    ],
    "token_count":13,
    "processed_text":"quotbig dick everyon itquot spam messag header sure delight fan big dick gender"
  },
  {
    "label":4,
    "text":"thank send good recip eat tweet lol",
    "cleaned_text":"thank send good recip eat tweet lol",
    "normalized_text":"thank send good recip eat tweet lol",
    "tokens":[
      "thank",
      "send",
      "good",
      "recip",
      "eat",
      "tweet",
      "lol"
    ],
    "token_count":7,
    "processed_text":"thank send good recip eat tweet lol"
  },
  {
    "label":0,
    "text":"well ive go dual version that need monitor laptop get confus",
    "cleaned_text":"well ive go dual version that need monitor laptop get confus",
    "normalized_text":"well ive go dual version that need monitor laptop get confus",
    "tokens":[
      "well",
      "ive",
      "go",
      "dual",
      "version",
      "need",
      "monitor",
      "laptop",
      "get",
      "confu"
    ],
    "token_count":10,
    "processed_text":"well ive go dual version need monitor laptop get confu"
  },
  {
    "label":4,
    "text":"beyonc marri shouldnt speak singl girl hell know",
    "cleaned_text":"beyonc marri shouldnt speak singl girl hell know",
    "normalized_text":"beyonc marri shouldnt speak singl girl hell know",
    "tokens":[
      "beyonc",
      "marri",
      "shouldnt",
      "speak",
      "singl",
      "girl",
      "hell",
      "know"
    ],
    "token_count":8,
    "processed_text":"beyonc marri shouldnt speak singl girl hell know"
  },
  {
    "label":4,
    "text":"wait momma wake gotta grab littl one next hour think take mom frisch cheesecak fac",
    "cleaned_text":"wait momma wake gotta grab littl one next hour think take mom frisch cheesecak fac",
    "normalized_text":"wait momma wake gotta grab littl one next hour think take mom frisch cheesecak fac",
    "tokens":[
      "wait",
      "momma",
      "wake",
      "got",
      "ta",
      "grab",
      "littl",
      "one",
      "next",
      "hour",
      "think",
      "take",
      "mom",
      "frisch",
      "cheesecak",
      "fac"
    ],
    "token_count":16,
    "processed_text":"wait momma wake got ta grab littl one next hour think take mom frisch cheesecak fac"
  },
  {
    "label":0,
    "text":"look recip diabet actual tast good hard recent diagnos type cant find thing good eat",
    "cleaned_text":"look recip diabet actual tast good hard recent diagnos type cant find thing good eat",
    "normalized_text":"look recip diabet actual tast good hard recent diagnos type cant find thing good eat",
    "tokens":[
      "look",
      "recip",
      "diabet",
      "actual",
      "tast",
      "good",
      "hard",
      "recent",
      "diagno",
      "type",
      "cant",
      "find",
      "thing",
      "good",
      "eat"
    ],
    "token_count":15,
    "processed_text":"look recip diabet actual tast good hard recent diagno type cant find thing good eat"
  },
  {
    "label":0,
    "text":"one like get earli cri babi poor kiki must feel goodstillteeth gggrrrrrrr",
    "cleaned_text":"one like get earli cri babi poor kiki must feel goodstillteeth gggrrrrrrr",
    "normalized_text":"one like get earli cri babi poor kiki must feel goodstillteeth gggrrrrrrr",
    "tokens":[
      "one",
      "like",
      "get",
      "earli",
      "cri",
      "babi",
      "poor",
      "kiki",
      "feel",
      "goodstillteeth",
      "gggrrrrrrr"
    ],
    "token_count":11,
    "processed_text":"one like get earli cri babi poor kiki feel goodstillteeth gggrrrrrrr"
  },
  {
    "label":4,
    "text":"watch quotmonkeylectr pro bike wheel light displayquot beat light bike",
    "cleaned_text":"watch quotmonkeylectr pro bike wheel light displayquot beat light bike",
    "normalized_text":"watch quotmonkeylectr pro bike wheel light displayquot beat light bike",
    "tokens":[
      "watch",
      "quotmonkeylectr",
      "pro",
      "bike",
      "wheel",
      "light",
      "displayquot",
      "beat",
      "light",
      "bike"
    ],
    "token_count":10,
    "processed_text":"watch quotmonkeylectr pro bike wheel light displayquot beat light bike"
  },
  {
    "label":4,
    "text":"summer final",
    "cleaned_text":"summer final",
    "normalized_text":"summer final",
    "tokens":[
      "summer",
      "final"
    ],
    "token_count":2,
    "processed_text":"summer final"
  },
  {
    "label":0,
    "text":"ok work updat dont see chang sm though",
    "cleaned_text":"ok work updat dont see chang sm though",
    "normalized_text":"ok work updat dont see chang sm though",
    "tokens":[
      "ok",
      "work",
      "updat",
      "dont",
      "see",
      "chang",
      "sm",
      "though"
    ],
    "token_count":8,
    "processed_text":"ok work updat dont see chang sm though"
  },
  {
    "label":4,
    "text":"go go wash hair",
    "cleaned_text":"go go wash hair",
    "normalized_text":"go go wash hair",
    "tokens":[
      "go",
      "go",
      "wash",
      "hair"
    ],
    "token_count":4,
    "processed_text":"go go wash hair"
  },
  {
    "label":0,
    "text":"sad face itun download crash laptop",
    "cleaned_text":"sad face itun download crash laptop",
    "normalized_text":"sad face itun download crash laptop",
    "tokens":[
      "sad",
      "face",
      "itun",
      "download",
      "crash",
      "laptop"
    ],
    "token_count":6,
    "processed_text":"sad face itun download crash laptop"
  },
  {
    "label":0,
    "text":"sorri im fuck ill",
    "cleaned_text":"sorri im fuck ill",
    "normalized_text":"sorri im fuck ill",
    "tokens":[
      "sorri",
      "im",
      "fuck",
      "ill"
    ],
    "token_count":4,
    "processed_text":"sorri im fuck ill"
  },
  {
    "label":4,
    "text":"bbq rib what menu tonight sure definit accompani movi rampr",
    "cleaned_text":"bbq rib what menu tonight sure definit accompani movi rampr",
    "normalized_text":"bbq rib what menu tonight sure definit accompani movi rampr",
    "tokens":[
      "bbq",
      "rib",
      "menu",
      "tonight",
      "sure",
      "definit",
      "accompani",
      "movi",
      "rampr"
    ],
    "token_count":9,
    "processed_text":"bbq rib menu tonight sure definit accompani movi rampr"
  },
  {
    "label":0,
    "text":"thank decid give old hq go account scoundrel",
    "cleaned_text":"thank decid give old hq go account scoundrel",
    "normalized_text":"thank decid give old hq go account scoundrel",
    "tokens":[
      "thank",
      "decid",
      "give",
      "old",
      "hq",
      "go",
      "account",
      "scoundrel"
    ],
    "token_count":8,
    "processed_text":"thank decid give old hq go account scoundrel"
  },
  {
    "label":4,
    "text":"mayb littl",
    "cleaned_text":"mayb littl",
    "normalized_text":"mayb littl",
    "tokens":[
      "mayb",
      "littl"
    ],
    "token_count":2,
    "processed_text":"mayb littl"
  },
  {
    "label":0,
    "text":"gotta csection",
    "cleaned_text":"gotta csection",
    "normalized_text":"gotta csection",
    "tokens":[
      "got",
      "ta",
      "csection"
    ],
    "token_count":3,
    "processed_text":"got ta csection"
  },
  {
    "label":0,
    "text":"wish id gone lion brand yarn booth make fair give away bag free yarn bum",
    "cleaned_text":"wish id gone lion brand yarn booth make fair give away bag free yarn bum",
    "normalized_text":"wish id gone lion brand yarn booth make fair give away bag free yarn bum",
    "tokens":[
      "wish",
      "id",
      "gone",
      "lion",
      "brand",
      "yarn",
      "booth",
      "make",
      "fair",
      "give",
      "away",
      "bag",
      "free",
      "yarn",
      "bum"
    ],
    "token_count":15,
    "processed_text":"wish id gone lion brand yarn booth make fair give away bag free yarn bum"
  },
  {
    "label":0,
    "text":"ugh dont feel good",
    "cleaned_text":"ugh dont feel good",
    "normalized_text":"ugh dont feel good",
    "tokens":[
      "ugh",
      "dont",
      "feel",
      "good"
    ],
    "token_count":4,
    "processed_text":"ugh dont feel good"
  },
  {
    "label":4,
    "text":"reuben roxi similar",
    "cleaned_text":"reuben roxi similar",
    "normalized_text":"reuben roxi similar",
    "tokens":[
      "reuben",
      "roxi",
      "similar"
    ],
    "token_count":3,
    "processed_text":"reuben roxi similar"
  },
  {
    "label":0,
    "text":"cant open mozilla firefox eeeeeekkk",
    "cleaned_text":"cant open mozilla firefox eeeeeekkk",
    "normalized_text":"cant open mozilla firefox eeeeeekkk",
    "tokens":[
      "cant",
      "open",
      "mozilla",
      "firefox",
      "eeeeeekkk"
    ],
    "token_count":5,
    "processed_text":"cant open mozilla firefox eeeeeekkk"
  },
  {
    "label":0,
    "text":"niki unfortun time tri regist onlin regi close moment settl volunt dday",
    "cleaned_text":"niki unfortun time tri regist onlin regi close moment settl volunt dday",
    "normalized_text":"niki unfortun time tri regist onlin regi close moment settl volunt dday",
    "tokens":[
      "niki",
      "unfortun",
      "time",
      "tri",
      "regist",
      "onlin",
      "regi",
      "close",
      "moment",
      "settl",
      "volunt",
      "dday"
    ],
    "token_count":12,
    "processed_text":"niki unfortun time tri regist onlin regi close moment settl volunt dday"
  },
  {
    "label":0,
    "text":"work today",
    "cleaned_text":"work today",
    "normalized_text":"work today",
    "tokens":[
      "work",
      "today"
    ],
    "token_count":2,
    "processed_text":"work today"
  },
  {
    "label":0,
    "text":"thing talk pathet",
    "cleaned_text":"thing talk pathet",
    "normalized_text":"thing talk pathet",
    "tokens":[
      "thing",
      "talk",
      "pathet"
    ],
    "token_count":3,
    "processed_text":"thing talk pathet"
  },
  {
    "label":4,
    "text":"thank im still work away get human cylon done first",
    "cleaned_text":"thank im still work away get human cylon done first",
    "normalized_text":"thank im still work away get human cylon done first",
    "tokens":[
      "thank",
      "im",
      "still",
      "work",
      "away",
      "get",
      "human",
      "cylon",
      "done",
      "first"
    ],
    "token_count":10,
    "processed_text":"thank im still work away get human cylon done first"
  },
  {
    "label":0,
    "text":"total agre half way done want complet today",
    "cleaned_text":"total agre half way done want complet today",
    "normalized_text":"total agre half way done want complet today",
    "tokens":[
      "total",
      "agr",
      "half",
      "way",
      "done",
      "want",
      "complet",
      "today"
    ],
    "token_count":8,
    "processed_text":"total agr half way done want complet today"
  },
  {
    "label":0,
    "text":"got love dm random person tell great intend",
    "cleaned_text":"got love dm random person tell great intend",
    "normalized_text":"got love dm random person tell great intend",
    "tokens":[
      "got",
      "love",
      "dm",
      "random",
      "person",
      "tell",
      "great",
      "intend"
    ],
    "token_count":8,
    "processed_text":"got love dm random person tell great intend"
  },
  {
    "label":0,
    "text":"wish could nick",
    "cleaned_text":"wish could nick",
    "normalized_text":"wish could nick",
    "tokens":[
      "wish",
      "nick"
    ],
    "token_count":2,
    "processed_text":"wish nick"
  },
  {
    "label":0,
    "text":"still price",
    "cleaned_text":"still price",
    "normalized_text":"still price",
    "tokens":[
      "still",
      "price"
    ],
    "token_count":2,
    "processed_text":"still price"
  },
  {
    "label":0,
    "text":"fell love hous yesterday saw price fell love",
    "cleaned_text":"fell love hous yesterday saw price fell love",
    "normalized_text":"fell love hous yesterday saw price fell love",
    "tokens":[
      "fell",
      "love",
      "hou",
      "yesterday",
      "saw",
      "price",
      "fell",
      "love"
    ],
    "token_count":8,
    "processed_text":"fell love hou yesterday saw price fell love"
  },
  {
    "label":0,
    "text":"look librari kanpur wanna read non fiction book librari r full fiction",
    "cleaned_text":"look librari kanpur wanna read non fiction book librari r full fiction",
    "normalized_text":"look librari kanpur wanna read non fiction book librari r full fiction",
    "tokens":[
      "look",
      "librari",
      "kanpur",
      "wan",
      "na",
      "read",
      "non",
      "fiction",
      "book",
      "librari",
      "full",
      "fiction"
    ],
    "token_count":12,
    "processed_text":"look librari kanpur wan na read non fiction book librari full fiction"
  },
  {
    "label":4,
    "text":"go pick special deliveri up warehous",
    "cleaned_text":"go pick special deliveri up warehous",
    "normalized_text":"go pick special deliveri up warehous",
    "tokens":[
      "go",
      "pick",
      "special",
      "deliveri",
      "wareh"
    ],
    "token_count":5,
    "processed_text":"go pick special deliveri wareh"
  },
  {
    "label":4,
    "text":"lordship well fine morn",
    "cleaned_text":"lordship well fine morn",
    "normalized_text":"lordship well fine morn",
    "tokens":[
      "lordship",
      "well",
      "fine",
      "morn"
    ],
    "token_count":4,
    "processed_text":"lordship well fine morn"
  },
  {
    "label":0,
    "text":"sorri guy let rephras last tweet wish could place induc coma day sorri mislead tweet",
    "cleaned_text":"sorri guy let rephras last tweet wish could place induc coma day sorri mislead tweet",
    "normalized_text":"sorri guy let rephras last tweet wish could place induc coma day sorri mislead tweet",
    "tokens":[
      "sorri",
      "guy",
      "let",
      "rephra",
      "last",
      "tweet",
      "wish",
      "place",
      "induc",
      "coma",
      "day",
      "sorri",
      "mislead",
      "tweet"
    ],
    "token_count":14,
    "processed_text":"sorri guy let rephra last tweet wish place induc coma day sorri mislead tweet"
  },
  {
    "label":4,
    "text":"omg amaz make cum pant watch hell yeah",
    "cleaned_text":"omg amaz make cum pant watch hell yeah",
    "normalized_text":"omg amaz make cum pant watch hell yeah",
    "tokens":[
      "omg",
      "amaz",
      "make",
      "cum",
      "pant",
      "watch",
      "hell",
      "yeah"
    ],
    "token_count":8,
    "processed_text":"omg amaz make cum pant watch hell yeah"
  },
  {
    "label":4,
    "text":"teen charg even servic tonight im proud littl booger",
    "cleaned_text":"teen charg even servic tonight im proud littl booger",
    "normalized_text":"teen charg even servic tonight im proud littl booger",
    "tokens":[
      "teen",
      "charg",
      "even",
      "servic",
      "tonight",
      "im",
      "proud",
      "littl",
      "booger"
    ],
    "token_count":9,
    "processed_text":"teen charg even servic tonight im proud littl booger"
  },
  {
    "label":0,
    "text":"got home work",
    "cleaned_text":"got home work",
    "normalized_text":"got home work",
    "tokens":[
      "got",
      "home",
      "work"
    ],
    "token_count":3,
    "processed_text":"got home work"
  },
  {
    "label":0,
    "text":"watch new knightrid sky worri say im actual enjoy far",
    "cleaned_text":"watch new knightrid sky worri say im actual enjoy far",
    "normalized_text":"watch new knightrid sky worri say im actual enjoy far",
    "tokens":[
      "watch",
      "new",
      "knightrid",
      "sky",
      "worri",
      "say",
      "im",
      "actual",
      "enjoy",
      "far"
    ],
    "token_count":10,
    "processed_text":"watch new knightrid sky worri say im actual enjoy far"
  },
  {
    "label":0,
    "text":"oh miss",
    "cleaned_text":"oh miss",
    "normalized_text":"oh miss",
    "tokens":[
      "oh",
      "miss"
    ],
    "token_count":2,
    "processed_text":"oh miss"
  },
  {
    "label":0,
    "text":"arent followerswhat appear sidebar",
    "cleaned_text":"arent followerswhat appear sidebar",
    "normalized_text":"arent followerswhat appear sidebar",
    "tokens":[
      "arent",
      "followerswhat",
      "appear",
      "sidebar"
    ],
    "token_count":4,
    "processed_text":"arent followerswhat appear sidebar"
  },
  {
    "label":4,
    "text":"happi first birthday littl boy daughter celebr wed",
    "cleaned_text":"happi first birthday littl boy daughter celebr wed",
    "normalized_text":"happi first birthday littl boy daughter celebr wed",
    "tokens":[
      "happi",
      "first",
      "birthday",
      "littl",
      "boy",
      "daughter",
      "celebr",
      "wed"
    ],
    "token_count":8,
    "processed_text":"happi first birthday littl boy daughter celebr wed"
  },
  {
    "label":4,
    "text":"found use version control system help well dont get cocki ahead though",
    "cleaned_text":"found use version control system help well dont get cocki ahead though",
    "normalized_text":"found use version control system help well dont get cocki ahead though",
    "tokens":[
      "found",
      "use",
      "version",
      "control",
      "system",
      "help",
      "well",
      "dont",
      "get",
      "cocki",
      "ahead",
      "though"
    ],
    "token_count":12,
    "processed_text":"found use version control system help well dont get cocki ahead though"
  },
  {
    "label":4,
    "text":"ye cupcak",
    "cleaned_text":"ye cupcak",
    "normalized_text":"ye cupcak",
    "tokens":[
      "ye",
      "cupcak"
    ],
    "token_count":2,
    "processed_text":"ye cupcak"
  },
  {
    "label":0,
    "text":"prowl want interact cannon prowlerfor lulz xd dont think player activ",
    "cleaned_text":"prowl want interact cannon prowlerfor lulz xd dont think player activ",
    "normalized_text":"prowl want interact cannon prowlerfor lulz xd dont think player activ",
    "tokens":[
      "prowl",
      "want",
      "interact",
      "cannon",
      "prowlerfor",
      "lulz",
      "xd",
      "dont",
      "think",
      "player",
      "activ"
    ],
    "token_count":11,
    "processed_text":"prowl want interact cannon prowlerfor lulz xd dont think player activ"
  },
  {
    "label":4,
    "text":"solv submachin wo help got system",
    "cleaned_text":"solv submachin wo help got system",
    "normalized_text":"solv submachin wo help got system",
    "tokens":[
      "solv",
      "submachin",
      "wo",
      "help",
      "got",
      "system"
    ],
    "token_count":6,
    "processed_text":"solv submachin wo help got system"
  },
  {
    "label":4,
    "text":"goppl make meant let analyz begin sigh wanna fish tank gallon",
    "cleaned_text":"goppl make meant let analyz begin sigh wanna fish tank gallon",
    "normalized_text":"goppl make meant let analyz begin sigh wanna fish tank gallon",
    "tokens":[
      "goppl",
      "make",
      "meant",
      "let",
      "analyz",
      "begin",
      "sigh",
      "wan",
      "na",
      "fish",
      "tank",
      "gallon"
    ],
    "token_count":12,
    "processed_text":"goppl make meant let analyz begin sigh wan na fish tank gallon"
  },
  {
    "label":4,
    "text":"mmmmkevin pereira hahaha cant believ brother watch e",
    "cleaned_text":"mmmmkevin pereira hahaha cant believ brother watch e",
    "normalized_text":"mmmmkevin pereira hahaha cant believ brother watch e",
    "tokens":[
      "mmmmkevin",
      "pereira",
      "hahaha",
      "cant",
      "believ",
      "brother",
      "watch"
    ],
    "token_count":7,
    "processed_text":"mmmmkevin pereira hahaha cant believ brother watch"
  },
  {
    "label":0,
    "text":"homework homework homework ugh",
    "cleaned_text":"homework homework homework ugh",
    "normalized_text":"homework homework homework ugh",
    "tokens":[
      "homework",
      "homework",
      "homework",
      "ugh"
    ],
    "token_count":4,
    "processed_text":"homework homework homework ugh"
  },
  {
    "label":4,
    "text":"dont think explain cant find",
    "cleaned_text":"dont think explain cant find",
    "normalized_text":"dont think explain cant find",
    "tokens":[
      "dont",
      "think",
      "explain",
      "cant",
      "find"
    ],
    "token_count":5,
    "processed_text":"dont think explain cant find"
  },
  {
    "label":4,
    "text":"comput everyon els appear lost space ill sort later",
    "cleaned_text":"comput everyon els appear lost space ill sort later",
    "normalized_text":"comput everyon els appear lost space ill sort later",
    "tokens":[
      "comput",
      "everyon",
      "el",
      "appear",
      "lost",
      "space",
      "ill",
      "sort",
      "later"
    ],
    "token_count":9,
    "processed_text":"comput everyon el appear lost space ill sort later"
  },
  {
    "label":4,
    "text":"blahhhh im bore today fun",
    "cleaned_text":"blahhhh im bore today fun",
    "normalized_text":"blahhhh im bore today fun",
    "tokens":[
      "blahhhh",
      "im",
      "bore",
      "today",
      "fun"
    ],
    "token_count":5,
    "processed_text":"blahhhh im bore today fun"
  },
  {
    "label":0,
    "text":"hi gloomi day pa weather",
    "cleaned_text":"hi gloomi day pa weather",
    "normalized_text":"hi gloomi day pa weather",
    "tokens":[
      "hi",
      "gloomi",
      "day",
      "pa",
      "weather"
    ],
    "token_count":5,
    "processed_text":"hi gloomi day pa weather"
  },
  {
    "label":4,
    "text":"make banana pud",
    "cleaned_text":"make banana pud",
    "normalized_text":"make banana pud",
    "tokens":[
      "make",
      "banana",
      "pud"
    ],
    "token_count":3,
    "processed_text":"make banana pud"
  },
  {
    "label":0,
    "text":"miss good stuff",
    "cleaned_text":"miss good stuff",
    "normalized_text":"miss good stuff",
    "tokens":[
      "miss",
      "good",
      "stuff"
    ],
    "token_count":3,
    "processed_text":"miss good stuff"
  },
  {
    "label":4,
    "text":"lol prophet your make good nutrit choic",
    "cleaned_text":"lol prophet your make good nutrit choic",
    "normalized_text":"lol prophet your make good nutrit choic",
    "tokens":[
      "lol",
      "prophet",
      "make",
      "good",
      "nutrit",
      "choic"
    ],
    "token_count":6,
    "processed_text":"lol prophet make good nutrit choic"
  },
  {
    "label":4,
    "text":"lot usual laundri thank god clean hous prais god cut hedg thank prais god",
    "cleaned_text":"lot usual laundri thank god clean hous prais god cut hedg thank prais god",
    "normalized_text":"lot usual laundri thank god clean hous prais god cut hedg thank prais god",
    "tokens":[
      "lot",
      "usual",
      "laundri",
      "thank",
      "god",
      "clean",
      "hou",
      "prai",
      "god",
      "cut",
      "hedg",
      "thank",
      "prai",
      "god"
    ],
    "token_count":14,
    "processed_text":"lot usual laundri thank god clean hou prai god cut hedg thank prai god"
  },
  {
    "label":0,
    "text":"umm doesnt let lol say",
    "cleaned_text":"umm doesnt let lol say",
    "normalized_text":"umm doesnt let lol say",
    "tokens":[
      "umm",
      "doesnt",
      "let",
      "lol",
      "say"
    ],
    "token_count":5,
    "processed_text":"umm doesnt let lol say"
  },
  {
    "label":4,
    "text":"mean movi night north bay still",
    "cleaned_text":"mean movi night north bay still",
    "normalized_text":"mean movi night north bay still",
    "tokens":[
      "mean",
      "movi",
      "night",
      "north",
      "bay",
      "still"
    ],
    "token_count":6,
    "processed_text":"mean movi night north bay still"
  },
  {
    "label":4,
    "text":"go sm jk tae",
    "cleaned_text":"go sm jk tae",
    "normalized_text":"go sm jk tae",
    "tokens":[
      "go",
      "sm",
      "jk",
      "tae"
    ],
    "token_count":4,
    "processed_text":"go sm jk tae"
  },
  {
    "label":4,
    "text":"finish quotbedtim storiesquot ador need start rent movi often",
    "cleaned_text":"finish quotbedtim storiesquot ador need start rent movi often",
    "normalized_text":"finish quotbedtim storiesquot ador need start rent movi often",
    "tokens":[
      "finish",
      "quotbedtim",
      "storiesquot",
      "ador",
      "need",
      "start",
      "rent",
      "movi",
      "often"
    ],
    "token_count":9,
    "processed_text":"finish quotbedtim storiesquot ador need start rent movi often"
  },
  {
    "label":0,
    "text":"miss statsoc miss ext babi",
    "cleaned_text":"miss statsoc miss ext babi",
    "normalized_text":"miss statsoc miss ext babi",
    "tokens":[
      "miss",
      "statsoc",
      "miss",
      "ext",
      "babi"
    ],
    "token_count":5,
    "processed_text":"miss statsoc miss ext babi"
  },
  {
    "label":4,
    "text":"aw thank aurelio immort quiz hat creation talent rhyme",
    "cleaned_text":"aw thank aurelio immort quiz hat creation talent rhyme",
    "normalized_text":"aw thank aurelio immort quiz hat creation talent rhyme",
    "tokens":[
      "aw",
      "thank",
      "aurelio",
      "immort",
      "quiz",
      "hat",
      "creation",
      "talent",
      "rhyme"
    ],
    "token_count":9,
    "processed_text":"aw thank aurelio immort quiz hat creation talent rhyme"
  },
  {
    "label":0,
    "text":"bomb got aircon car fulli fog oh car frike give smoke",
    "cleaned_text":"bomb got aircon car fulli fog oh car frike give smoke",
    "normalized_text":"bomb got aircon car fulli fog oh car frike give smoke",
    "tokens":[
      "bomb",
      "got",
      "aircon",
      "car",
      "fulli",
      "fog",
      "oh",
      "car",
      "frike",
      "give",
      "smoke"
    ],
    "token_count":11,
    "processed_text":"bomb got aircon car fulli fog oh car frike give smoke"
  },
  {
    "label":4,
    "text":"seen quotwolverinequot last night wait till becom june watch transform reveng fallen nice",
    "cleaned_text":"seen quotwolverinequot last night wait till becom june watch transform reveng fallen nice",
    "normalized_text":"seen quotwolverinequot last night wait till becom june watch transform reveng fallen nice",
    "tokens":[
      "seen",
      "last",
      "night",
      "wait",
      "till",
      "becom",
      "june",
      "watch",
      "transform",
      "reveng",
      "fallen",
      "nice"
    ],
    "token_count":12,
    "processed_text":"seen last night wait till becom june watch transform reveng fallen nice"
  },
  {
    "label":0,
    "text":"cluckingblossom thought trade probabl best band saw set short fail grab one cd",
    "cleaned_text":"cluckingblossom thought trade probabl best band saw set short fail grab one cd",
    "normalized_text":"cluckingblossom thought trade probabl best band saw set short fail grab one cd",
    "tokens":[
      "cluckingblossom",
      "thought",
      "trade",
      "probabl",
      "best",
      "band",
      "saw",
      "set",
      "short",
      "fail",
      "grab",
      "one",
      "cd"
    ],
    "token_count":13,
    "processed_text":"cluckingblossom thought trade probabl best band saw set short fail grab one cd"
  },
  {
    "label":4,
    "text":"march today dunno ill goin though need paintin gran friend first",
    "cleaned_text":"march today dunno ill goin though need paintin gran friend first",
    "normalized_text":"march today dunno ill goin though need paintin gran friend first",
    "tokens":[
      "march",
      "today",
      "dunno",
      "ill",
      "goin",
      "though",
      "need",
      "paintin",
      "gran",
      "friend",
      "first"
    ],
    "token_count":11,
    "processed_text":"march today dunno ill goin though need paintin gran friend first"
  },
  {
    "label":4,
    "text":"agreeit like poetri motionnow wait bluray uncut releas",
    "cleaned_text":"agreeit like poetri motionnow wait bluray uncut releas",
    "normalized_text":"agreeit like poetri motionnow wait bluray uncut releas",
    "tokens":[
      "agreeit",
      "like",
      "poetri",
      "motionnow",
      "wait",
      "bluray",
      "uncut",
      "relea"
    ],
    "token_count":8,
    "processed_text":"agreeit like poetri motionnow wait bluray uncut relea"
  },
  {
    "label":0,
    "text":"spring break back school day tilll summer",
    "cleaned_text":"spring break back school day tilll summer",
    "normalized_text":"spring break back school day tilll summer",
    "tokens":[
      "spring",
      "break",
      "back",
      "school",
      "day",
      "tilll",
      "summer"
    ],
    "token_count":7,
    "processed_text":"spring break back school day tilll summer"
  },
  {
    "label":4,
    "text":"ok ive told expect call u",
    "cleaned_text":"ok ive told expect call u",
    "normalized_text":"ok ive told expect call u",
    "tokens":[
      "ok",
      "ive",
      "told",
      "expect",
      "call"
    ],
    "token_count":5,
    "processed_text":"ok ive told expect call"
  },
  {
    "label":0,
    "text":"yesterday soo amaz love concert miss vfc lol big thank tc golden ticket im start loov",
    "cleaned_text":"yesterday soo amaz love concert miss vfc lol big thank tc golden ticket im start loov",
    "normalized_text":"yesterday soo amaz love concert miss vfc lol big thank tc golden ticket im start loov",
    "tokens":[
      "yesterday",
      "soo",
      "amaz",
      "love",
      "concert",
      "miss",
      "vfc",
      "lol",
      "big",
      "thank",
      "tc",
      "golden",
      "ticket",
      "im",
      "start",
      "loov"
    ],
    "token_count":16,
    "processed_text":"yesterday soo amaz love concert miss vfc lol big thank tc golden ticket im start loov"
  },
  {
    "label":4,
    "text":"didnt find thresher shark lost anchor",
    "cleaned_text":"didnt find thresher shark lost anchor",
    "normalized_text":"didnt find thresher shark lost anchor",
    "tokens":[
      "didnt",
      "find",
      "thresher",
      "shark",
      "lost",
      "anchor"
    ],
    "token_count":6,
    "processed_text":"didnt find thresher shark lost anchor"
  },
  {
    "label":4,
    "text":"hmm chat cousin kinahlol enx help em",
    "cleaned_text":"hmm chat cousin kinahlol enx help em",
    "normalized_text":"hmm chat cousin kinahlol enx help em",
    "tokens":[
      "hmm",
      "chat",
      "cousin",
      "kinahlol",
      "enx",
      "help",
      "em"
    ],
    "token_count":7,
    "processed_text":"hmm chat cousin kinahlol enx help em"
  },
  {
    "label":4,
    "text":"ahh know gg soo good love twittereddd",
    "cleaned_text":"ahh know gg soo good love twittereddd",
    "normalized_text":"ahh know gg soo good love twittereddd",
    "tokens":[
      "ahh",
      "know",
      "gg",
      "soo",
      "good",
      "love",
      "twittereddd"
    ],
    "token_count":7,
    "processed_text":"ahh know gg soo good love twittereddd"
  },
  {
    "label":4,
    "text":"love sale pitch may make poster stall quotit good shitquot",
    "cleaned_text":"love sale pitch may make poster stall quotit good shitquot",
    "normalized_text":"love sale pitch may make poster stall quotit good shitquot",
    "tokens":[
      "love",
      "sale",
      "pitch",
      "may",
      "make",
      "poster",
      "stall",
      "quotit",
      "good",
      "shitquot"
    ],
    "token_count":10,
    "processed_text":"love sale pitch may make poster stall quotit good shitquot"
  },
  {
    "label":0,
    "text":"roll putt rd hole",
    "cleaned_text":"roll putt rd hole",
    "normalized_text":"roll putt rd hole",
    "tokens":[
      "roll",
      "putt",
      "rd",
      "hole"
    ],
    "token_count":4,
    "processed_text":"roll putt rd hole"
  },
  {
    "label":4,
    "text":"want see chick",
    "cleaned_text":"want see chick",
    "normalized_text":"want see chick",
    "tokens":[
      "want",
      "see",
      "chick"
    ],
    "token_count":3,
    "processed_text":"want see chick"
  },
  {
    "label":0,
    "text":"want tri cherri three oliv last night bar didnt",
    "cleaned_text":"want tri cherri three oliv last night bar didnt",
    "normalized_text":"want tri cherri three oliv last night bar didnt",
    "tokens":[
      "want",
      "tri",
      "cherri",
      "three",
      "oliv",
      "last",
      "night",
      "bar",
      "didnt"
    ],
    "token_count":9,
    "processed_text":"want tri cherri three oliv last night bar didnt"
  },
  {
    "label":0,
    "text":"want see",
    "cleaned_text":"want see",
    "normalized_text":"want see",
    "tokens":[
      "want",
      "see"
    ],
    "token_count":2,
    "processed_text":"want see"
  },
  {
    "label":4,
    "text":"hahaha michel funni nighti night",
    "cleaned_text":"hahaha michel funni nighti night",
    "normalized_text":"hahaha michel funni nighti night",
    "tokens":[
      "hahaha",
      "michel",
      "funni",
      "nighti",
      "night"
    ],
    "token_count":5,
    "processed_text":"hahaha michel funni nighti night"
  },
  {
    "label":4,
    "text":"im glad yall dalla yall rehears tomorrow",
    "cleaned_text":"im glad yall dalla yall rehears tomorrow",
    "normalized_text":"im glad yall dalla yall rehears tomorrow",
    "tokens":[
      "im",
      "glad",
      "yall",
      "dalla",
      "yall",
      "rehear",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"im glad yall dalla yall rehear tomorrow"
  },
  {
    "label":4,
    "text":"beach",
    "cleaned_text":"beach",
    "normalized_text":"beach",
    "tokens":[
      "beach"
    ],
    "token_count":1,
    "processed_text":"beach"
  },
  {
    "label":4,
    "text":"sit outsid tan hope lesser shade pure bloomin end day lol",
    "cleaned_text":"sit outsid tan hope lesser shade pure bloomin end day lol",
    "normalized_text":"sit outsid tan hope lesser shade pure bloomin end day lol",
    "tokens":[
      "sit",
      "outsid",
      "tan",
      "hope",
      "lesser",
      "shade",
      "pure",
      "bloomin",
      "end",
      "day",
      "lol"
    ],
    "token_count":11,
    "processed_text":"sit outsid tan hope lesser shade pure bloomin end day lol"
  },
  {
    "label":4,
    "text":"year almost graduat come",
    "cleaned_text":"year almost graduat come",
    "normalized_text":"year almost graduat come",
    "tokens":[
      "year",
      "almost",
      "graduat",
      "come"
    ],
    "token_count":4,
    "processed_text":"year almost graduat come"
  },
  {
    "label":4,
    "text":"got back quotworkquot alreadi tire still go lunch made need friend",
    "cleaned_text":"got back quotworkquot alreadi tire still go lunch made need friend",
    "normalized_text":"got back quotworkquot alreadi tire still go lunch made need friend",
    "tokens":[
      "got",
      "back",
      "quotworkquot",
      "alreadi",
      "tire",
      "still",
      "go",
      "lunch",
      "made",
      "need",
      "friend"
    ],
    "token_count":11,
    "processed_text":"got back quotworkquot alreadi tire still go lunch made need friend"
  },
  {
    "label":4,
    "text":"show granada last night sick del hard act follow",
    "cleaned_text":"show granada last night sick del hard act follow",
    "normalized_text":"show granada last night sick del hard act follow",
    "tokens":[
      "show",
      "granada",
      "last",
      "night",
      "sick",
      "del",
      "hard",
      "act",
      "follow"
    ],
    "token_count":9,
    "processed_text":"show granada last night sick del hard act follow"
  },
  {
    "label":4,
    "text":"finish newest blink tattoo",
    "cleaned_text":"finish newest blink tattoo",
    "normalized_text":"finish newest blink tattoo",
    "tokens":[
      "finish",
      "newest",
      "blink",
      "tattoo"
    ],
    "token_count":4,
    "processed_text":"finish newest blink tattoo"
  },
  {
    "label":0,
    "text":"agre winner antm want alison win bad",
    "cleaned_text":"agre winner antm want alison win bad",
    "normalized_text":"agre winner antm want alison win bad",
    "tokens":[
      "agr",
      "winner",
      "antm",
      "want",
      "alison",
      "win",
      "bad"
    ],
    "token_count":7,
    "processed_text":"agr winner antm want alison win bad"
  },
  {
    "label":0,
    "text":"headach back",
    "cleaned_text":"headach back",
    "normalized_text":"headach back",
    "tokens":[
      "headach",
      "back"
    ],
    "token_count":2,
    "processed_text":"headach back"
  },
  {
    "label":4,
    "text":"ahhhh love trailer new moon",
    "cleaned_text":"ahhhh love trailer new moon",
    "normalized_text":"ahhhh love trailer new moon",
    "tokens":[
      "ahhhh",
      "love",
      "trailer",
      "new",
      "moon"
    ],
    "token_count":5,
    "processed_text":"ahhhh love trailer new moon"
  },
  {
    "label":0,
    "text":"would take sir unfortun im away though",
    "cleaned_text":"would take sir unfortun im away though",
    "normalized_text":"would take sir unfortun im away though",
    "tokens":[
      "take",
      "sir",
      "unfortun",
      "im",
      "away",
      "though"
    ],
    "token_count":6,
    "processed_text":"take sir unfortun im away though"
  },
  {
    "label":0,
    "text":"age sinc watch tbbt himym hero cousin fill gb extern hdd movi",
    "cleaned_text":"age sinc watch tbbt himym hero cousin fill gb extern hdd movi",
    "normalized_text":"age sinc watch tbbt himym hero cousin fill gb extern hdd movi",
    "tokens":[
      "age",
      "sinc",
      "watch",
      "tbbt",
      "himym",
      "hero",
      "cousin",
      "fill",
      "gb",
      "extern",
      "hdd",
      "movi"
    ],
    "token_count":12,
    "processed_text":"age sinc watch tbbt himym hero cousin fill gb extern hdd movi"
  },
  {
    "label":4,
    "text":"dont want revisionaa cram look foward mtv movi award tonight",
    "cleaned_text":"dont want revisionaa cram look foward mtv movi award tonight",
    "normalized_text":"dont want revisionaa cram look foward mtv movi award tonight",
    "tokens":[
      "dont",
      "want",
      "revisionaa",
      "cram",
      "look",
      "foward",
      "mtv",
      "movi",
      "award",
      "tonight"
    ],
    "token_count":10,
    "processed_text":"dont want revisionaa cram look foward mtv movi award tonight"
  },
  {
    "label":0,
    "text":"broke broken chair that mean buy new one weekend",
    "cleaned_text":"broke broken chair that mean buy new one weekend",
    "normalized_text":"broke broken chair that mean buy new one weekend",
    "tokens":[
      "broke",
      "broken",
      "chair",
      "mean",
      "buy",
      "new",
      "one",
      "weekend"
    ],
    "token_count":8,
    "processed_text":"broke broken chair mean buy new one weekend"
  },
  {
    "label":4,
    "text":"ut oh good luck",
    "cleaned_text":"ut oh good luck",
    "normalized_text":"ut oh good luck",
    "tokens":[
      "ut",
      "oh",
      "good",
      "luck"
    ],
    "token_count":4,
    "processed_text":"ut oh good luck"
  },
  {
    "label":0,
    "text":"wow babe colour amaz lt want",
    "cleaned_text":"wow babe colour amaz lt want",
    "normalized_text":"wow babe colour amaz lt want",
    "tokens":[
      "wow",
      "babe",
      "colour",
      "amaz",
      "lt",
      "want"
    ],
    "token_count":6,
    "processed_text":"wow babe colour amaz lt want"
  },
  {
    "label":0,
    "text":"made back safe accid year yeah im sad back work tomorrow",
    "cleaned_text":"made back safe accid year yeah im sad back work tomorrow",
    "normalized_text":"made back safe accid year yeah im sad back work tomorrow",
    "tokens":[
      "made",
      "back",
      "safe",
      "accid",
      "year",
      "yeah",
      "im",
      "sad",
      "back",
      "work",
      "tomorrow"
    ],
    "token_count":11,
    "processed_text":"made back safe accid year yeah im sad back work tomorrow"
  },
  {
    "label":4,
    "text":"usual take still video one song tour anodyn newburgh ill get figur ill use ryan",
    "cleaned_text":"usual take still video one song tour anodyn newburgh ill get figur ill use ryan",
    "normalized_text":"usual take still video one song tour anodyn newburgh ill get figur ill use ryan",
    "tokens":[
      "usual",
      "take",
      "still",
      "video",
      "one",
      "song",
      "tour",
      "anodyn",
      "newburgh",
      "ill",
      "get",
      "figur",
      "ill",
      "use",
      "ryan"
    ],
    "token_count":15,
    "processed_text":"usual take still video one song tour anodyn newburgh ill get figur ill use ryan"
  },
  {
    "label":0,
    "text":"yay babe cant wait see youuuuuu miss u",
    "cleaned_text":"yay babe cant wait see youuuuuu miss u",
    "normalized_text":"yay babe cant wait see youuuuuu miss u",
    "tokens":[
      "yay",
      "babe",
      "cant",
      "wait",
      "see",
      "youuuuuu",
      "miss"
    ],
    "token_count":7,
    "processed_text":"yay babe cant wait see youuuuuu miss"
  },
  {
    "label":4,
    "text":"sip coffe put quotmusikkiquot onto sansa",
    "cleaned_text":"sip coffe put quotmusikkiquot onto sansa",
    "normalized_text":"sip coffe put quotmusikkiquot onto sansa",
    "tokens":[
      "sip",
      "coff",
      "put",
      "quotmusikkiquot",
      "onto",
      "sansa"
    ],
    "token_count":6,
    "processed_text":"sip coff put quotmusikkiquot onto sansa"
  },
  {
    "label":0,
    "text":"well suck cabl internet",
    "cleaned_text":"well suck cabl internet",
    "normalized_text":"well suck cabl internet",
    "tokens":[
      "well",
      "suck",
      "cabl",
      "internet"
    ],
    "token_count":4,
    "processed_text":"well suck cabl internet"
  },
  {
    "label":4,
    "text":"want get hippogryph new pet",
    "cleaned_text":"want get hippogryph new pet",
    "normalized_text":"want get hippogryph new pet",
    "tokens":[
      "want",
      "get",
      "hippogryph",
      "new",
      "pet"
    ],
    "token_count":5,
    "processed_text":"want get hippogryph new pet"
  },
  {
    "label":0,
    "text":"that rubbish",
    "cleaned_text":"that rubbish",
    "normalized_text":"that rubbish",
    "tokens":[
      "rubbish"
    ],
    "token_count":1,
    "processed_text":"rubbish"
  },
  {
    "label":4,
    "text":"reserv gb iphon gs black cant wait friday",
    "cleaned_text":"reserv gb iphon gs black cant wait friday",
    "normalized_text":"reserv gb iphon gs black cant wait friday",
    "tokens":[
      "reserv",
      "gb",
      "iphon",
      "gs",
      "black",
      "cant",
      "wait",
      "friday"
    ],
    "token_count":8,
    "processed_text":"reserv gb iphon gs black cant wait friday"
  },
  {
    "label":4,
    "text":"congrat us hope new moon success next year",
    "cleaned_text":"congrat us hope new moon success next year",
    "normalized_text":"congrat us hope new moon success next year",
    "tokens":[
      "congrat",
      "us",
      "hope",
      "new",
      "moon",
      "success",
      "next",
      "year"
    ],
    "token_count":8,
    "processed_text":"congrat us hope new moon success next year"
  },
  {
    "label":4,
    "text":"yeah total creepi demi allow",
    "cleaned_text":"yeah total creepi demi allow",
    "normalized_text":"yeah total creepi demi allow",
    "tokens":[
      "yeah",
      "total",
      "creepi",
      "demi",
      "allow"
    ],
    "token_count":5,
    "processed_text":"yeah total creepi demi allow"
  },
  {
    "label":4,
    "text":"tweet goe cuz she nutz still awesom ly dude",
    "cleaned_text":"tweet goe cuz she nutz still awesom ly dude",
    "normalized_text":"tweet goe cuz she nutz still awesom ly dude",
    "tokens":[
      "tweet",
      "goe",
      "cuz",
      "nutz",
      "still",
      "awesom",
      "ly",
      "dude"
    ],
    "token_count":8,
    "processed_text":"tweet goe cuz nutz still awesom ly dude"
  },
  {
    "label":4,
    "text":"ok didnt quit work look like ill read direct stop clock",
    "cleaned_text":"ok didnt quit work look like ill read direct stop clock",
    "normalized_text":"ok didnt quit work look like ill read direct stop clock",
    "tokens":[
      "ok",
      "didnt",
      "quit",
      "work",
      "look",
      "like",
      "ill",
      "read",
      "direct",
      "stop",
      "clock"
    ],
    "token_count":11,
    "processed_text":"ok didnt quit work look like ill read direct stop clock"
  },
  {
    "label":0,
    "text":"feel terribl nuke stay puft",
    "cleaned_text":"feel terribl nuke stay puft",
    "normalized_text":"feel terribl nuke stay puft",
    "tokens":[
      "feel",
      "terribl",
      "nuke",
      "stay",
      "puft"
    ],
    "token_count":5,
    "processed_text":"feel terribl nuke stay puft"
  },
  {
    "label":4,
    "text":"new day new month mean new possibilit work feel energ gotta go lot accomplish",
    "cleaned_text":"new day new month mean new possibilit work feel energ gotta go lot accomplish",
    "normalized_text":"new day new month mean new possibilit work feel energ gotta go lot accomplish",
    "tokens":[
      "new",
      "day",
      "new",
      "month",
      "mean",
      "new",
      "possibilit",
      "work",
      "feel",
      "energ",
      "got",
      "ta",
      "go",
      "lot",
      "accomplish"
    ],
    "token_count":15,
    "processed_text":"new day new month mean new possibilit work feel energ got ta go lot accomplish"
  },
  {
    "label":0,
    "text":"twitter cell phone new go internet help thank",
    "cleaned_text":"twitter cell phone new go internet help thank",
    "normalized_text":"twitter cell phone new go internet help thank",
    "tokens":[
      "twitter",
      "cell",
      "phone",
      "new",
      "go",
      "internet",
      "help",
      "thank"
    ],
    "token_count":8,
    "processed_text":"twitter cell phone new go internet help thank"
  },
  {
    "label":4,
    "text":"isvalcor guild im heal",
    "cleaned_text":"isvalcor guild im heal",
    "normalized_text":"isvalcor guild im heal",
    "tokens":[
      "isvalcor",
      "guild",
      "im",
      "heal"
    ],
    "token_count":4,
    "processed_text":"isvalcor guild im heal"
  },
  {
    "label":4,
    "text":"speakin holyi hope everyon bless day",
    "cleaned_text":"speakin holyi hope everyon bless day",
    "normalized_text":"speakin holyi hope everyon bless day",
    "tokens":[
      "speakin",
      "holyi",
      "hope",
      "everyon",
      "bless",
      "day"
    ],
    "token_count":6,
    "processed_text":"speakin holyi hope everyon bless day"
  },
  {
    "label":0,
    "text":"im oklahoma lol need make anoth video toobut dont know",
    "cleaned_text":"im oklahoma lol need make anoth video toobut dont know",
    "normalized_text":"im oklahoma lol need make anoth video toobut dont know",
    "tokens":[
      "im",
      "oklahoma",
      "lol",
      "need",
      "make",
      "anoth",
      "video",
      "toobut",
      "dont",
      "know"
    ],
    "token_count":10,
    "processed_text":"im oklahoma lol need make anoth video toobut dont know"
  },
  {
    "label":0,
    "text":"ahhhhhhh man go work",
    "cleaned_text":"ahhhhhhh man go work",
    "normalized_text":"ahhhhhhh man go work",
    "tokens":[
      "ahhhhhhh",
      "man",
      "go",
      "work"
    ],
    "token_count":4,
    "processed_text":"ahhhhhhh man go work"
  },
  {
    "label":0,
    "text":"fuck guess im make chitiva tonight end er instead fml",
    "cleaned_text":"fuck guess im make chitiva tonight end er instead fml",
    "normalized_text":"fuck guess im make chitiva tonight end er instead fml",
    "tokens":[
      "fuck",
      "guess",
      "im",
      "make",
      "chitiva",
      "tonight",
      "end",
      "er",
      "instead",
      "fml"
    ],
    "token_count":10,
    "processed_text":"fuck guess im make chitiva tonight end er instead fml"
  },
  {
    "label":4,
    "text":"haha love ferri lt lt glad boy good time togeth",
    "cleaned_text":"haha love ferri lt lt glad boy good time togeth",
    "normalized_text":"haha love ferri lt lt glad boy good time togeth",
    "tokens":[
      "haha",
      "love",
      "ferri",
      "lt",
      "lt",
      "glad",
      "boy",
      "good",
      "time",
      "togeth"
    ],
    "token_count":10,
    "processed_text":"haha love ferri lt lt glad boy good time togeth"
  },
  {
    "label":0,
    "text":"awe sorri hear hate bad dream ive strang one",
    "cleaned_text":"awe sorri hear hate bad dream ive strang one",
    "normalized_text":"awe sorri hear hate bad dream ive strang one",
    "tokens":[
      "awe",
      "sorri",
      "hear",
      "hate",
      "bad",
      "dream",
      "ive",
      "strang",
      "one"
    ],
    "token_count":9,
    "processed_text":"awe sorri hear hate bad dream ive strang one"
  },
  {
    "label":4,
    "text":"wish could come show im way new zealand though mayb one day",
    "cleaned_text":"wish could come show im way new zealand though mayb one day",
    "normalized_text":"wish could come show im way new zealand though mayb one day",
    "tokens":[
      "wish",
      "come",
      "show",
      "im",
      "way",
      "new",
      "zealand",
      "though",
      "mayb",
      "one",
      "day"
    ],
    "token_count":11,
    "processed_text":"wish come show im way new zealand though mayb one day"
  },
  {
    "label":4,
    "text":"hahaha deff",
    "cleaned_text":"hahaha deff",
    "normalized_text":"hahaha deff",
    "tokens":[
      "hahaha",
      "deff"
    ],
    "token_count":2,
    "processed_text":"hahaha deff"
  },
  {
    "label":0,
    "text":"suddenli expel thoma fiss stori possibl cant consid",
    "cleaned_text":"suddenli expel thoma fiss stori possibl cant consid",
    "normalized_text":"suddenli expel thoma fiss stori possibl cant consid",
    "tokens":[
      "suddenli",
      "expel",
      "thoma",
      "fiss",
      "stori",
      "possibl",
      "cant",
      "consid"
    ],
    "token_count":8,
    "processed_text":"suddenli expel thoma fiss stori possibl cant consid"
  },
  {
    "label":4,
    "text":"total go redesign blog youtub futur graphic design lol hope get design sold store like f anchor blue",
    "cleaned_text":"total go redesign blog youtub futur graphic design lol hope get design sold store like f anchor blue",
    "normalized_text":"total go redesign blog youtub futur graphic design lol hope get design sold store like f anchor blue",
    "tokens":[
      "total",
      "go",
      "redesign",
      "blog",
      "youtub",
      "futur",
      "graphic",
      "design",
      "lol",
      "hope",
      "get",
      "design",
      "sold",
      "store",
      "like",
      "anchor",
      "blue"
    ],
    "token_count":17,
    "processed_text":"total go redesign blog youtub futur graphic design lol hope get design sold store like anchor blue"
  },
  {
    "label":4,
    "text":"voodoo beth amaz blast got new clubbin buddi",
    "cleaned_text":"voodoo beth amaz blast got new clubbin buddi",
    "normalized_text":"voodoo beth amaz blast got new clubbin buddi",
    "tokens":[
      "voodoo",
      "beth",
      "amaz",
      "blast",
      "got",
      "new",
      "clubbin",
      "buddi"
    ],
    "token_count":8,
    "processed_text":"voodoo beth amaz blast got new clubbin buddi"
  },
  {
    "label":4,
    "text":"immah buy supernatur dvd week hope",
    "cleaned_text":"immah buy supernatur dvd week hope",
    "normalized_text":"immah buy supernatur dvd week hope",
    "tokens":[
      "immah",
      "buy",
      "supernatur",
      "dvd",
      "week",
      "hope"
    ],
    "token_count":6,
    "processed_text":"immah buy supernatur dvd week hope"
  },
  {
    "label":0,
    "text":"horrib migrain im suppos help year old birthday parti today med better start kick",
    "cleaned_text":"horrib migrain im suppos help year old birthday parti today med better start kick",
    "normalized_text":"horrib migrain im suppos help year old birthday parti today med better start kick",
    "tokens":[
      "horrib",
      "migrain",
      "im",
      "suppo",
      "help",
      "year",
      "old",
      "birthday",
      "parti",
      "today",
      "med",
      "better",
      "start",
      "kick"
    ],
    "token_count":14,
    "processed_text":"horrib migrain im suppo help year old birthday parti today med better start kick"
  },
  {
    "label":0,
    "text":"probabl dinner suni next week mayb there fish",
    "cleaned_text":"probabl dinner suni next week mayb there fish",
    "normalized_text":"probabl dinner suni next week mayb there fish",
    "tokens":[
      "probabl",
      "dinner",
      "suni",
      "next",
      "week",
      "mayb",
      "fish"
    ],
    "token_count":7,
    "processed_text":"probabl dinner suni next week mayb fish"
  },
  {
    "label":4,
    "text":"quotour intent build tool simplic photoshopquot contest worst metaphor could score well",
    "cleaned_text":"quotour intent build tool simplic photoshopquot contest worst metaphor could score well",
    "normalized_text":"quotour intent build tool simplic photoshopquot contest worst metaphor could score well",
    "tokens":[
      "quotour",
      "intent",
      "build",
      "tool",
      "simplic",
      "photoshopquot",
      "contest",
      "worst",
      "metaphor",
      "score",
      "well"
    ],
    "token_count":11,
    "processed_text":"quotour intent build tool simplic photoshopquot contest worst metaphor score well"
  },
  {
    "label":0,
    "text":"bbqing rain good idea feel sick",
    "cleaned_text":"bbqing rain good idea feel sick",
    "normalized_text":"bbqing rain good idea feel sick",
    "tokens":[
      "bbqing",
      "rain",
      "good",
      "idea",
      "feel",
      "sick"
    ],
    "token_count":6,
    "processed_text":"bbqing rain good idea feel sick"
  },
  {
    "label":0,
    "text":"miss soulja boy danc",
    "cleaned_text":"miss soulja boy danc",
    "normalized_text":"miss soulja boy danc",
    "tokens":[
      "miss",
      "soulja",
      "boy",
      "danc"
    ],
    "token_count":4,
    "processed_text":"miss soulja boy danc"
  },
  {
    "label":4,
    "text":"lol salad finger like twitter awesum",
    "cleaned_text":"lol salad finger like twitter awesum",
    "normalized_text":"lol salad finger like twitter awesum",
    "tokens":[
      "lol",
      "salad",
      "finger",
      "like",
      "twitter",
      "awesum"
    ],
    "token_count":6,
    "processed_text":"lol salad finger like twitter awesum"
  },
  {
    "label":4,
    "text":"got mail",
    "cleaned_text":"got mail",
    "normalized_text":"got mail",
    "tokens":[
      "got",
      "mail"
    ],
    "token_count":2,
    "processed_text":"got mail"
  },
  {
    "label":0,
    "text":"pic phili usa last octob go back nxt june wish sooner",
    "cleaned_text":"pic phili usa last octob go back nxt june wish sooner",
    "normalized_text":"pic phili usa last octob go back nxt june wish sooner",
    "tokens":[
      "pic",
      "phili",
      "usa",
      "last",
      "octob",
      "go",
      "back",
      "nxt",
      "june",
      "wish",
      "sooner"
    ],
    "token_count":11,
    "processed_text":"pic phili usa last octob go back nxt june wish sooner"
  },
  {
    "label":0,
    "text":"massiv old",
    "cleaned_text":"massiv old",
    "normalized_text":"massiv old",
    "tokens":[
      "massiv",
      "old"
    ],
    "token_count":2,
    "processed_text":"massiv old"
  },
  {
    "label":4,
    "text":"aha better",
    "cleaned_text":"aha better",
    "normalized_text":"aha better",
    "tokens":[
      "aha",
      "better"
    ],
    "token_count":2,
    "processed_text":"aha better"
  },
  {
    "label":0,
    "text":"realli think though crowd wasnt lound last year though peopl scream dk outta voic",
    "cleaned_text":"realli think though crowd wasnt lound last year though peopl scream dk outta voic",
    "normalized_text":"realli think though crowd wasnt lound last year though peopl scream dk outta voic",
    "tokens":[
      "realli",
      "think",
      "though",
      "crowd",
      "wasnt",
      "lound",
      "last",
      "year",
      "though",
      "peopl",
      "scream",
      "dk",
      "outta",
      "voic"
    ],
    "token_count":14,
    "processed_text":"realli think though crowd wasnt lound last year though peopl scream dk outta voic"
  },
  {
    "label":0,
    "text":"sponsor miss call drop reg like voicepuls plu half price",
    "cleaned_text":"sponsor miss call drop reg like voicepuls plu half price",
    "normalized_text":"sponsor miss call drop reg like voicepuls plu half price",
    "tokens":[
      "sponsor",
      "miss",
      "call",
      "drop",
      "reg",
      "like",
      "voicepul",
      "plu",
      "half",
      "price"
    ],
    "token_count":10,
    "processed_text":"sponsor miss call drop reg like voicepul plu half price"
  },
  {
    "label":4,
    "text":"think rob stint asia",
    "cleaned_text":"think rob stint asia",
    "normalized_text":"think rob stint asia",
    "tokens":[
      "think",
      "rob",
      "stint",
      "asia"
    ],
    "token_count":4,
    "processed_text":"think rob stint asia"
  },
  {
    "label":0,
    "text":"neighbor got new dog happi miss sad isnt comfi anymor hurt bad ank yest morn go dr tom",
    "cleaned_text":"neighbor got new dog happi miss sad isnt comfi anymor hurt bad ank yest morn go dr tom",
    "normalized_text":"neighbor got new dog happi miss sad isnt comfi anymor hurt bad ank yest morn go dr tom",
    "tokens":[
      "neighbor",
      "got",
      "new",
      "dog",
      "happi",
      "miss",
      "sad",
      "isnt",
      "comfi",
      "anymor",
      "hurt",
      "bad",
      "ank",
      "yest",
      "morn",
      "go",
      "dr",
      "tom"
    ],
    "token_count":18,
    "processed_text":"neighbor got new dog happi miss sad isnt comfi anymor hurt bad ank yest morn go dr tom"
  },
  {
    "label":4,
    "text":"chang site profil zachari kristen simpli infatu still profil though im bed",
    "cleaned_text":"chang site profil zachari kristen simpli infatu still profil though im bed",
    "normalized_text":"chang site profil zachari kristen simpli infatu still profil though im bed",
    "tokens":[
      "chang",
      "site",
      "profil",
      "zachari",
      "kristen",
      "simpli",
      "infatu",
      "still",
      "profil",
      "though",
      "im",
      "bed"
    ],
    "token_count":12,
    "processed_text":"chang site profil zachari kristen simpli infatu still profil though im bed"
  },
  {
    "label":4,
    "text":"im go liz th somayb go see",
    "cleaned_text":"im go liz th somayb go see",
    "normalized_text":"im go liz th somayb go see",
    "tokens":[
      "im",
      "go",
      "liz",
      "th",
      "somayb",
      "go",
      "see"
    ],
    "token_count":7,
    "processed_text":"im go liz th somayb go see"
  },
  {
    "label":4,
    "text":"mr reay hang intern",
    "cleaned_text":"mr reay hang intern",
    "normalized_text":"mr reay hang intern",
    "tokens":[
      "mr",
      "reay",
      "hang",
      "intern"
    ],
    "token_count":4,
    "processed_text":"mr reay hang intern"
  },
  {
    "label":0,
    "text":"fell poni fair",
    "cleaned_text":"fell poni fair",
    "normalized_text":"fell poni fair",
    "tokens":[
      "fell",
      "poni",
      "fair"
    ],
    "token_count":3,
    "processed_text":"fell poni fair"
  },
  {
    "label":0,
    "text":"laptop actual right next im lazi turn",
    "cleaned_text":"laptop actual right next im lazi turn",
    "normalized_text":"laptop actual right next im lazi turn",
    "tokens":[
      "laptop",
      "actual",
      "right",
      "next",
      "im",
      "lazi",
      "turn"
    ],
    "token_count":7,
    "processed_text":"laptop actual right next im lazi turn"
  },
  {
    "label":4,
    "text":"bout go basebal game familyy",
    "cleaned_text":"bout go basebal game familyy",
    "normalized_text":"bout go basebal game familyy",
    "tokens":[
      "bout",
      "go",
      "baseb",
      "game",
      "familyy"
    ],
    "token_count":5,
    "processed_text":"bout go baseb game familyy"
  },
  {
    "label":0,
    "text":"cannot believ miss letterman last nite dont watch letterman ughhhhhh",
    "cleaned_text":"cannot believ miss letterman last nite dont watch letterman ughhhhhh",
    "normalized_text":"cannot believ miss letterman last nite dont watch letterman ughhhhhh",
    "tokens":[
      "believ",
      "miss",
      "letterman",
      "last",
      "nite",
      "dont",
      "watch",
      "letterman",
      "ughhhhhh"
    ],
    "token_count":9,
    "processed_text":"believ miss letterman last nite dont watch letterman ughhhhhh"
  },
  {
    "label":4,
    "text":"sexbox pass wii aint got shit collect",
    "cleaned_text":"sexbox pass wii aint got shit collect",
    "normalized_text":"sexbox pass wii aint got shit collect",
    "tokens":[
      "sexbox",
      "pass",
      "wii",
      "aint",
      "got",
      "shit",
      "collect"
    ],
    "token_count":7,
    "processed_text":"sexbox pass wii aint got shit collect"
  },
  {
    "label":4,
    "text":"iloveyousomuch",
    "cleaned_text":"iloveyousomuch",
    "normalized_text":"iloveyousomuch",
    "tokens":[
      "iloveyousomuch"
    ],
    "token_count":1,
    "processed_text":"iloveyousomuch"
  },
  {
    "label":0,
    "text":"hard sens hadnt revis easi sens familiar one screw",
    "cleaned_text":"hard sens hadnt revis easi sens familiar one screw",
    "normalized_text":"hard sens hadnt revis easi sens familiar one screw",
    "tokens":[
      "hard",
      "sen",
      "hadnt",
      "revi",
      "easi",
      "sen",
      "familiar",
      "one",
      "screw"
    ],
    "token_count":9,
    "processed_text":"hard sen hadnt revi easi sen familiar one screw"
  },
  {
    "label":0,
    "text":"u chose b hubbi nemor",
    "cleaned_text":"u chose b hubbi nemor",
    "normalized_text":"u chose b hubbi nemor",
    "tokens":[
      "chose",
      "hubbi",
      "nemor"
    ],
    "token_count":3,
    "processed_text":"chose hubbi nemor"
  },
  {
    "label":0,
    "text":"alaaaa forgot buy green tea la",
    "cleaned_text":"alaaaa forgot buy green tea la",
    "normalized_text":"alaaaa forgot buy green tea la",
    "tokens":[
      "alaaaa",
      "forgot",
      "buy",
      "green",
      "tea",
      "la"
    ],
    "token_count":6,
    "processed_text":"alaaaa forgot buy green tea la"
  },
  {
    "label":0,
    "text":"ps get hairzzz cutz miss sharp havent seen like day",
    "cleaned_text":"ps get hairzzz cutz miss sharp havent seen like day",
    "normalized_text":"ps get hairzzz cutz miss sharp havent seen like day",
    "tokens":[
      "ps",
      "get",
      "hairzzz",
      "cutz",
      "miss",
      "sharp",
      "havent",
      "seen",
      "like",
      "day"
    ],
    "token_count":10,
    "processed_text":"ps get hairzzz cutz miss sharp havent seen like day"
  },
  {
    "label":4,
    "text":"boy home",
    "cleaned_text":"boy home",
    "normalized_text":"boy home",
    "tokens":[
      "boy",
      "home"
    ],
    "token_count":2,
    "processed_text":"boy home"
  },
  {
    "label":4,
    "text":"nice look vega sound fun",
    "cleaned_text":"nice look vega sound fun",
    "normalized_text":"nice look vega sound fun",
    "tokens":[
      "nice",
      "look",
      "vega",
      "sound",
      "fun"
    ],
    "token_count":5,
    "processed_text":"nice look vega sound fun"
  },
  {
    "label":4,
    "text":"ticket paid hand yet wow im alon thank mark progress twitter wecandothi",
    "cleaned_text":"ticket paid hand yet wow im alon thank mark progress twitter wecandothi",
    "normalized_text":"ticket paid hand yet wow im alon thank mark progress twitter wecandothi",
    "tokens":[
      "ticket",
      "paid",
      "hand",
      "yet",
      "wow",
      "im",
      "alon",
      "thank",
      "mark",
      "progress",
      "twitter",
      "wecandothi"
    ],
    "token_count":12,
    "processed_text":"ticket paid hand yet wow im alon thank mark progress twitter wecandothi"
  },
  {
    "label":4,
    "text":"lenka voic fulfil sunday afternoon",
    "cleaned_text":"lenka voic fulfil sunday afternoon",
    "normalized_text":"lenka voic fulfil sunday afternoon",
    "tokens":[
      "lenka",
      "voic",
      "fulfil",
      "sunday",
      "afternoon"
    ],
    "token_count":5,
    "processed_text":"lenka voic fulfil sunday afternoon"
  },
  {
    "label":4,
    "text":"ooo yay review ye",
    "cleaned_text":"ooo yay review ye",
    "normalized_text":"ooo yay review ye",
    "tokens":[
      "ooo",
      "yay",
      "review",
      "ye"
    ],
    "token_count":4,
    "processed_text":"ooo yay review ye"
  },
  {
    "label":4,
    "text":"nice meet u im chile wait ur answer",
    "cleaned_text":"nice meet u im chile wait ur answer",
    "normalized_text":"nice meet u im chile wait ur answer",
    "tokens":[
      "nice",
      "meet",
      "im",
      "chile",
      "wait",
      "ur",
      "answer"
    ],
    "token_count":7,
    "processed_text":"nice meet im chile wait ur answer"
  },
  {
    "label":4,
    "text":"enjoy phillipin lucki you",
    "cleaned_text":"enjoy phillipin lucki you",
    "normalized_text":"enjoy phillipin lucki you",
    "tokens":[
      "enjoy",
      "phillipin",
      "lucki"
    ],
    "token_count":3,
    "processed_text":"enjoy phillipin lucki"
  },
  {
    "label":4,
    "text":"oh phewa long im one think",
    "cleaned_text":"oh phewa long im one think",
    "normalized_text":"oh phewa long im one think",
    "tokens":[
      "oh",
      "phewa",
      "long",
      "im",
      "one",
      "think"
    ],
    "token_count":6,
    "processed_text":"oh phewa long im one think"
  },
  {
    "label":4,
    "text":"bank get thing roll hooray day",
    "cleaned_text":"bank get thing roll hooray day",
    "normalized_text":"bank get thing roll hooray day",
    "tokens":[
      "bank",
      "get",
      "thing",
      "roll",
      "hooray",
      "day"
    ],
    "token_count":6,
    "processed_text":"bank get thing roll hooray day"
  },
  {
    "label":4,
    "text":"oh si tungtung itu ada inggrisnya ya g ga baca detail nya jadi kirain mrk chines thank u",
    "cleaned_text":"oh si tungtung itu ada inggrisnya ya g ga baca detail nya jadi kirain mrk chines thank u",
    "normalized_text":"oh si tungtung itu ada inggrisnya ya g ga baca detail nya jadi kirain mrk chines thank u",
    "tokens":[
      "oh",
      "si",
      "tungtung",
      "itu",
      "ada",
      "inggrisnya",
      "ya",
      "ga",
      "baca",
      "detail",
      "nya",
      "jadi",
      "kirain",
      "mrk",
      "chine",
      "thank"
    ],
    "token_count":16,
    "processed_text":"oh si tungtung itu ada inggrisnya ya ga baca detail nya jadi kirain mrk chine thank"
  },
  {
    "label":0,
    "text":"go back player championship coupl hr morn fli home nyc tonight",
    "cleaned_text":"go back player championship coupl hr morn fli home nyc tonight",
    "normalized_text":"go back player championship coupl hr morn fli home nyc tonight",
    "tokens":[
      "go",
      "back",
      "player",
      "championship",
      "coupl",
      "hr",
      "morn",
      "fli",
      "home",
      "nyc",
      "tonight"
    ],
    "token_count":11,
    "processed_text":"go back player championship coupl hr morn fli home nyc tonight"
  },
  {
    "label":0,
    "text":"dosent work say",
    "cleaned_text":"dosent work say",
    "normalized_text":"dosent work say",
    "tokens":[
      "dosent",
      "work",
      "say"
    ],
    "token_count":3,
    "processed_text":"dosent work say"
  },
  {
    "label":4,
    "text":"baltimor go boo im go spend today watch movi sailor moon today good day",
    "cleaned_text":"baltimor go boo im go spend today watch movi sailor moon today good day",
    "normalized_text":"baltimor go boo im go spend today watch movi sailor moon today good day",
    "tokens":[
      "baltimor",
      "go",
      "boo",
      "im",
      "go",
      "spend",
      "today",
      "watch",
      "movi",
      "sailor",
      "moon",
      "today",
      "good",
      "day"
    ],
    "token_count":14,
    "processed_text":"baltimor go boo im go spend today watch movi sailor moon today good day"
  },
  {
    "label":4,
    "text":"love need",
    "cleaned_text":"love need",
    "normalized_text":"love need",
    "tokens":[
      "love",
      "need"
    ],
    "token_count":2,
    "processed_text":"love need"
  },
  {
    "label":4,
    "text":"im still bed doin ipod soon get person brook comput ill hook u",
    "cleaned_text":"im still bed doin ipod soon get person brook comput ill hook u",
    "normalized_text":"im still bed doin ipod soon get person brook comput ill hook u",
    "tokens":[
      "im",
      "still",
      "bed",
      "doin",
      "ipod",
      "soon",
      "get",
      "person",
      "brook",
      "comput",
      "ill",
      "hook"
    ],
    "token_count":12,
    "processed_text":"im still bed doin ipod soon get person brook comput ill hook"
  },
  {
    "label":4,
    "text":"first time see matrix aah stun film",
    "cleaned_text":"first time see matrix aah stun film",
    "normalized_text":"first time see matrix aah stun film",
    "tokens":[
      "first",
      "time",
      "see",
      "matrix",
      "aah",
      "stun",
      "film"
    ],
    "token_count":7,
    "processed_text":"first time see matrix aah stun film"
  },
  {
    "label":4,
    "text":"weather greet us more magnific display thunderhead downpour work steadi full fun",
    "cleaned_text":"weather greet us more magnific display thunderhead downpour work steadi full fun",
    "normalized_text":"weather greet us more magnific display thunderhead downpour work steadi full fun",
    "tokens":[
      "weather",
      "greet",
      "us",
      "magnif",
      "display",
      "thunderhead",
      "downpour",
      "work",
      "steadi",
      "full",
      "fun"
    ],
    "token_count":11,
    "processed_text":"weather greet us magnif display thunderhead downpour work steadi full fun"
  },
  {
    "label":0,
    "text":"realli fanci bbq oh well beach half hour hope",
    "cleaned_text":"realli fanci bbq oh well beach half hour hope",
    "normalized_text":"realli fanci bbq oh well beach half hour hope",
    "tokens":[
      "realli",
      "fanci",
      "bbq",
      "oh",
      "well",
      "beach",
      "half",
      "hour",
      "hope"
    ],
    "token_count":9,
    "processed_text":"realli fanci bbq oh well beach half hour hope"
  },
  {
    "label":4,
    "text":"cant open eye properli mayb sleep lil longer itll fix",
    "cleaned_text":"cant open eye properli mayb sleep lil longer itll fix",
    "normalized_text":"cant open eye properli mayb sleep lil longer itll fix",
    "tokens":[
      "cant",
      "open",
      "eye",
      "properli",
      "mayb",
      "sleep",
      "lil",
      "longer",
      "itll",
      "fix"
    ],
    "token_count":10,
    "processed_text":"cant open eye properli mayb sleep lil longer itll fix"
  },
  {
    "label":4,
    "text":"yup whole concept pay forward good one",
    "cleaned_text":"yup whole concept pay forward good one",
    "normalized_text":"yup whole concept pay forward good one",
    "tokens":[
      "yup",
      "whole",
      "concept",
      "pay",
      "forward",
      "good",
      "one"
    ],
    "token_count":7,
    "processed_text":"yup whole concept pay forward good one"
  },
  {
    "label":4,
    "text":"around first week juli bike ride",
    "cleaned_text":"around first week juli bike ride",
    "normalized_text":"around first week juli bike ride",
    "tokens":[
      "around",
      "first",
      "week",
      "juli",
      "bike",
      "ride"
    ],
    "token_count":6,
    "processed_text":"around first week juli bike ride"
  },
  {
    "label":4,
    "text":"ello sweet tweet ahhhh anoth manic monday lol",
    "cleaned_text":"ello sweet tweet ahhhh anoth manic monday lol",
    "normalized_text":"ello sweet tweet ahhhh anoth manic monday lol",
    "tokens":[
      "ello",
      "sweet",
      "tweet",
      "ahhhh",
      "anoth",
      "manic",
      "monday",
      "lol"
    ],
    "token_count":8,
    "processed_text":"ello sweet tweet ahhhh anoth manic monday lol"
  },
  {
    "label":4,
    "text":"lmao actual he go pleasur ps album releas dont think he go go shirtless know",
    "cleaned_text":"lmao actual he go pleasur ps album releas dont think he go go shirtless know",
    "normalized_text":"lmao actual he go pleasur ps album releas dont think he go go shirtless know",
    "tokens":[
      "lmao",
      "actual",
      "go",
      "pleasur",
      "ps",
      "album",
      "relea",
      "dont",
      "think",
      "go",
      "go",
      "shirtless",
      "know"
    ],
    "token_count":13,
    "processed_text":"lmao actual go pleasur ps album relea dont think go go shirtless know"
  },
  {
    "label":0,
    "text":"cant help think listen jay",
    "cleaned_text":"cant help think listen jay",
    "normalized_text":"cant help think listen jay",
    "tokens":[
      "cant",
      "help",
      "think",
      "listen",
      "jay"
    ],
    "token_count":5,
    "processed_text":"cant help think listen jay"
  },
  {
    "label":0,
    "text":"miss min",
    "cleaned_text":"miss min",
    "normalized_text":"miss min",
    "tokens":[
      "miss",
      "min"
    ],
    "token_count":2,
    "processed_text":"miss min"
  },
  {
    "label":0,
    "text":"tri program new phone work",
    "cleaned_text":"tri program new phone work",
    "normalized_text":"tri program new phone work",
    "tokens":[
      "tri",
      "program",
      "new",
      "phone",
      "work"
    ],
    "token_count":5,
    "processed_text":"tri program new phone work"
  },
  {
    "label":0,
    "text":"emran hashmi everi music channel pm",
    "cleaned_text":"emran hashmi everi music channel pm",
    "normalized_text":"emran hashmi everi music channel pm",
    "tokens":[
      "emran",
      "hashmi",
      "everi",
      "music",
      "channel",
      "pm"
    ],
    "token_count":6,
    "processed_text":"emran hashmi everi music channel pm"
  },
  {
    "label":0,
    "text":"want alfredo u come north",
    "cleaned_text":"want alfredo u come north",
    "normalized_text":"want alfredo u come north",
    "tokens":[
      "want",
      "alfredo",
      "come",
      "north"
    ],
    "token_count":4,
    "processed_text":"want alfredo come north"
  },
  {
    "label":0,
    "text":"irememb mashal field miss",
    "cleaned_text":"irememb mashal field miss",
    "normalized_text":"irememb mashal field miss",
    "tokens":[
      "irememb",
      "mashal",
      "field",
      "miss"
    ],
    "token_count":4,
    "processed_text":"irememb mashal field miss"
  },
  {
    "label":4,
    "text":"ummm u wont need call cuz imma chair next u",
    "cleaned_text":"ummm u wont need call cuz imma chair next u",
    "normalized_text":"ummm u wont need call cuz imma chair next u",
    "tokens":[
      "ummm",
      "wont",
      "need",
      "call",
      "cuz",
      "imma",
      "chair",
      "next"
    ],
    "token_count":8,
    "processed_text":"ummm wont need call cuz imma chair next"
  },
  {
    "label":4,
    "text":"vou ver love man hj finalment",
    "cleaned_text":"vou ver love man hj finalment",
    "normalized_text":"vou ver love man hj finalment",
    "tokens":[
      "vou",
      "ver",
      "love",
      "man",
      "hj",
      "final"
    ],
    "token_count":6,
    "processed_text":"vou ver love man hj final"
  },
  {
    "label":4,
    "text":"lunch",
    "cleaned_text":"lunch",
    "normalized_text":"lunch",
    "tokens":[
      "lunch"
    ],
    "token_count":1,
    "processed_text":"lunch"
  },
  {
    "label":0,
    "text":"way first at danc class year ann",
    "cleaned_text":"way first at danc class year ann",
    "normalized_text":"way first at danc class year ann",
    "tokens":[
      "way",
      "first",
      "danc",
      "class",
      "year",
      "ann"
    ],
    "token_count":6,
    "processed_text":"way first danc class year ann"
  },
  {
    "label":4,
    "text":"watch paul blart mall cop know underworld rise lycan excel movi check em sometim",
    "cleaned_text":"watch paul blart mall cop know underworld rise lycan excel movi check em sometim",
    "normalized_text":"watch paul blart mall cop know underworld rise lycan excel movi check em sometim",
    "tokens":[
      "watch",
      "paul",
      "blart",
      "mall",
      "cop",
      "know",
      "underworld",
      "rise",
      "lycan",
      "excel",
      "movi",
      "check",
      "em",
      "sometim"
    ],
    "token_count":14,
    "processed_text":"watch paul blart mall cop know underworld rise lycan excel movi check em sometim"
  },
  {
    "label":0,
    "text":"total bite didnt realiz temp frigid yonder head back desert",
    "cleaned_text":"total bite didnt realiz temp frigid yonder head back desert",
    "normalized_text":"total bite didnt realiz temp frigid yonder head back desert",
    "tokens":[
      "total",
      "bite",
      "didnt",
      "realiz",
      "temp",
      "frigid",
      "yonder",
      "head",
      "back",
      "desert"
    ],
    "token_count":10,
    "processed_text":"total bite didnt realiz temp frigid yonder head back desert"
  },
  {
    "label":0,
    "text":"tummi hurt ampmi dad told go bed im rebel",
    "cleaned_text":"tummi hurt ampmi dad told go bed im rebel",
    "normalized_text":"tummi hurt ampmi dad told go bed im rebel",
    "tokens":[
      "tummi",
      "hurt",
      "ampmi",
      "dad",
      "told",
      "go",
      "bed",
      "im",
      "rebel"
    ],
    "token_count":9,
    "processed_text":"tummi hurt ampmi dad told go bed im rebel"
  },
  {
    "label":4,
    "text":"hmmm work stuff mainli get car back hospit today make happi",
    "cleaned_text":"hmmm work stuff mainli get car back hospit today make happi",
    "normalized_text":"hmmm work stuff mainli get car back hospit today make happi",
    "tokens":[
      "hmmm",
      "work",
      "stuff",
      "mainli",
      "get",
      "car",
      "back",
      "hospit",
      "today",
      "make",
      "happi"
    ],
    "token_count":11,
    "processed_text":"hmmm work stuff mainli get car back hospit today make happi"
  },
  {
    "label":4,
    "text":"mammer got quesadilla maker watch true blood tonight",
    "cleaned_text":"mammer got quesadilla maker watch true blood tonight",
    "normalized_text":"mammer got quesadilla maker watch true blood tonight",
    "tokens":[
      "mammer",
      "got",
      "quesadilla",
      "maker",
      "watch",
      "true",
      "blood",
      "tonight"
    ],
    "token_count":8,
    "processed_text":"mammer got quesadilla maker watch true blood tonight"
  },
  {
    "label":0,
    "text":"dublinwaterfordkinsalekenmarelisdoonvarnatrimdublin that took sip guin good",
    "cleaned_text":"dublinwaterfordkinsalekenmarelisdoonvarnatrimdublin that took sip guin good",
    "normalized_text":"dublinwaterfordkinsalekenmarelisdoonvarnatrimdublin that took sip guin good",
    "tokens":[
      "took",
      "sip",
      "guin",
      "good"
    ],
    "token_count":4,
    "processed_text":"took sip guin good"
  },
  {
    "label":4,
    "text":"yeah hope your great day",
    "cleaned_text":"yeah hope your great day",
    "normalized_text":"yeah hope your great day",
    "tokens":[
      "yeah",
      "hope",
      "great",
      "day"
    ],
    "token_count":4,
    "processed_text":"yeah hope great day"
  },
  {
    "label":0,
    "text":"one fishi die",
    "cleaned_text":"one fishi die",
    "normalized_text":"one fishi die",
    "tokens":[
      "one",
      "fishi",
      "die"
    ],
    "token_count":3,
    "processed_text":"one fishi die"
  },
  {
    "label":4,
    "text":"true competit advantag know everyon part inform entertain excit world",
    "cleaned_text":"true competit advantag know everyon part inform entertain excit world",
    "normalized_text":"true competit advantag know everyon part inform entertain excit world",
    "tokens":[
      "true",
      "competit",
      "advantag",
      "know",
      "everyon",
      "part",
      "inform",
      "entertain",
      "excit",
      "world"
    ],
    "token_count":10,
    "processed_text":"true competit advantag know everyon part inform entertain excit world"
  },
  {
    "label":4,
    "text":"dont neither sweet dream sweeti love",
    "cleaned_text":"dont neither sweet dream sweeti love",
    "normalized_text":"dont neither sweet dream sweeti love",
    "tokens":[
      "dont",
      "neither",
      "sweet",
      "dream",
      "sweeti",
      "love"
    ],
    "token_count":6,
    "processed_text":"dont neither sweet dream sweeti love"
  },
  {
    "label":0,
    "text":"short trip wheel back home",
    "cleaned_text":"short trip wheel back home",
    "normalized_text":"short trip wheel back home",
    "tokens":[
      "short",
      "trip",
      "wheel",
      "back",
      "home"
    ],
    "token_count":5,
    "processed_text":"short trip wheel back home"
  },
  {
    "label":4,
    "text":"littl fanci good told",
    "cleaned_text":"littl fanci good told",
    "normalized_text":"littl fanci good told",
    "tokens":[
      "littl",
      "fanci",
      "good",
      "told"
    ],
    "token_count":4,
    "processed_text":"littl fanci good told"
  },
  {
    "label":4,
    "text":"dont know eh christian want go watch",
    "cleaned_text":"dont know eh christian want go watch",
    "normalized_text":"dont know eh christian want go watch",
    "tokens":[
      "dont",
      "know",
      "eh",
      "christian",
      "want",
      "go",
      "watch"
    ],
    "token_count":7,
    "processed_text":"dont know eh christian want go watch"
  },
  {
    "label":0,
    "text":"headach fantast",
    "cleaned_text":"headach fantast",
    "normalized_text":"headach fantast",
    "tokens":[
      "headach",
      "fantast"
    ],
    "token_count":2,
    "processed_text":"headach fantast"
  },
  {
    "label":4,
    "text":"lol inde stephen spot stephen anywher",
    "cleaned_text":"lol inde stephen spot stephen anywher",
    "normalized_text":"lol inde stephen spot stephen anywher",
    "tokens":[
      "lol",
      "ind",
      "stephen",
      "spot",
      "stephen",
      "anywh"
    ],
    "token_count":6,
    "processed_text":"lol ind stephen spot stephen anywh"
  },
  {
    "label":4,
    "text":"realli want go laugh therapi place co need help co look funni lol",
    "cleaned_text":"realli want go laugh therapi place co need help co look funni lol",
    "normalized_text":"realli want go laugh therapi place co need help co look funni lol",
    "tokens":[
      "realli",
      "want",
      "go",
      "laugh",
      "therapi",
      "place",
      "co",
      "need",
      "help",
      "co",
      "look",
      "funni",
      "lol"
    ],
    "token_count":13,
    "processed_text":"realli want go laugh therapi place co need help co look funni lol"
  },
  {
    "label":4,
    "text":"ahh today good girlfriend amaz lt",
    "cleaned_text":"ahh today good girlfriend amaz lt",
    "normalized_text":"ahh today good girlfriend amaz lt",
    "tokens":[
      "ahh",
      "today",
      "good",
      "girlfriend",
      "amaz",
      "lt"
    ],
    "token_count":6,
    "processed_text":"ahh today good girlfriend amaz lt"
  },
  {
    "label":4,
    "text":"thank much your nice organ fs account btw",
    "cleaned_text":"thank much your nice organ fs account btw",
    "normalized_text":"thank much your nice organ fs account btw",
    "tokens":[
      "thank",
      "much",
      "nice",
      "organ",
      "fs",
      "account",
      "btw"
    ],
    "token_count":7,
    "processed_text":"thank much nice organ fs account btw"
  },
  {
    "label":0,
    "text":"oh realisedback work tonight",
    "cleaned_text":"oh realisedback work tonight",
    "normalized_text":"oh realisedback work tonight",
    "tokens":[
      "oh",
      "realisedback",
      "work",
      "tonight"
    ],
    "token_count":4,
    "processed_text":"oh realisedback work tonight"
  },
  {
    "label":0,
    "text":"rip ed mcmahon tough week phillyconnect broadcast lost gari papa day ago",
    "cleaned_text":"rip ed mcmahon tough week phillyconnect broadcast lost gari papa day ago",
    "normalized_text":"rip ed mcmahon tough week phillyconnect broadcast lost gari papa day ago",
    "tokens":[
      "rip",
      "ed",
      "mcmahon",
      "tough",
      "week",
      "phillyconnect",
      "broadcast",
      "lost",
      "gari",
      "papa",
      "day",
      "ago"
    ],
    "token_count":12,
    "processed_text":"rip ed mcmahon tough week phillyconnect broadcast lost gari papa day ago"
  },
  {
    "label":4,
    "text":"yeah puppi bad kinda like instal gig system run bash",
    "cleaned_text":"yeah puppi bad kinda like instal gig system run bash",
    "normalized_text":"yeah puppi bad kinda like instal gig system run bash",
    "tokens":[
      "yeah",
      "puppi",
      "bad",
      "kinda",
      "like",
      "instal",
      "gig",
      "system",
      "run",
      "bash"
    ],
    "token_count":10,
    "processed_text":"yeah puppi bad kinda like instal gig system run bash"
  },
  {
    "label":0,
    "text":"miss alreadi",
    "cleaned_text":"miss alreadi",
    "normalized_text":"miss alreadi",
    "tokens":[
      "miss",
      "alreadi"
    ],
    "token_count":2,
    "processed_text":"miss alreadi"
  },
  {
    "label":4,
    "text":"im speech right mwahaha",
    "cleaned_text":"im speech right mwahaha",
    "normalized_text":"im speech right mwahaha",
    "tokens":[
      "im",
      "speech",
      "right",
      "mwahaha"
    ],
    "token_count":4,
    "processed_text":"im speech right mwahaha"
  },
  {
    "label":0,
    "text":"th night row im wide awak awhil time ipod damn thing die earlier",
    "cleaned_text":"th night row im wide awak awhil time ipod damn thing die earlier",
    "normalized_text":"th night row im wide awak awhil time ipod damn thing die earlier",
    "tokens":[
      "th",
      "night",
      "row",
      "im",
      "wide",
      "awak",
      "awhil",
      "time",
      "ipod",
      "damn",
      "thing",
      "die",
      "earlier"
    ],
    "token_count":13,
    "processed_text":"th night row im wide awak awhil time ipod damn thing die earlier"
  },
  {
    "label":0,
    "text":"readi",
    "cleaned_text":"readi",
    "normalized_text":"readi",
    "tokens":[
      "readi"
    ],
    "token_count":1,
    "processed_text":"readi"
  },
  {
    "label":0,
    "text":"got follow lastnight drop follow kiinna sad",
    "cleaned_text":"got follow lastnight drop follow kiinna sad",
    "normalized_text":"got follow lastnight drop follow kiinna sad",
    "tokens":[
      "got",
      "follow",
      "lastnight",
      "drop",
      "follow",
      "kiinna",
      "sad"
    ],
    "token_count":7,
    "processed_text":"got follow lastnight drop follow kiinna sad"
  },
  {
    "label":0,
    "text":"nativ land miss",
    "cleaned_text":"nativ land miss",
    "normalized_text":"nativ land miss",
    "tokens":[
      "nativ",
      "land",
      "miss"
    ],
    "token_count":3,
    "processed_text":"nativ land miss"
  },
  {
    "label":4,
    "text":"ok agora na casa de rdua start train yay",
    "cleaned_text":"ok agora na casa de rdua start train yay",
    "normalized_text":"ok agora na casa de rdua start train yay",
    "tokens":[
      "ok",
      "agora",
      "na",
      "casa",
      "de",
      "rdua",
      "start",
      "train",
      "yay"
    ],
    "token_count":9,
    "processed_text":"ok agora na casa de rdua start train yay"
  },
  {
    "label":4,
    "text":"hope went legssss",
    "cleaned_text":"hope went legssss",
    "normalized_text":"hope went legssss",
    "tokens":[
      "hope",
      "went",
      "legssss"
    ],
    "token_count":3,
    "processed_text":"hope went legssss"
  },
  {
    "label":0,
    "text":"ran right tournament",
    "cleaned_text":"ran right tournament",
    "normalized_text":"ran right tournament",
    "tokens":[
      "ran",
      "right",
      "tournament"
    ],
    "token_count":3,
    "processed_text":"ran right tournament"
  },
  {
    "label":0,
    "text":"dont watch eastend tweet scare",
    "cleaned_text":"dont watch eastend tweet scare",
    "normalized_text":"dont watch eastend tweet scare",
    "tokens":[
      "dont",
      "watch",
      "eastend",
      "tweet",
      "scare"
    ],
    "token_count":5,
    "processed_text":"dont watch eastend tweet scare"
  },
  {
    "label":4,
    "text":"summer worst time start winter better cold make run faster get back insid",
    "cleaned_text":"summer worst time start winter better cold make run faster get back insid",
    "normalized_text":"summer worst time start winter better cold make run faster get back insid",
    "tokens":[
      "summer",
      "worst",
      "time",
      "start",
      "winter",
      "better",
      "cold",
      "make",
      "run",
      "faster",
      "get",
      "back",
      "insid"
    ],
    "token_count":13,
    "processed_text":"summer worst time start winter better cold make run faster get back insid"
  },
  {
    "label":4,
    "text":"ventur lunch take one holiday dress back boyfriend quot look like youv come xma tree",
    "cleaned_text":"ventur lunch take one holiday dress back boyfriend quot look like youv come xma tree",
    "normalized_text":"ventur lunch take one holiday dress back boyfriend quot look like youv come xma tree",
    "tokens":[
      "ventur",
      "lunch",
      "take",
      "one",
      "holiday",
      "dress",
      "back",
      "boyfriend",
      "quot",
      "look",
      "like",
      "youv",
      "come",
      "xma",
      "tree"
    ],
    "token_count":15,
    "processed_text":"ventur lunch take one holiday dress back boyfriend quot look like youv come xma tree"
  },
  {
    "label":4,
    "text":"decid help dad car",
    "cleaned_text":"decid help dad car",
    "normalized_text":"decid help dad car",
    "tokens":[
      "decid",
      "help",
      "dad",
      "car"
    ],
    "token_count":4,
    "processed_text":"decid help dad car"
  },
  {
    "label":4,
    "text":"sweet dream",
    "cleaned_text":"sweet dream",
    "normalized_text":"sweet dream",
    "tokens":[
      "sweet",
      "dream"
    ],
    "token_count":2,
    "processed_text":"sweet dream"
  },
  {
    "label":0,
    "text":"soo impress mr dave tolley last night well whole banddidnt stick around say hi thoughsorri",
    "cleaned_text":"soo impress mr dave tolley last night well whole banddidnt stick around say hi thoughsorri",
    "normalized_text":"soo impress mr dave tolley last night well whole banddidnt stick around say hi thoughsorri",
    "tokens":[
      "soo",
      "impress",
      "mr",
      "dave",
      "tolley",
      "last",
      "night",
      "well",
      "whole",
      "banddidnt",
      "stick",
      "around",
      "say",
      "hi",
      "thoughsorri"
    ],
    "token_count":15,
    "processed_text":"soo impress mr dave tolley last night well whole banddidnt stick around say hi thoughsorri"
  },
  {
    "label":0,
    "text":"crazi day come upawesom orient yuckyyi",
    "cleaned_text":"crazi day come upawesom orient yuckyyi",
    "normalized_text":"crazi day come upawesom orient yuckyyi",
    "tokens":[
      "crazi",
      "day",
      "come",
      "upawesom",
      "orient",
      "yuckyyi"
    ],
    "token_count":6,
    "processed_text":"crazi day come upawesom orient yuckyyi"
  },
  {
    "label":0,
    "text":"im sorri love would",
    "cleaned_text":"im sorri love would",
    "normalized_text":"im sorri love would",
    "tokens":[
      "im",
      "sorri",
      "love"
    ],
    "token_count":3,
    "processed_text":"im sorri love"
  },
  {
    "label":0,
    "text":"still ntt lynni look like gonna wait anoth night wouldv guess fo sho v night",
    "cleaned_text":"still ntt lynni look like gonna wait anoth night wouldv guess fo sho v night",
    "normalized_text":"still ntt lynni look like gonna wait anoth night wouldv guess fo sho v night",
    "tokens":[
      "still",
      "ntt",
      "lynni",
      "look",
      "like",
      "gon",
      "na",
      "wait",
      "anoth",
      "night",
      "wouldv",
      "guess",
      "fo",
      "sho",
      "night"
    ],
    "token_count":15,
    "processed_text":"still ntt lynni look like gon na wait anoth night wouldv guess fo sho night"
  },
  {
    "label":0,
    "text":"realiz im live librari babel",
    "cleaned_text":"realiz im live librari babel",
    "normalized_text":"realiz im live librari babel",
    "tokens":[
      "realiz",
      "im",
      "live",
      "librari",
      "babel"
    ],
    "token_count":5,
    "processed_text":"realiz im live librari babel"
  },
  {
    "label":0,
    "text":"realli fuck",
    "cleaned_text":"realli fuck",
    "normalized_text":"realli fuck",
    "tokens":[
      "realli",
      "fuck"
    ],
    "token_count":2,
    "processed_text":"realli fuck"
  },
  {
    "label":0,
    "text":"sweet made littl coffe amp tv milk carton sat desk sadli came cropper pint water rip",
    "cleaned_text":"sweet made littl coffe amp tv milk carton sat desk sadli came cropper pint water rip",
    "normalized_text":"sweet made littl coffe amp tv milk carton sat desk sadli came cropper pint water rip",
    "tokens":[
      "sweet",
      "made",
      "littl",
      "coff",
      "amp",
      "tv",
      "milk",
      "carton",
      "sat",
      "desk",
      "sadli",
      "came",
      "cropper",
      "pint",
      "water",
      "rip"
    ],
    "token_count":16,
    "processed_text":"sweet made littl coff amp tv milk carton sat desk sadli came cropper pint water rip"
  },
  {
    "label":4,
    "text":"finish paint bulletin board bar oper friend ian charm",
    "cleaned_text":"finish paint bulletin board bar oper friend ian charm",
    "normalized_text":"finish paint bulletin board bar oper friend ian charm",
    "tokens":[
      "finish",
      "paint",
      "bulletin",
      "board",
      "bar",
      "oper",
      "friend",
      "ian",
      "charm"
    ],
    "token_count":9,
    "processed_text":"finish paint bulletin board bar oper friend ian charm"
  },
  {
    "label":0,
    "text":"got movi beast come tv keep forget buy dvd",
    "cleaned_text":"got movi beast come tv keep forget buy dvd",
    "normalized_text":"got movi beast come tv keep forget buy dvd",
    "tokens":[
      "got",
      "movi",
      "beast",
      "come",
      "tv",
      "keep",
      "forget",
      "buy",
      "dvd"
    ],
    "token_count":9,
    "processed_text":"got movi beast come tv keep forget buy dvd"
  },
  {
    "label":4,
    "text":"go lowest mainten afford guy dont lot time cleanup run around much",
    "cleaned_text":"go lowest mainten afford guy dont lot time cleanup run around much",
    "normalized_text":"go lowest mainten afford guy dont lot time cleanup run around much",
    "tokens":[
      "go",
      "lowest",
      "mainten",
      "afford",
      "guy",
      "dont",
      "lot",
      "time",
      "cleanup",
      "run",
      "around",
      "much"
    ],
    "token_count":12,
    "processed_text":"go lowest mainten afford guy dont lot time cleanup run around much"
  },
  {
    "label":4,
    "text":"susan sarandon guest late night w jimmi fallon tonight mother lover come he former snl cast member",
    "cleaned_text":"susan sarandon guest late night w jimmi fallon tonight mother lover come he former snl cast member",
    "normalized_text":"susan sarandon guest late night w jimmi fallon tonight mother lover come he former snl cast member",
    "tokens":[
      "susan",
      "sarandon",
      "guest",
      "late",
      "night",
      "jimmi",
      "fallon",
      "tonight",
      "mother",
      "lover",
      "come",
      "former",
      "snl",
      "cast",
      "member"
    ],
    "token_count":15,
    "processed_text":"susan sarandon guest late night jimmi fallon tonight mother lover come former snl cast member"
  },
  {
    "label":0,
    "text":"back day trip montserrat fun last day tomorrow",
    "cleaned_text":"back day trip montserrat fun last day tomorrow",
    "normalized_text":"back day trip montserrat fun last day tomorrow",
    "tokens":[
      "back",
      "day",
      "trip",
      "montserrat",
      "fun",
      "last",
      "day",
      "tomorrow"
    ],
    "token_count":8,
    "processed_text":"back day trip montserrat fun last day tomorrow"
  },
  {
    "label":4,
    "text":"thank god class today",
    "cleaned_text":"thank god class today",
    "normalized_text":"thank god class today",
    "tokens":[
      "thank",
      "god",
      "class",
      "today"
    ],
    "token_count":4,
    "processed_text":"thank god class today"
  },
  {
    "label":4,
    "text":"got lot new follow weekend thank everyon hope enjoy safemama tweet",
    "cleaned_text":"got lot new follow weekend thank everyon hope enjoy safemama tweet",
    "normalized_text":"got lot new follow weekend thank everyon hope enjoy safemama tweet",
    "tokens":[
      "got",
      "lot",
      "new",
      "follow",
      "weekend",
      "thank",
      "everyon",
      "hope",
      "enjoy",
      "safemama",
      "tweet"
    ],
    "token_count":11,
    "processed_text":"got lot new follow weekend thank everyon hope enjoy safemama tweet"
  },
  {
    "label":4,
    "text":"well hashtag dont use im sure your ask",
    "cleaned_text":"well hashtag dont use im sure your ask",
    "normalized_text":"well hashtag dont use im sure your ask",
    "tokens":[
      "well",
      "hashtag",
      "dont",
      "use",
      "im",
      "sure",
      "ask"
    ],
    "token_count":7,
    "processed_text":"well hashtag dont use im sure ask"
  },
  {
    "label":4,
    "text":"sorri hear get well soon",
    "cleaned_text":"sorri hear get well soon",
    "normalized_text":"sorri hear get well soon",
    "tokens":[
      "sorri",
      "hear",
      "get",
      "well",
      "soon"
    ],
    "token_count":5,
    "processed_text":"sorri hear get well soon"
  },
  {
    "label":0,
    "text":"someon stop worldi wanna get",
    "cleaned_text":"someon stop worldi wanna get",
    "normalized_text":"someon stop worldi wanna get",
    "tokens":[
      "someon",
      "stop",
      "worldi",
      "wan",
      "na",
      "get"
    ],
    "token_count":6,
    "processed_text":"someon stop worldi wan na get"
  },
  {
    "label":0,
    "text":"sat incolleg think",
    "cleaned_text":"sat incolleg think",
    "normalized_text":"sat incolleg think",
    "tokens":[
      "sat",
      "incolleg",
      "think"
    ],
    "token_count":3,
    "processed_text":"sat incolleg think"
  },
  {
    "label":0,
    "text":"loveee call dutyi suck love hate im repli someon wont see need friend",
    "cleaned_text":"loveee call dutyi suck love hate im repli someon wont see need friend",
    "normalized_text":"loveee call dutyi suck love hate im repli someon wont see need friend",
    "tokens":[
      "lovee",
      "call",
      "dutyi",
      "suck",
      "love",
      "hate",
      "im",
      "repli",
      "someon",
      "wont",
      "see",
      "need",
      "friend"
    ],
    "token_count":13,
    "processed_text":"lovee call dutyi suck love hate im repli someon wont see need friend"
  },
  {
    "label":0,
    "text":"work hang mix well",
    "cleaned_text":"work hang mix well",
    "normalized_text":"work hang mix well",
    "tokens":[
      "work",
      "hang",
      "mix",
      "well"
    ],
    "token_count":4,
    "processed_text":"work hang mix well"
  },
  {
    "label":0,
    "text":"hittingclos call drive importantli rest peac big moos heart hurt amp ur famili",
    "cleaned_text":"hittingclos call drive importantli rest peac big moos heart hurt amp ur famili",
    "normalized_text":"hittingclos call drive importantli rest peac big moos heart hurt amp ur famili",
    "tokens":[
      "hittingclo",
      "call",
      "drive",
      "importantli",
      "rest",
      "peac",
      "big",
      "moo",
      "heart",
      "hurt",
      "amp",
      "ur",
      "famili"
    ],
    "token_count":13,
    "processed_text":"hittingclo call drive importantli rest peac big moo heart hurt amp ur famili"
  },
  {
    "label":4,
    "text":"haha twittter world im tirednight",
    "cleaned_text":"haha twittter world im tirednight",
    "normalized_text":"haha twittter world im tirednight",
    "tokens":[
      "haha",
      "twittter",
      "world",
      "im",
      "tirednight"
    ],
    "token_count":5,
    "processed_text":"haha twittter world im tirednight"
  },
  {
    "label":0,
    "text":"stupid ass barbecu stupid ass friend eat stupid ass food go stupid ass parti later",
    "cleaned_text":"stupid ass barbecu stupid ass friend eat stupid ass food go stupid ass parti later",
    "normalized_text":"stupid ass barbecu stupid ass friend eat stupid ass food go stupid ass parti later",
    "tokens":[
      "stupid",
      "ass",
      "barbecu",
      "stupid",
      "ass",
      "friend",
      "eat",
      "stupid",
      "ass",
      "food",
      "go",
      "stupid",
      "ass",
      "parti",
      "later"
    ],
    "token_count":15,
    "processed_text":"stupid ass barbecu stupid ass friend eat stupid ass food go stupid ass parti later"
  },
  {
    "label":0,
    "text":"blue neon van make sick",
    "cleaned_text":"blue neon van make sick",
    "normalized_text":"blue neon van make sick",
    "tokens":[
      "blue",
      "neon",
      "van",
      "make",
      "sick"
    ],
    "token_count":5,
    "processed_text":"blue neon van make sick"
  },
  {
    "label":0,
    "text":"im sorri love thought follow noooowww yay",
    "cleaned_text":"im sorri love thought follow noooowww yay",
    "normalized_text":"im sorri love thought follow noooowww yay",
    "tokens":[
      "im",
      "sorri",
      "love",
      "thought",
      "follow",
      "noooowww",
      "yay"
    ],
    "token_count":7,
    "processed_text":"im sorri love thought follow noooowww yay"
  },
  {
    "label":0,
    "text":"hangin mom bore someon help",
    "cleaned_text":"hangin mom bore someon help",
    "normalized_text":"hangin mom bore someon help",
    "tokens":[
      "hangin",
      "mom",
      "bore",
      "someon",
      "help"
    ],
    "token_count":5,
    "processed_text":"hangin mom bore someon help"
  },
  {
    "label":4,
    "text":"go dinner mum bub group friend great night",
    "cleaned_text":"go dinner mum bub group friend great night",
    "normalized_text":"go dinner mum bub group friend great night",
    "tokens":[
      "go",
      "dinner",
      "mum",
      "bub",
      "group",
      "friend",
      "great",
      "night"
    ],
    "token_count":8,
    "processed_text":"go dinner mum bub group friend great night"
  },
  {
    "label":4,
    "text":"haha win one x",
    "cleaned_text":"haha win one x",
    "normalized_text":"haha win one x",
    "tokens":[
      "haha",
      "win",
      "one"
    ],
    "token_count":3,
    "processed_text":"haha win one"
  },
  {
    "label":4,
    "text":"amaz day yesturday weather gorgous haha sarah mile bloodi tatton park later bbq",
    "cleaned_text":"amaz day yesturday weather gorgous haha sarah mile bloodi tatton park later bbq",
    "normalized_text":"amaz day yesturday weather gorgous haha sarah mile bloodi tatton park later bbq",
    "tokens":[
      "amaz",
      "day",
      "yesturday",
      "weather",
      "gorgou",
      "haha",
      "sarah",
      "mile",
      "bloodi",
      "tatton",
      "park",
      "later",
      "bbq"
    ],
    "token_count":13,
    "processed_text":"amaz day yesturday weather gorgou haha sarah mile bloodi tatton park later bbq"
  },
  {
    "label":4,
    "text":"im reunit robin ami today ct",
    "cleaned_text":"im reunit robin ami today ct",
    "normalized_text":"im reunit robin ami today ct",
    "tokens":[
      "im",
      "reunit",
      "robin",
      "ami",
      "today",
      "ct"
    ],
    "token_count":6,
    "processed_text":"im reunit robin ami today ct"
  },
  {
    "label":0,
    "text":"good idea sure pic bought month ago replac last one",
    "cleaned_text":"good idea sure pic bought month ago replac last one",
    "normalized_text":"good idea sure pic bought month ago replac last one",
    "tokens":[
      "good",
      "idea",
      "sure",
      "pic",
      "bought",
      "month",
      "ago",
      "replac",
      "last",
      "one"
    ],
    "token_count":10,
    "processed_text":"good idea sure pic bought month ago replac last one"
  },
  {
    "label":4,
    "text":"nice good time send us load news",
    "cleaned_text":"nice good time send us load news",
    "normalized_text":"nice good time send us load news",
    "tokens":[
      "nice",
      "good",
      "time",
      "send",
      "us",
      "load",
      "news"
    ],
    "token_count":7,
    "processed_text":"nice good time send us load news"
  },
  {
    "label":0,
    "text":"love kinda mess theater didnt show partli cloudi go packgot flight morn",
    "cleaned_text":"love kinda mess theater didnt show partli cloudi go packgot flight morn",
    "normalized_text":"love kinda mess theater didnt show partli cloudi go packgot flight morn",
    "tokens":[
      "love",
      "kinda",
      "mess",
      "theater",
      "didnt",
      "show",
      "partli",
      "cloudi",
      "go",
      "packgot",
      "flight",
      "morn"
    ],
    "token_count":12,
    "processed_text":"love kinda mess theater didnt show partli cloudi go packgot flight morn"
  },
  {
    "label":0,
    "text":"wolverin outrag enough gambit tho",
    "cleaned_text":"wolverin outrag enough gambit tho",
    "normalized_text":"wolverin outrag enough gambit tho",
    "tokens":[
      "wolverin",
      "outrag",
      "enough",
      "gambit",
      "tho"
    ],
    "token_count":5,
    "processed_text":"wolverin outrag enough gambit tho"
  },
  {
    "label":0,
    "text":"miss vancouv alreadi mani fun thing",
    "cleaned_text":"miss vancouv alreadi mani fun thing",
    "normalized_text":"miss vancouv alreadi mani fun thing",
    "tokens":[
      "miss",
      "vancouv",
      "alreadi",
      "mani",
      "fun",
      "thing"
    ],
    "token_count":6,
    "processed_text":"miss vancouv alreadi mani fun thing"
  },
  {
    "label":0,
    "text":"long beach home sweet home",
    "cleaned_text":"long beach home sweet home",
    "normalized_text":"long beach home sweet home",
    "tokens":[
      "long",
      "beach",
      "home",
      "sweet",
      "home"
    ],
    "token_count":5,
    "processed_text":"long beach home sweet home"
  },
  {
    "label":0,
    "text":"hey disneyrecord byeeeeeee disneyrecord live gt",
    "cleaned_text":"hey disneyrecord byeeeeeee disneyrecord live gt",
    "normalized_text":"hey disneyrecord byeeeeeee disneyrecord live gt",
    "tokens":[
      "hey",
      "disneyrecord",
      "byeeeeeee",
      "disneyrecord",
      "live",
      "gt"
    ],
    "token_count":6,
    "processed_text":"hey disneyrecord byeeeeeee disneyrecord live gt"
  },
  {
    "label":0,
    "text":"listen eagl call catalina islandthey almost readi fledg",
    "cleaned_text":"listen eagl call catalina islandthey almost readi fledg",
    "normalized_text":"listen eagl call catalina islandthey almost readi fledg",
    "tokens":[
      "listen",
      "eagl",
      "call",
      "catalina",
      "islandthey",
      "almost",
      "readi",
      "fledg"
    ],
    "token_count":8,
    "processed_text":"listen eagl call catalina islandthey almost readi fledg"
  },
  {
    "label":0,
    "text":"dog rosco die yestarday im sad",
    "cleaned_text":"dog rosco die yestarday im sad",
    "normalized_text":"dog rosco die yestarday im sad",
    "tokens":[
      "dog",
      "rosco",
      "die",
      "yestarday",
      "im",
      "sad"
    ],
    "token_count":6,
    "processed_text":"dog rosco die yestarday im sad"
  },
  {
    "label":0,
    "text":"want pic",
    "cleaned_text":"want pic",
    "normalized_text":"want pic",
    "tokens":[
      "want",
      "pic"
    ],
    "token_count":2,
    "processed_text":"want pic"
  },
  {
    "label":0,
    "text":"need confess made flavorless london broil tonight",
    "cleaned_text":"need confess made flavorless london broil tonight",
    "normalized_text":"need confess made flavorless london broil tonight",
    "tokens":[
      "need",
      "confess",
      "made",
      "flavorless",
      "london",
      "broil",
      "tonight"
    ],
    "token_count":7,
    "processed_text":"need confess made flavorless london broil tonight"
  },
  {
    "label":4,
    "text":"go casa impian today",
    "cleaned_text":"go casa impian today",
    "normalized_text":"go casa impian today",
    "tokens":[
      "go",
      "casa",
      "impian",
      "today"
    ],
    "token_count":4,
    "processed_text":"go casa impian today"
  },
  {
    "label":4,
    "text":"u adopt dont go school anymor amp u leav month shoot w lemur",
    "cleaned_text":"u adopt dont go school anymor amp u leav month shoot w lemur",
    "normalized_text":"u adopt dont go school anymor amp u leav month shoot w lemur",
    "tokens":[
      "adopt",
      "dont",
      "go",
      "school",
      "anymor",
      "amp",
      "leav",
      "month",
      "shoot",
      "lemur"
    ],
    "token_count":10,
    "processed_text":"adopt dont go school anymor amp leav month shoot lemur"
  },
  {
    "label":4,
    "text":"pic frost",
    "cleaned_text":"pic frost",
    "normalized_text":"pic frost",
    "tokens":[
      "pic",
      "frost"
    ],
    "token_count":2,
    "processed_text":"pic frost"
  },
  {
    "label":0,
    "text":"sprain wrist work make hard type anyth",
    "cleaned_text":"sprain wrist work make hard type anyth",
    "normalized_text":"sprain wrist work make hard type anyth",
    "tokens":[
      "sprain",
      "wrist",
      "work",
      "make",
      "hard",
      "type",
      "anyth"
    ],
    "token_count":7,
    "processed_text":"sprain wrist work make hard type anyth"
  },
  {
    "label":0,
    "text":"drive airport",
    "cleaned_text":"drive airport",
    "normalized_text":"drive airport",
    "tokens":[
      "drive",
      "airport"
    ],
    "token_count":2,
    "processed_text":"drive airport"
  },
  {
    "label":0,
    "text":"get new tire theyr expens",
    "cleaned_text":"get new tire theyr expens",
    "normalized_text":"get new tire theyr expens",
    "tokens":[
      "get",
      "new",
      "tire",
      "theyr",
      "expen"
    ],
    "token_count":5,
    "processed_text":"get new tire theyr expen"
  },
  {
    "label":4,
    "text":"oooo see andyshaun bradi trend topic",
    "cleaned_text":"oooo see andyshaun bradi trend topic",
    "normalized_text":"oooo see andyshaun bradi trend topic",
    "tokens":[
      "oooo",
      "see",
      "andyshaun",
      "bradi",
      "trend",
      "topic"
    ],
    "token_count":6,
    "processed_text":"oooo see andyshaun bradi trend topic"
  },
  {
    "label":4,
    "text":"wow like nambu lot tweeti much intuit especi multipl twitter account",
    "cleaned_text":"wow like nambu lot tweeti much intuit especi multipl twitter account",
    "normalized_text":"wow like nambu lot tweeti much intuit especi multipl twitter account",
    "tokens":[
      "wow",
      "like",
      "nambu",
      "lot",
      "tweeti",
      "much",
      "intuit",
      "especi",
      "multipl",
      "twitter",
      "account"
    ],
    "token_count":11,
    "processed_text":"wow like nambu lot tweeti much intuit especi multipl twitter account"
  },
  {
    "label":0,
    "text":"heeyyyyyi plan fell sitter got class earli anyway alreadi bed",
    "cleaned_text":"heeyyyyyi plan fell sitter got class earli anyway alreadi bed",
    "normalized_text":"heeyyyyyi plan fell sitter got class earli anyway alreadi bed",
    "tokens":[
      "heeyyyyyi",
      "plan",
      "fell",
      "sitter",
      "got",
      "class",
      "earli",
      "anyway",
      "alreadi",
      "bed"
    ],
    "token_count":10,
    "processed_text":"heeyyyyyi plan fell sitter got class earli anyway alreadi bed"
  },
  {
    "label":4,
    "text":"alex davi repli myspac sun ahhhh good time gtlt xd",
    "cleaned_text":"alex davi repli myspac sun ahhhh good time gtlt xd",
    "normalized_text":"alex davi repli myspac sun ahhhh good time gtlt xd",
    "tokens":[
      "alex",
      "davi",
      "repli",
      "myspac",
      "sun",
      "ahhhh",
      "good",
      "time",
      "gtlt",
      "xd"
    ],
    "token_count":10,
    "processed_text":"alex davi repli myspac sun ahhhh good time gtlt xd"
  },
  {
    "label":4,
    "text":"loooooov love meet interscop record week la finger cross",
    "cleaned_text":"loooooov love meet interscop record week la finger cross",
    "normalized_text":"loooooov love meet interscop record week la finger cross",
    "tokens":[
      "loooooov",
      "love",
      "meet",
      "interscop",
      "record",
      "week",
      "la",
      "finger",
      "cross"
    ],
    "token_count":9,
    "processed_text":"loooooov love meet interscop record week la finger cross"
  },
  {
    "label":4,
    "text":"thank lol awww know right make eat sleep move less lol",
    "cleaned_text":"thank lol awww know right make eat sleep move less lol",
    "normalized_text":"thank lol awww know right make eat sleep move less lol",
    "tokens":[
      "thank",
      "lol",
      "awww",
      "know",
      "right",
      "make",
      "eat",
      "sleep",
      "move",
      "less",
      "lol"
    ],
    "token_count":11,
    "processed_text":"thank lol awww know right make eat sleep move less lol"
  },
  {
    "label":0,
    "text":"sound like realli rough daypoor",
    "cleaned_text":"sound like realli rough daypoor",
    "normalized_text":"sound like realli rough daypoor",
    "tokens":[
      "sound",
      "like",
      "realli",
      "rough",
      "daypoor"
    ],
    "token_count":5,
    "processed_text":"sound like realli rough daypoor"
  },
  {
    "label":4,
    "text":"guess ill say goodnight im probabl gonna back lol casegoodnight twittil",
    "cleaned_text":"guess ill say goodnight im probabl gonna back lol casegoodnight twittil",
    "normalized_text":"guess ill say goodnight im probabl gonna back lol casegoodnight twittil",
    "tokens":[
      "guess",
      "ill",
      "say",
      "goodnight",
      "im",
      "probabl",
      "gon",
      "na",
      "back",
      "lol",
      "casegoodnight",
      "twittil"
    ],
    "token_count":12,
    "processed_text":"guess ill say goodnight im probabl gon na back lol casegoodnight twittil"
  },
  {
    "label":0,
    "text":"sigh oh woe betid wanna quotth greekquot seem extra prettier ibollock",
    "cleaned_text":"sigh oh woe betid wanna quotth greekquot seem extra prettier ibollock",
    "normalized_text":"sigh oh woe betid wanna quotth greekquot seem extra prettier ibollock",
    "tokens":[
      "sigh",
      "oh",
      "woe",
      "betid",
      "wan",
      "na",
      "quotth",
      "greekquot",
      "seem",
      "extra",
      "prettier",
      "ibollock"
    ],
    "token_count":12,
    "processed_text":"sigh oh woe betid wan na quotth greekquot seem extra prettier ibollock"
  },
  {
    "label":4,
    "text":"nighti night madam muffin",
    "cleaned_text":"nighti night madam muffin",
    "normalized_text":"nighti night madam muffin",
    "tokens":[
      "nighti",
      "night",
      "madam",
      "muffin"
    ],
    "token_count":4,
    "processed_text":"nighti night madam muffin"
  },
  {
    "label":4,
    "text":"opera work thank god safari",
    "cleaned_text":"opera work thank god safari",
    "normalized_text":"opera work thank god safari",
    "tokens":[
      "opera",
      "work",
      "thank",
      "god",
      "safari"
    ],
    "token_count":5,
    "processed_text":"opera work thank god safari"
  },
  {
    "label":4,
    "text":"stocking countrysid perfect lunch time watch",
    "cleaned_text":"stocking countrysid perfect lunch time watch",
    "normalized_text":"stocking countrysid perfect lunch time watch",
    "tokens":[
      "stock",
      "countrysid",
      "perfect",
      "lunch",
      "time",
      "watch"
    ],
    "token_count":6,
    "processed_text":"stock countrysid perfect lunch time watch"
  },
  {
    "label":0,
    "text":"oh man terribl news feel better buddi",
    "cleaned_text":"oh man terribl news feel better buddi",
    "normalized_text":"oh man terribl news feel better buddi",
    "tokens":[
      "oh",
      "man",
      "terribl",
      "news",
      "feel",
      "better",
      "buddi"
    ],
    "token_count":7,
    "processed_text":"oh man terribl news feel better buddi"
  },
  {
    "label":4,
    "text":"worst dresseddont give damn audreyther lot peopl love style alway",
    "cleaned_text":"worst dresseddont give damn audreyther lot peopl love style alway",
    "normalized_text":"worst dresseddont give damn audreyther lot peopl love style alway",
    "tokens":[
      "worst",
      "dresseddont",
      "give",
      "damn",
      "audreyth",
      "lot",
      "peopl",
      "love",
      "style",
      "alway"
    ],
    "token_count":10,
    "processed_text":"worst dresseddont give damn audreyth lot peopl love style alway"
  },
  {
    "label":4,
    "text":"get readi go hous yeeee ok ummm yeah gonna work spanish projectooo wearemakinnflannnn",
    "cleaned_text":"get readi go hous yeeee ok ummm yeah gonna work spanish projectooo wearemakinnflannnn",
    "normalized_text":"get readi go hous yeeee ok ummm yeah gonna work spanish projectooo wearemakinnflannnn",
    "tokens":[
      "get",
      "readi",
      "go",
      "hou",
      "yeeee",
      "ok",
      "ummm",
      "yeah",
      "gon",
      "na",
      "work",
      "spanish",
      "projectooo"
    ],
    "token_count":13,
    "processed_text":"get readi go hou yeeee ok ummm yeah gon na work spanish projectooo"
  },
  {
    "label":4,
    "text":"tomorrow ill ask swept seri",
    "cleaned_text":"tomorrow ill ask swept seri",
    "normalized_text":"tomorrow ill ask swept seri",
    "tokens":[
      "tomorrow",
      "ill",
      "ask",
      "swept",
      "seri"
    ],
    "token_count":5,
    "processed_text":"tomorrow ill ask swept seri"
  },
  {
    "label":4,
    "text":"never earli say everi new year eve",
    "cleaned_text":"never earli say everi new year eve",
    "normalized_text":"never earli say everi new year eve",
    "tokens":[
      "never",
      "earli",
      "say",
      "everi",
      "new",
      "year",
      "eve"
    ],
    "token_count":7,
    "processed_text":"never earli say everi new year eve"
  },
  {
    "label":4,
    "text":"th page noggin",
    "cleaned_text":"th page noggin",
    "normalized_text":"th page noggin",
    "tokens":[
      "th",
      "page",
      "noggin"
    ],
    "token_count":3,
    "processed_text":"th page noggin"
  },
  {
    "label":0,
    "text":"thank microsoft want play vs beta fug",
    "cleaned_text":"thank microsoft want play vs beta fug",
    "normalized_text":"thank microsoft want play vs beta fug",
    "tokens":[
      "thank",
      "microsoft",
      "want",
      "play",
      "vs",
      "beta",
      "fug"
    ],
    "token_count":7,
    "processed_text":"thank microsoft want play vs beta fug"
  },
  {
    "label":0,
    "text":"tiredblehi miss buddi chad christa talk christa todaybut chadlost numberboo",
    "cleaned_text":"tiredblehi miss buddi chad christa talk christa todaybut chadlost numberboo",
    "normalized_text":"tiredblehi miss buddi chad christa talk christa todaybut chadlost numberboo",
    "tokens":[
      "tiredblehi",
      "miss",
      "buddi",
      "chad",
      "christa",
      "talk",
      "christa",
      "todaybut",
      "chadlost",
      "numberboo"
    ],
    "token_count":10,
    "processed_text":"tiredblehi miss buddi chad christa talk christa todaybut chadlost numberboo"
  },
  {
    "label":0,
    "text":"wonder show like quothom along da rilesquot local tv anymor may bakya jolog bu",
    "cleaned_text":"wonder show like quothom along da rilesquot local tv anymor may bakya jolog bu",
    "normalized_text":"wonder show like quothom along da rilesquot local tv anymor may bakya jolog bu",
    "tokens":[
      "wonder",
      "show",
      "like",
      "quothom",
      "along",
      "da",
      "rilesquot",
      "local",
      "tv",
      "anymor",
      "may",
      "bakya",
      "jolog",
      "bu"
    ],
    "token_count":14,
    "processed_text":"wonder show like quothom along da rilesquot local tv anymor may bakya jolog bu"
  },
  {
    "label":4,
    "text":"get readi work plan night",
    "cleaned_text":"get readi work plan night",
    "normalized_text":"get readi work plan night",
    "tokens":[
      "get",
      "readi",
      "work",
      "plan",
      "night"
    ],
    "token_count":5,
    "processed_text":"get readi work plan night"
  },
  {
    "label":4,
    "text":"jonasturnright jonaswhatdididotoyourheart sampl new favorit",
    "cleaned_text":"jonasturnright jonaswhatdididotoyourheart sampl new favorit",
    "normalized_text":"jonasturnright jonaswhatdididotoyourheart sampl new favorit",
    "tokens":[
      "jonasturnright",
      "sampl",
      "new",
      "favorit"
    ],
    "token_count":4,
    "processed_text":"jonasturnright sampl new favorit"
  },
  {
    "label":4,
    "text":"come money ha knew",
    "cleaned_text":"come money ha knew",
    "normalized_text":"come money ha knew",
    "tokens":[
      "come",
      "money",
      "ha",
      "knew"
    ],
    "token_count":4,
    "processed_text":"come money ha knew"
  },
  {
    "label":0,
    "text":"realiz new harri potter movi come im go need kleenex sinc one favourit charact die",
    "cleaned_text":"realiz new harri potter movi come im go need kleenex sinc one favourit charact die",
    "normalized_text":"realiz new harri potter movi come im go need kleenex sinc one favourit charact die",
    "tokens":[
      "realiz",
      "new",
      "harri",
      "potter",
      "movi",
      "come",
      "im",
      "go",
      "need",
      "kleenex",
      "sinc",
      "one",
      "favourit",
      "charact",
      "die"
    ],
    "token_count":15,
    "processed_text":"realiz new harri potter movi come im go need kleenex sinc one favourit charact die"
  },
  {
    "label":4,
    "text":"u righti peac corp kinda galjust havent chanc yet",
    "cleaned_text":"u righti peac corp kinda galjust havent chanc yet",
    "normalized_text":"u righti peac corp kinda galjust havent chanc yet",
    "tokens":[
      "righti",
      "peac",
      "corp",
      "kinda",
      "galjust",
      "havent",
      "chanc",
      "yet"
    ],
    "token_count":8,
    "processed_text":"righti peac corp kinda galjust havent chanc yet"
  },
  {
    "label":0,
    "text":"tri hardest get car done pocket fund mean slow process",
    "cleaned_text":"tri hardest get car done pocket fund mean slow process",
    "normalized_text":"tri hardest get car done pocket fund mean slow process",
    "tokens":[
      "tri",
      "hardest",
      "get",
      "car",
      "done",
      "pocket",
      "fund",
      "mean",
      "slow",
      "process"
    ],
    "token_count":10,
    "processed_text":"tri hardest get car done pocket fund mean slow process"
  },
  {
    "label":0,
    "text":"one fish commit suicid",
    "cleaned_text":"one fish commit suicid",
    "normalized_text":"one fish commit suicid",
    "tokens":[
      "one",
      "fish",
      "commit",
      "suicid"
    ],
    "token_count":4,
    "processed_text":"one fish commit suicid"
  },
  {
    "label":0,
    "text":"got subwayyyyyi still want yogurt",
    "cleaned_text":"got subwayyyyyi still want yogurt",
    "normalized_text":"got subwayyyyyi still want yogurt",
    "tokens":[
      "got",
      "subwayyyyyi",
      "still",
      "want",
      "yogurt"
    ],
    "token_count":5,
    "processed_text":"got subwayyyyyi still want yogurt"
  },
  {
    "label":4,
    "text":"copywrit huge manchest firm",
    "cleaned_text":"copywrit huge manchest firm",
    "normalized_text":"copywrit huge manchest firm",
    "tokens":[
      "copywrit",
      "huge",
      "manchest",
      "firm"
    ],
    "token_count":4,
    "processed_text":"copywrit huge manchest firm"
  },
  {
    "label":0,
    "text":"car accid great fing way start day",
    "cleaned_text":"car accid great fing way start day",
    "normalized_text":"car accid great fing way start day",
    "tokens":[
      "car",
      "accid",
      "great",
      "fing",
      "way",
      "start",
      "day"
    ],
    "token_count":7,
    "processed_text":"car accid great fing way start day"
  },
  {
    "label":0,
    "text":"poor thing glad got look though",
    "cleaned_text":"poor thing glad got look though",
    "normalized_text":"poor thing glad got look though",
    "tokens":[
      "poor",
      "thing",
      "glad",
      "got",
      "look",
      "though"
    ],
    "token_count":6,
    "processed_text":"poor thing glad got look though"
  },
  {
    "label":4,
    "text":"celebr hoppusday happi hoppusday blink",
    "cleaned_text":"celebr hoppusday happi hoppusday blink",
    "normalized_text":"celebr hoppusday happi hoppusday blink",
    "tokens":[
      "celebr",
      "hoppusday",
      "happi",
      "hoppusday",
      "blink"
    ],
    "token_count":5,
    "processed_text":"celebr hoppusday happi hoppusday blink"
  },
  {
    "label":4,
    "text":"couldnt tame hair save life lol food could eat fruit day",
    "cleaned_text":"couldnt tame hair save life lol food could eat fruit day",
    "normalized_text":"couldnt tame hair save life lol food could eat fruit day",
    "tokens":[
      "couldnt",
      "tame",
      "hair",
      "save",
      "life",
      "lol",
      "food",
      "eat",
      "fruit",
      "day"
    ],
    "token_count":10,
    "processed_text":"couldnt tame hair save life lol food eat fruit day"
  },
  {
    "label":4,
    "text":"yep b back even shop bye",
    "cleaned_text":"yep b back even shop bye",
    "normalized_text":"yep b back even shop bye",
    "tokens":[
      "yep",
      "back",
      "even",
      "shop",
      "bye"
    ],
    "token_count":5,
    "processed_text":"yep back even shop bye"
  },
  {
    "label":0,
    "text":"oh didnt realiz suck",
    "cleaned_text":"oh didnt realiz suck",
    "normalized_text":"oh didnt realiz suck",
    "tokens":[
      "oh",
      "didnt",
      "realiz",
      "suck"
    ],
    "token_count":4,
    "processed_text":"oh didnt realiz suck"
  },
  {
    "label":0,
    "text":"wish go glasto footi tourni chariti call saturday tho let win mutha",
    "cleaned_text":"wish go glasto footi tourni chariti call saturday tho let win mutha",
    "normalized_text":"wish go glasto footi tourni chariti call saturday tho let win mutha",
    "tokens":[
      "wish",
      "go",
      "glasto",
      "footi",
      "tourni",
      "chariti",
      "call",
      "saturday",
      "tho",
      "let",
      "win",
      "mutha"
    ],
    "token_count":12,
    "processed_text":"wish go glasto footi tourni chariti call saturday tho let win mutha"
  },
  {
    "label":4,
    "text":"evannn follow pleas thank",
    "cleaned_text":"evannn follow pleas thank",
    "normalized_text":"evannn follow pleas thank",
    "tokens":[
      "evannn",
      "follow",
      "plea",
      "thank"
    ],
    "token_count":4,
    "processed_text":"evannn follow plea thank"
  },
  {
    "label":4,
    "text":"good quotearthquot us either quothannah montannaquot lol",
    "cleaned_text":"good quotearthquot us either quothannah montannaquot lol",
    "normalized_text":"good quotearthquot us either quothannah montannaquot lol",
    "tokens":[
      "good",
      "quotearthquot",
      "us",
      "either",
      "quothannah",
      "montannaquot",
      "lol"
    ],
    "token_count":7,
    "processed_text":"good quotearthquot us either quothannah montannaquot lol"
  },
  {
    "label":0,
    "text":"think ton idl work week doesnt help either im sure",
    "cleaned_text":"think ton idl work week doesnt help either im sure",
    "normalized_text":"think ton idl work week doesnt help either im sure",
    "tokens":[
      "think",
      "ton",
      "idl",
      "work",
      "week",
      "doesnt",
      "help",
      "either",
      "im",
      "sure"
    ],
    "token_count":10,
    "processed_text":"think ton idl work week doesnt help either im sure"
  },
  {
    "label":4,
    "text":"feel load better hope pill finger cross ill fine tomorrow",
    "cleaned_text":"feel load better hope pill finger cross ill fine tomorrow",
    "normalized_text":"feel load better hope pill finger cross ill fine tomorrow",
    "tokens":[
      "feel",
      "load",
      "better",
      "hope",
      "pill",
      "finger",
      "cross",
      "ill",
      "fine",
      "tomorrow"
    ],
    "token_count":10,
    "processed_text":"feel load better hope pill finger cross ill fine tomorrow"
  },
  {
    "label":0,
    "text":"enjoy secretari state line right",
    "cleaned_text":"enjoy secretari state line right",
    "normalized_text":"enjoy secretari state line right",
    "tokens":[
      "enjoy",
      "secretari",
      "state",
      "line",
      "right"
    ],
    "token_count":5,
    "processed_text":"enjoy secretari state line right"
  },
  {
    "label":4,
    "text":"fill applic taco bell yeah morgan go work togeth hope lt",
    "cleaned_text":"fill applic taco bell yeah morgan go work togeth hope lt",
    "normalized_text":"fill applic taco bell yeah morgan go work togeth hope lt",
    "tokens":[
      "fill",
      "applic",
      "taco",
      "bell",
      "yeah",
      "morgan",
      "go",
      "work",
      "togeth",
      "hope",
      "lt"
    ],
    "token_count":11,
    "processed_text":"fill applic taco bell yeah morgan go work togeth hope lt"
  },
  {
    "label":4,
    "text":"bitch ever get close man shall kill get fuck",
    "cleaned_text":"bitch ever get close man shall kill get fuck",
    "normalized_text":"bitch ever get close man shall kill get fuck",
    "tokens":[
      "bitch",
      "ever",
      "get",
      "close",
      "man",
      "kill",
      "get",
      "fuck"
    ],
    "token_count":8,
    "processed_text":"bitch ever get close man kill get fuck"
  },
  {
    "label":4,
    "text":"prefer leav phone connect net bare get call smse make full use mobil internet",
    "cleaned_text":"prefer leav phone connect net bare get call smse make full use mobil internet",
    "normalized_text":"prefer leav phone connect net bare get call smse make full use mobil internet",
    "tokens":[
      "prefer",
      "leav",
      "phone",
      "connect",
      "net",
      "bare",
      "get",
      "call",
      "smse",
      "make",
      "full",
      "use",
      "mobil",
      "internet"
    ],
    "token_count":14,
    "processed_text":"prefer leav phone connect net bare get call smse make full use mobil internet"
  },
  {
    "label":4,
    "text":"aww muah",
    "cleaned_text":"aww muah",
    "normalized_text":"aww muah",
    "tokens":[
      "aww",
      "muah"
    ],
    "token_count":2,
    "processed_text":"aww muah"
  },
  {
    "label":0,
    "text":"watch mentalist realli made miss psycholog class im glad learnt though",
    "cleaned_text":"watch mentalist realli made miss psycholog class im glad learnt though",
    "normalized_text":"watch mentalist realli made miss psycholog class im glad learnt though",
    "tokens":[
      "watch",
      "mentalist",
      "realli",
      "made",
      "miss",
      "psycholog",
      "class",
      "im",
      "glad",
      "learnt",
      "though"
    ],
    "token_count":11,
    "processed_text":"watch mentalist realli made miss psycholog class im glad learnt though"
  },
  {
    "label":4,
    "text":"dinner applebe pretend studi",
    "cleaned_text":"dinner applebe pretend studi",
    "normalized_text":"dinner applebe pretend studi",
    "tokens":[
      "dinner",
      "appleb",
      "pretend",
      "studi"
    ],
    "token_count":4,
    "processed_text":"dinner appleb pretend studi"
  },
  {
    "label":4,
    "text":"rubi seem take quotther one way itquot philosophi bit far geekcamp",
    "cleaned_text":"rubi seem take quotther one way itquot philosophi bit far geekcamp",
    "normalized_text":"rubi seem take quotther one way itquot philosophi bit far geekcamp",
    "tokens":[
      "rubi",
      "seem",
      "take",
      "quotther",
      "one",
      "way",
      "itquot",
      "philosophi",
      "bit",
      "far",
      "geekcamp"
    ],
    "token_count":11,
    "processed_text":"rubi seem take quotther one way itquot philosophi bit far geekcamp"
  },
  {
    "label":0,
    "text":"get readi skewl tire crap mschelseababylt",
    "cleaned_text":"get readi skewl tire crap mschelseababylt",
    "normalized_text":"get readi skewl tire crap mschelseababylt",
    "tokens":[
      "get",
      "readi",
      "skewl",
      "tire",
      "crap",
      "mschelseababylt"
    ],
    "token_count":6,
    "processed_text":"get readi skewl tire crap mschelseababylt"
  },
  {
    "label":0,
    "text":"bad day",
    "cleaned_text":"bad day",
    "normalized_text":"bad day",
    "tokens":[
      "bad",
      "day"
    ],
    "token_count":2,
    "processed_text":"bad day"
  },
  {
    "label":4,
    "text":"say pocket squar think look better",
    "cleaned_text":"say pocket squar think look better",
    "normalized_text":"say pocket squar think look better",
    "tokens":[
      "say",
      "pocket",
      "squar",
      "think",
      "look",
      "better"
    ],
    "token_count":6,
    "processed_text":"say pocket squar think look better"
  },
  {
    "label":0,
    "text":"sold vacuum craig list broke retract handl show lost deal result way go",
    "cleaned_text":"sold vacuum craig list broke retract handl show lost deal result way go",
    "normalized_text":"sold vacuum craig list broke retract handl show lost deal result way go",
    "tokens":[
      "sold",
      "vacuum",
      "craig",
      "list",
      "broke",
      "retract",
      "handl",
      "show",
      "lost",
      "deal",
      "result",
      "way",
      "go"
    ],
    "token_count":13,
    "processed_text":"sold vacuum craig list broke retract handl show lost deal result way go"
  },
  {
    "label":0,
    "text":"time lay unwindmiss like crazi",
    "cleaned_text":"time lay unwindmiss like crazi",
    "normalized_text":"time lay unwindmiss like crazi",
    "tokens":[
      "time",
      "lay",
      "unwindmiss",
      "like",
      "crazi"
    ],
    "token_count":5,
    "processed_text":"time lay unwindmiss like crazi"
  },
  {
    "label":0,
    "text":"goodnight phone get taken away sad life",
    "cleaned_text":"goodnight phone get taken away sad life",
    "normalized_text":"goodnight phone get taken away sad life",
    "tokens":[
      "goodnight",
      "phone",
      "get",
      "taken",
      "away",
      "sad",
      "life"
    ],
    "token_count":7,
    "processed_text":"goodnight phone get taken away sad life"
  },
  {
    "label":4,
    "text":"prob enjoy love messag long one tweet lol",
    "cleaned_text":"prob enjoy love messag long one tweet lol",
    "normalized_text":"prob enjoy love messag long one tweet lol",
    "tokens":[
      "prob",
      "enjoy",
      "love",
      "messag",
      "long",
      "one",
      "tweet",
      "lol"
    ],
    "token_count":8,
    "processed_text":"prob enjoy love messag long one tweet lol"
  },
  {
    "label":4,
    "text":"nod good night rest well know theyll give clear good one dear",
    "cleaned_text":"nod good night rest well know theyll give clear good one dear",
    "normalized_text":"nod good night rest well know theyll give clear good one dear",
    "tokens":[
      "nod",
      "good",
      "night",
      "rest",
      "well",
      "know",
      "theyll",
      "give",
      "clear",
      "good",
      "one",
      "dear"
    ],
    "token_count":12,
    "processed_text":"nod good night rest well know theyll give clear good one dear"
  },
  {
    "label":0,
    "text":"redey flight tomorrow im gonna miss guy",
    "cleaned_text":"redey flight tomorrow im gonna miss guy",
    "normalized_text":"redey flight tomorrow im gonna miss guy",
    "tokens":[
      "redey",
      "flight",
      "tomorrow",
      "im",
      "gon",
      "na",
      "miss",
      "guy"
    ],
    "token_count":8,
    "processed_text":"redey flight tomorrow im gon na miss guy"
  },
  {
    "label":4,
    "text":"goodnight twitterland hope ill sleep night anti spasm med seem work great",
    "cleaned_text":"goodnight twitterland hope ill sleep night anti spasm med seem work great",
    "normalized_text":"goodnight twitterland hope ill sleep night anti spasm med seem work great",
    "tokens":[
      "goodnight",
      "twitterland",
      "hope",
      "ill",
      "sleep",
      "night",
      "anti",
      "spasm",
      "med",
      "seem",
      "work",
      "great"
    ],
    "token_count":12,
    "processed_text":"goodnight twitterland hope ill sleep night anti spasm med seem work great"
  },
  {
    "label":4,
    "text":"take semiday woke w postdeliveri poststress nausea biz plan watch true blood se soon ellen tv",
    "cleaned_text":"take semiday woke w postdeliveri poststress nausea biz plan watch true blood se soon ellen tv",
    "normalized_text":"take semiday woke w postdeliveri poststress nausea biz plan watch true blood se soon ellen tv",
    "tokens":[
      "take",
      "semiday",
      "woke",
      "postdeliveri",
      "poststress",
      "nausea",
      "biz",
      "plan",
      "watch",
      "true",
      "blood",
      "se",
      "soon",
      "ellen",
      "tv"
    ],
    "token_count":15,
    "processed_text":"take semiday woke postdeliveri poststress nausea biz plan watch true blood se soon ellen tv"
  },
  {
    "label":0,
    "text":"nicho kingdom everyon say monday",
    "cleaned_text":"nicho kingdom everyon say monday",
    "normalized_text":"nicho kingdom everyon say monday",
    "tokens":[
      "nicho",
      "kingdom",
      "everyon",
      "say",
      "monday"
    ],
    "token_count":5,
    "processed_text":"nicho kingdom everyon say monday"
  },
  {
    "label":4,
    "text":"need ban togeth recogn arent alway go agre that ok tcot",
    "cleaned_text":"need ban togeth recogn arent alway go agre that ok tcot",
    "normalized_text":"need ban togeth recogn arent alway go agre that ok tcot",
    "tokens":[
      "need",
      "ban",
      "togeth",
      "recogn",
      "arent",
      "alway",
      "go",
      "agr",
      "ok",
      "tcot"
    ],
    "token_count":10,
    "processed_text":"need ban togeth recogn arent alway go agr ok tcot"
  },
  {
    "label":4,
    "text":"amaz made cri",
    "cleaned_text":"amaz made cri",
    "normalized_text":"amaz made cri",
    "tokens":[
      "amaz",
      "made",
      "cri"
    ],
    "token_count":3,
    "processed_text":"amaz made cri"
  },
  {
    "label":0,
    "text":"one today fun",
    "cleaned_text":"one today fun",
    "normalized_text":"one today fun",
    "tokens":[
      "one",
      "today",
      "fun"
    ],
    "token_count":3,
    "processed_text":"one today fun"
  },
  {
    "label":4,
    "text":"nightmar wax quotethn majorityquot chill radio follow rori hoy quottwilightquot doubl",
    "cleaned_text":"nightmar wax quotethn majorityquot chill radio follow rori hoy quottwilightquot doubl",
    "normalized_text":"nightmar wax quotethn majorityquot chill radio follow rori hoy quottwilightquot doubl",
    "tokens":[
      "nightmar",
      "wax",
      "quotethn",
      "majorityquot",
      "chill",
      "radio",
      "follow",
      "rori",
      "hoy",
      "doubl"
    ],
    "token_count":10,
    "processed_text":"nightmar wax quotethn majorityquot chill radio follow rori hoy doubl"
  },
  {
    "label":4,
    "text":"eat tobleron work colleagu bought caus birthday tomorrow",
    "cleaned_text":"eat tobleron work colleagu bought caus birthday tomorrow",
    "normalized_text":"eat tobleron work colleagu bought caus birthday tomorrow",
    "tokens":[
      "eat",
      "tobleron",
      "work",
      "colleagu",
      "bought",
      "cau",
      "birthday",
      "tomorrow"
    ],
    "token_count":8,
    "processed_text":"eat tobleron work colleagu bought cau birthday tomorrow"
  },
  {
    "label":0,
    "text":"oh boo im sorri mom thought tri convert religion due cursiv song one mix x",
    "cleaned_text":"oh boo im sorri mom thought tri convert religion due cursiv song one mix x",
    "normalized_text":"oh boo im sorri mom thought tri convert religion due cursiv song one mix x",
    "tokens":[
      "oh",
      "boo",
      "im",
      "sorri",
      "mom",
      "thought",
      "tri",
      "convert",
      "religion",
      "due",
      "cursiv",
      "song",
      "one",
      "mix"
    ],
    "token_count":14,
    "processed_text":"oh boo im sorri mom thought tri convert religion due cursiv song one mix"
  },
  {
    "label":4,
    "text":"eatng dumpl hahaha",
    "cleaned_text":"eatng dumpl hahaha",
    "normalized_text":"eatng dumpl hahaha",
    "tokens":[
      "eatng",
      "dumpl",
      "hahaha"
    ],
    "token_count":3,
    "processed_text":"eatng dumpl hahaha"
  },
  {
    "label":4,
    "text":"thank matt ill around pm see tomorrow",
    "cleaned_text":"thank matt ill around pm see tomorrow",
    "normalized_text":"thank matt ill around pm see tomorrow",
    "tokens":[
      "thank",
      "matt",
      "ill",
      "around",
      "pm",
      "see",
      "tomorrow"
    ],
    "token_count":7,
    "processed_text":"thank matt ill around pm see tomorrow"
  },
  {
    "label":4,
    "text":"absolut noth today nice",
    "cleaned_text":"absolut noth today nice",
    "normalized_text":"absolut noth today nice",
    "tokens":[
      "absolut",
      "noth",
      "today",
      "nice"
    ],
    "token_count":4,
    "processed_text":"absolut noth today nice"
  },
  {
    "label":0,
    "text":"sleepi annoy girl grab librari copi dthe mighti duck chanc",
    "cleaned_text":"sleepi annoy girl grab librari copi dthe mighti duck chanc",
    "normalized_text":"sleepi annoy girl grab librari copi dthe mighti duck chanc",
    "tokens":[
      "sleepi",
      "annoy",
      "girl",
      "grab",
      "librari",
      "copi",
      "dthe",
      "mighti",
      "duck",
      "chanc"
    ],
    "token_count":10,
    "processed_text":"sleepi annoy girl grab librari copi dthe mighti duck chanc"
  },
  {
    "label":0,
    "text":"get better ruthem",
    "cleaned_text":"get better ruthem",
    "normalized_text":"get better ruthem",
    "tokens":[
      "get",
      "better",
      "ruthem"
    ],
    "token_count":3,
    "processed_text":"get better ruthem"
  },
  {
    "label":0,
    "text":"yea that next step tri sell gumtre even twitter get nowher",
    "cleaned_text":"yea that next step tri sell gumtre even twitter get nowher",
    "normalized_text":"yea that next step tri sell gumtre even twitter get nowher",
    "tokens":[
      "yea",
      "next",
      "step",
      "tri",
      "sell",
      "gumtr",
      "even",
      "twitter",
      "get",
      "nowher"
    ],
    "token_count":10,
    "processed_text":"yea next step tri sell gumtr even twitter get nowher"
  },
  {
    "label":0,
    "text":"im pool applic",
    "cleaned_text":"im pool applic",
    "normalized_text":"im pool applic",
    "tokens":[
      "im",
      "pool",
      "applic"
    ],
    "token_count":3,
    "processed_text":"im pool applic"
  },
  {
    "label":4,
    "text":"gotta tidi hous listen music",
    "cleaned_text":"gotta tidi hous listen music",
    "normalized_text":"gotta tidi hous listen music",
    "tokens":[
      "got",
      "ta",
      "tidi",
      "hou",
      "listen",
      "music"
    ],
    "token_count":6,
    "processed_text":"got ta tidi hou listen music"
  },
  {
    "label":0,
    "text":"wow site still work",
    "cleaned_text":"wow site still work",
    "normalized_text":"wow site still work",
    "tokens":[
      "wow",
      "site",
      "still",
      "work"
    ],
    "token_count":4,
    "processed_text":"wow site still work"
  },
  {
    "label":0,
    "text":"lazi allll morningshoulda exercis worknow work allllll day",
    "cleaned_text":"lazi allll morningshoulda exercis worknow work allllll day",
    "normalized_text":"lazi allll morningshoulda exercis worknow work allllll day",
    "tokens":[
      "lazi",
      "allll",
      "morningshoulda",
      "exerci",
      "worknow",
      "work",
      "allllll",
      "day"
    ],
    "token_count":8,
    "processed_text":"lazi allll morningshoulda exerci worknow work allllll day"
  },
  {
    "label":0,
    "text":"read offlin messag na quotgt dude kailangan na niya talaga kantahin sayo yung pleas dont go",
    "cleaned_text":"read offlin messag na quotgt dude kailangan na niya talaga kantahin sayo yung pleas dont go",
    "normalized_text":"read offlin messag na quotgt dude kailangan na niya talaga kantahin sayo yung pleas dont go",
    "tokens":[
      "read",
      "offlin",
      "messag",
      "na",
      "quotgt",
      "dude",
      "kailangan",
      "na",
      "niya",
      "talaga",
      "kantahin",
      "sayo",
      "yung",
      "plea",
      "dont",
      "go"
    ],
    "token_count":16,
    "processed_text":"read offlin messag na quotgt dude kailangan na niya talaga kantahin sayo yung plea dont go"
  },
  {
    "label":4,
    "text":"im final graduat",
    "cleaned_text":"im final graduat",
    "normalized_text":"im final graduat",
    "tokens":[
      "im",
      "final",
      "graduat"
    ],
    "token_count":3,
    "processed_text":"im final graduat"
  },
  {
    "label":4,
    "text":"done uploadin comment",
    "cleaned_text":"done uploadin comment",
    "normalized_text":"done uploadin comment",
    "tokens":[
      "done",
      "uploadin",
      "comment"
    ],
    "token_count":3,
    "processed_text":"done uploadin comment"
  },
  {
    "label":0,
    "text":"imdi feel crap",
    "cleaned_text":"imdi feel crap",
    "normalized_text":"imdi feel crap",
    "tokens":[
      "imdi",
      "feel",
      "crap"
    ],
    "token_count":3,
    "processed_text":"imdi feel crap"
  },
  {
    "label":4,
    "text":"welcom twitter realli anoth time waster",
    "cleaned_text":"welcom twitter realli anoth time waster",
    "normalized_text":"welcom twitter realli anoth time waster",
    "tokens":[
      "welcom",
      "twitter",
      "realli",
      "anoth",
      "time",
      "waster"
    ],
    "token_count":6,
    "processed_text":"welcom twitter realli anoth time waster"
  },
  {
    "label":0,
    "text":"although mcdonald better england yesss good night pay moment though how",
    "cleaned_text":"although mcdonald better england yesss good night pay moment though how",
    "normalized_text":"although mcdonald better england yesss good night pay moment though how",
    "tokens":[
      "although",
      "mcdonald",
      "better",
      "england",
      "yesss",
      "good",
      "night",
      "pay",
      "moment",
      "though"
    ],
    "token_count":10,
    "processed_text":"although mcdonald better england yesss good night pay moment though"
  },
  {
    "label":0,
    "text":"slightli annoy right result cant sleep",
    "cleaned_text":"slightli annoy right result cant sleep",
    "normalized_text":"slightli annoy right result cant sleep",
    "tokens":[
      "slightli",
      "annoy",
      "right",
      "result",
      "cant",
      "sleep"
    ],
    "token_count":6,
    "processed_text":"slightli annoy right result cant sleep"
  },
  {
    "label":4,
    "text":"aawww damn got well sort lol great hour sleep",
    "cleaned_text":"aawww damn got well sort lol great hour sleep",
    "normalized_text":"aawww damn got well sort lol great hour sleep",
    "tokens":[
      "aawww",
      "damn",
      "got",
      "well",
      "sort",
      "lol",
      "great",
      "hour",
      "sleep"
    ],
    "token_count":9,
    "processed_text":"aawww damn got well sort lol great hour sleep"
  },
  {
    "label":0,
    "text":"miss much",
    "cleaned_text":"miss much",
    "normalized_text":"miss much",
    "tokens":[
      "miss",
      "much"
    ],
    "token_count":2,
    "processed_text":"miss much"
  },
  {
    "label":0,
    "text":"aw get wake choke wit da asthma iz turribl hugglz",
    "cleaned_text":"aw get wake choke wit da asthma iz turribl hugglz",
    "normalized_text":"aw get wake choke wit da asthma iz turribl hugglz",
    "tokens":[
      "aw",
      "get",
      "wake",
      "choke",
      "wit",
      "da",
      "asthma",
      "iz",
      "turribl",
      "hugglz"
    ],
    "token_count":10,
    "processed_text":"aw get wake choke wit da asthma iz turribl hugglz"
  },
  {
    "label":4,
    "text":"hey nice layout color love blue",
    "cleaned_text":"hey nice layout color love blue",
    "normalized_text":"hey nice layout color love blue",
    "tokens":[
      "hey",
      "nice",
      "layout",
      "color",
      "love",
      "blue"
    ],
    "token_count":6,
    "processed_text":"hey nice layout color love blue"
  },
  {
    "label":4,
    "text":"good see use twitter bit",
    "cleaned_text":"good see use twitter bit",
    "normalized_text":"good see use twitter bit",
    "tokens":[
      "good",
      "see",
      "use",
      "twitter",
      "bit"
    ],
    "token_count":5,
    "processed_text":"good see use twitter bit"
  },
  {
    "label":0,
    "text":"aww guy win event",
    "cleaned_text":"aww guy win event",
    "normalized_text":"aww guy win event",
    "tokens":[
      "aww",
      "guy",
      "win",
      "event"
    ],
    "token_count":4,
    "processed_text":"aww guy win event"
  },
  {
    "label":0,
    "text":"conscienc gastronom desir arent best bed fellow tell",
    "cleaned_text":"conscienc gastronom desir arent best bed fellow tell",
    "normalized_text":"conscienc gastronom desir arent best bed fellow tell",
    "tokens":[
      "conscienc",
      "gastronom",
      "desir",
      "arent",
      "best",
      "bed",
      "fellow",
      "tell"
    ],
    "token_count":8,
    "processed_text":"conscienc gastronom desir arent best bed fellow tell"
  },
  {
    "label":0,
    "text":"went bed earli last nightampstil couldnt get morn sittin chair feelin like im gonna knock",
    "cleaned_text":"went bed earli last nightampstil couldnt get morn sittin chair feelin like im gonna knock",
    "normalized_text":"went bed earli last nightampstil couldnt get morn sittin chair feelin like im gonna knock",
    "tokens":[
      "went",
      "bed",
      "earli",
      "last",
      "nightampstil",
      "couldnt",
      "get",
      "morn",
      "sittin",
      "chair",
      "feelin",
      "like",
      "im",
      "gon",
      "na",
      "knock"
    ],
    "token_count":16,
    "processed_text":"went bed earli last nightampstil couldnt get morn sittin chair feelin like im gon na knock"
  },
  {
    "label":4,
    "text":"broken yay lay",
    "cleaned_text":"broken yay lay",
    "normalized_text":"broken yay lay",
    "tokens":[
      "broken",
      "yay",
      "lay"
    ],
    "token_count":3,
    "processed_text":"broken yay lay"
  },
  {
    "label":4,
    "text":"your welcom good luck tech support today hope dont repeat time today lol",
    "cleaned_text":"your welcom good luck tech support today hope dont repeat time today lol",
    "normalized_text":"your welcom good luck tech support today hope dont repeat time today lol",
    "tokens":[
      "welcom",
      "good",
      "luck",
      "tech",
      "support",
      "today",
      "hope",
      "dont",
      "repeat",
      "time",
      "today",
      "lol"
    ],
    "token_count":12,
    "processed_text":"welcom good luck tech support today hope dont repeat time today lol"
  },
  {
    "label":0,
    "text":"go watch fc nant today think get relegatdd year",
    "cleaned_text":"go watch fc nant today think get relegatdd year",
    "normalized_text":"go watch fc nant today think get relegatdd year",
    "tokens":[
      "go",
      "watch",
      "fc",
      "nant",
      "today",
      "think",
      "get",
      "relegatdd",
      "year"
    ],
    "token_count":9,
    "processed_text":"go watch fc nant today think get relegatdd year"
  },
  {
    "label":0,
    "text":"omgg sim gave tummi ach theyr confus enjoy late hour",
    "cleaned_text":"omgg sim gave tummi ach theyr confus enjoy late hour",
    "normalized_text":"omgg sim gave tummi ach theyr confus enjoy late hour",
    "tokens":[
      "omgg",
      "sim",
      "gave",
      "tummi",
      "ach",
      "theyr",
      "confu",
      "enjoy",
      "late",
      "hour"
    ],
    "token_count":10,
    "processed_text":"omgg sim gave tummi ach theyr confu enjoy late hour"
  },
  {
    "label":4,
    "text":"new design gt made she awesom",
    "cleaned_text":"new design gt made she awesom",
    "normalized_text":"new design gt made she awesom",
    "tokens":[
      "new",
      "design",
      "gt",
      "made",
      "awesom"
    ],
    "token_count":5,
    "processed_text":"new design gt made awesom"
  },
  {
    "label":4,
    "text":"yeah deffo gawd soooo know cambridg bu system xx",
    "cleaned_text":"yeah deffo gawd soooo know cambridg bu system xx",
    "normalized_text":"yeah deffo gawd soooo know cambridg bu system xx",
    "tokens":[
      "yeah",
      "deffo",
      "gawd",
      "soooo",
      "know",
      "cambridg",
      "bu",
      "system",
      "xx"
    ],
    "token_count":9,
    "processed_text":"yeah deffo gawd soooo know cambridg bu system xx"
  },
  {
    "label":4,
    "text":"oh hope soon",
    "cleaned_text":"oh hope soon",
    "normalized_text":"oh hope soon",
    "tokens":[
      "oh",
      "hope",
      "soon"
    ],
    "token_count":3,
    "processed_text":"oh hope soon"
  },
  {
    "label":0,
    "text":"iphon updat guess ill wait get home work",
    "cleaned_text":"iphon updat guess ill wait get home work",
    "normalized_text":"iphon updat guess ill wait get home work",
    "tokens":[
      "iphon",
      "updat",
      "guess",
      "ill",
      "wait",
      "get",
      "home",
      "work"
    ],
    "token_count":8,
    "processed_text":"iphon updat guess ill wait get home work"
  },
  {
    "label":4,
    "text":"oh ok thank",
    "cleaned_text":"oh ok thank",
    "normalized_text":"oh ok thank",
    "tokens":[
      "oh",
      "ok",
      "thank"
    ],
    "token_count":3,
    "processed_text":"oh ok thank"
  },
  {
    "label":0,
    "text":"aaaargh fone stop workin prob wont repli txt",
    "cleaned_text":"aaaargh fone stop workin prob wont repli txt",
    "normalized_text":"aaaargh fone stop workin prob wont repli txt",
    "tokens":[
      "aaaargh",
      "fone",
      "stop",
      "workin",
      "prob",
      "wont",
      "repli",
      "txt"
    ],
    "token_count":8,
    "processed_text":"aaaargh fone stop workin prob wont repli txt"
  },
  {
    "label":4,
    "text":"hey met ive known one person name panda im preetti sure he guy haha havent met hi",
    "cleaned_text":"hey met ive known one person name panda im preetti sure he guy haha havent met hi",
    "normalized_text":"hey met ive known one person name panda im preetti sure he guy haha havent met hi",
    "tokens":[
      "hey",
      "met",
      "ive",
      "known",
      "one",
      "person",
      "name",
      "panda",
      "im",
      "preetti",
      "sure",
      "guy",
      "haha",
      "havent",
      "met",
      "hi"
    ],
    "token_count":16,
    "processed_text":"hey met ive known one person name panda im preetti sure guy haha havent met hi"
  },
  {
    "label":0,
    "text":"neck kill ill live got lucki worst part terrifi express driver fault",
    "cleaned_text":"neck kill ill live got lucki worst part terrifi express driver fault",
    "normalized_text":"neck kill ill live got lucki worst part terrifi express driver fault",
    "tokens":[
      "neck",
      "kill",
      "ill",
      "live",
      "got",
      "lucki",
      "worst",
      "part",
      "terrifi",
      "express",
      "driver",
      "fault"
    ],
    "token_count":12,
    "processed_text":"neck kill ill live got lucki worst part terrifi express driver fault"
  },
  {
    "label":4,
    "text":"lolthank",
    "cleaned_text":"lolthank",
    "normalized_text":"lolthank",
    "tokens":[
      "lolthank"
    ],
    "token_count":1,
    "processed_text":"lolthank"
  },
  {
    "label":4,
    "text":"todayiliv",
    "cleaned_text":"todayiliv",
    "normalized_text":"todayiliv",
    "tokens":[
      "todayiliv"
    ],
    "token_count":1,
    "processed_text":"todayiliv"
  },
  {
    "label":0,
    "text":"pink eye eye never fun",
    "cleaned_text":"pink eye eye never fun",
    "normalized_text":"pink eye eye never fun",
    "tokens":[
      "pink",
      "eye",
      "eye",
      "never",
      "fun"
    ],
    "token_count":5,
    "processed_text":"pink eye eye never fun"
  },
  {
    "label":0,
    "text":"omggggggggg go sleep yr old dementia patient came downstairssh cant sleep sleep tj",
    "cleaned_text":"omggggggggg go sleep yr old dementia patient came downstairssh cant sleep sleep tj",
    "normalized_text":"omggggggggg go sleep yr old dementia patient came downstairssh cant sleep sleep tj",
    "tokens":[
      "omggggggggg",
      "go",
      "sleep",
      "yr",
      "old",
      "dementia",
      "patient",
      "came",
      "downstairssh",
      "cant",
      "sleep",
      "sleep",
      "tj"
    ],
    "token_count":13,
    "processed_text":"omggggggggg go sleep yr old dementia patient came downstairssh cant sleep sleep tj"
  },
  {
    "label":4,
    "text":"eat easter egg haha bit late oh well yummi x",
    "cleaned_text":"eat easter egg haha bit late oh well yummi x",
    "normalized_text":"eat easter egg haha bit late oh well yummi x",
    "tokens":[
      "eat",
      "easter",
      "egg",
      "haha",
      "bit",
      "late",
      "oh",
      "well",
      "yummi"
    ],
    "token_count":9,
    "processed_text":"eat easter egg haha bit late oh well yummi"
  },
  {
    "label":4,
    "text":"yess",
    "cleaned_text":"yess",
    "normalized_text":"yess",
    "tokens":[
      "yess"
    ],
    "token_count":1,
    "processed_text":"yess"
  },
  {
    "label":0,
    "text":"cant find dashiki",
    "cleaned_text":"cant find dashiki",
    "normalized_text":"cant find dashiki",
    "tokens":[
      "cant",
      "find",
      "dashiki"
    ],
    "token_count":3,
    "processed_text":"cant find dashiki"
  },
  {
    "label":4,
    "text":"repli pleeeas watch cover fall love pleas viru",
    "cleaned_text":"repli pleeeas watch cover fall love pleas viru",
    "normalized_text":"repli pleeeas watch cover fall love pleas viru",
    "tokens":[
      "repli",
      "pleeea",
      "watch",
      "cover",
      "fall",
      "love",
      "plea",
      "viru"
    ],
    "token_count":8,
    "processed_text":"repli pleeea watch cover fall love plea viru"
  },
  {
    "label":0,
    "text":"friggin headach",
    "cleaned_text":"friggin headach",
    "normalized_text":"friggin headach",
    "tokens":[
      "friggin",
      "headach"
    ],
    "token_count":2,
    "processed_text":"friggin headach"
  },
  {
    "label":0,
    "text":"break back",
    "cleaned_text":"break back",
    "normalized_text":"break back",
    "tokens":[
      "break",
      "back"
    ],
    "token_count":2,
    "processed_text":"break back"
  },
  {
    "label":4,
    "text":"lycka till",
    "cleaned_text":"lycka till",
    "normalized_text":"lycka till",
    "tokens":[
      "lycka",
      "till"
    ],
    "token_count":2,
    "processed_text":"lycka till"
  },
  {
    "label":4,
    "text":"hah thank sir gotta watch despot rosi round cheek theyr worst",
    "cleaned_text":"hah thank sir gotta watch despot rosi round cheek theyr worst",
    "normalized_text":"hah thank sir gotta watch despot rosi round cheek theyr worst",
    "tokens":[
      "hah",
      "thank",
      "sir",
      "got",
      "ta",
      "watch",
      "despot",
      "rosi",
      "round",
      "cheek",
      "theyr",
      "worst"
    ],
    "token_count":12,
    "processed_text":"hah thank sir got ta watch despot rosi round cheek theyr worst"
  },
  {
    "label":0,
    "text":"awak im ill allergi manag two nightmar hour sleep",
    "cleaned_text":"awak im ill allergi manag two nightmar hour sleep",
    "normalized_text":"awak im ill allergi manag two nightmar hour sleep",
    "tokens":[
      "awak",
      "im",
      "ill",
      "allergi",
      "manag",
      "two",
      "nightmar",
      "hour",
      "sleep"
    ],
    "token_count":9,
    "processed_text":"awak im ill allergi manag two nightmar hour sleep"
  },
  {
    "label":4,
    "text":"twitter quit quiet recent need fresh blooood cute boy",
    "cleaned_text":"twitter quit quiet recent need fresh blooood cute boy",
    "normalized_text":"twitter quit quiet recent need fresh blooood cute boy",
    "tokens":[
      "twitter",
      "quit",
      "quiet",
      "recent",
      "need",
      "fresh",
      "blooood",
      "cute",
      "boy"
    ],
    "token_count":9,
    "processed_text":"twitter quit quiet recent need fresh blooood cute boy"
  },
  {
    "label":4,
    "text":"final hit gay lol dont text noon",
    "cleaned_text":"final hit gay lol dont text noon",
    "normalized_text":"final hit gay lol dont text noon",
    "tokens":[
      "final",
      "hit",
      "gay",
      "lol",
      "dont",
      "text",
      "noon"
    ],
    "token_count":7,
    "processed_text":"final hit gay lol dont text noon"
  },
  {
    "label":0,
    "text":"mike tyson daughter pass away like depress pray famili",
    "cleaned_text":"mike tyson daughter pass away like depress pray famili",
    "normalized_text":"mike tyson daughter pass away like depress pray famili",
    "tokens":[
      "mike",
      "tyson",
      "daughter",
      "pass",
      "away",
      "like",
      "depress",
      "pray",
      "famili"
    ],
    "token_count":9,
    "processed_text":"mike tyson daughter pass away like depress pray famili"
  },
  {
    "label":4,
    "text":"lol noundiessunday",
    "cleaned_text":"lol noundiessunday",
    "normalized_text":"lol noundiessunday",
    "tokens":[
      "lol",
      "noundiessunday"
    ],
    "token_count":2,
    "processed_text":"lol noundiessunday"
  },
  {
    "label":4,
    "text":"love pastor tonight god move craziest way get watch",
    "cleaned_text":"love pastor tonight god move craziest way get watch",
    "normalized_text":"love pastor tonight god move craziest way get watch",
    "tokens":[
      "love",
      "pastor",
      "tonight",
      "god",
      "move",
      "craziest",
      "way",
      "get",
      "watch"
    ],
    "token_count":9,
    "processed_text":"love pastor tonight god move craziest way get watch"
  },
  {
    "label":4,
    "text":"probabl way work work good day enjoy whisk away later",
    "cleaned_text":"probabl way work work good day enjoy whisk away later",
    "normalized_text":"probabl way work work good day enjoy whisk away later",
    "tokens":[
      "probabl",
      "way",
      "work",
      "work",
      "good",
      "day",
      "enjoy",
      "whisk",
      "away",
      "later"
    ],
    "token_count":10,
    "processed_text":"probabl way work work good day enjoy whisk away later"
  },
  {
    "label":0,
    "text":"brought world",
    "cleaned_text":"brought world",
    "normalized_text":"brought world",
    "tokens":[
      "brought",
      "world"
    ],
    "token_count":2,
    "processed_text":"brought world"
  },
  {
    "label":0,
    "text":"everyon babi im feel old",
    "cleaned_text":"everyon babi im feel old",
    "normalized_text":"everyon babi im feel old",
    "tokens":[
      "everyon",
      "babi",
      "im",
      "feel",
      "old"
    ],
    "token_count":5,
    "processed_text":"everyon babi im feel old"
  },
  {
    "label":0,
    "text":"woke edward poster fell mecant get back sleep",
    "cleaned_text":"woke edward poster fell mecant get back sleep",
    "normalized_text":"woke edward poster fell mecant get back sleep",
    "tokens":[
      "woke",
      "edward",
      "poster",
      "fell",
      "mecant",
      "get",
      "back",
      "sleep"
    ],
    "token_count":8,
    "processed_text":"woke edward poster fell mecant get back sleep"
  },
  {
    "label":4,
    "text":"thank much follow",
    "cleaned_text":"thank much follow",
    "normalized_text":"thank much follow",
    "tokens":[
      "thank",
      "much",
      "follow"
    ],
    "token_count":3,
    "processed_text":"thank much follow"
  },
  {
    "label":4,
    "text":"great night last night ate sandwich dont feel sick im go john hous littl",
    "cleaned_text":"great night last night ate sandwich dont feel sick im go john hous littl",
    "normalized_text":"great night last night ate sandwich dont feel sick im go john hous littl",
    "tokens":[
      "great",
      "night",
      "last",
      "night",
      "ate",
      "sandwich",
      "dont",
      "feel",
      "sick",
      "im",
      "go",
      "john",
      "hou",
      "littl"
    ],
    "token_count":14,
    "processed_text":"great night last night ate sandwich dont feel sick im go john hou littl"
  },
  {
    "label":4,
    "text":"let know go shutterstock",
    "cleaned_text":"let know go shutterstock",
    "normalized_text":"let know go shutterstock",
    "tokens":[
      "let",
      "know",
      "go",
      "shutterstock"
    ],
    "token_count":4,
    "processed_text":"let know go shutterstock"
  },
  {
    "label":0,
    "text":"suck",
    "cleaned_text":"suck",
    "normalized_text":"suck",
    "tokens":[
      "suck"
    ],
    "token_count":1,
    "processed_text":"suck"
  },
  {
    "label":4,
    "text":"cant wait see steph",
    "cleaned_text":"cant wait see steph",
    "normalized_text":"cant wait see steph",
    "tokens":[
      "cant",
      "wait",
      "see",
      "steph"
    ],
    "token_count":4,
    "processed_text":"cant wait see steph"
  },
  {
    "label":0,
    "text":"coupl texa air franc plane miss board",
    "cleaned_text":"coupl texa air franc plane miss board",
    "normalized_text":"coupl texa air franc plane miss board",
    "tokens":[
      "coupl",
      "texa",
      "air",
      "franc",
      "plane",
      "miss",
      "board"
    ],
    "token_count":7,
    "processed_text":"coupl texa air franc plane miss board"
  },
  {
    "label":0,
    "text":"unbeliv jealou want goooo",
    "cleaned_text":"unbeliv jealou want goooo",
    "normalized_text":"unbeliv jealou want goooo",
    "tokens":[
      "unbeliv",
      "jealou",
      "want",
      "goooo"
    ],
    "token_count":4,
    "processed_text":"unbeliv jealou want goooo"
  },
  {
    "label":4,
    "text":"oooh that clever bow econom wisdom sensei",
    "cleaned_text":"oooh that clever bow econom wisdom sensei",
    "normalized_text":"oooh that clever bow econom wisdom sensei",
    "tokens":[
      "oooh",
      "clever",
      "bow",
      "econom",
      "wisdom",
      "sensei"
    ],
    "token_count":6,
    "processed_text":"oooh clever bow econom wisdom sensei"
  },
  {
    "label":4,
    "text":"fun award bet look stun",
    "cleaned_text":"fun award bet look stun",
    "normalized_text":"fun award bet look stun",
    "tokens":[
      "fun",
      "award",
      "bet",
      "look",
      "stun"
    ],
    "token_count":5,
    "processed_text":"fun award bet look stun"
  },
  {
    "label":0,
    "text":"et bag salad manner bag crisp shock sixteen stone epiphani morn waist leg",
    "cleaned_text":"et bag salad manner bag crisp shock sixteen stone epiphani morn waist leg",
    "normalized_text":"et bag salad manner bag crisp shock sixteen stone epiphani morn waist leg",
    "tokens":[
      "et",
      "bag",
      "salad",
      "manner",
      "bag",
      "crisp",
      "shock",
      "sixteen",
      "stone",
      "epiphani",
      "morn",
      "waist",
      "leg"
    ],
    "token_count":13,
    "processed_text":"et bag salad manner bag crisp shock sixteen stone epiphani morn waist leg"
  },
  {
    "label":0,
    "text":"st grade boyfriend actual found facebook use call farrah rememb ultim compliment",
    "cleaned_text":"st grade boyfriend actual found facebook use call farrah rememb ultim compliment",
    "normalized_text":"st grade boyfriend actual found facebook use call farrah rememb ultim compliment",
    "tokens":[
      "st",
      "grade",
      "boyfriend",
      "actual",
      "found",
      "facebook",
      "use",
      "call",
      "farrah",
      "rememb",
      "ultim",
      "compliment"
    ],
    "token_count":12,
    "processed_text":"st grade boyfriend actual found facebook use call farrah rememb ultim compliment"
  },
  {
    "label":4,
    "text":"blah know run mile half consistantli oh well ill get back dont worri",
    "cleaned_text":"blah know run mile half consistantli oh well ill get back dont worri",
    "normalized_text":"blah know run mile half consistantli oh well ill get back dont worri",
    "tokens":[
      "blah",
      "know",
      "run",
      "mile",
      "half",
      "consistantli",
      "oh",
      "well",
      "ill",
      "get",
      "back",
      "dont",
      "worri"
    ],
    "token_count":13,
    "processed_text":"blah know run mile half consistantli oh well ill get back dont worri"
  },
  {
    "label":0,
    "text":"stretch ear mm realli sore",
    "cleaned_text":"stretch ear mm realli sore",
    "normalized_text":"stretch ear mm realli sore",
    "tokens":[
      "stretch",
      "ear",
      "mm",
      "realli",
      "sore"
    ],
    "token_count":5,
    "processed_text":"stretch ear mm realli sore"
  },
  {
    "label":0,
    "text":"eww nastypoor thing",
    "cleaned_text":"eww nastypoor thing",
    "normalized_text":"eww nastypoor thing",
    "tokens":[
      "eww",
      "nastypoor",
      "thing"
    ],
    "token_count":3,
    "processed_text":"eww nastypoor thing"
  },
  {
    "label":0,
    "text":"howev go pretend forgotteni tri rememb friend birthday avail",
    "cleaned_text":"howev go pretend forgotteni tri rememb friend birthday avail",
    "normalized_text":"howev go pretend forgotteni tri rememb friend birthday avail",
    "tokens":[
      "howev",
      "go",
      "pretend",
      "forgotteni",
      "tri",
      "rememb",
      "friend",
      "birthday",
      "avail"
    ],
    "token_count":9,
    "processed_text":"howev go pretend forgotteni tri rememb friend birthday avail"
  },
  {
    "label":0,
    "text":"im go unfollow two your give sparkli milkbrick shame",
    "cleaned_text":"im go unfollow two your give sparkli milkbrick shame",
    "normalized_text":"im go unfollow two your give sparkli milkbrick shame",
    "tokens":[
      "im",
      "go",
      "unfollow",
      "two",
      "give",
      "sparkli",
      "milkbrick",
      "shame"
    ],
    "token_count":8,
    "processed_text":"im go unfollow two give sparkli milkbrick shame"
  },
  {
    "label":0,
    "text":"got marin world earli im alreadi impati line park",
    "cleaned_text":"got marin world earli im alreadi impati line park",
    "normalized_text":"got marin world earli im alreadi impati line park",
    "tokens":[
      "got",
      "marin",
      "world",
      "earli",
      "im",
      "alreadi",
      "impati",
      "line",
      "park"
    ],
    "token_count":9,
    "processed_text":"got marin world earli im alreadi impati line park"
  },
  {
    "label":4,
    "text":"nope yet go right got home jake basebal gamebrrrrr im frozen",
    "cleaned_text":"nope yet go right got home jake basebal gamebrrrrr im frozen",
    "normalized_text":"nope yet go right got home jake basebal gamebrrrrr im frozen",
    "tokens":[
      "nope",
      "yet",
      "go",
      "right",
      "got",
      "home",
      "jake",
      "baseb",
      "gamebrrrrr",
      "im",
      "frozen"
    ],
    "token_count":11,
    "processed_text":"nope yet go right got home jake baseb gamebrrrrr im frozen"
  },
  {
    "label":0,
    "text":"lay bed think long day miss dsneadi soooo miss",
    "cleaned_text":"lay bed think long day miss dsneadi soooo miss",
    "normalized_text":"lay bed think long day miss dsneadi soooo miss",
    "tokens":[
      "lay",
      "bed",
      "think",
      "long",
      "day",
      "miss",
      "dsneadi",
      "soooo",
      "miss"
    ],
    "token_count":9,
    "processed_text":"lay bed think long day miss dsneadi soooo miss"
  },
  {
    "label":4,
    "text":"glad your great time fl",
    "cleaned_text":"glad your great time fl",
    "normalized_text":"glad your great time fl",
    "tokens":[
      "glad",
      "great",
      "time",
      "fl"
    ],
    "token_count":4,
    "processed_text":"glad great time fl"
  },
  {
    "label":4,
    "text":"ah cool thought might mp archiv someth",
    "cleaned_text":"ah cool thought might mp archiv someth",
    "normalized_text":"ah cool thought might mp archiv someth",
    "tokens":[
      "ah",
      "cool",
      "thought",
      "mp",
      "archiv",
      "someth"
    ],
    "token_count":6,
    "processed_text":"ah cool thought mp archiv someth"
  },
  {
    "label":4,
    "text":"feel dad car think wow still unbeliev cool never earthquak",
    "cleaned_text":"feel dad car think wow still unbeliev cool never earthquak",
    "normalized_text":"feel dad car think wow still unbeliev cool never earthquak",
    "tokens":[
      "feel",
      "dad",
      "car",
      "think",
      "wow",
      "still",
      "unbeliev",
      "cool",
      "never",
      "earthquak"
    ],
    "token_count":10,
    "processed_text":"feel dad car think wow still unbeliev cool never earthquak"
  },
  {
    "label":4,
    "text":"im reeealli good moodi cant wait til friday",
    "cleaned_text":"im reeealli good moodi cant wait til friday",
    "normalized_text":"im reeealli good moodi cant wait til friday",
    "tokens":[
      "im",
      "reeealli",
      "good",
      "moodi",
      "cant",
      "wait",
      "til",
      "friday"
    ],
    "token_count":8,
    "processed_text":"im reeealli good moodi cant wait til friday"
  },
  {
    "label":4,
    "text":"interest first volunt stage dragon appear",
    "cleaned_text":"interest first volunt stage dragon appear",
    "normalized_text":"interest first volunt stage dragon appear",
    "tokens":[
      "interest",
      "first",
      "volunt",
      "stage",
      "dragon",
      "appear"
    ],
    "token_count":6,
    "processed_text":"interest first volunt stage dragon appear"
  },
  {
    "label":4,
    "text":"soni nintendo match microsoft year even sound cheesi say everybodi win",
    "cleaned_text":"soni nintendo match microsoft year even sound cheesi say everybodi win",
    "normalized_text":"soni nintendo match microsoft year even sound cheesi say everybodi win",
    "tokens":[
      "soni",
      "nintendo",
      "match",
      "microsoft",
      "year",
      "even",
      "sound",
      "cheesi",
      "say",
      "everybodi",
      "win"
    ],
    "token_count":11,
    "processed_text":"soni nintendo match microsoft year even sound cheesi say everybodi win"
  },
  {
    "label":0,
    "text":"readi monday",
    "cleaned_text":"readi monday",
    "normalized_text":"readi monday",
    "tokens":[
      "readi",
      "monday"
    ],
    "token_count":2,
    "processed_text":"readi monday"
  },
  {
    "label":0,
    "text":"accept maestro dont proper cc",
    "cleaned_text":"accept maestro dont proper cc",
    "normalized_text":"accept maestro dont proper cc",
    "tokens":[
      "accept",
      "maestro",
      "dont",
      "proper",
      "cc"
    ],
    "token_count":5,
    "processed_text":"accept maestro dont proper cc"
  },
  {
    "label":4,
    "text":"current home",
    "cleaned_text":"current home",
    "normalized_text":"current home",
    "tokens":[
      "current",
      "home"
    ],
    "token_count":2,
    "processed_text":"current home"
  },
  {
    "label":0,
    "text":"kill babi sea turtl shame gt",
    "cleaned_text":"kill babi sea turtl shame gt",
    "normalized_text":"kill babi sea turtl shame gt",
    "tokens":[
      "kill",
      "babi",
      "sea",
      "turtl",
      "shame",
      "gt"
    ],
    "token_count":6,
    "processed_text":"kill babi sea turtl shame gt"
  },
  {
    "label":4,
    "text":"love demi pleas come back chile",
    "cleaned_text":"love demi pleas come back chile",
    "normalized_text":"love demi pleas come back chile",
    "tokens":[
      "love",
      "demi",
      "plea",
      "come",
      "back",
      "chile"
    ],
    "token_count":6,
    "processed_text":"love demi plea come back chile"
  },
  {
    "label":4,
    "text":"thank x",
    "cleaned_text":"thank x",
    "normalized_text":"thank x",
    "tokens":[
      "thank"
    ],
    "token_count":1,
    "processed_text":"thank"
  },
  {
    "label":4,
    "text":"state slow seem pick bit",
    "cleaned_text":"state slow seem pick bit",
    "normalized_text":"state slow seem pick bit",
    "tokens":[
      "state",
      "slow",
      "seem",
      "pick",
      "bit"
    ],
    "token_count":5,
    "processed_text":"state slow seem pick bit"
  },
  {
    "label":4,
    "text":"yeah watch video simpli research camera angl video effect noth",
    "cleaned_text":"yeah watch video simpli research camera angl video effect noth",
    "normalized_text":"yeah watch video simpli research camera angl video effect noth",
    "tokens":[
      "yeah",
      "watch",
      "video",
      "simpli",
      "research",
      "camera",
      "angl",
      "video",
      "effect",
      "noth"
    ],
    "token_count":10,
    "processed_text":"yeah watch video simpli research camera angl video effect noth"
  },
  {
    "label":4,
    "text":"ye paivi safe co kinda know lol enjoy talk",
    "cleaned_text":"ye paivi safe co kinda know lol enjoy talk",
    "normalized_text":"ye paivi safe co kinda know lol enjoy talk",
    "tokens":[
      "ye",
      "paivi",
      "safe",
      "co",
      "kinda",
      "know",
      "lol",
      "enjoy",
      "talk"
    ],
    "token_count":9,
    "processed_text":"ye paivi safe co kinda know lol enjoy talk"
  },
  {
    "label":4,
    "text":"yey thank x",
    "cleaned_text":"yey thank x",
    "normalized_text":"yey thank x",
    "tokens":[
      "yey",
      "thank"
    ],
    "token_count":2,
    "processed_text":"yey thank"
  },
  {
    "label":0,
    "text":"sim wont work g",
    "cleaned_text":"sim wont work g",
    "normalized_text":"sim wont work g",
    "tokens":[
      "sim",
      "wont",
      "work"
    ],
    "token_count":3,
    "processed_text":"sim wont work"
  },
  {
    "label":4,
    "text":"thought took rock instead",
    "cleaned_text":"thought took rock instead",
    "normalized_text":"thought took rock instead",
    "tokens":[
      "thought",
      "took",
      "rock",
      "instead"
    ],
    "token_count":4,
    "processed_text":"thought took rock instead"
  },
  {
    "label":4,
    "text":"good look televis charlott laptop lol xx",
    "cleaned_text":"good look televis charlott laptop lol xx",
    "normalized_text":"good look televis charlott laptop lol xx",
    "tokens":[
      "good",
      "look",
      "televi",
      "charlott",
      "laptop",
      "lol",
      "xx"
    ],
    "token_count":7,
    "processed_text":"good look televi charlott laptop lol xx"
  },
  {
    "label":4,
    "text":"yeahhhh u made night",
    "cleaned_text":"yeahhhh u made night",
    "normalized_text":"yeahhhh u made night",
    "tokens":[
      "yeahhhh",
      "made",
      "night"
    ],
    "token_count":3,
    "processed_text":"yeahhhh made night"
  },
  {
    "label":0,
    "text":"tri chang pictur disappear",
    "cleaned_text":"tri chang pictur disappear",
    "normalized_text":"tri chang pictur disappear",
    "tokens":[
      "tri",
      "chang",
      "pictur",
      "disappear"
    ],
    "token_count":4,
    "processed_text":"tri chang pictur disappear"
  },
  {
    "label":0,
    "text":"hot room sleep stair famili happen watch war movi downstair",
    "cleaned_text":"hot room sleep stair famili happen watch war movi downstair",
    "normalized_text":"hot room sleep stair famili happen watch war movi downstair",
    "tokens":[
      "hot",
      "room",
      "sleep",
      "stair",
      "famili",
      "happen",
      "watch",
      "war",
      "movi",
      "downstair"
    ],
    "token_count":10,
    "processed_text":"hot room sleep stair famili happen watch war movi downstair"
  },
  {
    "label":0,
    "text":"super confus first know play lost",
    "cleaned_text":"super confus first know play lost",
    "normalized_text":"super confus first know play lost",
    "tokens":[
      "super",
      "confu",
      "first",
      "know",
      "play",
      "lost"
    ],
    "token_count":6,
    "processed_text":"super confu first know play lost"
  },
  {
    "label":0,
    "text":"feel neeeek phone deadski least black eye pea keep pump",
    "cleaned_text":"feel neeeek phone deadski least black eye pea keep pump",
    "normalized_text":"feel neeeek phone deadski least black eye pea keep pump",
    "tokens":[
      "feel",
      "neeeek",
      "phone",
      "deadski",
      "least",
      "black",
      "eye",
      "pea",
      "keep",
      "pump"
    ],
    "token_count":10,
    "processed_text":"feel neeeek phone deadski least black eye pea keep pump"
  },
  {
    "label":0,
    "text":"lot work spend weekend bad though fun yesterday cousin wed",
    "cleaned_text":"lot work spend weekend bad though fun yesterday cousin wed",
    "normalized_text":"lot work spend weekend bad though fun yesterday cousin wed",
    "tokens":[
      "lot",
      "work",
      "spend",
      "weekend",
      "bad",
      "though",
      "fun",
      "yesterday",
      "cousin",
      "wed"
    ],
    "token_count":10,
    "processed_text":"lot work spend weekend bad though fun yesterday cousin wed"
  },
  {
    "label":0,
    "text":"realli look forward tomorrow hope exit come around",
    "cleaned_text":"realli look forward tomorrow hope exit come around",
    "normalized_text":"realli look forward tomorrow hope exit come around",
    "tokens":[
      "realli",
      "look",
      "forward",
      "tomorrow",
      "hope",
      "exit",
      "come",
      "around"
    ],
    "token_count":8,
    "processed_text":"realli look forward tomorrow hope exit come around"
  },
  {
    "label":4,
    "text":"birthdayi dayi love pari hilton xxx",
    "cleaned_text":"birthdayi dayi love pari hilton xxx",
    "normalized_text":"birthdayi dayi love pari hilton xxx",
    "tokens":[
      "birthdayi",
      "dayi",
      "love",
      "pari",
      "hilton",
      "xxx"
    ],
    "token_count":6,
    "processed_text":"birthdayi dayi love pari hilton xxx"
  },
  {
    "label":0,
    "text":"think shirt larger thicker therefor held water ice that stori",
    "cleaned_text":"think shirt larger thicker therefor held water ice that stori",
    "normalized_text":"think shirt larger thicker therefor held water ice that stori",
    "tokens":[
      "think",
      "shirt",
      "larger",
      "thicker",
      "therefor",
      "held",
      "water",
      "ice",
      "stori"
    ],
    "token_count":9,
    "processed_text":"think shirt larger thicker therefor held water ice stori"
  },
  {
    "label":0,
    "text":"goin home",
    "cleaned_text":"goin home",
    "normalized_text":"goin home",
    "tokens":[
      "goin",
      "home"
    ],
    "token_count":2,
    "processed_text":"goin home"
  },
  {
    "label":0,
    "text":"good morn today dont feel anyth right",
    "cleaned_text":"good morn today dont feel anyth right",
    "normalized_text":"good morn today dont feel anyth right",
    "tokens":[
      "good",
      "morn",
      "today",
      "dont",
      "feel",
      "anyth",
      "right"
    ],
    "token_count":7,
    "processed_text":"good morn today dont feel anyth right"
  },
  {
    "label":4,
    "text":"thank awesom follow friday mention",
    "cleaned_text":"thank awesom follow friday mention",
    "normalized_text":"thank awesom follow friday mention",
    "tokens":[
      "thank",
      "awesom",
      "follow",
      "friday",
      "mention"
    ],
    "token_count":5,
    "processed_text":"thank awesom follow friday mention"
  },
  {
    "label":4,
    "text":"good morninghop great breakfast miss ihoplov pancak",
    "cleaned_text":"good morninghop great breakfast miss ihoplov pancak",
    "normalized_text":"good morninghop great breakfast miss ihoplov pancak",
    "tokens":[
      "good",
      "morninghop",
      "great",
      "breakfast",
      "miss",
      "ihoplov",
      "pancak"
    ],
    "token_count":7,
    "processed_text":"good morninghop great breakfast miss ihoplov pancak"
  },
  {
    "label":0,
    "text":"almost see dream im dream there voic insid head say youll never reach",
    "cleaned_text":"almost see dream im dream there voic insid head say youll never reach",
    "normalized_text":"almost see dream im dream there voic insid head say youll never reach",
    "tokens":[
      "almost",
      "see",
      "dream",
      "im",
      "dream",
      "voic",
      "insid",
      "head",
      "say",
      "youll",
      "never",
      "reach"
    ],
    "token_count":12,
    "processed_text":"almost see dream im dream voic insid head say youll never reach"
  },
  {
    "label":0,
    "text":"that rather that caus attack first place",
    "cleaned_text":"that rather that caus attack first place",
    "normalized_text":"that rather that caus attack first place",
    "tokens":[
      "rather",
      "cau",
      "attack",
      "first",
      "place"
    ],
    "token_count":5,
    "processed_text":"rather cau attack first place"
  },
  {
    "label":4,
    "text":"dont realli mindjust make bit fuss say im mom",
    "cleaned_text":"dont realli mindjust make bit fuss say im mom",
    "normalized_text":"dont realli mindjust make bit fuss say im mom",
    "tokens":[
      "dont",
      "realli",
      "mindjust",
      "make",
      "bit",
      "fuss",
      "say",
      "im",
      "mom"
    ],
    "token_count":9,
    "processed_text":"dont realli mindjust make bit fuss say im mom"
  },
  {
    "label":0,
    "text":"hair use long wavi like yoursand cut shoulder lengthnow miss long haira littl regret",
    "cleaned_text":"hair use long wavi like yoursand cut shoulder lengthnow miss long haira littl regret",
    "normalized_text":"hair use long wavi like yoursand cut shoulder lengthnow miss long haira littl regret",
    "tokens":[
      "hair",
      "use",
      "long",
      "wavi",
      "like",
      "yoursand",
      "cut",
      "shoulder",
      "lengthnow",
      "miss",
      "long",
      "haira",
      "littl",
      "regret"
    ],
    "token_count":14,
    "processed_text":"hair use long wavi like yoursand cut shoulder lengthnow miss long haira littl regret"
  },
  {
    "label":0,
    "text":"headach back feel bad twitter im work readi call day",
    "cleaned_text":"headach back feel bad twitter im work readi call day",
    "normalized_text":"headach back feel bad twitter im work readi call day",
    "tokens":[
      "headach",
      "back",
      "feel",
      "bad",
      "twitter",
      "im",
      "work",
      "readi",
      "call",
      "day"
    ],
    "token_count":10,
    "processed_text":"headach back feel bad twitter im work readi call day"
  },
  {
    "label":0,
    "text":"im go one hot load wee silli job get done",
    "cleaned_text":"im go one hot load wee silli job get done",
    "normalized_text":"im go one hot load wee silli job get done",
    "tokens":[
      "im",
      "go",
      "one",
      "hot",
      "load",
      "wee",
      "silli",
      "job",
      "get",
      "done"
    ],
    "token_count":10,
    "processed_text":"im go one hot load wee silli job get done"
  },
  {
    "label":0,
    "text":"ran dead squirrel lawn mower saddest thing ever least alreadi dead messi ew",
    "cleaned_text":"ran dead squirrel lawn mower saddest thing ever least alreadi dead messi ew",
    "normalized_text":"ran dead squirrel lawn mower saddest thing ever least alreadi dead messi ew",
    "tokens":[
      "ran",
      "dead",
      "squirrel",
      "lawn",
      "mower",
      "saddest",
      "thing",
      "ever",
      "least",
      "alreadi",
      "dead",
      "messi",
      "ew"
    ],
    "token_count":13,
    "processed_text":"ran dead squirrel lawn mower saddest thing ever least alreadi dead messi ew"
  },
  {
    "label":0,
    "text":"hear thunder",
    "cleaned_text":"hear thunder",
    "normalized_text":"hear thunder",
    "tokens":[
      "hear",
      "thunder"
    ],
    "token_count":2,
    "processed_text":"hear thunder"
  },
  {
    "label":0,
    "text":"meetup",
    "cleaned_text":"meetup",
    "normalized_text":"meetup",
    "tokens":[
      "meetup"
    ],
    "token_count":1,
    "processed_text":"meetup"
  },
  {
    "label":4,
    "text":"wait amanda wake see movi",
    "cleaned_text":"wait amanda wake see movi",
    "normalized_text":"wait amanda wake see movi",
    "tokens":[
      "wait",
      "amanda",
      "wake",
      "see",
      "movi"
    ],
    "token_count":5,
    "processed_text":"wait amanda wake see movi"
  },
  {
    "label":4,
    "text":"goodnight everybodi thank make st june day rememb",
    "cleaned_text":"goodnight everybodi thank make st june day rememb",
    "normalized_text":"goodnight everybodi thank make st june day rememb",
    "tokens":[
      "goodnight",
      "everybodi",
      "thank",
      "make",
      "st",
      "june",
      "day",
      "rememb"
    ],
    "token_count":8,
    "processed_text":"goodnight everybodi thank make st june day rememb"
  },
  {
    "label":4,
    "text":"phone lena czerina libbi",
    "cleaned_text":"phone lena czerina libbi",
    "normalized_text":"phone lena czerina libbi",
    "tokens":[
      "phone",
      "lena",
      "czerina",
      "libbi"
    ],
    "token_count":4,
    "processed_text":"phone lena czerina libbi"
  },
  {
    "label":4,
    "text":"lol u like british british rent",
    "cleaned_text":"lol u like british british rent",
    "normalized_text":"lol u like british british rent",
    "tokens":[
      "lol",
      "like",
      "british",
      "british",
      "rent"
    ],
    "token_count":5,
    "processed_text":"lol like british british rent"
  },
  {
    "label":4,
    "text":"well didnt realiz tweetgrad go evalu rate quotstankquot profil reign mandatori",
    "cleaned_text":"well didnt realiz tweetgrad go evalu rate quotstankquot profil reign mandatori",
    "normalized_text":"well didnt realiz tweetgrad go evalu rate quotstankquot profil reign mandatori",
    "tokens":[
      "well",
      "didnt",
      "realiz",
      "tweetgrad",
      "go",
      "evalu",
      "rate",
      "quotstankquot",
      "profil",
      "reign",
      "mandatori"
    ],
    "token_count":11,
    "processed_text":"well didnt realiz tweetgrad go evalu rate quotstankquot profil reign mandatori"
  },
  {
    "label":4,
    "text":"well thank kindli",
    "cleaned_text":"well thank kindli",
    "normalized_text":"well thank kindli",
    "tokens":[
      "well",
      "thank",
      "kindli"
    ],
    "token_count":3,
    "processed_text":"well thank kindli"
  },
  {
    "label":4,
    "text":"ooh rebecca minkoff sampl sale tomorrow least look",
    "cleaned_text":"ooh rebecca minkoff sampl sale tomorrow least look",
    "normalized_text":"ooh rebecca minkoff sampl sale tomorrow least look",
    "tokens":[
      "ooh",
      "rebecca",
      "minkoff",
      "sampl",
      "sale",
      "tomorrow",
      "least",
      "look"
    ],
    "token_count":8,
    "processed_text":"ooh rebecca minkoff sampl sale tomorrow least look"
  },
  {
    "label":4,
    "text":"love bitch sometim make peopl hate let face hater reason stunt",
    "cleaned_text":"love bitch sometim make peopl hate let face hater reason stunt",
    "normalized_text":"love bitch sometim make peopl hate let face hater reason stunt",
    "tokens":[
      "love",
      "bitch",
      "sometim",
      "make",
      "peopl",
      "hate",
      "let",
      "face",
      "hater",
      "reason",
      "stunt"
    ],
    "token_count":11,
    "processed_text":"love bitch sometim make peopl hate let face hater reason stunt"
  },
  {
    "label":0,
    "text":"pain max also design main line herv leger like need need cash",
    "cleaned_text":"pain max also design main line herv leger like need need cash",
    "normalized_text":"pain max also design main line herv leger like need need cash",
    "tokens":[
      "pain",
      "max",
      "also",
      "design",
      "main",
      "line",
      "herv",
      "leger",
      "like",
      "need",
      "need",
      "cash"
    ],
    "token_count":12,
    "processed_text":"pain max also design main line herv leger like need need cash"
  },
  {
    "label":0,
    "text":"want go promot gear groov unforn ride may b go one anaheim may though",
    "cleaned_text":"want go promot gear groov unforn ride may b go one anaheim may though",
    "normalized_text":"want go promot gear groov unforn ride may b go one anaheim may though",
    "tokens":[
      "want",
      "go",
      "promot",
      "gear",
      "groov",
      "unforn",
      "ride",
      "may",
      "go",
      "one",
      "anaheim",
      "may",
      "though"
    ],
    "token_count":13,
    "processed_text":"want go promot gear groov unforn ride may go one anaheim may though"
  },
  {
    "label":0,
    "text":"never felt anti social life where",
    "cleaned_text":"never felt anti social life where",
    "normalized_text":"never felt anti social life where",
    "tokens":[
      "never",
      "felt",
      "anti",
      "social",
      "life"
    ],
    "token_count":5,
    "processed_text":"never felt anti social life"
  },
  {
    "label":4,
    "text":"ook im go watch cqc bye dude good bless yaa",
    "cleaned_text":"ook im go watch cqc bye dude good bless yaa",
    "normalized_text":"ook im go watch cqc bye dude good bless yaa",
    "tokens":[
      "ook",
      "im",
      "go",
      "watch",
      "cqc",
      "bye",
      "dude",
      "good",
      "bless",
      "yaa"
    ],
    "token_count":10,
    "processed_text":"ook im go watch cqc bye dude good bless yaa"
  },
  {
    "label":0,
    "text":"would realli like know fuck find sub tq honest rawr",
    "cleaned_text":"would realli like know fuck find sub tq honest rawr",
    "normalized_text":"would realli like know fuck find sub tq honest rawr",
    "tokens":[
      "realli",
      "like",
      "know",
      "fuck",
      "find",
      "sub",
      "tq",
      "honest",
      "rawr"
    ],
    "token_count":9,
    "processed_text":"realli like know fuck find sub tq honest rawr"
  },
  {
    "label":0,
    "text":"noth im tire enough go bed coz didnt wake till",
    "cleaned_text":"noth im tire enough go bed coz didnt wake till",
    "normalized_text":"noth im tire enough go bed coz didnt wake till",
    "tokens":[
      "noth",
      "im",
      "tire",
      "enough",
      "go",
      "bed",
      "coz",
      "didnt",
      "wake",
      "till"
    ],
    "token_count":10,
    "processed_text":"noth im tire enough go bed coz didnt wake till"
  },
  {
    "label":0,
    "text":"awwwsorri dont servic detroit yet theyr awesom",
    "cleaned_text":"awwwsorri dont servic detroit yet theyr awesom",
    "normalized_text":"awwwsorri dont servic detroit yet theyr awesom",
    "tokens":[
      "awwwsorri",
      "dont",
      "servic",
      "detroit",
      "yet",
      "theyr",
      "awesom"
    ],
    "token_count":7,
    "processed_text":"awwwsorri dont servic detroit yet theyr awesom"
  },
  {
    "label":0,
    "text":"month put final got thru marley ador laugh lot etc last min rough",
    "cleaned_text":"month put final got thru marley ador laugh lot etc last min rough",
    "normalized_text":"month put final got thru marley ador laugh lot etc last min rough",
    "tokens":[
      "month",
      "put",
      "final",
      "got",
      "thru",
      "marley",
      "ador",
      "laugh",
      "lot",
      "etc",
      "last",
      "min",
      "rough"
    ],
    "token_count":13,
    "processed_text":"month put final got thru marley ador laugh lot etc last min rough"
  },
  {
    "label":0,
    "text":"day todaytir felt sleepi btw im soooooo addict mafia war",
    "cleaned_text":"day todaytir felt sleepi btw im soooooo addict mafia war",
    "normalized_text":"day todaytir felt sleepi btw im soooooo addict mafia war",
    "tokens":[
      "day",
      "todaytir",
      "felt",
      "sleepi",
      "btw",
      "im",
      "soooooo",
      "addict",
      "mafia",
      "war"
    ],
    "token_count":10,
    "processed_text":"day todaytir felt sleepi btw im soooooo addict mafia war"
  },
  {
    "label":4,
    "text":"idiot im mumbai heck",
    "cleaned_text":"idiot im mumbai heck",
    "normalized_text":"idiot im mumbai heck",
    "tokens":[
      "idiot",
      "im",
      "mumbai",
      "heck"
    ],
    "token_count":4,
    "processed_text":"idiot im mumbai heck"
  },
  {
    "label":0,
    "text":"tri setup old wireless router window simpli crash gona buy appl airport express today",
    "cleaned_text":"tri setup old wireless router window simpli crash gona buy appl airport express today",
    "normalized_text":"tri setup old wireless router window simpli crash gona buy appl airport express today",
    "tokens":[
      "tri",
      "setup",
      "old",
      "wireless",
      "router",
      "window",
      "simpli",
      "crash",
      "gona",
      "buy",
      "appl",
      "airport",
      "express",
      "today"
    ],
    "token_count":14,
    "processed_text":"tri setup old wireless router window simpli crash gona buy appl airport express today"
  },
  {
    "label":4,
    "text":"quotabduct abductedquot apart causemak differ give children futur look forward",
    "cleaned_text":"quotabduct abductedquot apart causemak differ give children futur look forward",
    "normalized_text":"quotabduct abductedquot apart causemak differ give children futur look forward",
    "tokens":[
      "quotabduct",
      "abductedquot",
      "apart",
      "causemak",
      "differ",
      "give",
      "children",
      "futur",
      "look",
      "forward"
    ],
    "token_count":10,
    "processed_text":"quotabduct abductedquot apart causemak differ give children futur look forward"
  },
  {
    "label":4,
    "text":"work til pm today",
    "cleaned_text":"work til pm today",
    "normalized_text":"work til pm today",
    "tokens":[
      "work",
      "til",
      "pm",
      "today"
    ],
    "token_count":4,
    "processed_text":"work til pm today"
  },
  {
    "label":0,
    "text":"wish would got ass go tube im sad",
    "cleaned_text":"wish would got ass go tube im sad",
    "normalized_text":"wish would got ass go tube im sad",
    "tokens":[
      "wish",
      "got",
      "ass",
      "go",
      "tube",
      "im",
      "sad"
    ],
    "token_count":7,
    "processed_text":"wish got ass go tube im sad"
  },
  {
    "label":4,
    "text":"goodnight peopl",
    "cleaned_text":"goodnight peopl",
    "normalized_text":"goodnight peopl",
    "tokens":[
      "goodnight",
      "peopl"
    ],
    "token_count":2,
    "processed_text":"goodnight peopl"
  },
  {
    "label":4,
    "text":"watch sens sensibl earlier im watch pride prejudic jane austen daydef",
    "cleaned_text":"watch sens sensibl earlier im watch pride prejudic jane austen daydef",
    "normalized_text":"watch sens sensibl earlier im watch pride prejudic jane austen daydef",
    "tokens":[
      "watch",
      "sen",
      "sensibl",
      "earlier",
      "im",
      "watch",
      "pride",
      "prejud",
      "jane",
      "austen",
      "daydef"
    ],
    "token_count":11,
    "processed_text":"watch sen sensibl earlier im watch pride prejud jane austen daydef"
  },
  {
    "label":0,
    "text":"put dog well royali suck",
    "cleaned_text":"put dog well royali suck",
    "normalized_text":"put dog well royali suck",
    "tokens":[
      "put",
      "dog",
      "well",
      "royali",
      "suck"
    ],
    "token_count":5,
    "processed_text":"put dog well royali suck"
  },
  {
    "label":4,
    "text":"workinga alway whatcha doon",
    "cleaned_text":"workinga alway whatcha doon",
    "normalized_text":"workinga alway whatcha doon",
    "tokens":[
      "workinga",
      "alway",
      "whatcha",
      "doon"
    ],
    "token_count":4,
    "processed_text":"workinga alway whatcha doon"
  },
  {
    "label":0,
    "text":"went see doc today told thing wors expect need med drugstor heavi stuff",
    "cleaned_text":"went see doc today told thing wors expect need med drugstor heavi stuff",
    "normalized_text":"went see doc today told thing wors expect need med drugstor heavi stuff",
    "tokens":[
      "went",
      "see",
      "doc",
      "today",
      "told",
      "thing",
      "wor",
      "expect",
      "need",
      "med",
      "drugstor",
      "heavi",
      "stuff"
    ],
    "token_count":13,
    "processed_text":"went see doc today told thing wor expect need med drugstor heavi stuff"
  },
  {
    "label":4,
    "text":"hope guy design drunk driver",
    "cleaned_text":"hope guy design drunk driver",
    "normalized_text":"hope guy design drunk driver",
    "tokens":[
      "hope",
      "guy",
      "design",
      "drunk",
      "driver"
    ],
    "token_count":5,
    "processed_text":"hope guy design drunk driver"
  },
  {
    "label":4,
    "text":"haha imagin theyr right size",
    "cleaned_text":"haha imagin theyr right size",
    "normalized_text":"haha imagin theyr right size",
    "tokens":[
      "haha",
      "imagin",
      "theyr",
      "right",
      "size"
    ],
    "token_count":5,
    "processed_text":"haha imagin theyr right size"
  },
  {
    "label":0,
    "text":"awwwh would best birthday present june see rob defranco bikini spain",
    "cleaned_text":"awwwh would best birthday present june see rob defranco bikini spain",
    "normalized_text":"awwwh would best birthday present june see rob defranco bikini spain",
    "tokens":[
      "awwwh",
      "best",
      "birthday",
      "present",
      "june",
      "see",
      "rob",
      "defranco",
      "bikini",
      "spain"
    ],
    "token_count":10,
    "processed_text":"awwwh best birthday present june see rob defranco bikini spain"
  },
  {
    "label":4,
    "text":"yep fine like say feel nuthin ward",
    "cleaned_text":"yep fine like say feel nuthin ward",
    "normalized_text":"yep fine like say feel nuthin ward",
    "tokens":[
      "yep",
      "fine",
      "like",
      "say",
      "feel",
      "nuthin",
      "ward"
    ],
    "token_count":7,
    "processed_text":"yep fine like say feel nuthin ward"
  },
  {
    "label":4,
    "text":"realli appreci live asylum tweet",
    "cleaned_text":"realli appreci live asylum tweet",
    "normalized_text":"realli appreci live asylum tweet",
    "tokens":[
      "realli",
      "appreci",
      "live",
      "asylum",
      "tweet"
    ],
    "token_count":5,
    "processed_text":"realli appreci live asylum tweet"
  },
  {
    "label":4,
    "text":"watch red eye rachel mcadam amaz actress",
    "cleaned_text":"watch red eye rachel mcadam amaz actress",
    "normalized_text":"watch red eye rachel mcadam amaz actress",
    "tokens":[
      "watch",
      "red",
      "eye",
      "rachel",
      "mcadam",
      "amaz",
      "actress"
    ],
    "token_count":7,
    "processed_text":"watch red eye rachel mcadam amaz actress"
  },
  {
    "label":0,
    "text":"dito expireddomain",
    "cleaned_text":"dito expireddomain",
    "normalized_text":"dito expireddomain",
    "tokens":[
      "dito",
      "expireddomain"
    ],
    "token_count":2,
    "processed_text":"dito expireddomain"
  },
  {
    "label":4,
    "text":"hang jessss fun",
    "cleaned_text":"hang jessss fun",
    "normalized_text":"hang jessss fun",
    "tokens":[
      "hang",
      "jessss",
      "fun"
    ],
    "token_count":3,
    "processed_text":"hang jessss fun"
  },
  {
    "label":0,
    "text":"morn dear",
    "cleaned_text":"morn dear",
    "normalized_text":"morn dear",
    "tokens":[
      "morn",
      "dear"
    ],
    "token_count":2,
    "processed_text":"morn dear"
  },
  {
    "label":0,
    "text":"know dad sound like he crazi he follow doctor order rest relax michael",
    "cleaned_text":"know dad sound like he crazi he follow doctor order rest relax michael",
    "normalized_text":"know dad sound like he crazi he follow doctor order rest relax michael",
    "tokens":[
      "know",
      "dad",
      "sound",
      "like",
      "crazi",
      "follow",
      "doctor",
      "order",
      "rest",
      "relax",
      "michael"
    ],
    "token_count":11,
    "processed_text":"know dad sound like crazi follow doctor order rest relax michael"
  },
  {
    "label":4,
    "text":"hell dust amaz cook realli shouldnt grous",
    "cleaned_text":"hell dust amaz cook realli shouldnt grous",
    "normalized_text":"hell dust amaz cook realli shouldnt grous",
    "tokens":[
      "hell",
      "dust",
      "amaz",
      "cook",
      "realli",
      "shouldnt",
      "grou"
    ],
    "token_count":7,
    "processed_text":"hell dust amaz cook realli shouldnt grou"
  },
  {
    "label":0,
    "text":"serious dont care rake make happi net oss project expect abl build nant fluent nh build pita",
    "cleaned_text":"serious dont care rake make happi net oss project expect abl build nant fluent nh build pita",
    "normalized_text":"serious dont care rake make happi net oss project expect abl build nant fluent nh build pita",
    "tokens":[
      "seriou",
      "dont",
      "care",
      "rake",
      "make",
      "happi",
      "net",
      "oss",
      "project",
      "expect",
      "abl",
      "build",
      "nant",
      "fluent",
      "nh",
      "build",
      "pita"
    ],
    "token_count":17,
    "processed_text":"seriou dont care rake make happi net oss project expect abl build nant fluent nh build pita"
  },
  {
    "label":0,
    "text":"husband china wife dug scotland fair",
    "cleaned_text":"husband china wife dug scotland fair",
    "normalized_text":"husband china wife dug scotland fair",
    "tokens":[
      "husband",
      "china",
      "wife",
      "dug",
      "scotland",
      "fair"
    ],
    "token_count":6,
    "processed_text":"husband china wife dug scotland fair"
  },
  {
    "label":4,
    "text":"time make donut other time burn donut",
    "cleaned_text":"time make donut other time burn donut",
    "normalized_text":"time make donut other time burn donut",
    "tokens":[
      "time",
      "make",
      "donut",
      "time",
      "burn",
      "donut"
    ],
    "token_count":6,
    "processed_text":"time make donut time burn donut"
  },
  {
    "label":0,
    "text":"ye talk someon got",
    "cleaned_text":"ye talk someon got",
    "normalized_text":"ye talk someon got",
    "tokens":[
      "ye",
      "talk",
      "someon",
      "got"
    ],
    "token_count":4,
    "processed_text":"ye talk someon got"
  },
  {
    "label":0,
    "text":"beaten rifl shoot fluke last shot beat point rain",
    "cleaned_text":"beaten rifl shoot fluke last shot beat point rain",
    "normalized_text":"beaten rifl shoot fluke last shot beat point rain",
    "tokens":[
      "beaten",
      "rifl",
      "shoot",
      "fluke",
      "last",
      "shot",
      "beat",
      "point",
      "rain"
    ],
    "token_count":9,
    "processed_text":"beaten rifl shoot fluke last shot beat point rain"
  },
  {
    "label":0,
    "text":"im watch red carpet right saw taylor lautner miley zac efron want ceremoni",
    "cleaned_text":"im watch red carpet right saw taylor lautner miley zac efron want ceremoni",
    "normalized_text":"im watch red carpet right saw taylor lautner miley zac efron want ceremoni",
    "tokens":[
      "im",
      "watch",
      "red",
      "carpet",
      "right",
      "saw",
      "taylor",
      "lautner",
      "miley",
      "zac",
      "efron",
      "want",
      "ceremoni"
    ],
    "token_count":13,
    "processed_text":"im watch red carpet right saw taylor lautner miley zac efron want ceremoni"
  },
  {
    "label":0,
    "text":"hahah nah yet week mayb test internet want load lol",
    "cleaned_text":"hahah nah yet week mayb test internet want load lol",
    "normalized_text":"hahah nah yet week mayb test internet want load lol",
    "tokens":[
      "hahah",
      "nah",
      "yet",
      "week",
      "mayb",
      "test",
      "internet",
      "want",
      "load",
      "lol"
    ],
    "token_count":10,
    "processed_text":"hahah nah yet week mayb test internet want load lol"
  },
  {
    "label":4,
    "text":"sound greatjust need",
    "cleaned_text":"sound greatjust need",
    "normalized_text":"sound greatjust need",
    "tokens":[
      "sound",
      "greatjust",
      "need"
    ],
    "token_count":3,
    "processed_text":"sound greatjust need"
  },
  {
    "label":4,
    "text":"total watch greaseoh way start friday night",
    "cleaned_text":"total watch greaseoh way start friday night",
    "normalized_text":"total watch greaseoh way start friday night",
    "tokens":[
      "total",
      "watch",
      "greaseoh",
      "way",
      "start",
      "friday",
      "night"
    ],
    "token_count":7,
    "processed_text":"total watch greaseoh way start friday night"
  },
  {
    "label":0,
    "text":"want one new olympu pen ep produc slrqualiti photo w ultracompact conveni",
    "cleaned_text":"want one new olympu pen ep produc slrqualiti photo w ultracompact conveni",
    "normalized_text":"want one new olympu pen ep produc slrqualiti photo w ultracompact conveni",
    "tokens":[
      "want",
      "one",
      "new",
      "olympu",
      "pen",
      "ep",
      "produc",
      "slrqualiti",
      "photo",
      "ultracompact",
      "conveni"
    ],
    "token_count":11,
    "processed_text":"want one new olympu pen ep produc slrqualiti photo ultracompact conveni"
  },
  {
    "label":0,
    "text":"damnit option call tri work someth im sad least place live",
    "cleaned_text":"damnit option call tri work someth im sad least place live",
    "normalized_text":"damnit option call tri work someth im sad least place live",
    "tokens":[
      "damnit",
      "option",
      "call",
      "tri",
      "work",
      "someth",
      "im",
      "sad",
      "least",
      "place",
      "live"
    ],
    "token_count":11,
    "processed_text":"damnit option call tri work someth im sad least place live"
  },
  {
    "label":4,
    "text":"sure lost track mani time weve seen resud evil wife fav movi",
    "cleaned_text":"sure lost track mani time weve seen resud evil wife fav movi",
    "normalized_text":"sure lost track mani time weve seen resud evil wife fav movi",
    "tokens":[
      "sure",
      "lost",
      "track",
      "mani",
      "time",
      "weve",
      "seen",
      "resud",
      "evil",
      "wife",
      "fav",
      "movi"
    ],
    "token_count":12,
    "processed_text":"sure lost track mani time weve seen resud evil wife fav movi"
  },
  {
    "label":4,
    "text":"think programm dont know",
    "cleaned_text":"think programm dont know",
    "normalized_text":"think programm dont know",
    "tokens":[
      "think",
      "programm",
      "dont",
      "know"
    ],
    "token_count":4,
    "processed_text":"think programm dont know"
  },
  {
    "label":4,
    "text":"watch friend jst",
    "cleaned_text":"watch friend jst",
    "normalized_text":"watch friend jst",
    "tokens":[
      "watch",
      "friend",
      "jst"
    ],
    "token_count":3,
    "processed_text":"watch friend jst"
  },
  {
    "label":0,
    "text":"realli peopl noth better make actor live miser",
    "cleaned_text":"realli peopl noth better make actor live miser",
    "normalized_text":"realli peopl noth better make actor live miser",
    "tokens":[
      "realli",
      "peopl",
      "noth",
      "better",
      "make",
      "actor",
      "live",
      "miser"
    ],
    "token_count":8,
    "processed_text":"realli peopl noth better make actor live miser"
  },
  {
    "label":0,
    "text":"pepsith caffein keepin couldnt help sound refresh",
    "cleaned_text":"pepsith caffein keepin couldnt help sound refresh",
    "normalized_text":"pepsith caffein keepin couldnt help sound refresh",
    "tokens":[
      "pepsith",
      "caffein",
      "keepin",
      "couldnt",
      "help",
      "sound",
      "refresh"
    ],
    "token_count":7,
    "processed_text":"pepsith caffein keepin couldnt help sound refresh"
  },
  {
    "label":4,
    "text":"get readi nci come tv ugh stupid tooth ach",
    "cleaned_text":"get readi nci come tv ugh stupid tooth ach",
    "normalized_text":"get readi nci come tv ugh stupid tooth ach",
    "tokens":[
      "get",
      "readi",
      "nci",
      "come",
      "tv",
      "ugh",
      "stupid",
      "tooth",
      "ach"
    ],
    "token_count":9,
    "processed_text":"get readi nci come tv ugh stupid tooth ach"
  },
  {
    "label":4,
    "text":"woo af rose place night go",
    "cleaned_text":"woo af rose place night go",
    "normalized_text":"woo af rose place night go",
    "tokens":[
      "woo",
      "af",
      "rose",
      "place",
      "night",
      "go"
    ],
    "token_count":6,
    "processed_text":"woo af rose place night go"
  },
  {
    "label":4,
    "text":"happi mother day love mama",
    "cleaned_text":"happi mother day love mama",
    "normalized_text":"happi mother day love mama",
    "tokens":[
      "happi",
      "mother",
      "day",
      "love",
      "mama"
    ],
    "token_count":5,
    "processed_text":"happi mother day love mama"
  },
  {
    "label":0,
    "text":"us vs itali confeder cup n im stuck work cant ever match weekend",
    "cleaned_text":"us vs itali confeder cup n im stuck work cant ever match weekend",
    "normalized_text":"us vs itali confeder cup n im stuck work cant ever match weekend",
    "tokens":[
      "us",
      "vs",
      "itali",
      "confed",
      "cup",
      "im",
      "stuck",
      "work",
      "cant",
      "ever",
      "match",
      "weekend"
    ],
    "token_count":12,
    "processed_text":"us vs itali confed cup im stuck work cant ever match weekend"
  },
  {
    "label":0,
    "text":"hungryyi lyk helln der noth eat heremani dnt hav strength prepar sumth boo",
    "cleaned_text":"hungryyi lyk helln der noth eat heremani dnt hav strength prepar sumth boo",
    "normalized_text":"hungryyi lyk helln der noth eat heremani dnt hav strength prepar sumth boo",
    "tokens":[
      "hungryyi",
      "lyk",
      "helln",
      "der",
      "noth",
      "eat",
      "heremani",
      "dnt",
      "hav",
      "strength",
      "prepar",
      "sumth",
      "boo"
    ],
    "token_count":13,
    "processed_text":"hungryyi lyk helln der noth eat heremani dnt hav strength prepar sumth boo"
  },
  {
    "label":0,
    "text":"take ur littl pretti nail updat default pic doin min n wont work",
    "cleaned_text":"take ur littl pretti nail updat default pic doin min n wont work",
    "normalized_text":"take ur littl pretti nail updat default pic doin min n wont work",
    "tokens":[
      "take",
      "ur",
      "littl",
      "pretti",
      "nail",
      "updat",
      "default",
      "pic",
      "doin",
      "min",
      "wont",
      "work"
    ],
    "token_count":12,
    "processed_text":"take ur littl pretti nail updat default pic doin min wont work"
  },
  {
    "label":4,
    "text":"got home prom fun",
    "cleaned_text":"got home prom fun",
    "normalized_text":"got home prom fun",
    "tokens":[
      "got",
      "home",
      "prom",
      "fun"
    ],
    "token_count":4,
    "processed_text":"got home prom fun"
  },
  {
    "label":0,
    "text":"ugh anyway probabl best thing phil e put year one best year music ever",
    "cleaned_text":"ugh anyway probabl best thing phil e put year one best year music ever",
    "normalized_text":"ugh anyway probabl best thing phil e put year one best year music ever",
    "tokens":[
      "ugh",
      "anyway",
      "probabl",
      "best",
      "thing",
      "phil",
      "put",
      "year",
      "one",
      "best",
      "year",
      "music",
      "ever"
    ],
    "token_count":13,
    "processed_text":"ugh anyway probabl best thing phil put year one best year music ever"
  },
  {
    "label":0,
    "text":"good morn allclean timethen gymgloomi sunday",
    "cleaned_text":"good morn allclean timethen gymgloomi sunday",
    "normalized_text":"good morn allclean timethen gymgloomi sunday",
    "tokens":[
      "good",
      "morn",
      "allclean",
      "timethen",
      "gymgloomi",
      "sunday"
    ],
    "token_count":6,
    "processed_text":"good morn allclean timethen gymgloomi sunday"
  },
  {
    "label":4,
    "text":"hi wonder ive seen somewher stare princess diari last time ive multital buddi rite",
    "cleaned_text":"hi wonder ive seen somewher stare princess diari last time ive multital buddi rite",
    "normalized_text":"hi wonder ive seen somewher stare princess diari last time ive multital buddi rite",
    "tokens":[
      "hi",
      "wonder",
      "ive",
      "seen",
      "somewh",
      "stare",
      "princess",
      "diari",
      "last",
      "time",
      "ive",
      "multit",
      "buddi",
      "rite"
    ],
    "token_count":14,
    "processed_text":"hi wonder ive seen somewh stare princess diari last time ive multit buddi rite"
  },
  {
    "label":4,
    "text":"fan found",
    "cleaned_text":"fan found",
    "normalized_text":"fan found",
    "tokens":[
      "fan",
      "found"
    ],
    "token_count":2,
    "processed_text":"fan found"
  },
  {
    "label":4,
    "text":"mayb danc long sunday danc kitchen dont wish ur fam fun mine lol",
    "cleaned_text":"mayb danc long sunday danc kitchen dont wish ur fam fun mine lol",
    "normalized_text":"mayb danc long sunday danc kitchen dont wish ur fam fun mine lol",
    "tokens":[
      "mayb",
      "danc",
      "long",
      "sunday",
      "danc",
      "kitchen",
      "dont",
      "wish",
      "ur",
      "fam",
      "fun",
      "mine",
      "lol"
    ],
    "token_count":13,
    "processed_text":"mayb danc long sunday danc kitchen dont wish ur fam fun mine lol"
  },
  {
    "label":4,
    "text":"ye curio one",
    "cleaned_text":"ye curio one",
    "normalized_text":"ye curio one",
    "tokens":[
      "ye",
      "curio",
      "one"
    ],
    "token_count":3,
    "processed_text":"ye curio one"
  },
  {
    "label":0,
    "text":"air franc plane rio pari miss crash mayb im nervou hope theyr ok",
    "cleaned_text":"air franc plane rio pari miss crash mayb im nervou hope theyr ok",
    "normalized_text":"air franc plane rio pari miss crash mayb im nervou hope theyr ok",
    "tokens":[
      "air",
      "franc",
      "plane",
      "rio",
      "pari",
      "miss",
      "crash",
      "mayb",
      "im",
      "nervou",
      "hope",
      "theyr",
      "ok"
    ],
    "token_count":13,
    "processed_text":"air franc plane rio pari miss crash mayb im nervou hope theyr ok"
  },
  {
    "label":0,
    "text":"develop posterior shin splint run week",
    "cleaned_text":"develop posterior shin splint run week",
    "normalized_text":"develop posterior shin splint run week",
    "tokens":[
      "develop",
      "posterior",
      "shin",
      "splint",
      "run",
      "week"
    ],
    "token_count":6,
    "processed_text":"develop posterior shin splint run week"
  },
  {
    "label":0,
    "text":"watch bgt final like week actual happen know win everyth suspens gone",
    "cleaned_text":"watch bgt final like week actual happen know win everyth suspens gone",
    "normalized_text":"watch bgt final like week actual happen know win everyth suspens gone",
    "tokens":[
      "watch",
      "bgt",
      "final",
      "like",
      "week",
      "actual",
      "happen",
      "know",
      "win",
      "everyth",
      "suspen",
      "gone"
    ],
    "token_count":12,
    "processed_text":"watch bgt final like week actual happen know win everyth suspen gone"
  },
  {
    "label":4,
    "text":"hick english degre haha",
    "cleaned_text":"hick english degre haha",
    "normalized_text":"hick english degre haha",
    "tokens":[
      "hick",
      "english",
      "degr",
      "haha"
    ],
    "token_count":4,
    "processed_text":"hick english degr haha"
  },
  {
    "label":4,
    "text":"thank ltfinger crossedgt",
    "cleaned_text":"thank ltfinger crossedgt",
    "normalized_text":"thank ltfinger crossedgt",
    "tokens":[
      "thank",
      "ltfinger",
      "crossedgt"
    ],
    "token_count":3,
    "processed_text":"thank ltfinger crossedgt"
  },
  {
    "label":4,
    "text":"love day gonna win squarespac iphon",
    "cleaned_text":"love day gonna win squarespac iphon",
    "normalized_text":"love day gonna win squarespac iphon",
    "tokens":[
      "love",
      "day",
      "gon",
      "na",
      "win",
      "squarespac",
      "iphon"
    ],
    "token_count":7,
    "processed_text":"love day gon na win squarespac iphon"
  },
  {
    "label":4,
    "text":"book trip brother wed june look forward set foot dutch soil even south",
    "cleaned_text":"book trip brother wed june look forward set foot dutch soil even south",
    "normalized_text":"book trip brother wed june look forward set foot dutch soil even south",
    "tokens":[
      "book",
      "trip",
      "brother",
      "wed",
      "june",
      "look",
      "forward",
      "set",
      "foot",
      "dutch",
      "soil",
      "even",
      "south"
    ],
    "token_count":13,
    "processed_text":"book trip brother wed june look forward set foot dutch soil even south"
  },
  {
    "label":4,
    "text":"lol heard one",
    "cleaned_text":"lol heard one",
    "normalized_text":"lol heard one",
    "tokens":[
      "lol",
      "heard",
      "one"
    ],
    "token_count":3,
    "processed_text":"lol heard one"
  },
  {
    "label":4,
    "text":"sure",
    "cleaned_text":"sure",
    "normalized_text":"sure",
    "tokens":[
      "sure"
    ],
    "token_count":1,
    "processed_text":"sure"
  },
  {
    "label":4,
    "text":"sad make account activ hehe lt",
    "cleaned_text":"sad make account activ hehe lt",
    "normalized_text":"sad make account activ hehe lt",
    "tokens":[
      "sad",
      "make",
      "account",
      "activ",
      "hehe",
      "lt"
    ],
    "token_count":6,
    "processed_text":"sad make account activ hehe lt"
  },
  {
    "label":4,
    "text":"woke",
    "cleaned_text":"woke",
    "normalized_text":"woke",
    "tokens":[
      "woke"
    ],
    "token_count":1,
    "processed_text":"woke"
  },
  {
    "label":4,
    "text":"mom total scri someth came real cooool kinder weird still cooool hahah",
    "cleaned_text":"mom total scri someth came real cooool kinder weird still cooool hahah",
    "normalized_text":"mom total scri someth came real cooool kinder weird still cooool hahah",
    "tokens":[
      "mom",
      "total",
      "scri",
      "someth",
      "came",
      "real",
      "cooool",
      "kinder",
      "weird",
      "still",
      "cooool",
      "hahah"
    ],
    "token_count":12,
    "processed_text":"mom total scri someth came real cooool kinder weird still cooool hahah"
  },
  {
    "label":0,
    "text":"know bum",
    "cleaned_text":"know bum",
    "normalized_text":"know bum",
    "tokens":[
      "know",
      "bum"
    ],
    "token_count":2,
    "processed_text":"know bum"
  },
  {
    "label":0,
    "text":"soo sad enjoy sun x",
    "cleaned_text":"soo sad enjoy sun x",
    "normalized_text":"soo sad enjoy sun x",
    "tokens":[
      "soo",
      "sad",
      "enjoy",
      "sun"
    ],
    "token_count":4,
    "processed_text":"soo sad enjoy sun"
  },
  {
    "label":0,
    "text":"fuck",
    "cleaned_text":"fuck",
    "normalized_text":"fuck",
    "tokens":[
      "fuck"
    ],
    "token_count":1,
    "processed_text":"fuck"
  },
  {
    "label":0,
    "text":"wear foundat first time ages gotten sun sinc think nw light",
    "cleaned_text":"wear foundat first time ages gotten sun sinc think nw light",
    "normalized_text":"wear foundat first time ages gotten sun sinc think nw light",
    "tokens":[
      "wear",
      "foundat",
      "first",
      "time",
      "age",
      "gotten",
      "sun",
      "sinc",
      "think",
      "nw",
      "light"
    ],
    "token_count":11,
    "processed_text":"wear foundat first time age gotten sun sinc think nw light"
  },
  {
    "label":4,
    "text":"b thursday friday everyon go",
    "cleaned_text":"b thursday friday everyon go",
    "normalized_text":"b thursday friday everyon go",
    "tokens":[
      "thursday",
      "friday",
      "everyon",
      "go"
    ],
    "token_count":4,
    "processed_text":"thursday friday everyon go"
  },
  {
    "label":4,
    "text":"well laugh even knew someth desper goofi eat bean",
    "cleaned_text":"well laugh even knew someth desper goofi eat bean",
    "normalized_text":"well laugh even knew someth desper goofi eat bean",
    "tokens":[
      "well",
      "laugh",
      "even",
      "knew",
      "someth",
      "desper",
      "goofi",
      "eat",
      "bean"
    ],
    "token_count":9,
    "processed_text":"well laugh even knew someth desper goofi eat bean"
  },
  {
    "label":0,
    "text":"ok im sorri ill shut",
    "cleaned_text":"ok im sorri ill shut",
    "normalized_text":"ok im sorri ill shut",
    "tokens":[
      "ok",
      "im",
      "sorri",
      "ill",
      "shut"
    ],
    "token_count":5,
    "processed_text":"ok im sorri ill shut"
  },
  {
    "label":0,
    "text":"want friend give peer pressur yo",
    "cleaned_text":"want friend give peer pressur yo",
    "normalized_text":"want friend give peer pressur yo",
    "tokens":[
      "want",
      "friend",
      "give",
      "peer",
      "pressur",
      "yo"
    ],
    "token_count":6,
    "processed_text":"want friend give peer pressur yo"
  },
  {
    "label":0,
    "text":"michel ryan would awesom companion",
    "cleaned_text":"michel ryan would awesom companion",
    "normalized_text":"michel ryan would awesom companion",
    "tokens":[
      "michel",
      "ryan",
      "awesom",
      "companion"
    ],
    "token_count":4,
    "processed_text":"michel ryan awesom companion"
  },
  {
    "label":0,
    "text":"wrk eat blue berri muffin amp strawberri milk n bagel n cream chees n sum chip boy greedi goin view later",
    "cleaned_text":"wrk eat blue berri muffin amp strawberri milk n bagel n cream chees n sum chip boy greedi goin view later",
    "normalized_text":"wrk eat blue berri muffin amp strawberri milk n bagel n cream chees n sum chip boy greedi goin view later",
    "tokens":[
      "wrk",
      "eat",
      "blue",
      "berri",
      "muffin",
      "amp",
      "strawberri",
      "milk",
      "bagel",
      "cream",
      "chee",
      "sum",
      "chip",
      "boy",
      "greedi",
      "goin",
      "view",
      "later"
    ],
    "token_count":18,
    "processed_text":"wrk eat blue berri muffin amp strawberri milk bagel cream chee sum chip boy greedi goin view later"
  },
  {
    "label":0,
    "text":"heyeverybodyhow get fallow",
    "cleaned_text":"heyeverybodyhow get fallow",
    "normalized_text":"heyeverybodyhow get fallow",
    "tokens":[
      "heyeverybodyhow",
      "get",
      "fallow"
    ],
    "token_count":3,
    "processed_text":"heyeverybodyhow get fallow"
  },
  {
    "label":4,
    "text":"believ got fuck",
    "cleaned_text":"believ got fuck",
    "normalized_text":"believ got fuck",
    "tokens":[
      "believ",
      "got",
      "fuck"
    ],
    "token_count":3,
    "processed_text":"believ got fuck"
  },
  {
    "label":0,
    "text":"would love get sleep right hate earli flight",
    "cleaned_text":"would love get sleep right hate earli flight",
    "normalized_text":"would love get sleep right hate earli flight",
    "tokens":[
      "love",
      "get",
      "sleep",
      "right",
      "hate",
      "earli",
      "flight"
    ],
    "token_count":7,
    "processed_text":"love get sleep right hate earli flight"
  },
  {
    "label":0,
    "text":"wont let add u either",
    "cleaned_text":"wont let add u either",
    "normalized_text":"wont let add u either",
    "tokens":[
      "wont",
      "let",
      "add",
      "either"
    ],
    "token_count":4,
    "processed_text":"wont let add either"
  },
  {
    "label":0,
    "text":"omg need sleep shouldnt nap yesterday didnt getv sleep til gonn wish didnt work today hr shift uurrgghhh",
    "cleaned_text":"omg need sleep shouldnt nap yesterday didnt getv sleep til gonn wish didnt work today hr shift uurrgghhh",
    "normalized_text":"omg need sleep shouldnt nap yesterday didnt getv sleep til gonn wish didnt work today hr shift uurrgghhh",
    "tokens":[
      "omg",
      "need",
      "sleep",
      "shouldnt",
      "nap",
      "yesterday",
      "didnt",
      "getv",
      "sleep",
      "til",
      "gonn",
      "wish",
      "didnt",
      "work",
      "today",
      "hr",
      "shift",
      "uurrgghhh"
    ],
    "token_count":18,
    "processed_text":"omg need sleep shouldnt nap yesterday didnt getv sleep til gonn wish didnt work today hr shift uurrgghhh"
  },
  {
    "label":4,
    "text":"graduat sunday eat lunch soontob famili",
    "cleaned_text":"graduat sunday eat lunch soontob famili",
    "normalized_text":"graduat sunday eat lunch soontob famili",
    "tokens":[
      "graduat",
      "sunday",
      "eat",
      "lunch",
      "soontob",
      "famili"
    ],
    "token_count":6,
    "processed_text":"graduat sunday eat lunch soontob famili"
  },
  {
    "label":0,
    "text":"im happi keep year streak never run ga aliv cant say lock key car",
    "cleaned_text":"im happi keep year streak never run ga aliv cant say lock key car",
    "normalized_text":"im happi keep year streak never run ga aliv cant say lock key car",
    "tokens":[
      "im",
      "happi",
      "keep",
      "year",
      "streak",
      "never",
      "run",
      "ga",
      "aliv",
      "cant",
      "say",
      "lock",
      "key",
      "car"
    ],
    "token_count":14,
    "processed_text":"im happi keep year streak never run ga aliv cant say lock key car"
  },
  {
    "label":0,
    "text":"caught sun today lobster boy",
    "cleaned_text":"caught sun today lobster boy",
    "normalized_text":"caught sun today lobster boy",
    "tokens":[
      "caught",
      "sun",
      "today",
      "lobster",
      "boy"
    ],
    "token_count":5,
    "processed_text":"caught sun today lobster boy"
  },
  {
    "label":0,
    "text":"bloodi knee injuri doctor tomorrow",
    "cleaned_text":"bloodi knee injuri doctor tomorrow",
    "normalized_text":"bloodi knee injuri doctor tomorrow",
    "tokens":[
      "bloodi",
      "knee",
      "injuri",
      "doctor",
      "tomorrow"
    ],
    "token_count":5,
    "processed_text":"bloodi knee injuri doctor tomorrow"
  },
  {
    "label":0,
    "text":"blockmat swine flu class block suspend saturday",
    "cleaned_text":"blockmat swine flu class block suspend saturday",
    "normalized_text":"blockmat swine flu class block suspend saturday",
    "tokens":[
      "blockmat",
      "swine",
      "flu",
      "class",
      "block",
      "suspend",
      "saturday"
    ],
    "token_count":7,
    "processed_text":"blockmat swine flu class block suspend saturday"
  },
  {
    "label":0,
    "text":"monday aint even start yet im dead agre work full week",
    "cleaned_text":"monday aint even start yet im dead agre work full week",
    "normalized_text":"monday aint even start yet im dead agre work full week",
    "tokens":[
      "monday",
      "aint",
      "even",
      "start",
      "yet",
      "im",
      "dead",
      "agr",
      "work",
      "full",
      "week"
    ],
    "token_count":11,
    "processed_text":"monday aint even start yet im dead agr work full week"
  },
  {
    "label":0,
    "text":"fell asleep earli last nightthat game tough one felt like crap work day hope feel betterhav great day",
    "cleaned_text":"fell asleep earli last nightthat game tough one felt like crap work day hope feel betterhav great day",
    "normalized_text":"fell asleep earli last nightthat game tough one felt like crap work day hope feel betterhav great day",
    "tokens":[
      "fell",
      "asleep",
      "earli",
      "last",
      "nightthat",
      "game",
      "tough",
      "one",
      "felt",
      "like",
      "crap",
      "work",
      "day",
      "hope",
      "feel",
      "betterhav",
      "great",
      "day"
    ],
    "token_count":18,
    "processed_text":"fell asleep earli last nightthat game tough one felt like crap work day hope feel betterhav great day"
  },
  {
    "label":4,
    "text":"done ikea got idea take megan airport ignor hotti pool think come home pass",
    "cleaned_text":"done ikea got idea take megan airport ignor hotti pool think come home pass",
    "normalized_text":"done ikea got idea take megan airport ignor hotti pool think come home pass",
    "tokens":[
      "done",
      "ikea",
      "got",
      "idea",
      "take",
      "megan",
      "airport",
      "ignor",
      "hotti",
      "pool",
      "think",
      "come",
      "home",
      "pass"
    ],
    "token_count":14,
    "processed_text":"done ikea got idea take megan airport ignor hotti pool think come home pass"
  },
  {
    "label":0,
    "text":"oo miss contest honorari winner name twin lol",
    "cleaned_text":"oo miss contest honorari winner name twin lol",
    "normalized_text":"oo miss contest honorari winner name twin lol",
    "tokens":[
      "oo",
      "miss",
      "contest",
      "honorari",
      "winner",
      "name",
      "twin",
      "lol"
    ],
    "token_count":8,
    "processed_text":"oo miss contest honorari winner name twin lol"
  },
  {
    "label":4,
    "text":"yeah take pictur papi",
    "cleaned_text":"yeah take pictur papi",
    "normalized_text":"yeah take pictur papi",
    "tokens":[
      "yeah",
      "take",
      "pictur",
      "papi"
    ],
    "token_count":4,
    "processed_text":"yeah take pictur papi"
  },
  {
    "label":0,
    "text":"doesnt upper divis stand",
    "cleaned_text":"doesnt upper divis stand",
    "normalized_text":"doesnt upper divis stand",
    "tokens":[
      "doesnt",
      "upper",
      "divi",
      "stand"
    ],
    "token_count":4,
    "processed_text":"doesnt upper divi stand"
  },
  {
    "label":0,
    "text":"sunni work tomorrow tv tonight",
    "cleaned_text":"sunni work tomorrow tv tonight",
    "normalized_text":"sunni work tomorrow tv tonight",
    "tokens":[
      "sunni",
      "work",
      "tomorrow",
      "tv",
      "tonight"
    ],
    "token_count":5,
    "processed_text":"sunni work tomorrow tv tonight"
  },
  {
    "label":4,
    "text":"wanna see want go",
    "cleaned_text":"wanna see want go",
    "normalized_text":"wanna see want go",
    "tokens":[
      "wan",
      "na",
      "see",
      "want",
      "go"
    ],
    "token_count":5,
    "processed_text":"wan na see want go"
  },
  {
    "label":4,
    "text":"cold wana curl bed happi birthday best friend",
    "cleaned_text":"cold wana curl bed happi birthday best friend",
    "normalized_text":"cold wana curl bed happi birthday best friend",
    "tokens":[
      "cold",
      "wana",
      "curl",
      "bed",
      "happi",
      "birthday",
      "best",
      "friend"
    ],
    "token_count":8,
    "processed_text":"cold wana curl bed happi birthday best friend"
  },
  {
    "label":0,
    "text":"smell eat crisp across offic must heighten sens smell adamantium claw",
    "cleaned_text":"smell eat crisp across offic must heighten sens smell adamantium claw",
    "normalized_text":"smell eat crisp across offic must heighten sens smell adamantium claw",
    "tokens":[
      "smell",
      "eat",
      "crisp",
      "across",
      "offic",
      "heighten",
      "sen",
      "smell",
      "adamantium",
      "claw"
    ],
    "token_count":10,
    "processed_text":"smell eat crisp across offic heighten sen smell adamantium claw"
  },
  {
    "label":0,
    "text":"friday noth",
    "cleaned_text":"friday noth",
    "normalized_text":"friday noth",
    "tokens":[
      "friday",
      "noth"
    ],
    "token_count":2,
    "processed_text":"friday noth"
  },
  {
    "label":4,
    "text":"play one mission cool",
    "cleaned_text":"play one mission cool",
    "normalized_text":"play one mission cool",
    "tokens":[
      "play",
      "one",
      "mission",
      "cool"
    ],
    "token_count":4,
    "processed_text":"play one mission cool"
  },
  {
    "label":4,
    "text":"ok well im total ur goin twitterita ili",
    "cleaned_text":"ok well im total ur goin twitterita ili",
    "normalized_text":"ok well im total ur goin twitterita ili",
    "tokens":[
      "ok",
      "well",
      "im",
      "total",
      "ur",
      "goin",
      "twitterita",
      "ili"
    ],
    "token_count":8,
    "processed_text":"ok well im total ur goin twitterita ili"
  },
  {
    "label":0,
    "text":"say everyth bigger us caramel sunda mcdo size sa pina",
    "cleaned_text":"say everyth bigger us caramel sunda mcdo size sa pina",
    "normalized_text":"say everyth bigger us caramel sunda mcdo size sa pina",
    "tokens":[
      "say",
      "everyth",
      "bigger",
      "us",
      "caramel",
      "sunda",
      "mcdo",
      "size",
      "sa",
      "pina"
    ],
    "token_count":10,
    "processed_text":"say everyth bigger us caramel sunda mcdo size sa pina"
  },
  {
    "label":4,
    "text":"lian say lian wait fun pleasur",
    "cleaned_text":"lian say lian wait fun pleasur",
    "normalized_text":"lian say lian wait fun pleasur",
    "tokens":[
      "lian",
      "say",
      "lian",
      "wait",
      "fun",
      "pleasur"
    ],
    "token_count":6,
    "processed_text":"lian say lian wait fun pleasur"
  },
  {
    "label":4,
    "text":"ha great thank tweet much today work though",
    "cleaned_text":"ha great thank tweet much today work though",
    "normalized_text":"ha great thank tweet much today work though",
    "tokens":[
      "ha",
      "great",
      "thank",
      "tweet",
      "much",
      "today",
      "work",
      "though"
    ],
    "token_count":8,
    "processed_text":"ha great thank tweet much today work though"
  },
  {
    "label":4,
    "text":"never late make dream come true susan boyl proof",
    "cleaned_text":"never late make dream come true susan boyl proof",
    "normalized_text":"never late make dream come true susan boyl proof",
    "tokens":[
      "never",
      "late",
      "make",
      "dream",
      "come",
      "true",
      "susan",
      "boyl",
      "proof"
    ],
    "token_count":9,
    "processed_text":"never late make dream come true susan boyl proof"
  },
  {
    "label":4,
    "text":"time photo essay extrem marathon one notsocrazi",
    "cleaned_text":"time photo essay extrem marathon one notsocrazi",
    "normalized_text":"time photo essay extrem marathon one notsocrazi",
    "tokens":[
      "time",
      "photo",
      "essay",
      "extrem",
      "marathon",
      "one",
      "notsocrazi"
    ],
    "token_count":7,
    "processed_text":"time photo essay extrem marathon one notsocrazi"
  },
  {
    "label":4,
    "text":"love haha",
    "cleaned_text":"love haha",
    "normalized_text":"love haha",
    "tokens":[
      "love",
      "haha"
    ],
    "token_count":2,
    "processed_text":"love haha"
  },
  {
    "label":0,
    "text":"good mood amp dont ask either",
    "cleaned_text":"good mood amp dont ask either",
    "normalized_text":"good mood amp dont ask either",
    "tokens":[
      "good",
      "mood",
      "amp",
      "dont",
      "ask",
      "either"
    ],
    "token_count":6,
    "processed_text":"good mood amp dont ask either"
  },
  {
    "label":0,
    "text":"super super super super bum ucla dammmmmnnnnnnnnnn",
    "cleaned_text":"super super super super bum ucla dammmmmnnnnnnnnnn",
    "normalized_text":"super super super super bum ucla dammmmmnnnnnnnnnn",
    "tokens":[
      "super",
      "super",
      "super",
      "super",
      "bum",
      "ucla"
    ],
    "token_count":6,
    "processed_text":"super super super super bum ucla"
  },
  {
    "label":4,
    "text":"home wango tango fun realli tire wrap mom present pass lt",
    "cleaned_text":"home wango tango fun realli tire wrap mom present pass lt",
    "normalized_text":"home wango tango fun realli tire wrap mom present pass lt",
    "tokens":[
      "home",
      "wango",
      "tango",
      "fun",
      "realli",
      "tire",
      "wrap",
      "mom",
      "present",
      "pass",
      "lt"
    ],
    "token_count":11,
    "processed_text":"home wango tango fun realli tire wrap mom present pass lt"
  },
  {
    "label":0,
    "text":"firefox cannot sometim use delet button gmail",
    "cleaned_text":"firefox cannot sometim use delet button gmail",
    "normalized_text":"firefox cannot sometim use delet button gmail",
    "tokens":[
      "firefox",
      "sometim",
      "use",
      "delet",
      "button",
      "gmail"
    ],
    "token_count":6,
    "processed_text":"firefox sometim use delet button gmail"
  },
  {
    "label":0,
    "text":"extrem sad read book end food quit shock condit food suppli",
    "cleaned_text":"extrem sad read book end food quit shock condit food suppli",
    "normalized_text":"extrem sad read book end food quit shock condit food suppli",
    "tokens":[
      "extrem",
      "sad",
      "read",
      "book",
      "end",
      "food",
      "quit",
      "shock",
      "condit",
      "food",
      "suppli"
    ],
    "token_count":11,
    "processed_text":"extrem sad read book end food quit shock condit food suppli"
  },
  {
    "label":0,
    "text":"might possibl hav strep throat ju great",
    "cleaned_text":"might possibl hav strep throat ju great",
    "normalized_text":"might possibl hav strep throat ju great",
    "tokens":[
      "possibl",
      "hav",
      "strep",
      "throat",
      "ju",
      "great"
    ],
    "token_count":6,
    "processed_text":"possibl hav strep throat ju great"
  },
  {
    "label":0,
    "text":"mayb she upset andor need shower u hatin homeless meg",
    "cleaned_text":"mayb she upset andor need shower u hatin homeless meg",
    "normalized_text":"mayb she upset andor need shower u hatin homeless meg",
    "tokens":[
      "mayb",
      "upset",
      "andor",
      "need",
      "shower",
      "hatin",
      "homeless",
      "meg"
    ],
    "token_count":8,
    "processed_text":"mayb upset andor need shower hatin homeless meg"
  },
  {
    "label":4,
    "text":"say road trip w bae funfunfun haha glad made",
    "cleaned_text":"say road trip w bae funfunfun haha glad made",
    "normalized_text":"say road trip w bae funfunfun haha glad made",
    "tokens":[
      "say",
      "road",
      "trip",
      "bae",
      "funfunfun",
      "haha",
      "glad",
      "made"
    ],
    "token_count":8,
    "processed_text":"say road trip bae funfunfun haha glad made"
  },
  {
    "label":4,
    "text":"ff ty",
    "cleaned_text":"ff ty",
    "normalized_text":"ff ty",
    "tokens":[
      "ff",
      "ty"
    ],
    "token_count":2,
    "processed_text":"ff ty"
  },
  {
    "label":0,
    "text":"agre though rain rule picnic idea sigh",
    "cleaned_text":"agre though rain rule picnic idea sigh",
    "normalized_text":"agre though rain rule picnic idea sigh",
    "tokens":[
      "agr",
      "though",
      "rain",
      "rule",
      "picnic",
      "idea",
      "sigh"
    ],
    "token_count":7,
    "processed_text":"agr though rain rule picnic idea sigh"
  },
  {
    "label":4,
    "text":"think figur sir your statu corni",
    "cleaned_text":"think figur sir your statu corni",
    "normalized_text":"think figur sir your statu corni",
    "tokens":[
      "think",
      "figur",
      "sir",
      "statu",
      "corni"
    ],
    "token_count":5,
    "processed_text":"think figur sir statu corni"
  },
  {
    "label":4,
    "text":"housework rock ipod tweet ya later",
    "cleaned_text":"housework rock ipod tweet ya later",
    "normalized_text":"housework rock ipod tweet ya later",
    "tokens":[
      "housework",
      "rock",
      "ipod",
      "tweet",
      "ya",
      "later"
    ],
    "token_count":6,
    "processed_text":"housework rock ipod tweet ya later"
  },
  {
    "label":4,
    "text":"hiya gorgeou grace us presenc today mani us miss xxx",
    "cleaned_text":"hiya gorgeou grace us presenc today mani us miss xxx",
    "normalized_text":"hiya gorgeou grace us presenc today mani us miss xxx",
    "tokens":[
      "hiya",
      "gorgeou",
      "grace",
      "us",
      "presenc",
      "today",
      "mani",
      "us",
      "miss",
      "xxx"
    ],
    "token_count":10,
    "processed_text":"hiya gorgeou grace us presenc today mani us miss xxx"
  },
  {
    "label":0,
    "text":"well im sorri lt your amaz boom take",
    "cleaned_text":"well im sorri lt your amaz boom take",
    "normalized_text":"well im sorri lt your amaz boom take",
    "tokens":[
      "well",
      "im",
      "sorri",
      "lt",
      "amaz",
      "boom",
      "take"
    ],
    "token_count":7,
    "processed_text":"well im sorri lt amaz boom take"
  },
  {
    "label":4,
    "text":"least may nd first id realli like go see empir",
    "cleaned_text":"least may nd first id realli like go see empir",
    "normalized_text":"least may nd first id realli like go see empir",
    "tokens":[
      "least",
      "may",
      "nd",
      "first",
      "id",
      "realli",
      "like",
      "go",
      "see",
      "empir"
    ],
    "token_count":10,
    "processed_text":"least may nd first id realli like go see empir"
  },
  {
    "label":0,
    "text":"back home great time miss nedo",
    "cleaned_text":"back home great time miss nedo",
    "normalized_text":"back home great time miss nedo",
    "tokens":[
      "back",
      "home",
      "great",
      "time",
      "miss",
      "nedo"
    ],
    "token_count":6,
    "processed_text":"back home great time miss nedo"
  },
  {
    "label":0,
    "text":"actual miss listen toni horn morn morn wont listen day",
    "cleaned_text":"actual miss listen toni horn morn morn wont listen day",
    "normalized_text":"actual miss listen toni horn morn morn wont listen day",
    "tokens":[
      "actual",
      "miss",
      "listen",
      "toni",
      "horn",
      "morn",
      "morn",
      "wont",
      "listen",
      "day"
    ],
    "token_count":10,
    "processed_text":"actual miss listen toni horn morn morn wont listen day"
  },
  {
    "label":0,
    "text":"id love pair im fayettevil arkansa",
    "cleaned_text":"id love pair im fayettevil arkansa",
    "normalized_text":"id love pair im fayettevil arkansa",
    "tokens":[
      "id",
      "love",
      "pair",
      "im",
      "fayettevil",
      "arkansa"
    ],
    "token_count":6,
    "processed_text":"id love pair im fayettevil arkansa"
  },
  {
    "label":0,
    "text":"ha ha funni get get",
    "cleaned_text":"ha ha funni get get",
    "normalized_text":"ha ha funni get get",
    "tokens":[
      "ha",
      "ha",
      "funni",
      "get",
      "get"
    ],
    "token_count":5,
    "processed_text":"ha ha funni get get"
  },
  {
    "label":4,
    "text":"hey want see could send music still also time would follow pleas",
    "cleaned_text":"hey want see could send music still also time would follow pleas",
    "normalized_text":"hey want see could send music still also time would follow pleas",
    "tokens":[
      "hey",
      "want",
      "see",
      "send",
      "music",
      "still",
      "also",
      "time",
      "follow",
      "plea"
    ],
    "token_count":10,
    "processed_text":"hey want see send music still also time follow plea"
  },
  {
    "label":0,
    "text":"ahh know holiday end fast",
    "cleaned_text":"ahh know holiday end fast",
    "normalized_text":"ahh know holiday end fast",
    "tokens":[
      "ahh",
      "know",
      "holiday",
      "end",
      "fast"
    ],
    "token_count":5,
    "processed_text":"ahh know holiday end fast"
  },
  {
    "label":0,
    "text":"lmaooo omg gospel arob im mad lool",
    "cleaned_text":"lmaooo omg gospel arob im mad lool",
    "normalized_text":"lmaooo omg gospel arob im mad lool",
    "tokens":[
      "lmaooo",
      "omg",
      "gospel",
      "arob",
      "im",
      "mad",
      "lool"
    ],
    "token_count":7,
    "processed_text":"lmaooo omg gospel arob im mad lool"
  },
  {
    "label":0,
    "text":"impress weather x",
    "cleaned_text":"impress weather x",
    "normalized_text":"impress weather x",
    "tokens":[
      "impress",
      "weather"
    ],
    "token_count":2,
    "processed_text":"impress weather"
  },
  {
    "label":4,
    "text":"thank mr blakk kick ass today",
    "cleaned_text":"thank mr blakk kick ass today",
    "normalized_text":"thank mr blakk kick ass today",
    "tokens":[
      "thank",
      "mr",
      "blakk",
      "kick",
      "ass",
      "today"
    ],
    "token_count":6,
    "processed_text":"thank mr blakk kick ass today"
  },
  {
    "label":4,
    "text":"sorri couldnt collect",
    "cleaned_text":"sorri couldnt collect",
    "normalized_text":"sorri couldnt collect",
    "tokens":[
      "sorri",
      "couldnt",
      "collect"
    ],
    "token_count":3,
    "processed_text":"sorri couldnt collect"
  },
  {
    "label":4,
    "text":"question one get twitterberri",
    "cleaned_text":"question one get twitterberri",
    "normalized_text":"question one get twitterberri",
    "tokens":[
      "question",
      "one",
      "get",
      "twitterberri"
    ],
    "token_count":4,
    "processed_text":"question one get twitterberri"
  },
  {
    "label":0,
    "text":"im still feel objectifi",
    "cleaned_text":"im still feel objectifi",
    "normalized_text":"im still feel objectifi",
    "tokens":[
      "im",
      "still",
      "feel",
      "objectifi"
    ],
    "token_count":4,
    "processed_text":"im still feel objectifi"
  },
  {
    "label":4,
    "text":"awww enjoy time",
    "cleaned_text":"awww enjoy time",
    "normalized_text":"awww enjoy time",
    "tokens":[
      "awww",
      "enjoy",
      "time"
    ],
    "token_count":3,
    "processed_text":"awww enjoy time"
  },
  {
    "label":4,
    "text":"great hear get exposur anoth site",
    "cleaned_text":"great hear get exposur anoth site",
    "normalized_text":"great hear get exposur anoth site",
    "tokens":[
      "great",
      "hear",
      "get",
      "exposur",
      "anoth",
      "site"
    ],
    "token_count":6,
    "processed_text":"great hear get exposur anoth site"
  },
  {
    "label":0,
    "text":"hubbi gone busi next day jp nap",
    "cleaned_text":"hubbi gone busi next day jp nap",
    "normalized_text":"hubbi gone busi next day jp nap",
    "tokens":[
      "hubbi",
      "gone",
      "busi",
      "next",
      "day",
      "jp",
      "nap"
    ],
    "token_count":7,
    "processed_text":"hubbi gone busi next day jp nap"
  },
  {
    "label":0,
    "text":"im sad saturday napervil visit ticket sold found",
    "cleaned_text":"im sad saturday napervil visit ticket sold found",
    "normalized_text":"im sad saturday napervil visit ticket sold found",
    "tokens":[
      "im",
      "sad",
      "saturday",
      "napervil",
      "visit",
      "ticket",
      "sold",
      "found"
    ],
    "token_count":8,
    "processed_text":"im sad saturday napervil visit ticket sold found"
  },
  {
    "label":4,
    "text":"ye go tweet made laugh watch final episod",
    "cleaned_text":"ye go tweet made laugh watch final episod",
    "normalized_text":"ye go tweet made laugh watch final episod",
    "tokens":[
      "ye",
      "go",
      "tweet",
      "made",
      "laugh",
      "watch",
      "final",
      "episod"
    ],
    "token_count":8,
    "processed_text":"ye go tweet made laugh watch final episod"
  },
  {
    "label":4,
    "text":"well easi",
    "cleaned_text":"well easi",
    "normalized_text":"well easi",
    "tokens":[
      "well",
      "easi"
    ],
    "token_count":2,
    "processed_text":"well easi"
  },
  {
    "label":4,
    "text":"use greasemonkey script would great ppl still prefer use friendfe pull togeth",
    "cleaned_text":"use greasemonkey script would great ppl still prefer use friendfe pull togeth",
    "normalized_text":"use greasemonkey script would great ppl still prefer use friendfe pull togeth",
    "tokens":[
      "use",
      "greasemonkey",
      "script",
      "great",
      "ppl",
      "still",
      "prefer",
      "use",
      "friendf",
      "pull",
      "togeth"
    ],
    "token_count":11,
    "processed_text":"use greasemonkey script great ppl still prefer use friendf pull togeth"
  },
  {
    "label":0,
    "text":"im sick one keep compani",
    "cleaned_text":"im sick one keep compani",
    "normalized_text":"im sick one keep compani",
    "tokens":[
      "im",
      "sick",
      "one",
      "keep",
      "compani"
    ],
    "token_count":5,
    "processed_text":"im sick one keep compani"
  },
  {
    "label":4,
    "text":"bout head home soon",
    "cleaned_text":"bout head home soon",
    "normalized_text":"bout head home soon",
    "tokens":[
      "bout",
      "head",
      "home",
      "soon"
    ],
    "token_count":4,
    "processed_text":"bout head home soon"
  },
  {
    "label":4,
    "text":"today graduat day let hope someon doesnt mess",
    "cleaned_text":"today graduat day let hope someon doesnt mess",
    "normalized_text":"today graduat day let hope someon doesnt mess",
    "tokens":[
      "today",
      "graduat",
      "day",
      "let",
      "hope",
      "someon",
      "doesnt",
      "mess"
    ],
    "token_count":8,
    "processed_text":"today graduat day let hope someon doesnt mess"
  },
  {
    "label":4,
    "text":"minor psych",
    "cleaned_text":"minor psych",
    "normalized_text":"minor psych",
    "tokens":[
      "minor",
      "psych"
    ],
    "token_count":2,
    "processed_text":"minor psych"
  },
  {
    "label":4,
    "text":"who juliana",
    "cleaned_text":"who juliana",
    "normalized_text":"who juliana",
    "tokens":[
      "juliana"
    ],
    "token_count":1,
    "processed_text":"juliana"
  },
  {
    "label":0,
    "text":"hell debug",
    "cleaned_text":"hell debug",
    "normalized_text":"hell debug",
    "tokens":[
      "hell",
      "debug"
    ],
    "token_count":2,
    "processed_text":"hell debug"
  },
  {
    "label":0,
    "text":"nadal french open rafa guess jump rog fed band wagon",
    "cleaned_text":"nadal french open rafa guess jump rog fed band wagon",
    "normalized_text":"nadal french open rafa guess jump rog fed band wagon",
    "tokens":[
      "nadal",
      "french",
      "open",
      "rafa",
      "guess",
      "jump",
      "rog",
      "fed",
      "band",
      "wagon"
    ],
    "token_count":10,
    "processed_text":"nadal french open rafa guess jump rog fed band wagon"
  },
  {
    "label":4,
    "text":"go get coursework done turkish f ipod co sister friend kati amp peter pfft",
    "cleaned_text":"go get coursework done turkish f ipod co sister friend kati amp peter pfft",
    "normalized_text":"go get coursework done turkish f ipod co sister friend kati amp peter pfft",
    "tokens":[
      "go",
      "get",
      "coursework",
      "done",
      "turkish",
      "ipod",
      "co",
      "sister",
      "friend",
      "kati",
      "amp",
      "peter",
      "pfft"
    ],
    "token_count":13,
    "processed_text":"go get coursework done turkish ipod co sister friend kati amp peter pfft"
  },
  {
    "label":0,
    "text":"call trunk thing begin episod ask also poor juliet",
    "cleaned_text":"call trunk thing begin episod ask also poor juliet",
    "normalized_text":"call trunk thing begin episod ask also poor juliet",
    "tokens":[
      "call",
      "trunk",
      "thing",
      "begin",
      "episod",
      "ask",
      "also",
      "poor",
      "juliet"
    ],
    "token_count":9,
    "processed_text":"call trunk thing begin episod ask also poor juliet"
  },
  {
    "label":0,
    "text":"dog determin foot new seat follow around sit wherev may",
    "cleaned_text":"dog determin foot new seat follow around sit wherev may",
    "normalized_text":"dog determin foot new seat follow around sit wherev may",
    "tokens":[
      "dog",
      "determin",
      "foot",
      "new",
      "seat",
      "follow",
      "around",
      "sit",
      "wherev",
      "may"
    ],
    "token_count":10,
    "processed_text":"dog determin foot new seat follow around sit wherev may"
  },
  {
    "label":4,
    "text":"hey david cant wait see manila",
    "cleaned_text":"hey david cant wait see manila",
    "normalized_text":"hey david cant wait see manila",
    "tokens":[
      "hey",
      "david",
      "cant",
      "wait",
      "see",
      "manila"
    ],
    "token_count":6,
    "processed_text":"hey david cant wait see manila"
  },
  {
    "label":4,
    "text":"plugin activ happen",
    "cleaned_text":"plugin activ happen",
    "normalized_text":"plugin activ happen",
    "tokens":[
      "plugin",
      "activ",
      "happen"
    ],
    "token_count":3,
    "processed_text":"plugin activ happen"
  },
  {
    "label":0,
    "text":"yike sorri hear imagin",
    "cleaned_text":"yike sorri hear imagin",
    "normalized_text":"yike sorri hear imagin",
    "tokens":[
      "yike",
      "sorri",
      "hear",
      "imagin"
    ],
    "token_count":4,
    "processed_text":"yike sorri hear imagin"
  },
  {
    "label":4,
    "text":"final snug bedhad awesom weekend bbi friend pride qt real reunion statu",
    "cleaned_text":"final snug bedhad awesom weekend bbi friend pride qt real reunion statu",
    "normalized_text":"final snug bedhad awesom weekend bbi friend pride qt real reunion statu",
    "tokens":[
      "final",
      "snug",
      "bedhad",
      "awesom",
      "weekend",
      "bbi",
      "friend",
      "pride",
      "qt",
      "real",
      "reunion",
      "statu"
    ],
    "token_count":12,
    "processed_text":"final snug bedhad awesom weekend bbi friend pride qt real reunion statu"
  },
  {
    "label":0,
    "text":"serious everyon someon ughhhhhh fml",
    "cleaned_text":"serious everyon someon ughhhhhh fml",
    "normalized_text":"serious everyon someon ughhhhhh fml",
    "tokens":[
      "seriou",
      "everyon",
      "someon",
      "ughhhhhh",
      "fml"
    ],
    "token_count":5,
    "processed_text":"seriou everyon someon ughhhhhh fml"
  },
  {
    "label":4,
    "text":"hackin mobil",
    "cleaned_text":"hackin mobil",
    "normalized_text":"hackin mobil",
    "tokens":[
      "hackin",
      "mobil"
    ],
    "token_count":2,
    "processed_text":"hackin mobil"
  },
  {
    "label":0,
    "text":"earli morninggg",
    "cleaned_text":"earli morninggg",
    "normalized_text":"earli morninggg",
    "tokens":[
      "earli",
      "morninggg"
    ],
    "token_count":2,
    "processed_text":"earli morninggg"
  },
  {
    "label":4,
    "text":"hahanow u live himin pineappleund sea",
    "cleaned_text":"hahanow u live himin pineappleund sea",
    "normalized_text":"hahanow u live himin pineappleund sea",
    "tokens":[
      "hahanow",
      "live",
      "himin",
      "pineappleund",
      "sea"
    ],
    "token_count":5,
    "processed_text":"hahanow live himin pineappleund sea"
  },
  {
    "label":4,
    "text":"final finish bunni draw background ju pain ah well work tomorrow",
    "cleaned_text":"final finish bunni draw background ju pain ah well work tomorrow",
    "normalized_text":"final finish bunni draw background ju pain ah well work tomorrow",
    "tokens":[
      "final",
      "finish",
      "bunni",
      "draw",
      "background",
      "ju",
      "pain",
      "ah",
      "well",
      "work",
      "tomorrow"
    ],
    "token_count":11,
    "processed_text":"final finish bunni draw background ju pain ah well work tomorrow"
  },
  {
    "label":0,
    "text":"feel decidedli lacklustr pretti shabbi effort morn",
    "cleaned_text":"feel decidedli lacklustr pretti shabbi effort morn",
    "normalized_text":"feel decidedli lacklustr pretti shabbi effort morn",
    "tokens":[
      "feel",
      "decidedli",
      "lacklustr",
      "pretti",
      "shabbi",
      "effort",
      "morn"
    ],
    "token_count":7,
    "processed_text":"feel decidedli lacklustr pretti shabbi effort morn"
  },
  {
    "label":4,
    "text":"thank great race",
    "cleaned_text":"thank great race",
    "normalized_text":"thank great race",
    "tokens":[
      "thank",
      "great",
      "race"
    ],
    "token_count":3,
    "processed_text":"thank great race"
  },
  {
    "label":0,
    "text":"sort updat iphon half app dont work due lack jailbreak hack patch",
    "cleaned_text":"sort updat iphon half app dont work due lack jailbreak hack patch",
    "normalized_text":"sort updat iphon half app dont work due lack jailbreak hack patch",
    "tokens":[
      "sort",
      "updat",
      "iphon",
      "half",
      "app",
      "dont",
      "work",
      "due",
      "lack",
      "jailbreak",
      "hack",
      "patch"
    ],
    "token_count":12,
    "processed_text":"sort updat iphon half app dont work due lack jailbreak hack patch"
  },
  {
    "label":0,
    "text":"see",
    "cleaned_text":"see",
    "normalized_text":"see",
    "tokens":[
      "see"
    ],
    "token_count":1,
    "processed_text":"see"
  },
  {
    "label":4,
    "text":"agre jordan",
    "cleaned_text":"agre jordan",
    "normalized_text":"agre jordan",
    "tokens":[
      "agr",
      "jordan"
    ],
    "token_count":2,
    "processed_text":"agr jordan"
  },
  {
    "label":0,
    "text":"wish teleport manila",
    "cleaned_text":"wish teleport manila",
    "normalized_text":"wish teleport manila",
    "tokens":[
      "wish",
      "teleport",
      "manila"
    ],
    "token_count":3,
    "processed_text":"wish teleport manila"
  },
  {
    "label":0,
    "text":"one night go back school tomorrow vacat almost",
    "cleaned_text":"one night go back school tomorrow vacat almost",
    "normalized_text":"one night go back school tomorrow vacat almost",
    "tokens":[
      "one",
      "night",
      "go",
      "back",
      "school",
      "tomorrow",
      "vacat",
      "almost"
    ],
    "token_count":8,
    "processed_text":"one night go back school tomorrow vacat almost"
  },
  {
    "label":0,
    "text":"hahajht awk sedih syhuk",
    "cleaned_text":"hahajht awk sedih syhuk",
    "normalized_text":"hahajht awk sedih syhuk",
    "tokens":[
      "hahajht",
      "awk",
      "sedih",
      "syhuk"
    ],
    "token_count":4,
    "processed_text":"hahajht awk sedih syhuk"
  },
  {
    "label":0,
    "text":"wish didnt stay awak watch fight quid gone bye bye minut fight",
    "cleaned_text":"wish didnt stay awak watch fight quid gone bye bye minut fight",
    "normalized_text":"wish didnt stay awak watch fight quid gone bye bye minut fight",
    "tokens":[
      "wish",
      "didnt",
      "stay",
      "awak",
      "watch",
      "fight",
      "quid",
      "gone",
      "bye",
      "bye",
      "minut",
      "fight"
    ],
    "token_count":12,
    "processed_text":"wish didnt stay awak watch fight quid gone bye bye minut fight"
  },
  {
    "label":0,
    "text":"work time",
    "cleaned_text":"work time",
    "normalized_text":"work time",
    "tokens":[
      "work",
      "time"
    ],
    "token_count":2,
    "processed_text":"work time"
  },
  {
    "label":4,
    "text":"mmm yummi pic ador ashley tiara love dress",
    "cleaned_text":"mmm yummi pic ador ashley tiara love dress",
    "normalized_text":"mmm yummi pic ador ashley tiara love dress",
    "tokens":[
      "mmm",
      "yummi",
      "pic",
      "ador",
      "ashley",
      "tiara",
      "love",
      "dress"
    ],
    "token_count":8,
    "processed_text":"mmm yummi pic ador ashley tiara love dress"
  },
  {
    "label":0,
    "text":"homebi myselffor rest day",
    "cleaned_text":"homebi myselffor rest day",
    "normalized_text":"homebi myselffor rest day",
    "tokens":[
      "homebi",
      "myselffor",
      "rest",
      "day"
    ],
    "token_count":4,
    "processed_text":"homebi myselffor rest day"
  },
  {
    "label":0,
    "text":"horrifi treat peopl stay power even theyr readi kill",
    "cleaned_text":"horrifi treat peopl stay power even theyr readi kill",
    "normalized_text":"horrifi treat peopl stay power even theyr readi kill",
    "tokens":[
      "horrifi",
      "treat",
      "peopl",
      "stay",
      "power",
      "even",
      "theyr",
      "readi",
      "kill"
    ],
    "token_count":9,
    "processed_text":"horrifi treat peopl stay power even theyr readi kill"
  },
  {
    "label":4,
    "text":"hope",
    "cleaned_text":"hope",
    "normalized_text":"hope",
    "tokens":[
      "hope"
    ],
    "token_count":1,
    "processed_text":"hope"
  },
  {
    "label":4,
    "text":"id rather love cant help right hard",
    "cleaned_text":"id rather love cant help right hard",
    "normalized_text":"id rather love cant help right hard",
    "tokens":[
      "id",
      "rather",
      "love",
      "cant",
      "help",
      "right",
      "hard"
    ],
    "token_count":7,
    "processed_text":"id rather love cant help right hard"
  },
  {
    "label":0,
    "text":"im go brush tomorrow give foam bath clumpi trim",
    "cleaned_text":"im go brush tomorrow give foam bath clumpi trim",
    "normalized_text":"im go brush tomorrow give foam bath clumpi trim",
    "tokens":[
      "im",
      "go",
      "brush",
      "tomorrow",
      "give",
      "foam",
      "bath",
      "clumpi",
      "trim"
    ],
    "token_count":9,
    "processed_text":"im go brush tomorrow give foam bath clumpi trim"
  },
  {
    "label":4,
    "text":"test new trillian astra far good",
    "cleaned_text":"test new trillian astra far good",
    "normalized_text":"test new trillian astra far good",
    "tokens":[
      "test",
      "new",
      "trillian",
      "astra",
      "far",
      "good"
    ],
    "token_count":6,
    "processed_text":"test new trillian astra far good"
  },
  {
    "label":4,
    "text":"today great",
    "cleaned_text":"today great",
    "normalized_text":"today great",
    "tokens":[
      "today",
      "great"
    ],
    "token_count":2,
    "processed_text":"today great"
  },
  {
    "label":0,
    "text":"car fail mot today noooo probabl best though may scrap",
    "cleaned_text":"car fail mot today noooo probabl best though may scrap",
    "normalized_text":"car fail mot today noooo probabl best though may scrap",
    "tokens":[
      "car",
      "fail",
      "mot",
      "today",
      "noooo",
      "probabl",
      "best",
      "though",
      "may",
      "scrap"
    ],
    "token_count":10,
    "processed_text":"car fail mot today noooo probabl best though may scrap"
  },
  {
    "label":4,
    "text":"quotguy im drunk lemm tweet thisquot run peopl mind lol wow goodnight lt good clean fun day today",
    "cleaned_text":"quotguy im drunk lemm tweet thisquot run peopl mind lol wow goodnight lt good clean fun day today",
    "normalized_text":"quotguy im drunk lemm tweet thisquot run peopl mind lol wow goodnight lt good clean fun day today",
    "tokens":[
      "quotguy",
      "im",
      "drunk",
      "lemm",
      "tweet",
      "thisquot",
      "run",
      "peopl",
      "mind",
      "lol",
      "wow",
      "goodnight",
      "lt",
      "good",
      "clean",
      "fun",
      "day",
      "today"
    ],
    "token_count":18,
    "processed_text":"quotguy im drunk lemm tweet thisquot run peopl mind lol wow goodnight lt good clean fun day today"
  },
  {
    "label":0,
    "text":"realli studi ive notion math yesterday realli worth",
    "cleaned_text":"realli studi ive notion math yesterday realli worth",
    "normalized_text":"realli studi ive notion math yesterday realli worth",
    "tokens":[
      "realli",
      "studi",
      "ive",
      "notion",
      "math",
      "yesterday",
      "realli",
      "worth"
    ],
    "token_count":8,
    "processed_text":"realli studi ive notion math yesterday realli worth"
  },
  {
    "label":0,
    "text":"u wanna meet",
    "cleaned_text":"u wanna meet",
    "normalized_text":"u wanna meet",
    "tokens":[
      "wan",
      "na",
      "meet"
    ],
    "token_count":3,
    "processed_text":"wan na meet"
  },
  {
    "label":0,
    "text":"hey tom im realli bad day right person awesom make feel better serious",
    "cleaned_text":"hey tom im realli bad day right person awesom make feel better serious",
    "normalized_text":"hey tom im realli bad day right person awesom make feel better serious",
    "tokens":[
      "hey",
      "tom",
      "im",
      "realli",
      "bad",
      "day",
      "right",
      "person",
      "awesom",
      "make",
      "feel",
      "better",
      "seriou"
    ],
    "token_count":13,
    "processed_text":"hey tom im realli bad day right person awesom make feel better seriou"
  },
  {
    "label":4,
    "text":"hey im twitter",
    "cleaned_text":"hey im twitter",
    "normalized_text":"hey im twitter",
    "tokens":[
      "hey",
      "im",
      "twitter"
    ],
    "token_count":3,
    "processed_text":"hey im twitter"
  },
  {
    "label":0,
    "text":"boreddd woke feel asleep eleven someth odd had scari dream",
    "cleaned_text":"boreddd woke feel asleep eleven someth odd had scari dream",
    "normalized_text":"boreddd woke feel asleep eleven someth odd had scari dream",
    "tokens":[
      "boreddd",
      "woke",
      "feel",
      "asleep",
      "eleven",
      "someth",
      "odd",
      "scari",
      "dream"
    ],
    "token_count":9,
    "processed_text":"boreddd woke feel asleep eleven someth odd scari dream"
  },
  {
    "label":0,
    "text":"keep tri contact bratday havent luck know method contact",
    "cleaned_text":"keep tri contact bratday havent luck know method contact",
    "normalized_text":"keep tri contact bratday havent luck know method contact",
    "tokens":[
      "keep",
      "tri",
      "contact",
      "bratday",
      "havent",
      "luck",
      "know",
      "method",
      "contact"
    ],
    "token_count":9,
    "processed_text":"keep tri contact bratday havent luck know method contact"
  },
  {
    "label":0,
    "text":"car troubl",
    "cleaned_text":"car troubl",
    "normalized_text":"car troubl",
    "tokens":[
      "car",
      "troubl"
    ],
    "token_count":2,
    "processed_text":"car troubl"
  },
  {
    "label":0,
    "text":"uhhh hahah finish yet",
    "cleaned_text":"uhhh hahah finish yet",
    "normalized_text":"uhhh hahah finish yet",
    "tokens":[
      "uhhh",
      "hahah",
      "finish",
      "yet"
    ],
    "token_count":4,
    "processed_text":"uhhh hahah finish yet"
  },
  {
    "label":4,
    "text":"thunderstorm make great cuddl time",
    "cleaned_text":"thunderstorm make great cuddl time",
    "normalized_text":"thunderstorm make great cuddl time",
    "tokens":[
      "thunderstorm",
      "make",
      "great",
      "cuddl",
      "time"
    ],
    "token_count":5,
    "processed_text":"thunderstorm make great cuddl time"
  },
  {
    "label":0,
    "text":"seven month rest peac jacob",
    "cleaned_text":"seven month rest peac jacob",
    "normalized_text":"seven month rest peac jacob",
    "tokens":[
      "seven",
      "month",
      "rest",
      "peac",
      "jacob"
    ],
    "token_count":5,
    "processed_text":"seven month rest peac jacob"
  },
  {
    "label":0,
    "text":"ahhh okay ill main stage day prolli miss",
    "cleaned_text":"ahhh okay ill main stage day prolli miss",
    "normalized_text":"ahhh okay ill main stage day prolli miss",
    "tokens":[
      "ahhh",
      "okay",
      "ill",
      "main",
      "stage",
      "day",
      "prolli",
      "miss"
    ],
    "token_count":8,
    "processed_text":"ahhh okay ill main stage day prolli miss"
  },
  {
    "label":0,
    "text":"wonder ppl long distanc relationship need physic contact",
    "cleaned_text":"wonder ppl long distanc relationship need physic contact",
    "normalized_text":"wonder ppl long distanc relationship need physic contact",
    "tokens":[
      "wonder",
      "ppl",
      "long",
      "distanc",
      "relationship",
      "need",
      "physic",
      "contact"
    ],
    "token_count":8,
    "processed_text":"wonder ppl long distanc relationship need physic contact"
  },
  {
    "label":0,
    "text":"go bed gonna wait longer phone ring great ill sleep",
    "cleaned_text":"go bed gonna wait longer phone ring great ill sleep",
    "normalized_text":"go bed gonna wait longer phone ring great ill sleep",
    "tokens":[
      "go",
      "bed",
      "gon",
      "na",
      "wait",
      "longer",
      "phone",
      "ring",
      "great",
      "ill",
      "sleep"
    ],
    "token_count":11,
    "processed_text":"go bed gon na wait longer phone ring great ill sleep"
  },
  {
    "label":0,
    "text":"actual translat sporti though love handl need hit road",
    "cleaned_text":"actual translat sporti though love handl need hit road",
    "normalized_text":"actual translat sporti though love handl need hit road",
    "tokens":[
      "actual",
      "translat",
      "sporti",
      "though",
      "love",
      "handl",
      "need",
      "hit",
      "road"
    ],
    "token_count":9,
    "processed_text":"actual translat sporti though love handl need hit road"
  },
  {
    "label":4,
    "text":"good luck amp fun",
    "cleaned_text":"good luck amp fun",
    "normalized_text":"good luck amp fun",
    "tokens":[
      "good",
      "luck",
      "amp",
      "fun"
    ],
    "token_count":4,
    "processed_text":"good luck amp fun"
  },
  {
    "label":4,
    "text":"rob look ador pictur decemb camilla",
    "cleaned_text":"rob look ador pictur decemb camilla",
    "normalized_text":"rob look ador pictur decemb camilla",
    "tokens":[
      "rob",
      "look",
      "ador",
      "pictur",
      "decemb",
      "camilla"
    ],
    "token_count":6,
    "processed_text":"rob look ador pictur decemb camilla"
  },
  {
    "label":0,
    "text":"cant believ weekend almost",
    "cleaned_text":"cant believ weekend almost",
    "normalized_text":"cant believ weekend almost",
    "tokens":[
      "cant",
      "believ",
      "weekend",
      "almost"
    ],
    "token_count":4,
    "processed_text":"cant believ weekend almost"
  },
  {
    "label":4,
    "text":"celebr still much enough time tweet though theyr way red carpet love wordtwitterrif",
    "cleaned_text":"celebr still much enough time tweet though theyr way red carpet love wordtwitterrif",
    "normalized_text":"celebr still much enough time tweet though theyr way red carpet love wordtwitterrif",
    "tokens":[
      "celebr",
      "still",
      "much",
      "enough",
      "time",
      "tweet",
      "though",
      "theyr",
      "way",
      "red",
      "carpet",
      "love",
      "wordtwitterrif"
    ],
    "token_count":13,
    "processed_text":"celebr still much enough time tweet though theyr way red carpet love wordtwitterrif"
  },
  {
    "label":4,
    "text":"cours got",
    "cleaned_text":"cours got",
    "normalized_text":"cours got",
    "tokens":[
      "cour",
      "got"
    ],
    "token_count":2,
    "processed_text":"cour got"
  },
  {
    "label":4,
    "text":"whooooa rock goin nut tryin figur",
    "cleaned_text":"whooooa rock goin nut tryin figur",
    "normalized_text":"whooooa rock goin nut tryin figur",
    "tokens":[
      "whooooa",
      "rock",
      "goin",
      "nut",
      "tryin",
      "figur"
    ],
    "token_count":6,
    "processed_text":"whooooa rock goin nut tryin figur"
  },
  {
    "label":4,
    "text":"got back show bethel hangout justin brittani vinni matti watch gossip girl pop popcorn",
    "cleaned_text":"got back show bethel hangout justin brittani vinni matti watch gossip girl pop popcorn",
    "normalized_text":"got back show bethel hangout justin brittani vinni matti watch gossip girl pop popcorn",
    "tokens":[
      "got",
      "back",
      "show",
      "bethel",
      "hangout",
      "justin",
      "brittani",
      "vinni",
      "matti",
      "watch",
      "gossip",
      "girl",
      "pop",
      "popcorn"
    ],
    "token_count":14,
    "processed_text":"got back show bethel hangout justin brittani vinni matti watch gossip girl pop popcorn"
  },
  {
    "label":4,
    "text":"hello twitter",
    "cleaned_text":"hello twitter",
    "normalized_text":"hello twitter",
    "tokens":[
      "hello",
      "twitter"
    ],
    "token_count":2,
    "processed_text":"hello twitter"
  },
  {
    "label":0,
    "text":"hot weather plu rain equal humid equal bad hair day",
    "cleaned_text":"hot weather plu rain equal humid equal bad hair day",
    "normalized_text":"hot weather plu rain equal humid equal bad hair day",
    "tokens":[
      "hot",
      "weather",
      "plu",
      "rain",
      "equal",
      "humid",
      "equal",
      "bad",
      "hair",
      "day"
    ],
    "token_count":10,
    "processed_text":"hot weather plu rain equal humid equal bad hair day"
  },
  {
    "label":4,
    "text":"smetim u ju need lse connectin wit wrldleavin phne rest alll day relax",
    "cleaned_text":"smetim u ju need lse connectin wit wrldleavin phne rest alll day relax",
    "normalized_text":"smetim u ju need lse connectin wit wrldleavin phne rest alll day relax",
    "tokens":[
      "smetim",
      "ju",
      "need",
      "lse",
      "connectin",
      "wit",
      "wrldleavin",
      "phne",
      "rest",
      "alll",
      "day",
      "relax"
    ],
    "token_count":12,
    "processed_text":"smetim ju need lse connectin wit wrldleavin phne rest alll day relax"
  },
  {
    "label":0,
    "text":"hate final detail get site readi go live done bar small detail",
    "cleaned_text":"hate final detail get site readi go live done bar small detail",
    "normalized_text":"hate final detail get site readi go live done bar small detail",
    "tokens":[
      "hate",
      "final",
      "detail",
      "get",
      "site",
      "readi",
      "go",
      "live",
      "done",
      "bar",
      "small",
      "detail"
    ],
    "token_count":12,
    "processed_text":"hate final detail get site readi go live done bar small detail"
  },
  {
    "label":0,
    "text":"hmwk",
    "cleaned_text":"hmwk",
    "normalized_text":"hmwk",
    "tokens":[
      "hmwk"
    ],
    "token_count":1,
    "processed_text":"hmwk"
  },
  {
    "label":4,
    "text":"hahaha kram tonight he finciti",
    "cleaned_text":"hahaha kram tonight he finciti",
    "normalized_text":"hahaha kram tonight he finciti",
    "tokens":[
      "hahaha",
      "kram",
      "tonight",
      "finciti"
    ],
    "token_count":4,
    "processed_text":"hahaha kram tonight finciti"
  },
  {
    "label":0,
    "text":"get ice skate onhel offici frozen enjoy creme brule ryan bar amp grill",
    "cleaned_text":"get ice skate onhel offici frozen enjoy creme brule ryan bar amp grill",
    "normalized_text":"get ice skate onhel offici frozen enjoy creme brule ryan bar amp grill",
    "tokens":[
      "get",
      "ice",
      "skate",
      "onhel",
      "offici",
      "frozen",
      "enjoy",
      "creme",
      "brule",
      "ryan",
      "bar",
      "amp",
      "grill"
    ],
    "token_count":13,
    "processed_text":"get ice skate onhel offici frozen enjoy creme brule ryan bar amp grill"
  },
  {
    "label":4,
    "text":"sup suga long time see back la yet",
    "cleaned_text":"sup suga long time see back la yet",
    "normalized_text":"sup suga long time see back la yet",
    "tokens":[
      "sup",
      "suga",
      "long",
      "time",
      "see",
      "back",
      "la",
      "yet"
    ],
    "token_count":8,
    "processed_text":"sup suga long time see back la yet"
  },
  {
    "label":0,
    "text":"hahaha remind case still",
    "cleaned_text":"hahaha remind case still",
    "normalized_text":"hahaha remind case still",
    "tokens":[
      "hahaha",
      "remind",
      "case",
      "still"
    ],
    "token_count":4,
    "processed_text":"hahaha remind case still"
  },
  {
    "label":0,
    "text":"work way casualti discographi pull weed later",
    "cleaned_text":"work way casualti discographi pull weed later",
    "normalized_text":"work way casualti discographi pull weed later",
    "tokens":[
      "work",
      "way",
      "casualti",
      "discographi",
      "pull",
      "weed",
      "later"
    ],
    "token_count":7,
    "processed_text":"work way casualti discographi pull weed later"
  },
  {
    "label":0,
    "text":"omw home call night pick trash today saw mom took babi park beauti saturdayniw go hm empti bed",
    "cleaned_text":"omw home call night pick trash today saw mom took babi park beauti saturdayniw go hm empti bed",
    "normalized_text":"omw home call night pick trash today saw mom took babi park beauti saturdayniw go hm empti bed",
    "tokens":[
      "omw",
      "home",
      "call",
      "night",
      "pick",
      "trash",
      "today",
      "saw",
      "mom",
      "took",
      "babi",
      "park",
      "beauti",
      "saturdayniw",
      "go",
      "hm",
      "empti",
      "bed"
    ],
    "token_count":18,
    "processed_text":"omw home call night pick trash today saw mom took babi park beauti saturdayniw go hm empti bed"
  },
  {
    "label":0,
    "text":"julia fever happi father day dad",
    "cleaned_text":"julia fever happi father day dad",
    "normalized_text":"julia fever happi father day dad",
    "tokens":[
      "julia",
      "fever",
      "happi",
      "father",
      "day",
      "dad"
    ],
    "token_count":6,
    "processed_text":"julia fever happi father day dad"
  },
  {
    "label":4,
    "text":"thank much today",
    "cleaned_text":"thank much today",
    "normalized_text":"thank much today",
    "tokens":[
      "thank",
      "much",
      "today"
    ],
    "token_count":3,
    "processed_text":"thank much today"
  },
  {
    "label":0,
    "text":"fish either realli realli weight tumor goodnight",
    "cleaned_text":"fish either realli realli weight tumor goodnight",
    "normalized_text":"fish either realli realli weight tumor goodnight",
    "tokens":[
      "fish",
      "either",
      "realli",
      "realli",
      "weight",
      "tumor",
      "goodnight"
    ],
    "token_count":7,
    "processed_text":"fish either realli realli weight tumor goodnight"
  },
  {
    "label":0,
    "text":"aw im sorri wast mine your alon",
    "cleaned_text":"aw im sorri wast mine your alon",
    "normalized_text":"aw im sorri wast mine your alon",
    "tokens":[
      "aw",
      "im",
      "sorri",
      "wast",
      "mine",
      "alon"
    ],
    "token_count":6,
    "processed_text":"aw im sorri wast mine alon"
  },
  {
    "label":0,
    "text":"cant seee tweeeett",
    "cleaned_text":"cant seee tweeeett",
    "normalized_text":"cant seee tweeeett",
    "tokens":[
      "cant",
      "seee",
      "tweeeett"
    ],
    "token_count":3,
    "processed_text":"cant seee tweeeett"
  },
  {
    "label":4,
    "text":"oooh heard cover ice box niiiic done",
    "cleaned_text":"oooh heard cover ice box niiiic done",
    "normalized_text":"oooh heard cover ice box niiiic done",
    "tokens":[
      "oooh",
      "heard",
      "cover",
      "ice",
      "box",
      "niiiic",
      "done"
    ],
    "token_count":7,
    "processed_text":"oooh heard cover ice box niiiic done"
  },
  {
    "label":0,
    "text":"tire th form",
    "cleaned_text":"tire th form",
    "normalized_text":"tire th form",
    "tokens":[
      "tire",
      "th",
      "form"
    ],
    "token_count":3,
    "processed_text":"tire th form"
  },
  {
    "label":0,
    "text":"accident said quotbrbquot fridg",
    "cleaned_text":"accident said quotbrbquot fridg",
    "normalized_text":"accident said quotbrbquot fridg",
    "tokens":[
      "accid",
      "said",
      "quotbrbquot",
      "fridg"
    ],
    "token_count":4,
    "processed_text":"accid said quotbrbquot fridg"
  },
  {
    "label":0,
    "text":"lion omin especi suppos domin scrum",
    "cleaned_text":"lion omin especi suppos domin scrum",
    "normalized_text":"lion omin especi suppos domin scrum",
    "tokens":[
      "lion",
      "omin",
      "especi",
      "suppo",
      "domin",
      "scrum"
    ],
    "token_count":6,
    "processed_text":"lion omin especi suppo domin scrum"
  },
  {
    "label":0,
    "text":"girl hair salon ask quotshal trim eyebrowsquot old feel",
    "cleaned_text":"girl hair salon ask quotshal trim eyebrowsquot old feel",
    "normalized_text":"girl hair salon ask quotshal trim eyebrowsquot old feel",
    "tokens":[
      "girl",
      "hair",
      "salon",
      "ask",
      "quotshal",
      "trim",
      "eyebrowsquot",
      "old",
      "feel"
    ],
    "token_count":9,
    "processed_text":"girl hair salon ask quotshal trim eyebrowsquot old feel"
  },
  {
    "label":0,
    "text":"feel sick brush well good tho",
    "cleaned_text":"feel sick brush well good tho",
    "normalized_text":"feel sick brush well good tho",
    "tokens":[
      "feel",
      "sick",
      "brush",
      "well",
      "good",
      "tho"
    ],
    "token_count":6,
    "processed_text":"feel sick brush well good tho"
  },
  {
    "label":0,
    "text":"caus everybodi changin dont feel",
    "cleaned_text":"caus everybodi changin dont feel",
    "normalized_text":"caus everybodi changin dont feel",
    "tokens":[
      "cau",
      "everybodi",
      "changin",
      "dont",
      "feel"
    ],
    "token_count":5,
    "processed_text":"cau everybodi changin dont feel"
  },
  {
    "label":0,
    "text":"u alreadi homework miss homework",
    "cleaned_text":"u alreadi homework miss homework",
    "normalized_text":"u alreadi homework miss homework",
    "tokens":[
      "alreadi",
      "homework",
      "miss",
      "homework"
    ],
    "token_count":4,
    "processed_text":"alreadi homework miss homework"
  },
  {
    "label":0,
    "text":"sorri hear lol",
    "cleaned_text":"sorri hear lol",
    "normalized_text":"sorri hear lol",
    "tokens":[
      "sorri",
      "hear",
      "lol"
    ],
    "token_count":3,
    "processed_text":"sorri hear lol"
  },
  {
    "label":4,
    "text":"kewl also found traffic much less morn",
    "cleaned_text":"kewl also found traffic much less morn",
    "normalized_text":"kewl also found traffic much less morn",
    "tokens":[
      "kewl",
      "also",
      "found",
      "traffic",
      "much",
      "less",
      "morn"
    ],
    "token_count":7,
    "processed_text":"kewl also found traffic much less morn"
  },
  {
    "label":0,
    "text":"ye take half bug spray kill one roachthat thing fn huge st one weve seen month thoughnot bad",
    "cleaned_text":"ye take half bug spray kill one roachthat thing fn huge st one weve seen month thoughnot bad",
    "normalized_text":"ye take half bug spray kill one roachthat thing fn huge st one weve seen month thoughnot bad",
    "tokens":[
      "ye",
      "take",
      "half",
      "bug",
      "spray",
      "kill",
      "one",
      "roachthat",
      "thing",
      "fn",
      "huge",
      "st",
      "one",
      "weve",
      "seen",
      "month",
      "thoughnot",
      "bad"
    ],
    "token_count":18,
    "processed_text":"ye take half bug spray kill one roachthat thing fn huge st one weve seen month thoughnot bad"
  },
  {
    "label":0,
    "text":"oh sad farrah fawcett heard tv got long live love charli angel idol",
    "cleaned_text":"oh sad farrah fawcett heard tv got long live love charli angel idol",
    "normalized_text":"oh sad farrah fawcett heard tv got long live love charli angel idol",
    "tokens":[
      "oh",
      "sad",
      "farrah",
      "fawcett",
      "heard",
      "tv",
      "got",
      "long",
      "live",
      "love",
      "charli",
      "angel",
      "idol"
    ],
    "token_count":13,
    "processed_text":"oh sad farrah fawcett heard tv got long live love charli angel idol"
  },
  {
    "label":0,
    "text":"alright gotta make quick list approv food gotta pick way home tonight eryth calori amp bread",
    "cleaned_text":"alright gotta make quick list approv food gotta pick way home tonight eryth calori amp bread",
    "normalized_text":"alright gotta make quick list approv food gotta pick way home tonight eryth calori amp bread",
    "tokens":[
      "alright",
      "got",
      "ta",
      "make",
      "quick",
      "list",
      "approv",
      "food",
      "got",
      "ta",
      "pick",
      "way",
      "home",
      "tonight",
      "eryth",
      "calori",
      "amp",
      "bread"
    ],
    "token_count":18,
    "processed_text":"alright got ta make quick list approv food got ta pick way home tonight eryth calori amp bread"
  },
  {
    "label":4,
    "text":"ditto mayb squib look power lol xx",
    "cleaned_text":"ditto mayb squib look power lol xx",
    "normalized_text":"ditto mayb squib look power lol xx",
    "tokens":[
      "ditto",
      "mayb",
      "squib",
      "look",
      "power",
      "lol",
      "xx"
    ],
    "token_count":7,
    "processed_text":"ditto mayb squib look power lol xx"
  },
  {
    "label":0,
    "text":"wish better",
    "cleaned_text":"wish better",
    "normalized_text":"wish better",
    "tokens":[
      "wish",
      "better"
    ],
    "token_count":2,
    "processed_text":"wish better"
  },
  {
    "label":4,
    "text":"introduc typekit real web design take giant step forward",
    "cleaned_text":"introduc typekit real web design take giant step forward",
    "normalized_text":"introduc typekit real web design take giant step forward",
    "tokens":[
      "introduc",
      "typekit",
      "real",
      "web",
      "design",
      "take",
      "giant",
      "step",
      "forward"
    ],
    "token_count":9,
    "processed_text":"introduc typekit real web design take giant step forward"
  },
  {
    "label":0,
    "text":"ahaha sunset ballroom feet hurt",
    "cleaned_text":"ahaha sunset ballroom feet hurt",
    "normalized_text":"ahaha sunset ballroom feet hurt",
    "tokens":[
      "ahaha",
      "sunset",
      "ballroom",
      "feet",
      "hurt"
    ],
    "token_count":5,
    "processed_text":"ahaha sunset ballroom feet hurt"
  },
  {
    "label":4,
    "text":"omfg todaaaaaaaaaaaaaaaaay see soon tom",
    "cleaned_text":"omfg todaaaaaaaaaaaaaaaaay see soon tom",
    "normalized_text":"omfg todaaaaaaaaaaaaaaaaay see soon tom",
    "tokens":[
      "omfg",
      "see",
      "soon",
      "tom"
    ],
    "token_count":4,
    "processed_text":"omfg see soon tom"
  },
  {
    "label":4,
    "text":"go go shop eat cheesecak factori girlfriend",
    "cleaned_text":"go go shop eat cheesecak factori girlfriend",
    "normalized_text":"go go shop eat cheesecak factori girlfriend",
    "tokens":[
      "go",
      "go",
      "shop",
      "eat",
      "cheesecak",
      "factori",
      "girlfriend"
    ],
    "token_count":7,
    "processed_text":"go go shop eat cheesecak factori girlfriend"
  },
  {
    "label":4,
    "text":"watch uninvit heather asya",
    "cleaned_text":"watch uninvit heather asya",
    "normalized_text":"watch uninvit heather asya",
    "tokens":[
      "watch",
      "uninvit",
      "heather",
      "asya"
    ],
    "token_count":4,
    "processed_text":"watch uninvit heather asya"
  },
  {
    "label":0,
    "text":"cbf dish",
    "cleaned_text":"cbf dish",
    "normalized_text":"cbf dish",
    "tokens":[
      "cbf",
      "dish"
    ],
    "token_count":2,
    "processed_text":"cbf dish"
  },
  {
    "label":0,
    "text":"start read twilight didnt know much id miss love read bellaamp edward theyr cute",
    "cleaned_text":"start read twilight didnt know much id miss love read bellaamp edward theyr cute",
    "normalized_text":"start read twilight didnt know much id miss love read bellaamp edward theyr cute",
    "tokens":[
      "start",
      "read",
      "twilight",
      "didnt",
      "know",
      "much",
      "id",
      "miss",
      "love",
      "read",
      "bellaamp",
      "edward",
      "theyr",
      "cute"
    ],
    "token_count":14,
    "processed_text":"start read twilight didnt know much id miss love read bellaamp edward theyr cute"
  },
  {
    "label":4,
    "text":"scorpio best period ppl know cross scorpio",
    "cleaned_text":"scorpio best period ppl know cross scorpio",
    "normalized_text":"scorpio best period ppl know cross scorpio",
    "tokens":[
      "scorpio",
      "best",
      "period",
      "ppl",
      "know",
      "cross",
      "scorpio"
    ],
    "token_count":7,
    "processed_text":"scorpio best period ppl know cross scorpio"
  },
  {
    "label":4,
    "text":"ill walk love anim",
    "cleaned_text":"ill walk love anim",
    "normalized_text":"ill walk love anim",
    "tokens":[
      "ill",
      "walk",
      "love",
      "anim"
    ],
    "token_count":4,
    "processed_text":"ill walk love anim"
  },
  {
    "label":0,
    "text":"made want go put dont eat pictur cooki monster netflix queue dont",
    "cleaned_text":"made want go put dont eat pictur cooki monster netflix queue dont",
    "normalized_text":"made want go put dont eat pictur cooki monster netflix queue dont",
    "tokens":[
      "made",
      "want",
      "go",
      "put",
      "dont",
      "eat",
      "pictur",
      "cooki",
      "monster",
      "netflix",
      "queue",
      "dont"
    ],
    "token_count":12,
    "processed_text":"made want go put dont eat pictur cooki monster netflix queue dont"
  },
  {
    "label":0,
    "text":"anoth day mindnumb dull nobodi money consequ cant anyth im miss glc keighley",
    "cleaned_text":"anoth day mindnumb dull nobodi money consequ cant anyth im miss glc keighley",
    "normalized_text":"anoth day mindnumb dull nobodi money consequ cant anyth im miss glc keighley",
    "tokens":[
      "anoth",
      "day",
      "mindnumb",
      "dull",
      "nobodi",
      "money",
      "consequ",
      "cant",
      "anyth",
      "im",
      "miss",
      "glc",
      "keighley"
    ],
    "token_count":13,
    "processed_text":"anoth day mindnumb dull nobodi money consequ cant anyth im miss glc keighley"
  },
  {
    "label":4,
    "text":"brii",
    "cleaned_text":"brii",
    "normalized_text":"brii",
    "tokens":[
      "brii"
    ],
    "token_count":1,
    "processed_text":"brii"
  },
  {
    "label":0,
    "text":"first day summer im alreadi bore",
    "cleaned_text":"first day summer im alreadi bore",
    "normalized_text":"first day summer im alreadi bore",
    "tokens":[
      "first",
      "day",
      "summer",
      "im",
      "alreadi",
      "bore"
    ],
    "token_count":6,
    "processed_text":"first day summer im alreadi bore"
  },
  {
    "label":4,
    "text":"gnight tweet star",
    "cleaned_text":"gnight tweet star",
    "normalized_text":"gnight tweet star",
    "tokens":[
      "gnight",
      "tweet",
      "star"
    ],
    "token_count":3,
    "processed_text":"gnight tweet star"
  },
  {
    "label":0,
    "text":"alreadi gezz that new record",
    "cleaned_text":"alreadi gezz that new record",
    "normalized_text":"alreadi gezz that new record",
    "tokens":[
      "alreadi",
      "gezz",
      "new",
      "record"
    ],
    "token_count":4,
    "processed_text":"alreadi gezz new record"
  },
  {
    "label":0,
    "text":"ahh nice thank good info also reason im unabl time chang profil pic lock",
    "cleaned_text":"ahh nice thank good info also reason im unabl time chang profil pic lock",
    "normalized_text":"ahh nice thank good info also reason im unabl time chang profil pic lock",
    "tokens":[
      "ahh",
      "nice",
      "thank",
      "good",
      "info",
      "also",
      "reason",
      "im",
      "unabl",
      "time",
      "chang",
      "profil",
      "pic",
      "lock"
    ],
    "token_count":14,
    "processed_text":"ahh nice thank good info also reason im unabl time chang profil pic lock"
  },
  {
    "label":0,
    "text":"wish dollar show england",
    "cleaned_text":"wish dollar show england",
    "normalized_text":"wish dollar show england",
    "tokens":[
      "wish",
      "dollar",
      "show",
      "england"
    ],
    "token_count":4,
    "processed_text":"wish dollar show england"
  },
  {
    "label":0,
    "text":"that realli sucki least day get better right",
    "cleaned_text":"that realli sucki least day get better right",
    "normalized_text":"that realli sucki least day get better right",
    "tokens":[
      "realli",
      "sucki",
      "least",
      "day",
      "get",
      "better",
      "right"
    ],
    "token_count":7,
    "processed_text":"realli sucki least day get better right"
  },
  {
    "label":4,
    "text":"go tb",
    "cleaned_text":"go tb",
    "normalized_text":"go tb",
    "tokens":[
      "go",
      "tb"
    ],
    "token_count":2,
    "processed_text":"go tb"
  },
  {
    "label":0,
    "text":"omg spankin elliott day punishin kid hard hate see cri elijah still sleep",
    "cleaned_text":"omg spankin elliott day punishin kid hard hate see cri elijah still sleep",
    "normalized_text":"omg spankin elliott day punishin kid hard hate see cri elijah still sleep",
    "tokens":[
      "omg",
      "spankin",
      "elliott",
      "day",
      "punishin",
      "kid",
      "hard",
      "hate",
      "see",
      "cri",
      "elijah",
      "still",
      "sleep"
    ],
    "token_count":13,
    "processed_text":"omg spankin elliott day punishin kid hard hate see cri elijah still sleep"
  },
  {
    "label":4,
    "text":"yay glad made time congrat ilylt",
    "cleaned_text":"yay glad made time congrat ilylt",
    "normalized_text":"yay glad made time congrat ilylt",
    "tokens":[
      "yay",
      "glad",
      "made",
      "time",
      "congrat",
      "ilylt"
    ],
    "token_count":6,
    "processed_text":"yay glad made time congrat ilylt"
  },
  {
    "label":0,
    "text":"news economi amp cut shuttin dwn lbriestownbookstr dyinglov wlkin aisl book whr u buy ur book",
    "cleaned_text":"news economi amp cut shuttin dwn lbriestownbookstr dyinglov wlkin aisl book whr u buy ur book",
    "normalized_text":"news economi amp cut shuttin dwn lbriestownbookstr dyinglov wlkin aisl book whr u buy ur book",
    "tokens":[
      "news",
      "economi",
      "amp",
      "cut",
      "shuttin",
      "dwn",
      "dyinglov",
      "wlkin",
      "aisl",
      "book",
      "whr",
      "buy",
      "ur",
      "book"
    ],
    "token_count":14,
    "processed_text":"news economi amp cut shuttin dwn dyinglov wlkin aisl book whr buy ur book"
  },
  {
    "label":0,
    "text":"didnt get sleep go laundri",
    "cleaned_text":"didnt get sleep go laundri",
    "normalized_text":"didnt get sleep go laundri",
    "tokens":[
      "didnt",
      "get",
      "sleep",
      "go",
      "laundri"
    ],
    "token_count":5,
    "processed_text":"didnt get sleep go laundri"
  },
  {
    "label":0,
    "text":"miss privat bzn home eg stalk lollol",
    "cleaned_text":"miss privat bzn home eg stalk lollol",
    "normalized_text":"miss privat bzn home eg stalk lollol",
    "tokens":[
      "miss",
      "privat",
      "bzn",
      "home",
      "eg",
      "stalk",
      "lollol"
    ],
    "token_count":7,
    "processed_text":"miss privat bzn home eg stalk lollol"
  },
  {
    "label":4,
    "text":"ok girli ju smile morn",
    "cleaned_text":"ok girli ju smile morn",
    "normalized_text":"ok girli ju smile morn",
    "tokens":[
      "ok",
      "girli",
      "ju",
      "smile",
      "morn"
    ],
    "token_count":5,
    "processed_text":"ok girli ju smile morn"
  },
  {
    "label":4,
    "text":"magician girl bday parti love magic good resourc kid learn magic goodenergi thx",
    "cleaned_text":"magician girl bday parti love magic good resourc kid learn magic goodenergi thx",
    "normalized_text":"magician girl bday parti love magic good resourc kid learn magic goodenergi thx",
    "tokens":[
      "magician",
      "girl",
      "bday",
      "parti",
      "love",
      "magic",
      "good",
      "resourc",
      "kid",
      "learn",
      "magic",
      "goodenergi",
      "thx"
    ],
    "token_count":13,
    "processed_text":"magician girl bday parti love magic good resourc kid learn magic goodenergi thx"
  },
  {
    "label":0,
    "text":"cold shower suck",
    "cleaned_text":"cold shower suck",
    "normalized_text":"cold shower suck",
    "tokens":[
      "cold",
      "shower",
      "suck"
    ],
    "token_count":3,
    "processed_text":"cold shower suck"
  },
  {
    "label":4,
    "text":"busybusi week ahead good news get see quotvegasquot weekend",
    "cleaned_text":"busybusi week ahead good news get see quotvegasquot weekend",
    "normalized_text":"busybusi week ahead good news get see quotvegasquot weekend",
    "tokens":[
      "busybusi",
      "week",
      "ahead",
      "good",
      "news",
      "get",
      "see",
      "quotvegasquot",
      "weekend"
    ],
    "token_count":9,
    "processed_text":"busybusi week ahead good news get see quotvegasquot weekend"
  },
  {
    "label":0,
    "text":"awwww finish chat bestiebut im alon im scare lonelyquotbquot",
    "cleaned_text":"awwww finish chat bestiebut im alon im scare lonelyquotbquot",
    "normalized_text":"awwww finish chat bestiebut im alon im scare lonelyquotbquot",
    "tokens":[
      "awwww",
      "finish",
      "chat",
      "bestiebut",
      "im",
      "alon",
      "im",
      "scare",
      "lonelyquotbquot"
    ],
    "token_count":9,
    "processed_text":"awwww finish chat bestiebut im alon im scare lonelyquotbquot"
  },
  {
    "label":4,
    "text":"exactli lol",
    "cleaned_text":"exactli lol",
    "normalized_text":"exactli lol",
    "tokens":[
      "exactli",
      "lol"
    ],
    "token_count":2,
    "processed_text":"exactli lol"
  },
  {
    "label":4,
    "text":"font internet wed invit",
    "cleaned_text":"font internet wed invit",
    "normalized_text":"font internet wed invit",
    "tokens":[
      "font",
      "internet",
      "wed",
      "invit"
    ],
    "token_count":4,
    "processed_text":"font internet wed invit"
  },
  {
    "label":0,
    "text":"cant rememb myspac login email address password",
    "cleaned_text":"cant rememb myspac login email address password",
    "normalized_text":"cant rememb myspac login email address password",
    "tokens":[
      "cant",
      "rememb",
      "myspac",
      "login",
      "email",
      "address",
      "password"
    ],
    "token_count":7,
    "processed_text":"cant rememb myspac login email address password"
  },
  {
    "label":0,
    "text":"tri contact jess loool",
    "cleaned_text":"tri contact jess loool",
    "normalized_text":"tri contact jess loool",
    "tokens":[
      "tri",
      "contact",
      "jess",
      "loool"
    ],
    "token_count":4,
    "processed_text":"tri contact jess loool"
  },
  {
    "label":0,
    "text":"cri wht u",
    "cleaned_text":"cri wht u",
    "normalized_text":"cri wht u",
    "tokens":[
      "cri",
      "wht"
    ],
    "token_count":2,
    "processed_text":"cri wht"
  },
  {
    "label":0,
    "text":"bought new cd listen work today forgot home",
    "cleaned_text":"bought new cd listen work today forgot home",
    "normalized_text":"bought new cd listen work today forgot home",
    "tokens":[
      "bought",
      "new",
      "cd",
      "listen",
      "work",
      "today",
      "forgot",
      "home"
    ],
    "token_count":8,
    "processed_text":"bought new cd listen work today forgot home"
  },
  {
    "label":4,
    "text":"rt went store get munchi back condo eat lunch",
    "cleaned_text":"rt went store get munchi back condo eat lunch",
    "normalized_text":"rt went store get munchi back condo eat lunch",
    "tokens":[
      "rt",
      "went",
      "store",
      "get",
      "munchi",
      "back",
      "condo",
      "eat",
      "lunch"
    ],
    "token_count":9,
    "processed_text":"rt went store get munchi back condo eat lunch"
  },
  {
    "label":4,
    "text":"yay shave itchi face",
    "cleaned_text":"yay shave itchi face",
    "normalized_text":"yay shave itchi face",
    "tokens":[
      "yay",
      "shave",
      "itchi",
      "face"
    ],
    "token_count":4,
    "processed_text":"yay shave itchi face"
  },
  {
    "label":4,
    "text":"got home work hour half ago follow yay im work art photoshop huzzah",
    "cleaned_text":"got home work hour half ago follow yay im work art photoshop huzzah",
    "normalized_text":"got home work hour half ago follow yay im work art photoshop huzzah",
    "tokens":[
      "got",
      "home",
      "work",
      "hour",
      "half",
      "ago",
      "follow",
      "yay",
      "im",
      "work",
      "art",
      "photoshop",
      "huzzah"
    ],
    "token_count":13,
    "processed_text":"got home work hour half ago follow yay im work art photoshop huzzah"
  },
  {
    "label":0,
    "text":"h cri public im read mau cant help",
    "cleaned_text":"h cri public im read mau cant help",
    "normalized_text":"h cri public im read mau cant help",
    "tokens":[
      "cri",
      "public",
      "im",
      "read",
      "mau",
      "cant",
      "help"
    ],
    "token_count":7,
    "processed_text":"cri public im read mau cant help"
  },
  {
    "label":4,
    "text":"ive awak two hour yet im still realli awak urgh make video cousin si today",
    "cleaned_text":"ive awak two hour yet im still realli awak urgh make video cousin si today",
    "normalized_text":"ive awak two hour yet im still realli awak urgh make video cousin si today",
    "tokens":[
      "ive",
      "awak",
      "two",
      "hour",
      "yet",
      "im",
      "still",
      "realli",
      "awak",
      "urgh",
      "make",
      "video",
      "cousin",
      "si",
      "today"
    ],
    "token_count":15,
    "processed_text":"ive awak two hour yet im still realli awak urgh make video cousin si today"
  },
  {
    "label":4,
    "text":"thank start tweet peopl alreadi follow see resend quothi quot pleas",
    "cleaned_text":"thank start tweet peopl alreadi follow see resend quothi quot pleas",
    "normalized_text":"thank start tweet peopl alreadi follow see resend quothi quot pleas",
    "tokens":[
      "thank",
      "start",
      "tweet",
      "peopl",
      "alreadi",
      "follow",
      "see",
      "resend",
      "quothi",
      "quot",
      "plea"
    ],
    "token_count":11,
    "processed_text":"thank start tweet peopl alreadi follow see resend quothi quot plea"
  },
  {
    "label":4,
    "text":"guess answer dont love wealthi",
    "cleaned_text":"guess answer dont love wealthi",
    "normalized_text":"guess answer dont love wealthi",
    "tokens":[
      "guess",
      "answer",
      "dont",
      "love",
      "wealthi"
    ],
    "token_count":5,
    "processed_text":"guess answer dont love wealthi"
  },
  {
    "label":0,
    "text":"sun sun sun bad cant enjoy im watchin gilmor girl",
    "cleaned_text":"sun sun sun bad cant enjoy im watchin gilmor girl",
    "normalized_text":"sun sun sun bad cant enjoy im watchin gilmor girl",
    "tokens":[
      "sun",
      "sun",
      "sun",
      "bad",
      "cant",
      "enjoy",
      "im",
      "watchin",
      "gilmor",
      "girl"
    ],
    "token_count":10,
    "processed_text":"sun sun sun bad cant enjoy im watchin gilmor girl"
  },
  {
    "label":0,
    "text":"wish iwa chees heaven lol",
    "cleaned_text":"wish iwa chees heaven lol",
    "normalized_text":"wish iwa chees heaven lol",
    "tokens":[
      "wish",
      "iwa",
      "chee",
      "heaven",
      "lol"
    ],
    "token_count":5,
    "processed_text":"wish iwa chee heaven lol"
  },
  {
    "label":4,
    "text":"got home night shoot workshop good time long beach",
    "cleaned_text":"got home night shoot workshop good time long beach",
    "normalized_text":"got home night shoot workshop good time long beach",
    "tokens":[
      "got",
      "home",
      "night",
      "shoot",
      "workshop",
      "good",
      "time",
      "long",
      "beach"
    ],
    "token_count":9,
    "processed_text":"got home night shoot workshop good time long beach"
  },
  {
    "label":0,
    "text":"nooo what",
    "cleaned_text":"nooo what",
    "normalized_text":"nooo what",
    "tokens":[
      "nooo"
    ],
    "token_count":1,
    "processed_text":"nooo"
  },
  {
    "label":4,
    "text":"thank dad believ",
    "cleaned_text":"thank dad believ",
    "normalized_text":"thank dad believ",
    "tokens":[
      "thank",
      "dad",
      "believ"
    ],
    "token_count":3,
    "processed_text":"thank dad believ"
  },
  {
    "label":4,
    "text":"want tangerin one handi bc im realli hungri",
    "cleaned_text":"want tangerin one handi bc im realli hungri",
    "normalized_text":"want tangerin one handi bc im realli hungri",
    "tokens":[
      "want",
      "tangerin",
      "one",
      "handi",
      "bc",
      "im",
      "realli",
      "hungri"
    ],
    "token_count":8,
    "processed_text":"want tangerin one handi bc im realli hungri"
  },
  {
    "label":4,
    "text":"yeah im happi dont know",
    "cleaned_text":"yeah im happi dont know",
    "normalized_text":"yeah im happi dont know",
    "tokens":[
      "yeah",
      "im",
      "happi",
      "dont",
      "know"
    ],
    "token_count":5,
    "processed_text":"yeah im happi dont know"
  },
  {
    "label":4,
    "text":"haha there lot celeb updat myspac teehe that get hook",
    "cleaned_text":"haha there lot celeb updat myspac teehe that get hook",
    "normalized_text":"haha there lot celeb updat myspac teehe that get hook",
    "tokens":[
      "haha",
      "lot",
      "celeb",
      "updat",
      "myspac",
      "teeh",
      "get",
      "hook"
    ],
    "token_count":8,
    "processed_text":"haha lot celeb updat myspac teeh get hook"
  },
  {
    "label":4,
    "text":"love tu jab ayegi tu dekhna aa jaaa na",
    "cleaned_text":"love tu jab ayegi tu dekhna aa jaaa na",
    "normalized_text":"love tu jab ayegi tu dekhna aa jaaa na",
    "tokens":[
      "love",
      "tu",
      "jab",
      "ayegi",
      "tu",
      "dekhna",
      "aa",
      "jaaa",
      "na"
    ],
    "token_count":9,
    "processed_text":"love tu jab ayegi tu dekhna aa jaaa na"
  },
  {
    "label":0,
    "text":"pfft your dog beat dog dog love everyon your dog hate",
    "cleaned_text":"pfft your dog beat dog dog love everyon your dog hate",
    "normalized_text":"pfft your dog beat dog dog love everyon your dog hate",
    "tokens":[
      "pfft",
      "dog",
      "beat",
      "dog",
      "dog",
      "love",
      "everyon",
      "dog",
      "hate"
    ],
    "token_count":9,
    "processed_text":"pfft dog beat dog dog love everyon dog hate"
  },
  {
    "label":0,
    "text":"miss u ptu",
    "cleaned_text":"miss u ptu",
    "normalized_text":"miss u ptu",
    "tokens":[
      "miss",
      "ptu"
    ],
    "token_count":2,
    "processed_text":"miss ptu"
  },
  {
    "label":0,
    "text":"gotta go work",
    "cleaned_text":"gotta go work",
    "normalized_text":"gotta go work",
    "tokens":[
      "got",
      "ta",
      "go",
      "work"
    ],
    "token_count":4,
    "processed_text":"got ta go work"
  },
  {
    "label":0,
    "text":"realli crave beauti mind roll semo sushi close tmr",
    "cleaned_text":"realli crave beauti mind roll semo sushi close tmr",
    "normalized_text":"realli crave beauti mind roll semo sushi close tmr",
    "tokens":[
      "realli",
      "crave",
      "beauti",
      "mind",
      "roll",
      "semo",
      "sushi",
      "close",
      "tmr"
    ],
    "token_count":9,
    "processed_text":"realli crave beauti mind roll semo sushi close tmr"
  },
  {
    "label":4,
    "text":"your london kotaku london meet go plz",
    "cleaned_text":"your london kotaku london meet go plz",
    "normalized_text":"your london kotaku london meet go plz",
    "tokens":[
      "london",
      "kotaku",
      "london",
      "meet",
      "go",
      "plz"
    ],
    "token_count":6,
    "processed_text":"london kotaku london meet go plz"
  },
  {
    "label":0,
    "text":"way ruin dream",
    "cleaned_text":"way ruin dream",
    "normalized_text":"way ruin dream",
    "tokens":[
      "way",
      "ruin",
      "dream"
    ],
    "token_count":3,
    "processed_text":"way ruin dream"
  },
  {
    "label":4,
    "text":"mozert requiem tonight",
    "cleaned_text":"mozert requiem tonight",
    "normalized_text":"mozert requiem tonight",
    "tokens":[
      "mozert",
      "requiem",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"mozert requiem tonight"
  },
  {
    "label":0,
    "text":"wrong new moon werewolf werewolv must crappi",
    "cleaned_text":"wrong new moon werewolf werewolv must crappi",
    "normalized_text":"wrong new moon werewolf werewolv must crappi",
    "tokens":[
      "wrong",
      "new",
      "moon",
      "werewolf",
      "werewolv",
      "crappi"
    ],
    "token_count":6,
    "processed_text":"wrong new moon werewolf werewolv crappi"
  },
  {
    "label":4,
    "text":"get email anyth yeah mine took pictur go tri tomoz someth xx",
    "cleaned_text":"get email anyth yeah mine took pictur go tri tomoz someth xx",
    "normalized_text":"get email anyth yeah mine took pictur go tri tomoz someth xx",
    "tokens":[
      "get",
      "email",
      "anyth",
      "yeah",
      "mine",
      "took",
      "pictur",
      "go",
      "tri",
      "tomoz",
      "someth",
      "xx"
    ],
    "token_count":12,
    "processed_text":"get email anyth yeah mine took pictur go tri tomoz someth xx"
  },
  {
    "label":4,
    "text":"yeah need rest us tomorrow night",
    "cleaned_text":"yeah need rest us tomorrow night",
    "normalized_text":"yeah need rest us tomorrow night",
    "tokens":[
      "yeah",
      "need",
      "rest",
      "us",
      "tomorrow",
      "night"
    ],
    "token_count":6,
    "processed_text":"yeah need rest us tomorrow night"
  },
  {
    "label":4,
    "text":"congratul hope get see decor",
    "cleaned_text":"congratul hope get see decor",
    "normalized_text":"congratul hope get see decor",
    "tokens":[
      "congratul",
      "hope",
      "get",
      "see",
      "decor"
    ],
    "token_count":5,
    "processed_text":"congratul hope get see decor"
  },
  {
    "label":4,
    "text":"even rest food past wk still enjoy coffeeshop food best",
    "cleaned_text":"even rest food past wk still enjoy coffeeshop food best",
    "normalized_text":"even rest food past wk still enjoy coffeeshop food best",
    "tokens":[
      "even",
      "rest",
      "food",
      "past",
      "wk",
      "still",
      "enjoy",
      "coffeeshop",
      "food",
      "best"
    ],
    "token_count":10,
    "processed_text":"even rest food past wk still enjoy coffeeshop food best"
  },
  {
    "label":0,
    "text":"look everywher jack find either morri",
    "cleaned_text":"look everywher jack find either morri",
    "normalized_text":"look everywher jack find either morri",
    "tokens":[
      "look",
      "everywh",
      "jack",
      "find",
      "either",
      "morri"
    ],
    "token_count":6,
    "processed_text":"look everywh jack find either morri"
  },
  {
    "label":0,
    "text":"im realli tire work tomorrow hate work moment poo",
    "cleaned_text":"im realli tire work tomorrow hate work moment poo",
    "normalized_text":"im realli tire work tomorrow hate work moment poo",
    "tokens":[
      "im",
      "realli",
      "tire",
      "work",
      "tomorrow",
      "hate",
      "work",
      "moment",
      "poo"
    ],
    "token_count":9,
    "processed_text":"im realli tire work tomorrow hate work moment poo"
  },
  {
    "label":0,
    "text":"go stir crazi go bed escap dealership still dont know job yet",
    "cleaned_text":"go stir crazi go bed escap dealership still dont know job yet",
    "normalized_text":"go stir crazi go bed escap dealership still dont know job yet",
    "tokens":[
      "go",
      "stir",
      "crazi",
      "go",
      "bed",
      "escap",
      "dealership",
      "still",
      "dont",
      "know",
      "job",
      "yet"
    ],
    "token_count":12,
    "processed_text":"go stir crazi go bed escap dealership still dont know job yet"
  },
  {
    "label":4,
    "text":"today event went well everyth alright noth left that call great teamwork",
    "cleaned_text":"today event went well everyth alright noth left that call great teamwork",
    "normalized_text":"today event went well everyth alright noth left that call great teamwork",
    "tokens":[
      "today",
      "event",
      "went",
      "well",
      "everyth",
      "alright",
      "noth",
      "left",
      "call",
      "great",
      "teamwork"
    ],
    "token_count":11,
    "processed_text":"today event went well everyth alright noth left call great teamwork"
  },
  {
    "label":0,
    "text":"wonder cant access multipli",
    "cleaned_text":"wonder cant access multipli",
    "normalized_text":"wonder cant access multipli",
    "tokens":[
      "wonder",
      "cant",
      "access",
      "multipli"
    ],
    "token_count":4,
    "processed_text":"wonder cant access multipli"
  },
  {
    "label":0,
    "text":"night twitter hehe realli need get popcornmi firend last twilight book iam realli want read",
    "cleaned_text":"night twitter hehe realli need get popcornmi firend last twilight book iam realli want read",
    "normalized_text":"night twitter hehe realli need get popcornmi firend last twilight book iam realli want read",
    "tokens":[
      "night",
      "twitter",
      "hehe",
      "realli",
      "need",
      "get",
      "popcornmi",
      "firend",
      "last",
      "twilight",
      "book",
      "iam",
      "realli",
      "want",
      "read"
    ],
    "token_count":15,
    "processed_text":"night twitter hehe realli need get popcornmi firend last twilight book iam realli want read"
  },
  {
    "label":0,
    "text":"raiin raiin raiin never sunni forr",
    "cleaned_text":"raiin raiin raiin never sunni forr",
    "normalized_text":"raiin raiin raiin never sunni forr",
    "tokens":[
      "raiin",
      "raiin",
      "raiin",
      "never",
      "sunni",
      "forr"
    ],
    "token_count":6,
    "processed_text":"raiin raiin raiin never sunni forr"
  },
  {
    "label":4,
    "text":"listen give jennif ur right beauti song",
    "cleaned_text":"listen give jennif ur right beauti song",
    "normalized_text":"listen give jennif ur right beauti song",
    "tokens":[
      "listen",
      "give",
      "jennif",
      "ur",
      "right",
      "beauti",
      "song"
    ],
    "token_count":7,
    "processed_text":"listen give jennif ur right beauti song"
  },
  {
    "label":4,
    "text":"hey cuti great run danc africa",
    "cleaned_text":"hey cuti great run danc africa",
    "normalized_text":"hey cuti great run danc africa",
    "tokens":[
      "hey",
      "cuti",
      "great",
      "run",
      "danc",
      "africa"
    ],
    "token_count":6,
    "processed_text":"hey cuti great run danc africa"
  },
  {
    "label":4,
    "text":"would chanc will trade prawn want bean carrot butter chicken saffron",
    "cleaned_text":"would chanc will trade prawn want bean carrot butter chicken saffron",
    "normalized_text":"would chanc will trade prawn want bean carrot butter chicken saffron",
    "tokens":[
      "chanc",
      "trade",
      "prawn",
      "want",
      "bean",
      "carrot",
      "butter",
      "chicken",
      "saffron"
    ],
    "token_count":9,
    "processed_text":"chanc trade prawn want bean carrot butter chicken saffron"
  },
  {
    "label":4,
    "text":"omg love ult fri misss shaunnaboo come logan work",
    "cleaned_text":"omg love ult fri misss shaunnaboo come logan work",
    "normalized_text":"omg love ult fri misss shaunnaboo come logan work",
    "tokens":[
      "omg",
      "love",
      "ult",
      "fri",
      "misss",
      "shaunnaboo",
      "come",
      "logan",
      "work"
    ],
    "token_count":9,
    "processed_text":"omg love ult fri misss shaunnaboo come logan work"
  },
  {
    "label":0,
    "text":"well netbal suck",
    "cleaned_text":"well netbal suck",
    "normalized_text":"well netbal suck",
    "tokens":[
      "well",
      "netbal",
      "suck"
    ],
    "token_count":3,
    "processed_text":"well netbal suck"
  },
  {
    "label":0,
    "text":"muscl sore",
    "cleaned_text":"muscl sore",
    "normalized_text":"muscl sore",
    "tokens":[
      "muscl",
      "sore"
    ],
    "token_count":2,
    "processed_text":"muscl sore"
  },
  {
    "label":0,
    "text":"go hous clean day sinc start spit outsid littl",
    "cleaned_text":"go hous clean day sinc start spit outsid littl",
    "normalized_text":"go hous clean day sinc start spit outsid littl",
    "tokens":[
      "go",
      "hou",
      "clean",
      "day",
      "sinc",
      "start",
      "spit",
      "outsid",
      "littl"
    ],
    "token_count":9,
    "processed_text":"go hou clean day sinc start spit outsid littl"
  },
  {
    "label":4,
    "text":"aww love mia",
    "cleaned_text":"aww love mia",
    "normalized_text":"aww love mia",
    "tokens":[
      "aww",
      "love",
      "mia"
    ],
    "token_count":3,
    "processed_text":"aww love mia"
  },
  {
    "label":4,
    "text":"quotoff hizzl fo shizzlequot phrase day xx",
    "cleaned_text":"quotoff hizzl fo shizzlequot phrase day xx",
    "normalized_text":"quotoff hizzl fo shizzlequot phrase day xx",
    "tokens":[
      "quotoff",
      "hizzl",
      "fo",
      "shizzlequot",
      "phrase",
      "day",
      "xx"
    ],
    "token_count":7,
    "processed_text":"quotoff hizzl fo shizzlequot phrase day xx"
  },
  {
    "label":0,
    "text":"put damn candl",
    "cleaned_text":"put damn candl",
    "normalized_text":"put damn candl",
    "tokens":[
      "put",
      "damn",
      "candl"
    ],
    "token_count":3,
    "processed_text":"put damn candl"
  },
  {
    "label":4,
    "text":"dont know much english im biiig fan much love chile success kiss",
    "cleaned_text":"dont know much english im biiig fan much love chile success kiss",
    "normalized_text":"dont know much english im biiig fan much love chile success kiss",
    "tokens":[
      "dont",
      "know",
      "much",
      "english",
      "im",
      "biiig",
      "fan",
      "much",
      "love",
      "chile",
      "success",
      "kiss"
    ],
    "token_count":12,
    "processed_text":"dont know much english im biiig fan much love chile success kiss"
  },
  {
    "label":0,
    "text":"hate work thing",
    "cleaned_text":"hate work thing",
    "normalized_text":"hate work thing",
    "tokens":[
      "hate",
      "work",
      "thing"
    ],
    "token_count":3,
    "processed_text":"hate work thing"
  },
  {
    "label":0,
    "text":"sad feet macbook fell sad feet macbook fell",
    "cleaned_text":"sad feet macbook fell sad feet macbook fell",
    "normalized_text":"sad feet macbook fell sad feet macbook fell",
    "tokens":[
      "sad",
      "feet",
      "macbook",
      "fell",
      "sad",
      "feet",
      "macbook",
      "fell"
    ],
    "token_count":8,
    "processed_text":"sad feet macbook fell sad feet macbook fell"
  },
  {
    "label":0,
    "text":"rememb mash potato fridg well oh wait theyr two week ago talk",
    "cleaned_text":"rememb mash potato fridg well oh wait theyr two week ago talk",
    "normalized_text":"rememb mash potato fridg well oh wait theyr two week ago talk",
    "tokens":[
      "rememb",
      "mash",
      "potato",
      "fridg",
      "well",
      "oh",
      "wait",
      "theyr",
      "two",
      "week",
      "ago",
      "talk"
    ],
    "token_count":12,
    "processed_text":"rememb mash potato fridg well oh wait theyr two week ago talk"
  },
  {
    "label":4,
    "text":"long vancouv",
    "cleaned_text":"long vancouv",
    "normalized_text":"long vancouv",
    "tokens":[
      "long",
      "vancouv"
    ],
    "token_count":2,
    "processed_text":"long vancouv"
  },
  {
    "label":4,
    "text":"eat dinner cinnamon toast crunch cereal gona make scrambl egg bt didnt feel like",
    "cleaned_text":"eat dinner cinnamon toast crunch cereal gona make scrambl egg bt didnt feel like",
    "normalized_text":"eat dinner cinnamon toast crunch cereal gona make scrambl egg bt didnt feel like",
    "tokens":[
      "eat",
      "dinner",
      "cinnamon",
      "toast",
      "crunch",
      "cereal",
      "gona",
      "make",
      "scrambl",
      "egg",
      "bt",
      "didnt",
      "feel",
      "like"
    ],
    "token_count":14,
    "processed_text":"eat dinner cinnamon toast crunch cereal gona make scrambl egg bt didnt feel like"
  },
  {
    "label":0,
    "text":"team fortress want backburn",
    "cleaned_text":"team fortress want backburn",
    "normalized_text":"team fortress want backburn",
    "tokens":[
      "team",
      "fortress",
      "want",
      "backburn"
    ],
    "token_count":4,
    "processed_text":"team fortress want backburn"
  },
  {
    "label":4,
    "text":"hey niec jade kwok anoth jade kowk",
    "cleaned_text":"hey niec jade kwok anoth jade kowk",
    "normalized_text":"hey niec jade kwok anoth jade kowk",
    "tokens":[
      "hey",
      "niec",
      "jade",
      "kwok",
      "anoth",
      "jade",
      "kowk"
    ],
    "token_count":7,
    "processed_text":"hey niec jade kwok anoth jade kowk"
  },
  {
    "label":4,
    "text":"listeneddifferenti likeyit dope u listen diff thingsst blognow get blip acct",
    "cleaned_text":"listeneddifferenti likeyit dope u listen diff thingsst blognow get blip acct",
    "normalized_text":"listeneddifferenti likeyit dope u listen diff thingsst blognow get blip acct",
    "tokens":[
      "likeyit",
      "dope",
      "listen",
      "diff",
      "thingsst",
      "blognow",
      "get",
      "blip",
      "acct"
    ],
    "token_count":9,
    "processed_text":"likeyit dope listen diff thingsst blognow get blip acct"
  },
  {
    "label":4,
    "text":"friend scott",
    "cleaned_text":"friend scott",
    "normalized_text":"friend scott",
    "tokens":[
      "friend",
      "scott"
    ],
    "token_count":2,
    "processed_text":"friend scott"
  },
  {
    "label":0,
    "text":"there fucken cockroach closet fast catch",
    "cleaned_text":"there fucken cockroach closet fast catch",
    "normalized_text":"there fucken cockroach closet fast catch",
    "tokens":[
      "fucken",
      "cockroach",
      "closet",
      "fast",
      "catch"
    ],
    "token_count":5,
    "processed_text":"fucken cockroach closet fast catch"
  },
  {
    "label":0,
    "text":"meet morn afternoon tweet ya soon",
    "cleaned_text":"meet morn afternoon tweet ya soon",
    "normalized_text":"meet morn afternoon tweet ya soon",
    "tokens":[
      "meet",
      "morn",
      "afternoon",
      "tweet",
      "ya",
      "soon"
    ],
    "token_count":6,
    "processed_text":"meet morn afternoon tweet ya soon"
  },
  {
    "label":0,
    "text":"watch dogtown im go cri",
    "cleaned_text":"watch dogtown im go cri",
    "normalized_text":"watch dogtown im go cri",
    "tokens":[
      "watch",
      "dogtown",
      "im",
      "go",
      "cri"
    ],
    "token_count":5,
    "processed_text":"watch dogtown im go cri"
  },
  {
    "label":4,
    "text":"call ppl phone late night convo actual pretti sexi like shadi best",
    "cleaned_text":"call ppl phone late night convo actual pretti sexi like shadi best",
    "normalized_text":"call ppl phone late night convo actual pretti sexi like shadi best",
    "tokens":[
      "call",
      "ppl",
      "phone",
      "late",
      "night",
      "convo",
      "actual",
      "pretti",
      "sexi",
      "like",
      "shadi",
      "best"
    ],
    "token_count":12,
    "processed_text":"call ppl phone late night convo actual pretti sexi like shadi best"
  },
  {
    "label":4,
    "text":"yo diddi congrat realli though know believ god refrain call us follow like fan",
    "cleaned_text":"yo diddi congrat realli though know believ god refrain call us follow like fan",
    "normalized_text":"yo diddi congrat realli though know believ god refrain call us follow like fan",
    "tokens":[
      "yo",
      "diddi",
      "congrat",
      "realli",
      "though",
      "know",
      "believ",
      "god",
      "refrain",
      "call",
      "us",
      "follow",
      "like",
      "fan"
    ],
    "token_count":14,
    "processed_text":"yo diddi congrat realli though know believ god refrain call us follow like fan"
  },
  {
    "label":4,
    "text":"thank serenad",
    "cleaned_text":"thank serenad",
    "normalized_text":"thank serenad",
    "tokens":[
      "thank",
      "serenad"
    ],
    "token_count":2,
    "processed_text":"thank serenad"
  },
  {
    "label":4,
    "text":"e highlight mario galaxi new super mario bro wii modern warfar assassin creed project natal fallout dlc point lookout",
    "cleaned_text":"e highlight mario galaxi new super mario bro wii modern warfar assassin creed project natal fallout dlc point lookout",
    "normalized_text":"e highlight mario galaxi new super mario bro wii modern warfar assassin creed project natal fallout dlc point lookout",
    "tokens":[
      "highlight",
      "mario",
      "galaxi",
      "new",
      "super",
      "mario",
      "bro",
      "wii",
      "modern",
      "warfar",
      "assassin",
      "creed",
      "project",
      "natal",
      "fallout",
      "dlc",
      "point",
      "lookout"
    ],
    "token_count":18,
    "processed_text":"highlight mario galaxi new super mario bro wii modern warfar assassin creed project natal fallout dlc point lookout"
  },
  {
    "label":0,
    "text":"would attract peopl optic",
    "cleaned_text":"would attract peopl optic",
    "normalized_text":"would attract peopl optic",
    "tokens":[
      "attract",
      "peopl",
      "optic"
    ],
    "token_count":3,
    "processed_text":"attract peopl optic"
  },
  {
    "label":4,
    "text":"heyi im haha come mexicantown grab bite enjoy time",
    "cleaned_text":"heyi im haha come mexicantown grab bite enjoy time",
    "normalized_text":"heyi im haha come mexicantown grab bite enjoy time",
    "tokens":[
      "heyi",
      "im",
      "haha",
      "come",
      "mexicantown",
      "grab",
      "bite",
      "enjoy",
      "time"
    ],
    "token_count":9,
    "processed_text":"heyi im haha come mexicantown grab bite enjoy time"
  },
  {
    "label":4,
    "text":"hey bbi love u",
    "cleaned_text":"hey bbi love u",
    "normalized_text":"hey bbi love u",
    "tokens":[
      "hey",
      "bbi",
      "love"
    ],
    "token_count":3,
    "processed_text":"hey bbi love"
  },
  {
    "label":4,
    "text":"less day",
    "cleaned_text":"less day",
    "normalized_text":"less day",
    "tokens":[
      "less",
      "day"
    ],
    "token_count":2,
    "processed_text":"less day"
  },
  {
    "label":0,
    "text":"still sick laassssttt sunday omg mad learn u alway carri chang cloth weather man say rain",
    "cleaned_text":"still sick laassssttt sunday omg mad learn u alway carri chang cloth weather man say rain",
    "normalized_text":"still sick laassssttt sunday omg mad learn u alway carri chang cloth weather man say rain",
    "tokens":[
      "still",
      "sick",
      "laassssttt",
      "sunday",
      "omg",
      "mad",
      "learn",
      "alway",
      "carri",
      "chang",
      "cloth",
      "weather",
      "man",
      "say",
      "rain"
    ],
    "token_count":15,
    "processed_text":"still sick laassssttt sunday omg mad learn alway carri chang cloth weather man say rain"
  },
  {
    "label":4,
    "text":"wow repli lucki",
    "cleaned_text":"wow repli lucki",
    "normalized_text":"wow repli lucki",
    "tokens":[
      "wow",
      "repli",
      "lucki"
    ],
    "token_count":3,
    "processed_text":"wow repli lucki"
  },
  {
    "label":4,
    "text":"besti lot shenanigan",
    "cleaned_text":"besti lot shenanigan",
    "normalized_text":"besti lot shenanigan",
    "tokens":[
      "besti",
      "lot",
      "shenanigan"
    ],
    "token_count":3,
    "processed_text":"besti lot shenanigan"
  },
  {
    "label":0,
    "text":"damnwhi everyon upset didnt mean tv die thing happen yet alway seem happen sigh",
    "cleaned_text":"damnwhi everyon upset didnt mean tv die thing happen yet alway seem happen sigh",
    "normalized_text":"damnwhi everyon upset didnt mean tv die thing happen yet alway seem happen sigh",
    "tokens":[
      "damnwhi",
      "everyon",
      "upset",
      "didnt",
      "mean",
      "tv",
      "die",
      "thing",
      "happen",
      "yet",
      "alway",
      "seem",
      "happen",
      "sigh"
    ],
    "token_count":14,
    "processed_text":"damnwhi everyon upset didnt mean tv die thing happen yet alway seem happen sigh"
  },
  {
    "label":0,
    "text":"dude stop your scare doesnt want know wimper",
    "cleaned_text":"dude stop your scare doesnt want know wimper",
    "normalized_text":"dude stop your scare doesnt want know wimper",
    "tokens":[
      "dude",
      "stop",
      "scare",
      "doesnt",
      "want",
      "know",
      "wimper"
    ],
    "token_count":7,
    "processed_text":"dude stop scare doesnt want know wimper"
  },
  {
    "label":4,
    "text":"home earli",
    "cleaned_text":"home earli",
    "normalized_text":"home earli",
    "tokens":[
      "home",
      "earli"
    ],
    "token_count":2,
    "processed_text":"home earli"
  },
  {
    "label":0,
    "text":"said lehitraot rosenhous thu end shabbattweetup parsha naso sniff amaz time",
    "cleaned_text":"said lehitraot rosenhous thu end shabbattweetup parsha naso sniff amaz time",
    "normalized_text":"said lehitraot rosenhous thu end shabbattweetup parsha naso sniff amaz time",
    "tokens":[
      "said",
      "lehitraot",
      "rosenh",
      "thu",
      "end",
      "shabbattweetup",
      "parsha",
      "naso",
      "sniff",
      "amaz",
      "time"
    ],
    "token_count":11,
    "processed_text":"said lehitraot rosenh thu end shabbattweetup parsha naso sniff amaz time"
  },
  {
    "label":0,
    "text":"arthur public adjust need cant find dm tweet thank roof leak",
    "cleaned_text":"arthur public adjust need cant find dm tweet thank roof leak",
    "normalized_text":"arthur public adjust need cant find dm tweet thank roof leak",
    "tokens":[
      "arthur",
      "public",
      "adjust",
      "need",
      "cant",
      "find",
      "dm",
      "tweet",
      "thank",
      "roof",
      "leak"
    ],
    "token_count":11,
    "processed_text":"arthur public adjust need cant find dm tweet thank roof leak"
  },
  {
    "label":4,
    "text":"hey love",
    "cleaned_text":"hey love",
    "normalized_text":"hey love",
    "tokens":[
      "hey",
      "love"
    ],
    "token_count":2,
    "processed_text":"hey love"
  },
  {
    "label":0,
    "text":"im home",
    "cleaned_text":"im home",
    "normalized_text":"im home",
    "tokens":[
      "im",
      "home"
    ],
    "token_count":2,
    "processed_text":"im home"
  },
  {
    "label":4,
    "text":"dont know like",
    "cleaned_text":"dont know like",
    "normalized_text":"dont know like",
    "tokens":[
      "dont",
      "know",
      "like"
    ],
    "token_count":3,
    "processed_text":"dont know like"
  },
  {
    "label":4,
    "text":"get readi church omg mother day rli bore txt",
    "cleaned_text":"get readi church omg mother day rli bore txt",
    "normalized_text":"get readi church omg mother day rli bore txt",
    "tokens":[
      "get",
      "readi",
      "church",
      "omg",
      "mother",
      "day",
      "rli",
      "bore",
      "txt"
    ],
    "token_count":9,
    "processed_text":"get readi church omg mother day rli bore txt"
  },
  {
    "label":4,
    "text":"video love even",
    "cleaned_text":"video love even",
    "normalized_text":"video love even",
    "tokens":[
      "video",
      "love",
      "even"
    ],
    "token_count":3,
    "processed_text":"video love even"
  },
  {
    "label":4,
    "text":"quotyour music compat smmithyyi lowquot ah well ad anyway",
    "cleaned_text":"quotyour music compat smmithyyi lowquot ah well ad anyway",
    "normalized_text":"quotyour music compat smmithyyi lowquot ah well ad anyway",
    "tokens":[
      "quotyour",
      "music",
      "compat",
      "smmithyyi",
      "lowquot",
      "ah",
      "well",
      "ad",
      "anyway"
    ],
    "token_count":9,
    "processed_text":"quotyour music compat smmithyyi lowquot ah well ad anyway"
  },
  {
    "label":0,
    "text":"wick sunburn face",
    "cleaned_text":"wick sunburn face",
    "normalized_text":"wick sunburn face",
    "tokens":[
      "wick",
      "sunburn",
      "face"
    ],
    "token_count":3,
    "processed_text":"wick sunburn face"
  },
  {
    "label":0,
    "text":"told mom today im buy ticket wednesday told wast money",
    "cleaned_text":"told mom today im buy ticket wednesday told wast money",
    "normalized_text":"told mom today im buy ticket wednesday told wast money",
    "tokens":[
      "told",
      "mom",
      "today",
      "im",
      "buy",
      "ticket",
      "wednesday",
      "told",
      "wast",
      "money"
    ],
    "token_count":10,
    "processed_text":"told mom today im buy ticket wednesday told wast money"
  },
  {
    "label":0,
    "text":"stuff plan buy",
    "cleaned_text":"stuff plan buy",
    "normalized_text":"stuff plan buy",
    "tokens":[
      "stuff",
      "plan",
      "buy"
    ],
    "token_count":3,
    "processed_text":"stuff plan buy"
  },
  {
    "label":0,
    "text":"told lol jk im siiiiick bird flu",
    "cleaned_text":"told lol jk im siiiiick bird flu",
    "normalized_text":"told lol jk im siiiiick bird flu",
    "tokens":[
      "told",
      "lol",
      "jk",
      "im",
      "siiiiick",
      "bird",
      "flu"
    ],
    "token_count":7,
    "processed_text":"told lol jk im siiiiick bird flu"
  },
  {
    "label":0,
    "text":"want feel good back normall",
    "cleaned_text":"want feel good back normall",
    "normalized_text":"want feel good back normall",
    "tokens":[
      "want",
      "feel",
      "good",
      "back",
      "normal"
    ],
    "token_count":5,
    "processed_text":"want feel good back normal"
  },
  {
    "label":4,
    "text":"need new headphon found hannah montana one playcom",
    "cleaned_text":"need new headphon found hannah montana one playcom",
    "normalized_text":"need new headphon found hannah montana one playcom",
    "tokens":[
      "need",
      "new",
      "headphon",
      "found",
      "hannah",
      "montana",
      "one",
      "playcom"
    ],
    "token_count":8,
    "processed_text":"need new headphon found hannah montana one playcom"
  },
  {
    "label":0,
    "text":"cant wait tomorrow miss vanessa",
    "cleaned_text":"cant wait tomorrow miss vanessa",
    "normalized_text":"cant wait tomorrow miss vanessa",
    "tokens":[
      "cant",
      "wait",
      "tomorrow",
      "miss",
      "vanessa"
    ],
    "token_count":5,
    "processed_text":"cant wait tomorrow miss vanessa"
  },
  {
    "label":0,
    "text":"apprentic wouldnt without margaret",
    "cleaned_text":"apprentic wouldnt without margaret",
    "normalized_text":"apprentic wouldnt without margaret",
    "tokens":[
      "apprent",
      "wouldnt",
      "without",
      "margaret"
    ],
    "token_count":4,
    "processed_text":"apprent wouldnt without margaret"
  },
  {
    "label":4,
    "text":"prepair go gotland weekend",
    "cleaned_text":"prepair go gotland weekend",
    "normalized_text":"prepair go gotland weekend",
    "tokens":[
      "prepair",
      "go",
      "gotland",
      "weekend"
    ],
    "token_count":4,
    "processed_text":"prepair go gotland weekend"
  },
  {
    "label":0,
    "text":"uggggh block ym skype",
    "cleaned_text":"uggggh block ym skype",
    "normalized_text":"uggggh block ym skype",
    "tokens":[
      "uggggh",
      "block",
      "ym",
      "skype"
    ],
    "token_count":4,
    "processed_text":"uggggh block ym skype"
  },
  {
    "label":0,
    "text":"plz follow peter would hate see lose",
    "cleaned_text":"plz follow peter would hate see lose",
    "normalized_text":"plz follow peter would hate see lose",
    "tokens":[
      "plz",
      "follow",
      "peter",
      "hate",
      "see",
      "lose"
    ],
    "token_count":6,
    "processed_text":"plz follow peter hate see lose"
  },
  {
    "label":0,
    "text":"pack gone tomorrow",
    "cleaned_text":"pack gone tomorrow",
    "normalized_text":"pack gone tomorrow",
    "tokens":[
      "pack",
      "gone",
      "tomorrow"
    ],
    "token_count":3,
    "processed_text":"pack gone tomorrow"
  },
  {
    "label":4,
    "text":"quotif want someth said ask man want someth done ask womanquot mt",
    "cleaned_text":"quotif want someth said ask man want someth done ask womanquot mt",
    "normalized_text":"quotif want someth said ask man want someth done ask womanquot mt",
    "tokens":[
      "quotif",
      "want",
      "someth",
      "said",
      "ask",
      "man",
      "want",
      "someth",
      "done",
      "ask",
      "womanquot",
      "mt"
    ],
    "token_count":12,
    "processed_text":"quotif want someth said ask man want someth done ask womanquot mt"
  },
  {
    "label":0,
    "text":"good morn everyon told quotth earli bird get wormquot im work",
    "cleaned_text":"good morn everyon told quotth earli bird get wormquot im work",
    "normalized_text":"good morn everyon told quotth earli bird get wormquot im work",
    "tokens":[
      "good",
      "morn",
      "everyon",
      "told",
      "quotth",
      "earli",
      "bird",
      "get",
      "wormquot",
      "im",
      "work"
    ],
    "token_count":11,
    "processed_text":"good morn everyon told quotth earli bird get wormquot im work"
  },
  {
    "label":0,
    "text":"feelin like fever meet",
    "cleaned_text":"feelin like fever meet",
    "normalized_text":"feelin like fever meet",
    "tokens":[
      "feelin",
      "like",
      "fever",
      "meet"
    ],
    "token_count":4,
    "processed_text":"feelin like fever meet"
  },
  {
    "label":0,
    "text":"bozeman anybodi come across diamond tenni bracelet rubi tenni bracelet emerald ring ct tanz ring dm miss gem",
    "cleaned_text":"bozeman anybodi come across diamond tenni bracelet rubi tenni bracelet emerald ring ct tanz ring dm miss gem",
    "normalized_text":"bozeman anybodi come across diamond tenni bracelet rubi tenni bracelet emerald ring ct tanz ring dm miss gem",
    "tokens":[
      "bozeman",
      "anybodi",
      "come",
      "across",
      "diamond",
      "tenni",
      "bracelet",
      "rubi",
      "tenni",
      "bracelet",
      "emerald",
      "ring",
      "ct",
      "tanz",
      "ring",
      "dm",
      "miss",
      "gem"
    ],
    "token_count":18,
    "processed_text":"bozeman anybodi come across diamond tenni bracelet rubi tenni bracelet emerald ring ct tanz ring dm miss gem"
  },
  {
    "label":0,
    "text":"know want meet pheefo",
    "cleaned_text":"know want meet pheefo",
    "normalized_text":"know want meet pheefo",
    "tokens":[
      "know",
      "want",
      "meet",
      "pheefo"
    ],
    "token_count":4,
    "processed_text":"know want meet pheefo"
  },
  {
    "label":0,
    "text":"studi chem",
    "cleaned_text":"studi chem",
    "normalized_text":"studi chem",
    "tokens":[
      "studi",
      "chem"
    ],
    "token_count":2,
    "processed_text":"studi chem"
  },
  {
    "label":4,
    "text":"pictur almost selena gomez",
    "cleaned_text":"pictur almost selena gomez",
    "normalized_text":"pictur almost selena gomez",
    "tokens":[
      "pictur",
      "almost",
      "selena",
      "gomez"
    ],
    "token_count":4,
    "processed_text":"pictur almost selena gomez"
  },
  {
    "label":0,
    "text":"extern hdd sometim fail get recogn window think problem might conflict usb hub fun use anymor",
    "cleaned_text":"extern hdd sometim fail get recogn window think problem might conflict usb hub fun use anymor",
    "normalized_text":"extern hdd sometim fail get recogn window think problem might conflict usb hub fun use anymor",
    "tokens":[
      "extern",
      "hdd",
      "sometim",
      "fail",
      "get",
      "recogn",
      "window",
      "think",
      "problem",
      "conflict",
      "usb",
      "hub",
      "fun",
      "use",
      "anymor"
    ],
    "token_count":15,
    "processed_text":"extern hdd sometim fail get recogn window think problem conflict usb hub fun use anymor"
  },
  {
    "label":0,
    "text":"go collag im buy car walk around campu degre weather",
    "cleaned_text":"go collag im buy car walk around campu degre weather",
    "normalized_text":"go collag im buy car walk around campu degre weather",
    "tokens":[
      "go",
      "collag",
      "im",
      "buy",
      "car",
      "walk",
      "around",
      "campu",
      "degr",
      "weather"
    ],
    "token_count":10,
    "processed_text":"go collag im buy car walk around campu degr weather"
  },
  {
    "label":4,
    "text":"nice hello final la",
    "cleaned_text":"nice hello final la",
    "normalized_text":"nice hello final la",
    "tokens":[
      "nice",
      "hello",
      "final",
      "la"
    ],
    "token_count":4,
    "processed_text":"nice hello final la"
  },
  {
    "label":0,
    "text":"thank actual good photo sayin better im suppos sort photograph",
    "cleaned_text":"thank actual good photo sayin better im suppos sort photograph",
    "normalized_text":"thank actual good photo sayin better im suppos sort photograph",
    "tokens":[
      "thank",
      "actual",
      "good",
      "photo",
      "sayin",
      "better",
      "im",
      "suppo",
      "sort",
      "photograph"
    ],
    "token_count":10,
    "processed_text":"thank actual good photo sayin better im suppo sort photograph"
  },
  {
    "label":4,
    "text":"cant get along mean doubt hug lol",
    "cleaned_text":"cant get along mean doubt hug lol",
    "normalized_text":"cant get along mean doubt hug lol",
    "tokens":[
      "cant",
      "get",
      "along",
      "mean",
      "doubt",
      "hug",
      "lol"
    ],
    "token_count":7,
    "processed_text":"cant get along mean doubt hug lol"
  },
  {
    "label":0,
    "text":"back new orlean head work hour need get sleep springfield area",
    "cleaned_text":"back new orlean head work hour need get sleep springfield area",
    "normalized_text":"back new orlean head work hour need get sleep springfield area",
    "tokens":[
      "back",
      "new",
      "orlean",
      "head",
      "work",
      "hour",
      "need",
      "get",
      "sleep",
      "springfield",
      "area"
    ],
    "token_count":11,
    "processed_text":"back new orlean head work hour need get sleep springfield area"
  },
  {
    "label":0,
    "text":"want cant",
    "cleaned_text":"want cant",
    "normalized_text":"want cant",
    "tokens":[
      "want",
      "cant"
    ],
    "token_count":2,
    "processed_text":"want cant"
  },
  {
    "label":0,
    "text":"goodby",
    "cleaned_text":"goodby",
    "normalized_text":"goodby",
    "tokens":[
      "goodbi"
    ],
    "token_count":1,
    "processed_text":"goodbi"
  },
  {
    "label":4,
    "text":"great night w boyzzzzz alway",
    "cleaned_text":"great night w boyzzzzz alway",
    "normalized_text":"great night w boyzzzzz alway",
    "tokens":[
      "great",
      "night",
      "boyzzzzz",
      "alway"
    ],
    "token_count":4,
    "processed_text":"great night boyzzzzz alway"
  },
  {
    "label":0,
    "text":"wish id known whale kommetji sooner hope sort",
    "cleaned_text":"wish id known whale kommetji sooner hope sort",
    "normalized_text":"wish id known whale kommetji sooner hope sort",
    "tokens":[
      "wish",
      "id",
      "known",
      "whale",
      "kommetji",
      "sooner",
      "hope",
      "sort"
    ],
    "token_count":8,
    "processed_text":"wish id known whale kommetji sooner hope sort"
  },
  {
    "label":4,
    "text":"never deni awesom im deni manlyhood told blow would",
    "cleaned_text":"never deni awesom im deni manlyhood told blow would",
    "normalized_text":"never deni awesom im deni manlyhood told blow would",
    "tokens":[
      "never",
      "deni",
      "awesom",
      "im",
      "deni",
      "manlyhood",
      "told",
      "blow"
    ],
    "token_count":8,
    "processed_text":"never deni awesom im deni manlyhood told blow"
  },
  {
    "label":0,
    "text":"peopl cannot read cigarett smell",
    "cleaned_text":"peopl cannot read cigarett smell",
    "normalized_text":"peopl cannot read cigarett smell",
    "tokens":[
      "peopl",
      "read",
      "cigarett",
      "smell"
    ],
    "token_count":4,
    "processed_text":"peopl read cigarett smell"
  },
  {
    "label":0,
    "text":"zooooommg tatter",
    "cleaned_text":"zooooommg tatter",
    "normalized_text":"zooooommg tatter",
    "tokens":[
      "zooooommg",
      "tatter"
    ],
    "token_count":2,
    "processed_text":"zooooommg tatter"
  },
  {
    "label":4,
    "text":"find profil nice background",
    "cleaned_text":"find profil nice background",
    "normalized_text":"find profil nice background",
    "tokens":[
      "find",
      "profil",
      "nice",
      "background"
    ],
    "token_count":4,
    "processed_text":"find profil nice background"
  },
  {
    "label":0,
    "text":"im sorri cant hang tonight work suck",
    "cleaned_text":"im sorri cant hang tonight work suck",
    "normalized_text":"im sorri cant hang tonight work suck",
    "tokens":[
      "im",
      "sorri",
      "cant",
      "hang",
      "tonight",
      "work",
      "suck"
    ],
    "token_count":7,
    "processed_text":"im sorri cant hang tonight work suck"
  },
  {
    "label":4,
    "text":"huzzah wine alway help btw cupcak nom today didnt chanc chat ya oh vell",
    "cleaned_text":"huzzah wine alway help btw cupcak nom today didnt chanc chat ya oh vell",
    "normalized_text":"huzzah wine alway help btw cupcak nom today didnt chanc chat ya oh vell",
    "tokens":[
      "huzzah",
      "wine",
      "alway",
      "help",
      "btw",
      "cupcak",
      "nom",
      "today",
      "didnt",
      "chanc",
      "chat",
      "ya",
      "oh",
      "vell"
    ],
    "token_count":14,
    "processed_text":"huzzah wine alway help btw cupcak nom today didnt chanc chat ya oh vell"
  },
  {
    "label":0,
    "text":"feel tire use",
    "cleaned_text":"feel tire use",
    "normalized_text":"feel tire use",
    "tokens":[
      "feel",
      "tire",
      "use"
    ],
    "token_count":3,
    "processed_text":"feel tire use"
  },
  {
    "label":4,
    "text":"haha celebr cookoff talkin bout gener go gen",
    "cleaned_text":"haha celebr cookoff talkin bout gener go gen",
    "normalized_text":"haha celebr cookoff talkin bout gener go gen",
    "tokens":[
      "haha",
      "celebr",
      "cookoff",
      "talkin",
      "bout",
      "gener",
      "go",
      "gen"
    ],
    "token_count":8,
    "processed_text":"haha celebr cookoff talkin bout gener go gen"
  },
  {
    "label":4,
    "text":"aah masa cuma ketemu bentar huhu want tell lot stori",
    "cleaned_text":"aah masa cuma ketemu bentar huhu want tell lot stori",
    "normalized_text":"aah masa cuma ketemu bentar huhu want tell lot stori",
    "tokens":[
      "aah",
      "masa",
      "cuma",
      "ketemu",
      "bentar",
      "huhu",
      "want",
      "tell",
      "lot",
      "stori"
    ],
    "token_count":10,
    "processed_text":"aah masa cuma ketemu bentar huhu want tell lot stori"
  },
  {
    "label":0,
    "text":"got devour mosquito even",
    "cleaned_text":"got devour mosquito even",
    "normalized_text":"got devour mosquito even",
    "tokens":[
      "got",
      "devour",
      "mosquito",
      "even"
    ],
    "token_count":4,
    "processed_text":"got devour mosquito even"
  },
  {
    "label":0,
    "text":"aww dont vomit yoohoo isnt real milk",
    "cleaned_text":"aww dont vomit yoohoo isnt real milk",
    "normalized_text":"aww dont vomit yoohoo isnt real milk",
    "tokens":[
      "aww",
      "dont",
      "vomit",
      "yoohoo",
      "isnt",
      "real",
      "milk"
    ],
    "token_count":7,
    "processed_text":"aww dont vomit yoohoo isnt real milk"
  },
  {
    "label":4,
    "text":"saw yesterday",
    "cleaned_text":"saw yesterday",
    "normalized_text":"saw yesterday",
    "tokens":[
      "saw",
      "yesterday"
    ],
    "token_count":2,
    "processed_text":"saw yesterday"
  },
  {
    "label":4,
    "text":"pass commerci insur exam",
    "cleaned_text":"pass commerci insur exam",
    "normalized_text":"pass commerci insur exam",
    "tokens":[
      "pass",
      "commerci",
      "insur",
      "exam"
    ],
    "token_count":4,
    "processed_text":"pass commerci insur exam"
  },
  {
    "label":4,
    "text":"go hs hous",
    "cleaned_text":"go hs hous",
    "normalized_text":"go hs hous",
    "tokens":[
      "go",
      "hs",
      "hou"
    ],
    "token_count":3,
    "processed_text":"go hs hou"
  },
  {
    "label":0,
    "text":"princess bride go go work realli need buy",
    "cleaned_text":"princess bride go go work realli need buy",
    "normalized_text":"princess bride go go work realli need buy",
    "tokens":[
      "princess",
      "bride",
      "go",
      "go",
      "work",
      "realli",
      "need",
      "buy"
    ],
    "token_count":8,
    "processed_text":"princess bride go go work realli need buy"
  },
  {
    "label":4,
    "text":"readi tomorrow dwt woot woot hope tuesday grove gonna see derek mark ahhhh",
    "cleaned_text":"readi tomorrow dwt woot woot hope tuesday grove gonna see derek mark ahhhh",
    "normalized_text":"readi tomorrow dwt woot woot hope tuesday grove gonna see derek mark ahhhh",
    "tokens":[
      "readi",
      "tomorrow",
      "dwt",
      "woot",
      "woot",
      "hope",
      "tuesday",
      "grove",
      "gon",
      "na",
      "see",
      "derek",
      "mark",
      "ahhhh"
    ],
    "token_count":14,
    "processed_text":"readi tomorrow dwt woot woot hope tuesday grove gon na see derek mark ahhhh"
  },
  {
    "label":0,
    "text":"rightt lol boy make sad sometim",
    "cleaned_text":"rightt lol boy make sad sometim",
    "normalized_text":"rightt lol boy make sad sometim",
    "tokens":[
      "rightt",
      "lol",
      "boy",
      "make",
      "sad",
      "sometim"
    ],
    "token_count":6,
    "processed_text":"rightt lol boy make sad sometim"
  },
  {
    "label":0,
    "text":"wasnt mexico cancel your confus cant go mexico flu crisi isnt yet",
    "cleaned_text":"wasnt mexico cancel your confus cant go mexico flu crisi isnt yet",
    "normalized_text":"wasnt mexico cancel your confus cant go mexico flu crisi isnt yet",
    "tokens":[
      "wasnt",
      "mexico",
      "cancel",
      "confu",
      "cant",
      "go",
      "mexico",
      "flu",
      "crisi",
      "isnt",
      "yet"
    ],
    "token_count":11,
    "processed_text":"wasnt mexico cancel confu cant go mexico flu crisi isnt yet"
  },
  {
    "label":0,
    "text":"ok caught contract tmobil doesnt expir januaryplu sure get atampt servic",
    "cleaned_text":"ok caught contract tmobil doesnt expir januaryplu sure get atampt servic",
    "normalized_text":"ok caught contract tmobil doesnt expir januaryplu sure get atampt servic",
    "tokens":[
      "ok",
      "caught",
      "contract",
      "tmobil",
      "doesnt",
      "expir",
      "januaryplu",
      "sure",
      "get",
      "atampt",
      "servic"
    ],
    "token_count":11,
    "processed_text":"ok caught contract tmobil doesnt expir januaryplu sure get atampt servic"
  },
  {
    "label":0,
    "text":"cute boy come work either",
    "cleaned_text":"cute boy come work either",
    "normalized_text":"cute boy come work either",
    "tokens":[
      "cute",
      "boy",
      "come",
      "work",
      "either"
    ],
    "token_count":5,
    "processed_text":"cute boy come work either"
  },
  {
    "label":4,
    "text":"week left studi god week til turn excit time",
    "cleaned_text":"week left studi god week til turn excit time",
    "normalized_text":"week left studi god week til turn excit time",
    "tokens":[
      "week",
      "left",
      "studi",
      "god",
      "week",
      "til",
      "turn",
      "excit",
      "time"
    ],
    "token_count":9,
    "processed_text":"week left studi god week til turn excit time"
  },
  {
    "label":4,
    "text":"woot way go jim btw long time talk",
    "cleaned_text":"woot way go jim btw long time talk",
    "normalized_text":"woot way go jim btw long time talk",
    "tokens":[
      "woot",
      "way",
      "go",
      "jim",
      "btw",
      "long",
      "time",
      "talk"
    ],
    "token_count":8,
    "processed_text":"woot way go jim btw long time talk"
  },
  {
    "label":4,
    "text":"perfect eek woke",
    "cleaned_text":"perfect eek woke",
    "normalized_text":"perfect eek woke",
    "tokens":[
      "perfect",
      "eek",
      "woke"
    ],
    "token_count":3,
    "processed_text":"perfect eek woke"
  },
  {
    "label":4,
    "text":"final til noodl girlss",
    "cleaned_text":"final til noodl girlss",
    "normalized_text":"final til noodl girlss",
    "tokens":[
      "final",
      "til",
      "noodl",
      "girlss"
    ],
    "token_count":4,
    "processed_text":"final til noodl girlss"
  },
  {
    "label":4,
    "text":"mani indian guy work nokia finland seem toler nostalgia cook curri food",
    "cleaned_text":"mani indian guy work nokia finland seem toler nostalgia cook curri food",
    "normalized_text":"mani indian guy work nokia finland seem toler nostalgia cook curri food",
    "tokens":[
      "mani",
      "indian",
      "guy",
      "work",
      "nokia",
      "finland",
      "seem",
      "toler",
      "nostalgia",
      "cook",
      "curri",
      "food"
    ],
    "token_count":12,
    "processed_text":"mani indian guy work nokia finland seem toler nostalgia cook curri food"
  },
  {
    "label":0,
    "text":"jeffre pleas unblock let follow wasnt nasti noth wrong ive bought u",
    "cleaned_text":"jeffre pleas unblock let follow wasnt nasti noth wrong ive bought u",
    "normalized_text":"jeffre pleas unblock let follow wasnt nasti noth wrong ive bought u",
    "tokens":[
      "jeffr",
      "plea",
      "unblock",
      "let",
      "follow",
      "wasnt",
      "nasti",
      "noth",
      "wrong",
      "ive",
      "bought"
    ],
    "token_count":11,
    "processed_text":"jeffr plea unblock let follow wasnt nasti noth wrong ive bought"
  },
  {
    "label":0,
    "text":"awwwww got rd one soon",
    "cleaned_text":"awwwww got rd one soon",
    "normalized_text":"awwwww got rd one soon",
    "tokens":[
      "awwwww",
      "got",
      "rd",
      "one",
      "soon"
    ],
    "token_count":5,
    "processed_text":"awwwww got rd one soon"
  },
  {
    "label":0,
    "text":"mayb shouldnt eaten mcdonaldsnow stomach ach",
    "cleaned_text":"mayb shouldnt eaten mcdonaldsnow stomach ach",
    "normalized_text":"mayb shouldnt eaten mcdonaldsnow stomach ach",
    "tokens":[
      "mayb",
      "shouldnt",
      "eaten",
      "mcdonaldsnow",
      "stomach",
      "ach"
    ],
    "token_count":6,
    "processed_text":"mayb shouldnt eaten mcdonaldsnow stomach ach"
  },
  {
    "label":0,
    "text":"ye pleas",
    "cleaned_text":"ye pleas",
    "normalized_text":"ye pleas",
    "tokens":[
      "ye",
      "plea"
    ],
    "token_count":2,
    "processed_text":"ye plea"
  },
  {
    "label":4,
    "text":"movi mm everyon love good chick flick",
    "cleaned_text":"movi mm everyon love good chick flick",
    "normalized_text":"movi mm everyon love good chick flick",
    "tokens":[
      "movi",
      "mm",
      "everyon",
      "love",
      "good",
      "chick",
      "flick"
    ],
    "token_count":7,
    "processed_text":"movi mm everyon love good chick flick"
  },
  {
    "label":4,
    "text":"absolut would delight see",
    "cleaned_text":"absolut would delight see",
    "normalized_text":"absolut would delight see",
    "tokens":[
      "absolut",
      "delight",
      "see"
    ],
    "token_count":3,
    "processed_text":"absolut delight see"
  },
  {
    "label":4,
    "text":"take advic im use",
    "cleaned_text":"take advic im use",
    "normalized_text":"take advic im use",
    "tokens":[
      "take",
      "advic",
      "im",
      "use"
    ],
    "token_count":4,
    "processed_text":"take advic im use"
  },
  {
    "label":4,
    "text":"leav switzerland train hour excit",
    "cleaned_text":"leav switzerland train hour excit",
    "normalized_text":"leav switzerland train hour excit",
    "tokens":[
      "leav",
      "switzerland",
      "train",
      "hour",
      "excit"
    ],
    "token_count":5,
    "processed_text":"leav switzerland train hour excit"
  },
  {
    "label":4,
    "text":"yeah wonder sinc docstogo earlymid may seem make sens wait compar amp pick best",
    "cleaned_text":"yeah wonder sinc docstogo earlymid may seem make sens wait compar amp pick best",
    "normalized_text":"yeah wonder sinc docstogo earlymid may seem make sens wait compar amp pick best",
    "tokens":[
      "yeah",
      "wonder",
      "sinc",
      "docstogo",
      "earlymid",
      "may",
      "seem",
      "make",
      "sen",
      "wait",
      "compar",
      "amp",
      "pick",
      "best"
    ],
    "token_count":14,
    "processed_text":"yeah wonder sinc docstogo earlymid may seem make sen wait compar amp pick best"
  },
  {
    "label":0,
    "text":"uhhhh ate toooooo much",
    "cleaned_text":"uhhhh ate toooooo much",
    "normalized_text":"uhhhh ate toooooo much",
    "tokens":[
      "uhhhh",
      "ate",
      "toooooo",
      "much"
    ],
    "token_count":4,
    "processed_text":"uhhhh ate toooooo much"
  },
  {
    "label":0,
    "text":"depress fuuck life",
    "cleaned_text":"depress fuuck life",
    "normalized_text":"depress fuuck life",
    "tokens":[
      "depress",
      "fuuck",
      "life"
    ],
    "token_count":3,
    "processed_text":"depress fuuck life"
  },
  {
    "label":0,
    "text":"listen live skeptic speak cant find link",
    "cleaned_text":"listen live skeptic speak cant find link",
    "normalized_text":"listen live skeptic speak cant find link",
    "tokens":[
      "listen",
      "live",
      "skeptic",
      "speak",
      "cant",
      "find",
      "link"
    ],
    "token_count":7,
    "processed_text":"listen live skeptic speak cant find link"
  },
  {
    "label":4,
    "text":"haha sayin think andi nocturn",
    "cleaned_text":"haha sayin think andi nocturn",
    "normalized_text":"haha sayin think andi nocturn",
    "tokens":[
      "haha",
      "sayin",
      "think",
      "andi",
      "nocturn"
    ],
    "token_count":5,
    "processed_text":"haha sayin think andi nocturn"
  },
  {
    "label":0,
    "text":"wish might make pritt stick glitter readyamateurish busi card come",
    "cleaned_text":"wish might make pritt stick glitter readyamateurish busi card come",
    "normalized_text":"wish might make pritt stick glitter readyamateurish busi card come",
    "tokens":[
      "wish",
      "make",
      "pritt",
      "stick",
      "glitter",
      "readyamateurish",
      "busi",
      "card",
      "come"
    ],
    "token_count":9,
    "processed_text":"wish make pritt stick glitter readyamateurish busi card come"
  },
  {
    "label":4,
    "text":"yaaaay cant wait c",
    "cleaned_text":"yaaaay cant wait c",
    "normalized_text":"yaaaay cant wait c",
    "tokens":[
      "yaaaay",
      "cant",
      "wait"
    ],
    "token_count":3,
    "processed_text":"yaaaay cant wait"
  },
  {
    "label":0,
    "text":"realli tri look situat new set eye isnt work well",
    "cleaned_text":"realli tri look situat new set eye isnt work well",
    "normalized_text":"realli tri look situat new set eye isnt work well",
    "tokens":[
      "realli",
      "tri",
      "look",
      "situat",
      "new",
      "set",
      "eye",
      "isnt",
      "work",
      "well"
    ],
    "token_count":10,
    "processed_text":"realli tri look situat new set eye isnt work well"
  },
  {
    "label":0,
    "text":"hate technolog doesnt work right frustrat beyond belief",
    "cleaned_text":"hate technolog doesnt work right frustrat beyond belief",
    "normalized_text":"hate technolog doesnt work right frustrat beyond belief",
    "tokens":[
      "hate",
      "technolog",
      "doesnt",
      "work",
      "right",
      "frustrat",
      "beyond",
      "belief"
    ],
    "token_count":8,
    "processed_text":"hate technolog doesnt work right frustrat beyond belief"
  },
  {
    "label":0,
    "text":"die need food",
    "cleaned_text":"die need food",
    "normalized_text":"die need food",
    "tokens":[
      "die",
      "need",
      "food"
    ],
    "token_count":3,
    "processed_text":"die need food"
  },
  {
    "label":4,
    "text":"ask question mate thank input",
    "cleaned_text":"ask question mate thank input",
    "normalized_text":"ask question mate thank input",
    "tokens":[
      "ask",
      "question",
      "mate",
      "thank",
      "input"
    ],
    "token_count":5,
    "processed_text":"ask question mate thank input"
  },
  {
    "label":0,
    "text":"yeah rememb im leav sea",
    "cleaned_text":"yeah rememb im leav sea",
    "normalized_text":"yeah rememb im leav sea",
    "tokens":[
      "yeah",
      "rememb",
      "im",
      "leav",
      "sea"
    ],
    "token_count":5,
    "processed_text":"yeah rememb im leav sea"
  },
  {
    "label":0,
    "text":"step away kyte week ago much twitter day long",
    "cleaned_text":"step away kyte week ago much twitter day long",
    "normalized_text":"step away kyte week ago much twitter day long",
    "tokens":[
      "step",
      "away",
      "kyte",
      "week",
      "ago",
      "much",
      "twitter",
      "day",
      "long"
    ],
    "token_count":9,
    "processed_text":"step away kyte week ago much twitter day long"
  },
  {
    "label":0,
    "text":"worri belov liverpoolfc money money money",
    "cleaned_text":"worri belov liverpoolfc money money money",
    "normalized_text":"worri belov liverpoolfc money money money",
    "tokens":[
      "worri",
      "belov",
      "liverpoolfc",
      "money",
      "money",
      "money"
    ],
    "token_count":6,
    "processed_text":"worri belov liverpoolfc money money money"
  },
  {
    "label":0,
    "text":"hate everyon asleep shop later",
    "cleaned_text":"hate everyon asleep shop later",
    "normalized_text":"hate everyon asleep shop later",
    "tokens":[
      "hate",
      "everyon",
      "asleep",
      "shop",
      "later"
    ],
    "token_count":5,
    "processed_text":"hate everyon asleep shop later"
  },
  {
    "label":0,
    "text":"eye hurt much look comput screen",
    "cleaned_text":"eye hurt much look comput screen",
    "normalized_text":"eye hurt much look comput screen",
    "tokens":[
      "eye",
      "hurt",
      "much",
      "look",
      "comput",
      "screen"
    ],
    "token_count":6,
    "processed_text":"eye hurt much look comput screen"
  },
  {
    "label":4,
    "text":"talk phone",
    "cleaned_text":"talk phone",
    "normalized_text":"talk phone",
    "tokens":[
      "talk",
      "phone"
    ],
    "token_count":2,
    "processed_text":"talk phone"
  },
  {
    "label":0,
    "text":"want stay bed",
    "cleaned_text":"want stay bed",
    "normalized_text":"want stay bed",
    "tokens":[
      "want",
      "stay",
      "bed"
    ],
    "token_count":3,
    "processed_text":"want stay bed"
  },
  {
    "label":4,
    "text":"yep bud light prelim perfect lazi put lime",
    "cleaned_text":"yep bud light prelim perfect lazi put lime",
    "normalized_text":"yep bud light prelim perfect lazi put lime",
    "tokens":[
      "yep",
      "bud",
      "light",
      "prelim",
      "perfect",
      "lazi",
      "put",
      "lime"
    ],
    "token_count":8,
    "processed_text":"yep bud light prelim perfect lazi put lime"
  },
  {
    "label":4,
    "text":"follo friday everyon follow amaz guy",
    "cleaned_text":"follo friday everyon follow amaz guy",
    "normalized_text":"follo friday everyon follow amaz guy",
    "tokens":[
      "follo",
      "friday",
      "everyon",
      "follow",
      "amaz",
      "guy"
    ],
    "token_count":6,
    "processed_text":"follo friday everyon follow amaz guy"
  },
  {
    "label":4,
    "text":"ey que te hiso el twitter",
    "cleaned_text":"ey que te hiso el twitter",
    "normalized_text":"ey que te hiso el twitter",
    "tokens":[
      "ey",
      "que",
      "te",
      "hiso",
      "el",
      "twitter"
    ],
    "token_count":6,
    "processed_text":"ey que te hiso el twitter"
  },
  {
    "label":0,
    "text":"homework arrgh",
    "cleaned_text":"homework arrgh",
    "normalized_text":"homework arrgh",
    "tokens":[
      "homework",
      "arrgh"
    ],
    "token_count":2,
    "processed_text":"homework arrgh"
  },
  {
    "label":0,
    "text":"cant wait till sunday next fridayalthough wont good thought caus gotta get earli next dayno parti",
    "cleaned_text":"cant wait till sunday next fridayalthough wont good thought caus gotta get earli next dayno parti",
    "normalized_text":"cant wait till sunday next fridayalthough wont good thought caus gotta get earli next dayno parti",
    "tokens":[
      "cant",
      "wait",
      "till",
      "sunday",
      "next",
      "fridayalthough",
      "wont",
      "good",
      "thought",
      "cau",
      "got",
      "ta",
      "get",
      "earli",
      "next",
      "dayno",
      "parti"
    ],
    "token_count":17,
    "processed_text":"cant wait till sunday next fridayalthough wont good thought cau got ta get earli next dayno parti"
  },
  {
    "label":4,
    "text":"listen say birthday today meeeeeeeeeee",
    "cleaned_text":"listen say birthday today meeeeeeeeeee",
    "normalized_text":"listen say birthday today meeeeeeeeeee",
    "tokens":[
      "listen",
      "say",
      "birthday",
      "today",
      "meeeeeeeeeee"
    ],
    "token_count":5,
    "processed_text":"listen say birthday today meeeeeeeeeee"
  },
  {
    "label":4,
    "text":"heh pretti sure alreadi",
    "cleaned_text":"heh pretti sure alreadi",
    "normalized_text":"heh pretti sure alreadi",
    "tokens":[
      "heh",
      "pretti",
      "sure",
      "alreadi"
    ],
    "token_count":4,
    "processed_text":"heh pretti sure alreadi"
  },
  {
    "label":4,
    "text":"hate monday best day",
    "cleaned_text":"hate monday best day",
    "normalized_text":"hate monday best day",
    "tokens":[
      "hate",
      "monday",
      "best",
      "day"
    ],
    "token_count":4,
    "processed_text":"hate monday best day"
  },
  {
    "label":0,
    "text":"go work tonight",
    "cleaned_text":"go work tonight",
    "normalized_text":"go work tonight",
    "tokens":[
      "go",
      "work",
      "tonight"
    ],
    "token_count":3,
    "processed_text":"go work tonight"
  },
  {
    "label":0,
    "text":"makeup hair mother natur hate tummi hurt im fall asleep raini im exhaust",
    "cleaned_text":"makeup hair mother natur hate tummi hurt im fall asleep raini im exhaust",
    "normalized_text":"makeup hair mother natur hate tummi hurt im fall asleep raini im exhaust",
    "tokens":[
      "makeup",
      "hair",
      "mother",
      "natur",
      "hate",
      "tummi",
      "hurt",
      "im",
      "fall",
      "asleep",
      "raini",
      "im",
      "exhaust"
    ],
    "token_count":13,
    "processed_text":"makeup hair mother natur hate tummi hurt im fall asleep raini im exhaust"
  },
  {
    "label":0,
    "text":"realli wish peopl wouldnt cheat contest creat mutpl acct least taylor support",
    "cleaned_text":"realli wish peopl wouldnt cheat contest creat mutpl acct least taylor support",
    "normalized_text":"realli wish peopl wouldnt cheat contest creat mutpl acct least taylor support",
    "tokens":[
      "realli",
      "wish",
      "peopl",
      "wouldnt",
      "cheat",
      "contest",
      "creat",
      "mutpl",
      "acct",
      "least",
      "taylor",
      "support"
    ],
    "token_count":12,
    "processed_text":"realli wish peopl wouldnt cheat contest creat mutpl acct least taylor support"
  },
  {
    "label":4,
    "text":"anyway aha",
    "cleaned_text":"anyway aha",
    "normalized_text":"anyway aha",
    "tokens":[
      "anyway",
      "aha"
    ],
    "token_count":2,
    "processed_text":"anyway aha"
  },
  {
    "label":4,
    "text":"get amp tell think lol time sim imagin happen real lifehmm",
    "cleaned_text":"get amp tell think lol time sim imagin happen real lifehmm",
    "normalized_text":"get amp tell think lol time sim imagin happen real lifehmm",
    "tokens":[
      "get",
      "amp",
      "tell",
      "think",
      "lol",
      "time",
      "sim",
      "imagin",
      "happen",
      "real",
      "lifehmm"
    ],
    "token_count":11,
    "processed_text":"get amp tell think lol time sim imagin happen real lifehmm"
  },
  {
    "label":4,
    "text":"got home rm visit someon special",
    "cleaned_text":"got home rm visit someon special",
    "normalized_text":"got home rm visit someon special",
    "tokens":[
      "got",
      "home",
      "rm",
      "visit",
      "someon",
      "special"
    ],
    "token_count":6,
    "processed_text":"got home rm visit someon special"
  },
  {
    "label":4,
    "text":"hey would awesom check store",
    "cleaned_text":"hey would awesom check store",
    "normalized_text":"hey would awesom check store",
    "tokens":[
      "hey",
      "awesom",
      "check",
      "store"
    ],
    "token_count":4,
    "processed_text":"hey awesom check store"
  },
  {
    "label":0,
    "text":"back work newyear holiday",
    "cleaned_text":"back work newyear holiday",
    "normalized_text":"back work newyear holiday",
    "tokens":[
      "back",
      "work",
      "newyear",
      "holiday"
    ],
    "token_count":4,
    "processed_text":"back work newyear holiday"
  },
  {
    "label":4,
    "text":"im twitter im go hospit see best friend babi boy",
    "cleaned_text":"im twitter im go hospit see best friend babi boy",
    "normalized_text":"im twitter im go hospit see best friend babi boy",
    "tokens":[
      "im",
      "twitter",
      "im",
      "go",
      "hospit",
      "see",
      "best",
      "friend",
      "babi",
      "boy"
    ],
    "token_count":10,
    "processed_text":"im twitter im go hospit see best friend babi boy"
  },
  {
    "label":4,
    "text":"wait bu cold got yogurt oreo bfast",
    "cleaned_text":"wait bu cold got yogurt oreo bfast",
    "normalized_text":"wait bu cold got yogurt oreo bfast",
    "tokens":[
      "wait",
      "bu",
      "cold",
      "got",
      "yogurt",
      "oreo",
      "bfast"
    ],
    "token_count":7,
    "processed_text":"wait bu cold got yogurt oreo bfast"
  },
  {
    "label":4,
    "text":"well yesterday drive lesson need lay goood",
    "cleaned_text":"well yesterday drive lesson need lay goood",
    "normalized_text":"well yesterday drive lesson need lay goood",
    "tokens":[
      "well",
      "yesterday",
      "drive",
      "lesson",
      "need",
      "lay",
      "goood"
    ],
    "token_count":7,
    "processed_text":"well yesterday drive lesson need lay goood"
  },
  {
    "label":0,
    "text":"ngeliat km dan fb mu sekarang bikin kesel mulu wont break coz still love bingungggg putu ga ya pt ga yaa",
    "cleaned_text":"ngeliat km dan fb mu sekarang bikin kesel mulu wont break coz still love bingungggg putu ga ya pt ga yaa",
    "normalized_text":"ngeliat km dan fb mu sekarang bikin kesel mulu wont break coz still love bingungggg putu ga ya pt ga yaa",
    "tokens":[
      "ngeliat",
      "km",
      "dan",
      "fb",
      "mu",
      "sekarang",
      "bikin",
      "kesel",
      "mulu",
      "wont",
      "break",
      "coz",
      "still",
      "love",
      "bingungggg",
      "putu",
      "ga",
      "ya",
      "pt",
      "ga",
      "yaa"
    ],
    "token_count":21,
    "processed_text":"ngeliat km dan fb mu sekarang bikin kesel mulu wont break coz still love bingungggg putu ga ya pt ga yaa"
  },
  {
    "label":4,
    "text":"that awesom stop starbuck everi countri",
    "cleaned_text":"that awesom stop starbuck everi countri",
    "normalized_text":"that awesom stop starbuck everi countri",
    "tokens":[
      "awesom",
      "stop",
      "starbuck",
      "everi",
      "countri"
    ],
    "token_count":5,
    "processed_text":"awesom stop starbuck everi countri"
  },
  {
    "label":0,
    "text":"know either",
    "cleaned_text":"know either",
    "normalized_text":"know either",
    "tokens":[
      "know",
      "either"
    ],
    "token_count":2,
    "processed_text":"know either"
  },
  {
    "label":0,
    "text":"monkey stereotyp",
    "cleaned_text":"monkey stereotyp",
    "normalized_text":"monkey stereotyp",
    "tokens":[
      "monkey",
      "stereotyp"
    ],
    "token_count":2,
    "processed_text":"monkey stereotyp"
  },
  {
    "label":0,
    "text":"immun system lead believ get sick",
    "cleaned_text":"immun system lead believ get sick",
    "normalized_text":"immun system lead believ get sick",
    "tokens":[
      "immun",
      "system",
      "lead",
      "believ",
      "get",
      "sick"
    ],
    "token_count":6,
    "processed_text":"immun system lead believ get sick"
  },
  {
    "label":0,
    "text":"im sorri hear certainli pleasant outsid boo rain",
    "cleaned_text":"im sorri hear certainli pleasant outsid boo rain",
    "normalized_text":"im sorri hear certainli pleasant outsid boo rain",
    "tokens":[
      "im",
      "sorri",
      "hear",
      "certainli",
      "pleasant",
      "outsid",
      "boo",
      "rain"
    ],
    "token_count":8,
    "processed_text":"im sorri hear certainli pleasant outsid boo rain"
  },
  {
    "label":0,
    "text":"oh poor pm p meee",
    "cleaned_text":"oh poor pm p meee",
    "normalized_text":"oh poor pm p meee",
    "tokens":[
      "oh",
      "poor",
      "pm",
      "meee"
    ],
    "token_count":4,
    "processed_text":"oh poor pm meee"
  },
  {
    "label":0,
    "text":"quotpleas wait minut vote againquot realli put damper fun thumbsup down part film youtub",
    "cleaned_text":"quotpleas wait minut vote againquot realli put damper fun thumbsup down part film youtub",
    "normalized_text":"quotpleas wait minut vote againquot realli put damper fun thumbsup down part film youtub",
    "tokens":[
      "quotplea",
      "wait",
      "minut",
      "vote",
      "againquot",
      "realli",
      "put",
      "damper",
      "fun",
      "thumbsup",
      "part",
      "film",
      "youtub"
    ],
    "token_count":13,
    "processed_text":"quotplea wait minut vote againquot realli put damper fun thumbsup part film youtub"
  },
  {
    "label":4,
    "text":"cours xoxoo",
    "cleaned_text":"cours xoxoo",
    "normalized_text":"cours xoxoo",
    "tokens":[
      "cour",
      "xoxoo"
    ],
    "token_count":2,
    "processed_text":"cour xoxoo"
  },
  {
    "label":4,
    "text":"play link",
    "cleaned_text":"play link",
    "normalized_text":"play link",
    "tokens":[
      "play",
      "link"
    ],
    "token_count":2,
    "processed_text":"play link"
  },
  {
    "label":0,
    "text":"hi miss sorri didnt come grad parti",
    "cleaned_text":"hi miss sorri didnt come grad parti",
    "normalized_text":"hi miss sorri didnt come grad parti",
    "tokens":[
      "hi",
      "miss",
      "sorri",
      "didnt",
      "come",
      "grad",
      "parti"
    ],
    "token_count":7,
    "processed_text":"hi miss sorri didnt come grad parti"
  },
  {
    "label":4,
    "text":"ye somethin slave wit man lol",
    "cleaned_text":"ye somethin slave wit man lol",
    "normalized_text":"ye somethin slave wit man lol",
    "tokens":[
      "ye",
      "somethin",
      "slave",
      "wit",
      "man",
      "lol"
    ],
    "token_count":6,
    "processed_text":"ye somethin slave wit man lol"
  },
  {
    "label":4,
    "text":"follow pastor judah sermon inspir warm hug indonesia ps your old",
    "cleaned_text":"follow pastor judah sermon inspir warm hug indonesia ps your old",
    "normalized_text":"follow pastor judah sermon inspir warm hug indonesia ps your old",
    "tokens":[
      "follow",
      "pastor",
      "judah",
      "sermon",
      "inspir",
      "warm",
      "hug",
      "indonesia",
      "ps",
      "old"
    ],
    "token_count":10,
    "processed_text":"follow pastor judah sermon inspir warm hug indonesia ps old"
  },
  {
    "label":4,
    "text":"dave buster im excit ha go longgg night",
    "cleaned_text":"dave buster im excit ha go longgg night",
    "normalized_text":"dave buster im excit ha go longgg night",
    "tokens":[
      "dave",
      "buster",
      "im",
      "excit",
      "ha",
      "go",
      "longgg",
      "night"
    ],
    "token_count":8,
    "processed_text":"dave buster im excit ha go longgg night"
  },
  {
    "label":0,
    "text":"hannah listen quotfearlessquot repeat get",
    "cleaned_text":"hannah listen quotfearlessquot repeat get",
    "normalized_text":"hannah listen quotfearlessquot repeat get",
    "tokens":[
      "hannah",
      "listen",
      "repeat",
      "get"
    ],
    "token_count":4,
    "processed_text":"hannah listen repeat get"
  },
  {
    "label":0,
    "text":"dont even know mean",
    "cleaned_text":"dont even know mean",
    "normalized_text":"dont even know mean",
    "tokens":[
      "dont",
      "even",
      "know",
      "mean"
    ],
    "token_count":4,
    "processed_text":"dont even know mean"
  },
  {
    "label":4,
    "text":"give friendli hello",
    "cleaned_text":"give friendli hello",
    "normalized_text":"give friendli hello",
    "tokens":[
      "give",
      "friendli",
      "hello"
    ],
    "token_count":3,
    "processed_text":"give friendli hello"
  },
  {
    "label":0,
    "text":"unfortun cant eat im fat diet due medic reason",
    "cleaned_text":"unfortun cant eat im fat diet due medic reason",
    "normalized_text":"unfortun cant eat im fat diet due medic reason",
    "tokens":[
      "unfortun",
      "cant",
      "eat",
      "im",
      "fat",
      "diet",
      "due",
      "medic",
      "reason"
    ],
    "token_count":9,
    "processed_text":"unfortun cant eat im fat diet due medic reason"
  },
  {
    "label":0,
    "text":"aint go duval n e",
    "cleaned_text":"aint go duval n e",
    "normalized_text":"aint go duval n e",
    "tokens":[
      "aint",
      "go",
      "duval"
    ],
    "token_count":3,
    "processed_text":"aint go duval"
  },
  {
    "label":4,
    "text":"yesss im use right",
    "cleaned_text":"yesss im use right",
    "normalized_text":"yesss im use right",
    "tokens":[
      "yesss",
      "im",
      "use",
      "right"
    ],
    "token_count":4,
    "processed_text":"yesss im use right"
  },
  {
    "label":0,
    "text":"want",
    "cleaned_text":"want",
    "normalized_text":"want",
    "tokens":[
      "want"
    ],
    "token_count":1,
    "processed_text":"want"
  },
  {
    "label":0,
    "text":"fair got stuck cameron diaz",
    "cleaned_text":"fair got stuck cameron diaz",
    "normalized_text":"fair got stuck cameron diaz",
    "tokens":[
      "fair",
      "got",
      "stuck",
      "cameron",
      "diaz"
    ],
    "token_count":5,
    "processed_text":"fair got stuck cameron diaz"
  },
  {
    "label":4,
    "text":"beef wyour idea tx bbq sauc condiment ingredi shouldnt need good stuff",
    "cleaned_text":"beef wyour idea tx bbq sauc condiment ingredi shouldnt need good stuff",
    "normalized_text":"beef wyour idea tx bbq sauc condiment ingredi shouldnt need good stuff",
    "tokens":[
      "beef",
      "wyour",
      "idea",
      "tx",
      "bbq",
      "sauc",
      "condiment",
      "ingredi",
      "shouldnt",
      "need",
      "good",
      "stuff"
    ],
    "token_count":12,
    "processed_text":"beef wyour idea tx bbq sauc condiment ingredi shouldnt need good stuff"
  },
  {
    "label":0,
    "text":"suck bout kate",
    "cleaned_text":"suck bout kate",
    "normalized_text":"suck bout kate",
    "tokens":[
      "suck",
      "bout",
      "kate"
    ],
    "token_count":3,
    "processed_text":"suck bout kate"
  },
  {
    "label":4,
    "text":"race hast park net win",
    "cleaned_text":"race hast park net win",
    "normalized_text":"race hast park net win",
    "tokens":[
      "race",
      "hast",
      "park",
      "net",
      "win"
    ],
    "token_count":5,
    "processed_text":"race hast park net win"
  },
  {
    "label":4,
    "text":"im sit burger king whopper tabl front",
    "cleaned_text":"im sit burger king whopper tabl front",
    "normalized_text":"im sit burger king whopper tabl front",
    "tokens":[
      "im",
      "sit",
      "burger",
      "king",
      "whopper",
      "tabl",
      "front"
    ],
    "token_count":7,
    "processed_text":"im sit burger king whopper tabl front"
  },
  {
    "label":0,
    "text":"guh suck see sometim go make sad aw im nearli done revamp",
    "cleaned_text":"guh suck see sometim go make sad aw im nearli done revamp",
    "normalized_text":"guh suck see sometim go make sad aw im nearli done revamp",
    "tokens":[
      "guh",
      "suck",
      "see",
      "sometim",
      "go",
      "make",
      "sad",
      "aw",
      "im",
      "nearli",
      "done",
      "revamp"
    ],
    "token_count":12,
    "processed_text":"guh suck see sometim go make sad aw im nearli done revamp"
  },
  {
    "label":4,
    "text":"tri contain excit bc niec way yay marisa lee",
    "cleaned_text":"tri contain excit bc niec way yay marisa lee",
    "normalized_text":"tri contain excit bc niec way yay marisa lee",
    "tokens":[
      "tri",
      "contain",
      "excit",
      "bc",
      "niec",
      "way",
      "yay",
      "marisa",
      "lee"
    ],
    "token_count":9,
    "processed_text":"tri contain excit bc niec way yay marisa lee"
  },
  {
    "label":0,
    "text":"joke fli bc make yum fri chickennot actual go",
    "cleaned_text":"joke fli bc make yum fri chickennot actual go",
    "normalized_text":"joke fli bc make yum fri chickennot actual go",
    "tokens":[
      "joke",
      "fli",
      "bc",
      "make",
      "yum",
      "fri",
      "chickennot",
      "actual",
      "go"
    ],
    "token_count":9,
    "processed_text":"joke fli bc make yum fri chickennot actual go"
  },
  {
    "label":4,
    "text":"lol gotta safe trip",
    "cleaned_text":"lol gotta safe trip",
    "normalized_text":"lol gotta safe trip",
    "tokens":[
      "lol",
      "got",
      "ta",
      "safe",
      "trip"
    ],
    "token_count":5,
    "processed_text":"lol got ta safe trip"
  },
  {
    "label":0,
    "text":"miss fckng alpina",
    "cleaned_text":"miss fckng alpina",
    "normalized_text":"miss fckng alpina",
    "tokens":[
      "miss",
      "fckng",
      "alpina"
    ],
    "token_count":3,
    "processed_text":"miss fckng alpina"
  },
  {
    "label":4,
    "text":"there rockstar fridg",
    "cleaned_text":"there rockstar fridg",
    "normalized_text":"there rockstar fridg",
    "tokens":[
      "rockstar",
      "fridg"
    ],
    "token_count":2,
    "processed_text":"rockstar fridg"
  },
  {
    "label":4,
    "text":"ok thank youll get one soon",
    "cleaned_text":"ok thank youll get one soon",
    "normalized_text":"ok thank youll get one soon",
    "tokens":[
      "ok",
      "thank",
      "youll",
      "get",
      "one",
      "soon"
    ],
    "token_count":6,
    "processed_text":"ok thank youll get one soon"
  },
  {
    "label":0,
    "text":"said bye alexi tri cri pass chatham plain old suck",
    "cleaned_text":"said bye alexi tri cri pass chatham plain old suck",
    "normalized_text":"said bye alexi tri cri pass chatham plain old suck",
    "tokens":[
      "said",
      "bye",
      "alexi",
      "tri",
      "cri",
      "pass",
      "chatham",
      "plain",
      "old",
      "suck"
    ],
    "token_count":10,
    "processed_text":"said bye alexi tri cri pass chatham plain old suck"
  },
  {
    "label":4,
    "text":"fun sun flint creek",
    "cleaned_text":"fun sun flint creek",
    "normalized_text":"fun sun flint creek",
    "tokens":[
      "fun",
      "sun",
      "flint",
      "creek"
    ],
    "token_count":4,
    "processed_text":"fun sun flint creek"
  },
  {
    "label":0,
    "text":"got school quiet hate want come home love x x",
    "cleaned_text":"got school quiet hate want come home love x x",
    "normalized_text":"got school quiet hate want come home love x x",
    "tokens":[
      "got",
      "school",
      "quiet",
      "hate",
      "want",
      "come",
      "home",
      "love"
    ],
    "token_count":8,
    "processed_text":"got school quiet hate want come home love"
  },
  {
    "label":0,
    "text":"tweetdeck cant get tweetdeck work",
    "cleaned_text":"tweetdeck cant get tweetdeck work",
    "normalized_text":"tweetdeck cant get tweetdeck work",
    "tokens":[
      "tweetdeck",
      "cant",
      "get",
      "tweetdeck",
      "work"
    ],
    "token_count":5,
    "processed_text":"tweetdeck cant get tweetdeck work"
  },
  {
    "label":4,
    "text":"hey taylor hate tri tri cant get sleep ilovey pleas repli itll mean world xo",
    "cleaned_text":"hey taylor hate tri tri cant get sleep ilovey pleas repli itll mean world xo",
    "normalized_text":"hey taylor hate tri tri cant get sleep ilovey pleas repli itll mean world xo",
    "tokens":[
      "hey",
      "taylor",
      "hate",
      "tri",
      "tri",
      "cant",
      "get",
      "sleep",
      "ilovey",
      "plea",
      "repli",
      "itll",
      "mean",
      "world",
      "xo"
    ],
    "token_count":15,
    "processed_text":"hey taylor hate tri tri cant get sleep ilovey plea repli itll mean world xo"
  },
  {
    "label":4,
    "text":"dont trust",
    "cleaned_text":"dont trust",
    "normalized_text":"dont trust",
    "tokens":[
      "dont",
      "trust"
    ],
    "token_count":2,
    "processed_text":"dont trust"
  },
  {
    "label":4,
    "text":"im scare think kill anim make hair",
    "cleaned_text":"im scare think kill anim make hair",
    "normalized_text":"im scare think kill anim make hair",
    "tokens":[
      "im",
      "scare",
      "think",
      "kill",
      "anim",
      "make",
      "hair"
    ],
    "token_count":7,
    "processed_text":"im scare think kill anim make hair"
  },
  {
    "label":4,
    "text":"ohhh heyi look know noww lmaoo",
    "cleaned_text":"ohhh heyi look know noww lmaoo",
    "normalized_text":"ohhh heyi look know noww lmaoo",
    "tokens":[
      "ohhh",
      "heyi",
      "look",
      "know",
      "noww",
      "lmaoo"
    ],
    "token_count":6,
    "processed_text":"ohhh heyi look know noww lmaoo"
  },
  {
    "label":4,
    "text":"watch simpson",
    "cleaned_text":"watch simpson",
    "normalized_text":"watch simpson",
    "tokens":[
      "watch",
      "simpson"
    ],
    "token_count":2,
    "processed_text":"watch simpson"
  },
  {
    "label":4,
    "text":"lift dremel rear leg time dog concentr balanc cant fight",
    "cleaned_text":"lift dremel rear leg time dog concentr balanc cant fight",
    "normalized_text":"lift dremel rear leg time dog concentr balanc cant fight",
    "tokens":[
      "lift",
      "dremel",
      "rear",
      "leg",
      "time",
      "dog",
      "concentr",
      "balanc",
      "cant",
      "fight"
    ],
    "token_count":10,
    "processed_text":"lift dremel rear leg time dog concentr balanc cant fight"
  },
  {
    "label":0,
    "text":"booo dont think ill abl make co im last cut first cut",
    "cleaned_text":"booo dont think ill abl make co im last cut first cut",
    "normalized_text":"booo dont think ill abl make co im last cut first cut",
    "tokens":[
      "booo",
      "dont",
      "think",
      "ill",
      "abl",
      "make",
      "co",
      "im",
      "last",
      "cut",
      "first",
      "cut"
    ],
    "token_count":12,
    "processed_text":"booo dont think ill abl make co im last cut first cut"
  },
  {
    "label":0,
    "text":"caus hate beer love frambois though im appar alcohol snob",
    "cleaned_text":"caus hate beer love frambois though im appar alcohol snob",
    "normalized_text":"caus hate beer love frambois though im appar alcohol snob",
    "tokens":[
      "cau",
      "hate",
      "beer",
      "love",
      "framboi",
      "though",
      "im",
      "appar",
      "alcohol",
      "snob"
    ],
    "token_count":10,
    "processed_text":"cau hate beer love framboi though im appar alcohol snob"
  },
  {
    "label":0,
    "text":"youll nearbi meeee",
    "cleaned_text":"youll nearbi meeee",
    "normalized_text":"youll nearbi meeee",
    "tokens":[
      "youll",
      "nearbi",
      "meeee"
    ],
    "token_count":3,
    "processed_text":"youll nearbi meeee"
  },
  {
    "label":4,
    "text":"sorri parti cowork well worth celebr see final cant wait action",
    "cleaned_text":"sorri parti cowork well worth celebr see final cant wait action",
    "normalized_text":"sorri parti cowork well worth celebr see final cant wait action",
    "tokens":[
      "sorri",
      "parti",
      "cowork",
      "well",
      "worth",
      "celebr",
      "see",
      "final",
      "cant",
      "wait",
      "action"
    ],
    "token_count":11,
    "processed_text":"sorri parti cowork well worth celebr see final cant wait action"
  },
  {
    "label":0,
    "text":"hate want milk",
    "cleaned_text":"hate want milk",
    "normalized_text":"hate want milk",
    "tokens":[
      "hate",
      "want",
      "milk"
    ],
    "token_count":3,
    "processed_text":"hate want milk"
  },
  {
    "label":0,
    "text":"sorri si",
    "cleaned_text":"sorri si",
    "normalized_text":"sorri si",
    "tokens":[
      "sorri",
      "si"
    ],
    "token_count":2,
    "processed_text":"sorri si"
  },
  {
    "label":0,
    "text":"yeah ehhh complic yung karma karma help how school",
    "cleaned_text":"yeah ehhh complic yung karma karma help how school",
    "normalized_text":"yeah ehhh complic yung karma karma help how school",
    "tokens":[
      "yeah",
      "ehhh",
      "complic",
      "yung",
      "karma",
      "karma",
      "help",
      "school"
    ],
    "token_count":8,
    "processed_text":"yeah ehhh complic yung karma karma help school"
  },
  {
    "label":4,
    "text":"oh ye song vid realli good",
    "cleaned_text":"oh ye song vid realli good",
    "normalized_text":"oh ye song vid realli good",
    "tokens":[
      "oh",
      "ye",
      "song",
      "vid",
      "realli",
      "good"
    ],
    "token_count":6,
    "processed_text":"oh ye song vid realli good"
  },
  {
    "label":0,
    "text":"come",
    "cleaned_text":"come",
    "normalized_text":"come",
    "tokens":[
      "come"
    ],
    "token_count":1,
    "processed_text":"come"
  },
  {
    "label":4,
    "text":"hey thanxx realli appreci yea charg tho go myspacecom price left sise",
    "cleaned_text":"hey thanxx realli appreci yea charg tho go myspacecom price left sise",
    "normalized_text":"hey thanxx realli appreci yea charg tho go myspacecom price left sise",
    "tokens":[
      "hey",
      "thanxx",
      "realli",
      "appreci",
      "yea",
      "charg",
      "tho",
      "go",
      "myspacecom",
      "price",
      "left",
      "sise"
    ],
    "token_count":12,
    "processed_text":"hey thanxx realli appreci yea charg tho go myspacecom price left sise"
  },
  {
    "label":4,
    "text":"illa still good",
    "cleaned_text":"illa still good",
    "normalized_text":"illa still good",
    "tokens":[
      "illa",
      "still",
      "good"
    ],
    "token_count":3,
    "processed_text":"illa still good"
  },
  {
    "label":4,
    "text":"aww jealou fun",
    "cleaned_text":"aww jealou fun",
    "normalized_text":"aww jealou fun",
    "tokens":[
      "aww",
      "jealou",
      "fun"
    ],
    "token_count":3,
    "processed_text":"aww jealou fun"
  },
  {
    "label":4,
    "text":"hahahahahahahah well long",
    "cleaned_text":"hahahahahahahah well long",
    "normalized_text":"hahahahahahahah well long",
    "tokens":[
      "hahahahahahahah",
      "well",
      "long"
    ],
    "token_count":3,
    "processed_text":"hahahahahahahah well long"
  },
  {
    "label":0,
    "text":"annoy hate wait",
    "cleaned_text":"annoy hate wait",
    "normalized_text":"annoy hate wait",
    "tokens":[
      "annoy",
      "hate",
      "wait"
    ],
    "token_count":3,
    "processed_text":"annoy hate wait"
  },
  {
    "label":4,
    "text":"go sleep earlier",
    "cleaned_text":"go sleep earlier",
    "normalized_text":"go sleep earlier",
    "tokens":[
      "go",
      "sleep",
      "earlier"
    ],
    "token_count":3,
    "processed_text":"go sleep earlier"
  },
  {
    "label":0,
    "text":"laundri tri hurri catch sum da game amp sad miss da launch lucian carniv somebodi buss wine",
    "cleaned_text":"laundri tri hurri catch sum da game amp sad miss da launch lucian carniv somebodi buss wine",
    "normalized_text":"laundri tri hurri catch sum da game amp sad miss da launch lucian carniv somebodi buss wine",
    "tokens":[
      "laundri",
      "tri",
      "hurri",
      "catch",
      "sum",
      "da",
      "game",
      "amp",
      "sad",
      "miss",
      "da",
      "launch",
      "lucian",
      "carniv",
      "somebodi",
      "buss",
      "wine"
    ],
    "token_count":17,
    "processed_text":"laundri tri hurri catch sum da game amp sad miss da launch lucian carniv somebodi buss wine"
  },
  {
    "label":0,
    "text":"way see nan she sick hospit",
    "cleaned_text":"way see nan she sick hospit",
    "normalized_text":"way see nan she sick hospit",
    "tokens":[
      "way",
      "see",
      "nan",
      "sick",
      "hospit"
    ],
    "token_count":5,
    "processed_text":"way see nan sick hospit"
  },
  {
    "label":4,
    "text":"insight gave amaz idea deal kid time much appreci thank hale",
    "cleaned_text":"insight gave amaz idea deal kid time much appreci thank hale",
    "normalized_text":"insight gave amaz idea deal kid time much appreci thank hale",
    "tokens":[
      "insight",
      "gave",
      "amaz",
      "idea",
      "deal",
      "kid",
      "time",
      "much",
      "appreci",
      "thank",
      "hale"
    ],
    "token_count":11,
    "processed_text":"insight gave amaz idea deal kid time much appreci thank hale"
  },
  {
    "label":0,
    "text":"happen miss hour updat",
    "cleaned_text":"happen miss hour updat",
    "normalized_text":"happen miss hour updat",
    "tokens":[
      "happen",
      "miss",
      "hour",
      "updat"
    ],
    "token_count":4,
    "processed_text":"happen miss hour updat"
  },
  {
    "label":4,
    "text":"help chant cake cake lol bd celebr need power big time help",
    "cleaned_text":"help chant cake cake lol bd celebr need power big time help",
    "normalized_text":"help chant cake cake lol bd celebr need power big time help",
    "tokens":[
      "help",
      "chant",
      "cake",
      "cake",
      "lol",
      "bd",
      "celebr",
      "need",
      "power",
      "big",
      "time",
      "help"
    ],
    "token_count":12,
    "processed_text":"help chant cake cake lol bd celebr need power big time help"
  },
  {
    "label":0,
    "text":"got mine updat copypast sideway text voic note etc make want get gs",
    "cleaned_text":"got mine updat copypast sideway text voic note etc make want get gs",
    "normalized_text":"got mine updat copypast sideway text voic note etc make want get gs",
    "tokens":[
      "got",
      "mine",
      "updat",
      "copypast",
      "sideway",
      "text",
      "voic",
      "note",
      "etc",
      "make",
      "want",
      "get",
      "gs"
    ],
    "token_count":13,
    "processed_text":"got mine updat copypast sideway text voic note etc make want get gs"
  },
  {
    "label":4,
    "text":"bu",
    "cleaned_text":"bu",
    "normalized_text":"bu",
    "tokens":[
      "bu"
    ],
    "token_count":1,
    "processed_text":"bu"
  },
  {
    "label":4,
    "text":"hope movi good",
    "cleaned_text":"hope movi good",
    "normalized_text":"hope movi good",
    "tokens":[
      "hope",
      "movi",
      "good"
    ],
    "token_count":3,
    "processed_text":"hope movi good"
  },
  {
    "label":0,
    "text":"dont like rain",
    "cleaned_text":"dont like rain",
    "normalized_text":"dont like rain",
    "tokens":[
      "dont",
      "like",
      "rain"
    ],
    "token_count":3,
    "processed_text":"dont like rain"
  },
  {
    "label":0,
    "text":"hate someon say go someth dont ugh",
    "cleaned_text":"hate someon say go someth dont ugh",
    "normalized_text":"hate someon say go someth dont ugh",
    "tokens":[
      "hate",
      "someon",
      "say",
      "go",
      "someth",
      "dont",
      "ugh"
    ],
    "token_count":7,
    "processed_text":"hate someon say go someth dont ugh"
  },
  {
    "label":0,
    "text":"hug soni send new one time fix",
    "cleaned_text":"hug soni send new one time fix",
    "normalized_text":"hug soni send new one time fix",
    "tokens":[
      "hug",
      "soni",
      "send",
      "new",
      "one",
      "time",
      "fix"
    ],
    "token_count":7,
    "processed_text":"hug soni send new one time fix"
  },
  {
    "label":4,
    "text":"member pet good day",
    "cleaned_text":"member pet good day",
    "normalized_text":"member pet good day",
    "tokens":[
      "member",
      "pet",
      "good",
      "day"
    ],
    "token_count":4,
    "processed_text":"member pet good day"
  },
  {
    "label":4,
    "text":"sound like plan",
    "cleaned_text":"sound like plan",
    "normalized_text":"sound like plan",
    "tokens":[
      "sound",
      "like",
      "plan"
    ],
    "token_count":3,
    "processed_text":"sound like plan"
  },
  {
    "label":0,
    "text":"know may fault realli wish stream werent option given demand internet connect",
    "cleaned_text":"know may fault realli wish stream werent option given demand internet connect",
    "normalized_text":"know may fault realli wish stream werent option given demand internet connect",
    "tokens":[
      "know",
      "may",
      "fault",
      "realli",
      "wish",
      "stream",
      "werent",
      "option",
      "given",
      "demand",
      "internet",
      "connect"
    ],
    "token_count":12,
    "processed_text":"know may fault realli wish stream werent option given demand internet connect"
  },
  {
    "label":0,
    "text":"rain delay wasnt depress enough read tr knight realli leav grey",
    "cleaned_text":"rain delay wasnt depress enough read tr knight realli leav grey",
    "normalized_text":"rain delay wasnt depress enough read tr knight realli leav grey",
    "tokens":[
      "rain",
      "delay",
      "wasnt",
      "depress",
      "enough",
      "read",
      "tr",
      "knight",
      "realli",
      "leav",
      "grey"
    ],
    "token_count":11,
    "processed_text":"rain delay wasnt depress enough read tr knight realli leav grey"
  },
  {
    "label":4,
    "text":"school get better near end food fight occur truth dare game rambuncti shout amaz day varelah",
    "cleaned_text":"school get better near end food fight occur truth dare game rambuncti shout amaz day varelah",
    "normalized_text":"school get better near end food fight occur truth dare game rambuncti shout amaz day varelah",
    "tokens":[
      "school",
      "get",
      "better",
      "near",
      "end",
      "food",
      "fight",
      "occur",
      "truth",
      "dare",
      "game",
      "rambuncti",
      "shout",
      "amaz",
      "day",
      "varelah"
    ],
    "token_count":16,
    "processed_text":"school get better near end food fight occur truth dare game rambuncti shout amaz day varelah"
  },
  {
    "label":4,
    "text":"oh ador song entir cd simpli gorgeou",
    "cleaned_text":"oh ador song entir cd simpli gorgeou",
    "normalized_text":"oh ador song entir cd simpli gorgeou",
    "tokens":[
      "oh",
      "ador",
      "song",
      "entir",
      "cd",
      "simpli",
      "gorgeou"
    ],
    "token_count":7,
    "processed_text":"oh ador song entir cd simpli gorgeou"
  },
  {
    "label":4,
    "text":"tri water frozen",
    "cleaned_text":"tri water frozen",
    "normalized_text":"tri water frozen",
    "tokens":[
      "tri",
      "water",
      "frozen"
    ],
    "token_count":3,
    "processed_text":"tri water frozen"
  },
  {
    "label":0,
    "text":"thought rachel maddow",
    "cleaned_text":"thought rachel maddow",
    "normalized_text":"thought rachel maddow",
    "tokens":[
      "thought",
      "rachel",
      "maddow"
    ],
    "token_count":3,
    "processed_text":"thought rachel maddow"
  },
  {
    "label":4,
    "text":"hello everyon",
    "cleaned_text":"hello everyon",
    "normalized_text":"hello everyon",
    "tokens":[
      "hello",
      "everyon"
    ],
    "token_count":2,
    "processed_text":"hello everyon"
  },
  {
    "label":0,
    "text":"rough night hope everyon ok get rest soon",
    "cleaned_text":"rough night hope everyon ok get rest soon",
    "normalized_text":"rough night hope everyon ok get rest soon",
    "tokens":[
      "rough",
      "night",
      "hope",
      "everyon",
      "ok",
      "get",
      "rest",
      "soon"
    ],
    "token_count":8,
    "processed_text":"rough night hope everyon ok get rest soon"
  },
  {
    "label":0,
    "text":"havent gotten prank call make laugh long",
    "cleaned_text":"havent gotten prank call make laugh long",
    "normalized_text":"havent gotten prank call make laugh long",
    "tokens":[
      "havent",
      "gotten",
      "prank",
      "call",
      "make",
      "laugh",
      "long"
    ],
    "token_count":7,
    "processed_text":"havent gotten prank call make laugh long"
  },
  {
    "label":0,
    "text":"there noth funni romant holocaust datenight",
    "cleaned_text":"there noth funni romant holocaust datenight",
    "normalized_text":"there noth funni romant holocaust datenight",
    "tokens":[
      "noth",
      "funni",
      "romant",
      "holocaust",
      "datenight"
    ],
    "token_count":5,
    "processed_text":"noth funni romant holocaust datenight"
  },
  {
    "label":4,
    "text":"wait see answer question watch apprentic final miss interview episod bugger",
    "cleaned_text":"wait see answer question watch apprentic final miss interview episod bugger",
    "normalized_text":"wait see answer question watch apprentic final miss interview episod bugger",
    "tokens":[
      "wait",
      "see",
      "answer",
      "question",
      "watch",
      "apprent",
      "final",
      "miss",
      "interview",
      "episod",
      "bugger"
    ],
    "token_count":11,
    "processed_text":"wait see answer question watch apprent final miss interview episod bugger"
  },
  {
    "label":4,
    "text":"idea haha",
    "cleaned_text":"idea haha",
    "normalized_text":"idea haha",
    "tokens":[
      "idea",
      "haha"
    ],
    "token_count":2,
    "processed_text":"idea haha"
  },
  {
    "label":0,
    "text":"ouch hurt finger",
    "cleaned_text":"ouch hurt finger",
    "normalized_text":"ouch hurt finger",
    "tokens":[
      "ouch",
      "hurt",
      "finger"
    ],
    "token_count":3,
    "processed_text":"ouch hurt finger"
  },
  {
    "label":0,
    "text":"bell rang lunch need toilet",
    "cleaned_text":"bell rang lunch need toilet",
    "normalized_text":"bell rang lunch need toilet",
    "tokens":[
      "bell",
      "rang",
      "lunch",
      "need",
      "toilet"
    ],
    "token_count":5,
    "processed_text":"bell rang lunch need toilet"
  },
  {
    "label":4,
    "text":"bettter tri amp get ur bday kiss",
    "cleaned_text":"bettter tri amp get ur bday kiss",
    "normalized_text":"bettter tri amp get ur bday kiss",
    "tokens":[
      "bettter",
      "tri",
      "amp",
      "get",
      "ur",
      "bday",
      "kiss"
    ],
    "token_count":7,
    "processed_text":"bettter tri amp get ur bday kiss"
  },
  {
    "label":0,
    "text":"dont know time ill yet im allow drive though",
    "cleaned_text":"dont know time ill yet im allow drive though",
    "normalized_text":"dont know time ill yet im allow drive though",
    "tokens":[
      "dont",
      "know",
      "time",
      "ill",
      "yet",
      "im",
      "allow",
      "drive",
      "though"
    ],
    "token_count":9,
    "processed_text":"dont know time ill yet im allow drive though"
  },
  {
    "label":4,
    "text":"exclus interview ss",
    "cleaned_text":"exclus interview ss",
    "normalized_text":"exclus interview ss",
    "tokens":[
      "exclu",
      "interview",
      "ss"
    ],
    "token_count":3,
    "processed_text":"exclu interview ss"
  },
  {
    "label":4,
    "text":"first int w trnr evah didnt know expect sinc tweep give great suggest like",
    "cleaned_text":"first int w trnr evah didnt know expect sinc tweep give great suggest like",
    "normalized_text":"first int w trnr evah didnt know expect sinc tweep give great suggest like",
    "tokens":[
      "first",
      "int",
      "trnr",
      "evah",
      "didnt",
      "know",
      "expect",
      "sinc",
      "tweep",
      "give",
      "great",
      "suggest",
      "like"
    ],
    "token_count":13,
    "processed_text":"first int trnr evah didnt know expect sinc tweep give great suggest like"
  },
  {
    "label":0,
    "text":"come never talk skype anymor",
    "cleaned_text":"come never talk skype anymor",
    "normalized_text":"come never talk skype anymor",
    "tokens":[
      "come",
      "never",
      "talk",
      "skype",
      "anymor"
    ],
    "token_count":5,
    "processed_text":"come never talk skype anymor"
  },
  {
    "label":4,
    "text":"love new stuff myspac layout last time dodger game brooklyn",
    "cleaned_text":"love new stuff myspac layout last time dodger game brooklyn",
    "normalized_text":"love new stuff myspac layout last time dodger game brooklyn",
    "tokens":[
      "love",
      "new",
      "stuff",
      "myspac",
      "layout",
      "last",
      "time",
      "dodger",
      "game",
      "brooklyn"
    ],
    "token_count":10,
    "processed_text":"love new stuff myspac layout last time dodger game brooklyn"
  },
  {
    "label":4,
    "text":"thank video riley love room especi wall",
    "cleaned_text":"thank video riley love room especi wall",
    "normalized_text":"thank video riley love room especi wall",
    "tokens":[
      "thank",
      "video",
      "riley",
      "love",
      "room",
      "especi",
      "wall"
    ],
    "token_count":7,
    "processed_text":"thank video riley love room especi wall"
  },
  {
    "label":0,
    "text":"noth long weekend",
    "cleaned_text":"noth long weekend",
    "normalized_text":"noth long weekend",
    "tokens":[
      "noth",
      "long",
      "weekend"
    ],
    "token_count":3,
    "processed_text":"noth long weekend"
  },
  {
    "label":0,
    "text":"well soon tear said book make shoe cloth find time read",
    "cleaned_text":"well soon tear said book make shoe cloth find time read",
    "normalized_text":"well soon tear said book make shoe cloth find time read",
    "tokens":[
      "well",
      "soon",
      "tear",
      "said",
      "book",
      "make",
      "shoe",
      "cloth",
      "find",
      "time",
      "read"
    ],
    "token_count":11,
    "processed_text":"well soon tear said book make shoe cloth find time read"
  },
  {
    "label":4,
    "text":"ok one rum hit sack vote count",
    "cleaned_text":"ok one rum hit sack vote count",
    "normalized_text":"ok one rum hit sack vote count",
    "tokens":[
      "ok",
      "one",
      "rum",
      "hit",
      "sack",
      "vote",
      "count"
    ],
    "token_count":7,
    "processed_text":"ok one rum hit sack vote count"
  },
  {
    "label":0,
    "text":"nice day belfast bbq cancel",
    "cleaned_text":"nice day belfast bbq cancel",
    "normalized_text":"nice day belfast bbq cancel",
    "tokens":[
      "nice",
      "day",
      "belfast",
      "bbq",
      "cancel"
    ],
    "token_count":5,
    "processed_text":"nice day belfast bbq cancel"
  },
  {
    "label":4,
    "text":"here demo commun wordpress theme",
    "cleaned_text":"here demo commun wordpress theme",
    "normalized_text":"here demo commun wordpress theme",
    "tokens":[
      "demo",
      "commun",
      "wordpress",
      "theme"
    ],
    "token_count":4,
    "processed_text":"demo commun wordpress theme"
  },
  {
    "label":4,
    "text":"notic chang site",
    "cleaned_text":"notic chang site",
    "normalized_text":"notic chang site",
    "tokens":[
      "notic",
      "chang",
      "site"
    ],
    "token_count":3,
    "processed_text":"notic chang site"
  },
  {
    "label":0,
    "text":"sad anoth tragedi",
    "cleaned_text":"sad anoth tragedi",
    "normalized_text":"sad anoth tragedi",
    "tokens":[
      "sad",
      "anoth",
      "tragedi"
    ],
    "token_count":3,
    "processed_text":"sad anoth tragedi"
  },
  {
    "label":0,
    "text":"dit work wait roll around bad still hour away mayb ill eat dinner instead eat brought order",
    "cleaned_text":"dit work wait roll around bad still hour away mayb ill eat dinner instead eat brought order",
    "normalized_text":"dit work wait roll around bad still hour away mayb ill eat dinner instead eat brought order",
    "tokens":[
      "dit",
      "work",
      "wait",
      "roll",
      "around",
      "bad",
      "still",
      "hour",
      "away",
      "mayb",
      "ill",
      "eat",
      "dinner",
      "instead",
      "eat",
      "brought",
      "order"
    ],
    "token_count":17,
    "processed_text":"dit work wait roll around bad still hour away mayb ill eat dinner instead eat brought order"
  },
  {
    "label":4,
    "text":"wealthi thru christ love familyand amaz soulsbless",
    "cleaned_text":"wealthi thru christ love familyand amaz soulsbless",
    "normalized_text":"wealthi thru christ love familyand amaz soulsbless",
    "tokens":[
      "wealthi",
      "thru",
      "christ",
      "love",
      "familyand",
      "amaz",
      "soulsbless"
    ],
    "token_count":7,
    "processed_text":"wealthi thru christ love familyand amaz soulsbless"
  },
  {
    "label":4,
    "text":"haha chri panget nya eh peac lol",
    "cleaned_text":"haha chri panget nya eh peac lol",
    "normalized_text":"haha chri panget nya eh peac lol",
    "tokens":[
      "haha",
      "chri",
      "panget",
      "nya",
      "eh",
      "peac",
      "lol"
    ],
    "token_count":7,
    "processed_text":"haha chri panget nya eh peac lol"
  },
  {
    "label":4,
    "text":"tom show day took ball duck rememb love thank perfect show",
    "cleaned_text":"tom show day took ball duck rememb love thank perfect show",
    "normalized_text":"tom show day took ball duck rememb love thank perfect show",
    "tokens":[
      "tom",
      "show",
      "day",
      "took",
      "ball",
      "duck",
      "rememb",
      "love",
      "thank",
      "perfect",
      "show"
    ],
    "token_count":11,
    "processed_text":"tom show day took ball duck rememb love thank perfect show"
  },
  {
    "label":4,
    "text":"thank check commun",
    "cleaned_text":"thank check commun",
    "normalized_text":"thank check commun",
    "tokens":[
      "thank",
      "check",
      "commun"
    ],
    "token_count":3,
    "processed_text":"thank check commun"
  },
  {
    "label":0,
    "text":"im enjoy gay day without im even lesbian miss",
    "cleaned_text":"im enjoy gay day without im even lesbian miss",
    "normalized_text":"im enjoy gay day without im even lesbian miss",
    "tokens":[
      "im",
      "enjoy",
      "gay",
      "day",
      "without",
      "im",
      "even",
      "lesbian",
      "miss"
    ],
    "token_count":9,
    "processed_text":"im enjoy gay day without im even lesbian miss"
  },
  {
    "label":0,
    "text":"via wow never knew bad thing mexico right would feel safe",
    "cleaned_text":"via wow never knew bad thing mexico right would feel safe",
    "normalized_text":"via wow never knew bad thing mexico right would feel safe",
    "tokens":[
      "via",
      "wow",
      "never",
      "knew",
      "bad",
      "thing",
      "mexico",
      "right",
      "feel",
      "safe"
    ],
    "token_count":10,
    "processed_text":"via wow never knew bad thing mexico right feel safe"
  },
  {
    "label":4,
    "text":"go hit beach jami cyur twitter love xoxoxo",
    "cleaned_text":"go hit beach jami cyur twitter love xoxoxo",
    "normalized_text":"go hit beach jami cyur twitter love xoxoxo",
    "tokens":[
      "go",
      "hit",
      "beach",
      "jami",
      "cyur",
      "twitter",
      "love",
      "xoxoxo"
    ],
    "token_count":8,
    "processed_text":"go hit beach jami cyur twitter love xoxoxo"
  },
  {
    "label":4,
    "text":"your welcom howd celebr",
    "cleaned_text":"your welcom howd celebr",
    "normalized_text":"your welcom howd celebr",
    "tokens":[
      "welcom",
      "howd",
      "celebr"
    ],
    "token_count":3,
    "processed_text":"welcom howd celebr"
  },
  {
    "label":0,
    "text":"think genet danc trait latina unfortun latino men",
    "cleaned_text":"think genet danc trait latina unfortun latino men",
    "normalized_text":"think genet danc trait latina unfortun latino men",
    "tokens":[
      "think",
      "genet",
      "danc",
      "trait",
      "latina",
      "unfortun",
      "latino",
      "men"
    ],
    "token_count":8,
    "processed_text":"think genet danc trait latina unfortun latino men"
  },
  {
    "label":0,
    "text":"get mt tat today im realli gonna wish best friend",
    "cleaned_text":"get mt tat today im realli gonna wish best friend",
    "normalized_text":"get mt tat today im realli gonna wish best friend",
    "tokens":[
      "get",
      "mt",
      "tat",
      "today",
      "im",
      "realli",
      "gon",
      "na",
      "wish",
      "best",
      "friend"
    ],
    "token_count":11,
    "processed_text":"get mt tat today im realli gon na wish best friend"
  },
  {
    "label":4,
    "text":"fresh princ bel air haha",
    "cleaned_text":"fresh princ bel air haha",
    "normalized_text":"fresh princ bel air haha",
    "tokens":[
      "fresh",
      "princ",
      "bel",
      "air",
      "haha"
    ],
    "token_count":5,
    "processed_text":"fresh princ bel air haha"
  },
  {
    "label":4,
    "text":"brodster tuuubee",
    "cleaned_text":"brodster tuuubee",
    "normalized_text":"brodster tuuubee",
    "tokens":[
      "brodster",
      "tuuube"
    ],
    "token_count":2,
    "processed_text":"brodster tuuube"
  },
  {
    "label":4,
    "text":"yup got tru today id play young kid amp wasnt initi enthusiast worth",
    "cleaned_text":"yup got tru today id play young kid amp wasnt initi enthusiast worth",
    "normalized_text":"yup got tru today id play young kid amp wasnt initi enthusiast worth",
    "tokens":[
      "yup",
      "got",
      "tru",
      "today",
      "id",
      "play",
      "young",
      "kid",
      "amp",
      "wasnt",
      "initi",
      "enthusiast",
      "worth"
    ],
    "token_count":13,
    "processed_text":"yup got tru today id play young kid amp wasnt initi enthusiast worth"
  },
  {
    "label":0,
    "text":"everyon go watch transform im stuck kawawah naman",
    "cleaned_text":"everyon go watch transform im stuck kawawah naman",
    "normalized_text":"everyon go watch transform im stuck kawawah naman",
    "tokens":[
      "everyon",
      "go",
      "watch",
      "transform",
      "im",
      "stuck",
      "kawawah",
      "naman"
    ],
    "token_count":8,
    "processed_text":"everyon go watch transform im stuck kawawah naman"
  },
  {
    "label":0,
    "text":"uh oh wing make nervou",
    "cleaned_text":"uh oh wing make nervou",
    "normalized_text":"uh oh wing make nervou",
    "tokens":[
      "uh",
      "oh",
      "wing",
      "make",
      "nervou"
    ],
    "token_count":5,
    "processed_text":"uh oh wing make nervou"
  },
  {
    "label":4,
    "text":"go cook spag bol look forward go pub later",
    "cleaned_text":"go cook spag bol look forward go pub later",
    "normalized_text":"go cook spag bol look forward go pub later",
    "tokens":[
      "go",
      "cook",
      "spag",
      "bol",
      "look",
      "forward",
      "go",
      "pub",
      "later"
    ],
    "token_count":9,
    "processed_text":"go cook spag bol look forward go pub later"
  },
  {
    "label":4,
    "text":"get readi kfest today",
    "cleaned_text":"get readi kfest today",
    "normalized_text":"get readi kfest today",
    "tokens":[
      "get",
      "readi",
      "kfest",
      "today"
    ],
    "token_count":4,
    "processed_text":"get readi kfest today"
  },
  {
    "label":0,
    "text":"cant sleep wish nick stay awak",
    "cleaned_text":"cant sleep wish nick stay awak",
    "normalized_text":"cant sleep wish nick stay awak",
    "tokens":[
      "cant",
      "sleep",
      "wish",
      "nick",
      "stay",
      "awak"
    ],
    "token_count":6,
    "processed_text":"cant sleep wish nick stay awak"
  },
  {
    "label":0,
    "text":"go good long swim",
    "cleaned_text":"go good long swim",
    "normalized_text":"go good long swim",
    "tokens":[
      "go",
      "good",
      "long",
      "swim"
    ],
    "token_count":4,
    "processed_text":"go good long swim"
  },
  {
    "label":0,
    "text":"cheesecak fat",
    "cleaned_text":"cheesecak fat",
    "normalized_text":"cheesecak fat",
    "tokens":[
      "cheesecak",
      "fat"
    ],
    "token_count":2,
    "processed_text":"cheesecak fat"
  },
  {
    "label":4,
    "text":"shouldnt requir think much im skool",
    "cleaned_text":"shouldnt requir think much im skool",
    "normalized_text":"shouldnt requir think much im skool",
    "tokens":[
      "shouldnt",
      "requir",
      "think",
      "much",
      "im",
      "skool"
    ],
    "token_count":6,
    "processed_text":"shouldnt requir think much im skool"
  },
  {
    "label":4,
    "text":"oh yeah home tracki dack adrian wine salmon prepar chicken simpson heater blanki edgar movi readi go meow",
    "cleaned_text":"oh yeah home tracki dack adrian wine salmon prepar chicken simpson heater blanki edgar movi readi go meow",
    "normalized_text":"oh yeah home tracki dack adrian wine salmon prepar chicken simpson heater blanki edgar movi readi go meow",
    "tokens":[
      "oh",
      "yeah",
      "home",
      "tracki",
      "dack",
      "adrian",
      "wine",
      "salmon",
      "prepar",
      "chicken",
      "simpson",
      "heater",
      "blanki",
      "edgar",
      "movi",
      "readi",
      "go",
      "meow"
    ],
    "token_count":18,
    "processed_text":"oh yeah home tracki dack adrian wine salmon prepar chicken simpson heater blanki edgar movi readi go meow"
  },
  {
    "label":4,
    "text":"thank u veryy muchh",
    "cleaned_text":"thank u veryy muchh",
    "normalized_text":"thank u veryy muchh",
    "tokens":[
      "thank",
      "veryy",
      "muchh"
    ],
    "token_count":3,
    "processed_text":"thank veryy muchh"
  },
  {
    "label":0,
    "text":"decid stop develop magento webshop rebuild quotwebpark stylequot took almost week develop get",
    "cleaned_text":"decid stop develop magento webshop rebuild quotwebpark stylequot took almost week develop get",
    "normalized_text":"decid stop develop magento webshop rebuild quotwebpark stylequot took almost week develop get",
    "tokens":[
      "decid",
      "stop",
      "develop",
      "magento",
      "webshop",
      "rebuild",
      "quotwebpark",
      "stylequot",
      "took",
      "almost",
      "week",
      "develop",
      "get"
    ],
    "token_count":13,
    "processed_text":"decid stop develop magento webshop rebuild quotwebpark stylequot took almost week develop get"
  },
  {
    "label":0,
    "text":"list tip iceberg",
    "cleaned_text":"list tip iceberg",
    "normalized_text":"list tip iceberg",
    "tokens":[
      "list",
      "tip",
      "iceberg"
    ],
    "token_count":3,
    "processed_text":"list tip iceberg"
  },
  {
    "label":4,
    "text":"greatt way end semesterr",
    "cleaned_text":"greatt way end semesterr",
    "normalized_text":"greatt way end semesterr",
    "tokens":[
      "greatt",
      "way",
      "end",
      "semesterr"
    ],
    "token_count":4,
    "processed_text":"greatt way end semesterr"
  },
  {
    "label":0,
    "text":"wish could see kathi griffin tonight",
    "cleaned_text":"wish could see kathi griffin tonight",
    "normalized_text":"wish could see kathi griffin tonight",
    "tokens":[
      "wish",
      "see",
      "kathi",
      "griffin",
      "tonight"
    ],
    "token_count":5,
    "processed_text":"wish see kathi griffin tonight"
  },
  {
    "label":0,
    "text":"ohhhhhhhh cinderella miss",
    "cleaned_text":"ohhhhhhhh cinderella miss",
    "normalized_text":"ohhhhhhhh cinderella miss",
    "tokens":[
      "ohhhhhhhh",
      "cinderella",
      "miss"
    ],
    "token_count":3,
    "processed_text":"ohhhhhhhh cinderella miss"
  },
  {
    "label":0,
    "text":"im sorri new moon trailer crap theyr realli bad actor imo sound dead compar book",
    "cleaned_text":"im sorri new moon trailer crap theyr realli bad actor imo sound dead compar book",
    "normalized_text":"im sorri new moon trailer crap theyr realli bad actor imo sound dead compar book",
    "tokens":[
      "im",
      "sorri",
      "new",
      "moon",
      "trailer",
      "crap",
      "theyr",
      "realli",
      "bad",
      "actor",
      "imo",
      "sound",
      "dead",
      "compar",
      "book"
    ],
    "token_count":15,
    "processed_text":"im sorri new moon trailer crap theyr realli bad actor imo sound dead compar book"
  },
  {
    "label":0,
    "text":"whoaon sale hour ticket partnerup sold appar im go",
    "cleaned_text":"whoaon sale hour ticket partnerup sold appar im go",
    "normalized_text":"whoaon sale hour ticket partnerup sold appar im go",
    "tokens":[
      "whoaon",
      "sale",
      "hour",
      "ticket",
      "partnerup",
      "sold",
      "appar",
      "im",
      "go"
    ],
    "token_count":9,
    "processed_text":"whoaon sale hour ticket partnerup sold appar im go"
  },
  {
    "label":0,
    "text":"realli hope loco stop pose said mani peopl",
    "cleaned_text":"realli hope loco stop pose said mani peopl",
    "normalized_text":"realli hope loco stop pose said mani peopl",
    "tokens":[
      "realli",
      "hope",
      "loco",
      "stop",
      "pose",
      "said",
      "mani",
      "peopl"
    ],
    "token_count":8,
    "processed_text":"realli hope loco stop pose said mani peopl"
  },
  {
    "label":0,
    "text":"thought team",
    "cleaned_text":"thought team",
    "normalized_text":"thought team",
    "tokens":[
      "thought",
      "team"
    ],
    "token_count":2,
    "processed_text":"thought team"
  },
  {
    "label":4,
    "text":"boght greenampblack oliv amp feta chees flagey market mmmm delici",
    "cleaned_text":"boght greenampblack oliv amp feta chees flagey market mmmm delici",
    "normalized_text":"boght greenampblack oliv amp feta chees flagey market mmmm delici",
    "tokens":[
      "boght",
      "greenampblack",
      "oliv",
      "amp",
      "feta",
      "chee",
      "flagey",
      "market",
      "mmmm",
      "delici"
    ],
    "token_count":10,
    "processed_text":"boght greenampblack oliv amp feta chee flagey market mmmm delici"
  },
  {
    "label":4,
    "text":"ill stomach hurt head throat day till birthday hope get better soon",
    "cleaned_text":"ill stomach hurt head throat day till birthday hope get better soon",
    "normalized_text":"ill stomach hurt head throat day till birthday hope get better soon",
    "tokens":[
      "ill",
      "stomach",
      "hurt",
      "head",
      "throat",
      "day",
      "till",
      "birthday",
      "hope",
      "get",
      "better",
      "soon"
    ],
    "token_count":12,
    "processed_text":"ill stomach hurt head throat day till birthday hope get better soon"
  },
  {
    "label":4,
    "text":"train today super power blind sunshin r u brown desir woke nap dayb",
    "cleaned_text":"train today super power blind sunshin r u brown desir woke nap dayb",
    "normalized_text":"train today super power blind sunshin r u brown desir woke nap dayb",
    "tokens":[
      "train",
      "today",
      "super",
      "power",
      "blind",
      "sunshin",
      "brown",
      "desir",
      "woke",
      "nap",
      "dayb"
    ],
    "token_count":11,
    "processed_text":"train today super power blind sunshin brown desir woke nap dayb"
  },
  {
    "label":0,
    "text":"done gay art evalu onto geogrpahi",
    "cleaned_text":"done gay art evalu onto geogrpahi",
    "normalized_text":"done gay art evalu onto geogrpahi",
    "tokens":[
      "done",
      "gay",
      "art",
      "evalu",
      "onto",
      "geogrpahi"
    ],
    "token_count":6,
    "processed_text":"done gay art evalu onto geogrpahi"
  },
  {
    "label":4,
    "text":"look like draw love order day",
    "cleaned_text":"look like draw love order day",
    "normalized_text":"look like draw love order day",
    "tokens":[
      "look",
      "like",
      "draw",
      "love",
      "order",
      "day"
    ],
    "token_count":6,
    "processed_text":"look like draw love order day"
  },
  {
    "label":0,
    "text":"dun get hug feel left",
    "cleaned_text":"dun get hug feel left",
    "normalized_text":"dun get hug feel left",
    "tokens":[
      "dun",
      "get",
      "hug",
      "feel",
      "left"
    ],
    "token_count":5,
    "processed_text":"dun get hug feel left"
  },
  {
    "label":4,
    "text":"abl sleep nightmar fun though dean winchest wasnt bad",
    "cleaned_text":"abl sleep nightmar fun though dean winchest wasnt bad",
    "normalized_text":"abl sleep nightmar fun though dean winchest wasnt bad",
    "tokens":[
      "abl",
      "sleep",
      "nightmar",
      "fun",
      "though",
      "dean",
      "winchest",
      "wasnt",
      "bad"
    ],
    "token_count":9,
    "processed_text":"abl sleep nightmar fun though dean winchest wasnt bad"
  },
  {
    "label":0,
    "text":"stop frown blobfish",
    "cleaned_text":"stop frown blobfish",
    "normalized_text":"stop frown blobfish",
    "tokens":[
      "stop",
      "frown",
      "blobfish"
    ],
    "token_count":3,
    "processed_text":"stop frown blobfish"
  },
  {
    "label":4,
    "text":"ok ask firstrecord matchstalk men matchstalk cat dog brian michael",
    "cleaned_text":"ok ask firstrecord matchstalk men matchstalk cat dog brian michael",
    "normalized_text":"ok ask firstrecord matchstalk men matchstalk cat dog brian michael",
    "tokens":[
      "ok",
      "ask",
      "firstrecord",
      "matchstalk",
      "men",
      "matchstalk",
      "cat",
      "dog",
      "brian",
      "michael"
    ],
    "token_count":10,
    "processed_text":"ok ask firstrecord matchstalk men matchstalk cat dog brian michael"
  },
  {
    "label":4,
    "text":"school day",
    "cleaned_text":"school day",
    "normalized_text":"school day",
    "tokens":[
      "school",
      "day"
    ],
    "token_count":2,
    "processed_text":"school day"
  },
  {
    "label":4,
    "text":"alway want tri send recip yummi hope summer go well",
    "cleaned_text":"alway want tri send recip yummi hope summer go well",
    "normalized_text":"alway want tri send recip yummi hope summer go well",
    "tokens":[
      "alway",
      "want",
      "tri",
      "send",
      "recip",
      "yummi",
      "hope",
      "summer",
      "go",
      "well"
    ],
    "token_count":10,
    "processed_text":"alway want tri send recip yummi hope summer go well"
  },
  {
    "label":4,
    "text":"good luck xxx",
    "cleaned_text":"good luck xxx",
    "normalized_text":"good luck xxx",
    "tokens":[
      "good",
      "luck",
      "xxx"
    ],
    "token_count":3,
    "processed_text":"good luck xxx"
  },
  {
    "label":0,
    "text":"order camera dont know im go get time san francisco",
    "cleaned_text":"order camera dont know im go get time san francisco",
    "normalized_text":"order camera dont know im go get time san francisco",
    "tokens":[
      "order",
      "camera",
      "dont",
      "know",
      "im",
      "go",
      "get",
      "time",
      "san",
      "francisco"
    ],
    "token_count":10,
    "processed_text":"order camera dont know im go get time san francisco"
  },
  {
    "label":4,
    "text":"even go respond",
    "cleaned_text":"even go respond",
    "normalized_text":"even go respond",
    "tokens":[
      "even",
      "go",
      "respond"
    ],
    "token_count":3,
    "processed_text":"even go respond"
  },
  {
    "label":0,
    "text":"doubl date w bf im gonna throw",
    "cleaned_text":"doubl date w bf im gonna throw",
    "normalized_text":"doubl date w bf im gonna throw",
    "tokens":[
      "doubl",
      "date",
      "bf",
      "im",
      "gon",
      "na",
      "throw"
    ],
    "token_count":7,
    "processed_text":"doubl date bf im gon na throw"
  },
  {
    "label":0,
    "text":"oh wish could see tna live",
    "cleaned_text":"oh wish could see tna live",
    "normalized_text":"oh wish could see tna live",
    "tokens":[
      "oh",
      "wish",
      "see",
      "tna",
      "live"
    ],
    "token_count":5,
    "processed_text":"oh wish see tna live"
  },
  {
    "label":4,
    "text":"hihi blast church today youth day today take church want want fun",
    "cleaned_text":"hihi blast church today youth day today take church want want fun",
    "normalized_text":"hihi blast church today youth day today take church want want fun",
    "tokens":[
      "hihi",
      "blast",
      "church",
      "today",
      "youth",
      "day",
      "today",
      "take",
      "church",
      "want",
      "want",
      "fun"
    ],
    "token_count":12,
    "processed_text":"hihi blast church today youth day today take church want want fun"
  },
  {
    "label":0,
    "text":"yay u start work still got",
    "cleaned_text":"yay u start work still got",
    "normalized_text":"yay u start work still got",
    "tokens":[
      "yay",
      "start",
      "work",
      "still",
      "got"
    ],
    "token_count":5,
    "processed_text":"yay start work still got"
  },
  {
    "label":4,
    "text":"wasss suppos sleepin caus didnt go badd gotta get snuggli",
    "cleaned_text":"wasss suppos sleepin caus didnt go badd gotta get snuggli",
    "normalized_text":"wasss suppos sleepin caus didnt go badd gotta get snuggli",
    "tokens":[
      "wasss",
      "suppo",
      "sleepin",
      "cau",
      "didnt",
      "go",
      "badd",
      "got",
      "ta",
      "get",
      "snuggli"
    ],
    "token_count":11,
    "processed_text":"wasss suppo sleepin cau didnt go badd got ta get snuggli"
  },
  {
    "label":0,
    "text":"feel refresh gonna hit sack bit weather hot weekend soony",
    "cleaned_text":"feel refresh gonna hit sack bit weather hot weekend soony",
    "normalized_text":"feel refresh gonna hit sack bit weather hot weekend soony",
    "tokens":[
      "feel",
      "refresh",
      "gon",
      "na",
      "hit",
      "sack",
      "bit",
      "weather",
      "hot",
      "weekend",
      "sooni"
    ],
    "token_count":11,
    "processed_text":"feel refresh gon na hit sack bit weather hot weekend sooni"
  },
  {
    "label":4,
    "text":"woke felt good need whitne wore last night",
    "cleaned_text":"woke felt good need whitne wore last night",
    "normalized_text":"woke felt good need whitne wore last night",
    "tokens":[
      "woke",
      "felt",
      "good",
      "need",
      "whitn",
      "wore",
      "last",
      "night"
    ],
    "token_count":8,
    "processed_text":"woke felt good need whitn wore last night"
  },
  {
    "label":4,
    "text":"that hurrah ever one hope enjoy",
    "cleaned_text":"that hurrah ever one hope enjoy",
    "normalized_text":"that hurrah ever one hope enjoy",
    "tokens":[
      "hurrah",
      "ever",
      "one",
      "hope",
      "enjoy"
    ],
    "token_count":5,
    "processed_text":"hurrah ever one hope enjoy"
  },
  {
    "label":4,
    "text":"subscrib thank subscrib your inspir continu im",
    "cleaned_text":"subscrib thank subscrib your inspir continu im",
    "normalized_text":"subscrib thank subscrib your inspir continu im",
    "tokens":[
      "subscrib",
      "thank",
      "subscrib",
      "inspir",
      "continu",
      "im"
    ],
    "token_count":6,
    "processed_text":"subscrib thank subscrib inspir continu im"
  },
  {
    "label":0,
    "text":"blake done miranda amaz lee ann reba next",
    "cleaned_text":"blake done miranda amaz lee ann reba next",
    "normalized_text":"blake done miranda amaz lee ann reba next",
    "tokens":[
      "blake",
      "done",
      "miranda",
      "amaz",
      "lee",
      "ann",
      "reba",
      "next"
    ],
    "token_count":8,
    "processed_text":"blake done miranda amaz lee ann reba next"
  },
  {
    "label":0,
    "text":"weather affect moodno sun til tue two sunni degre day mid month didnt move somewher warmer",
    "cleaned_text":"weather affect moodno sun til tue two sunni degre day mid month didnt move somewher warmer",
    "normalized_text":"weather affect moodno sun til tue two sunni degre day mid month didnt move somewher warmer",
    "tokens":[
      "weather",
      "affect",
      "moodno",
      "sun",
      "til",
      "tue",
      "two",
      "sunni",
      "degr",
      "day",
      "mid",
      "month",
      "didnt",
      "move",
      "somewh",
      "warmer"
    ],
    "token_count":16,
    "processed_text":"weather affect moodno sun til tue two sunni degr day mid month didnt move somewh warmer"
  },
  {
    "label":4,
    "text":"got done walk hazz hoursgo tan",
    "cleaned_text":"got done walk hazz hoursgo tan",
    "normalized_text":"got done walk hazz hoursgo tan",
    "tokens":[
      "got",
      "done",
      "walk",
      "hazz",
      "hoursgo",
      "tan"
    ],
    "token_count":6,
    "processed_text":"got done walk hazz hoursgo tan"
  },
  {
    "label":4,
    "text":"amaz homemad breakfast prepar ohsodomest gf bitch cook lol",
    "cleaned_text":"amaz homemad breakfast prepar ohsodomest gf bitch cook lol",
    "normalized_text":"amaz homemad breakfast prepar ohsodomest gf bitch cook lol",
    "tokens":[
      "amaz",
      "homemad",
      "breakfast",
      "prepar",
      "ohsodomest",
      "gf",
      "bitch",
      "cook",
      "lol"
    ],
    "token_count":9,
    "processed_text":"amaz homemad breakfast prepar ohsodomest gf bitch cook lol"
  },
  {
    "label":4,
    "text":"also inbetween class",
    "cleaned_text":"also inbetween class",
    "normalized_text":"also inbetween class",
    "tokens":[
      "also",
      "inbetween",
      "class"
    ],
    "token_count":3,
    "processed_text":"also inbetween class"
  },
  {
    "label":0,
    "text":"aquarium pacif boston cook steak veggi biscuit mash potato chubbi run day irvin",
    "cleaned_text":"aquarium pacif boston cook steak veggi biscuit mash potato chubbi run day irvin",
    "normalized_text":"aquarium pacif boston cook steak veggi biscuit mash potato chubbi run day irvin",
    "tokens":[
      "aquarium",
      "pacif",
      "boston",
      "cook",
      "steak",
      "veggi",
      "biscuit",
      "mash",
      "potato",
      "chubbi",
      "run",
      "day",
      "irvin"
    ],
    "token_count":13,
    "processed_text":"aquarium pacif boston cook steak veggi biscuit mash potato chubbi run day irvin"
  },
  {
    "label":0,
    "text":"hate happen could give anoth name",
    "cleaned_text":"hate happen could give anoth name",
    "normalized_text":"hate happen could give anoth name",
    "tokens":[
      "hate",
      "happen",
      "give",
      "anoth",
      "name"
    ],
    "token_count":5,
    "processed_text":"hate happen give anoth name"
  },
  {
    "label":4,
    "text":"start support group im sit corner listen black sabbath wonder",
    "cleaned_text":"start support group im sit corner listen black sabbath wonder",
    "normalized_text":"start support group im sit corner listen black sabbath wonder",
    "tokens":[
      "start",
      "support",
      "group",
      "im",
      "sit",
      "corner",
      "listen",
      "black",
      "sabbath",
      "wonder"
    ],
    "token_count":10,
    "processed_text":"start support group im sit corner listen black sabbath wonder"
  },
  {
    "label":0,
    "text":"paranoia disappoint set everi time read tweet",
    "cleaned_text":"paranoia disappoint set everi time read tweet",
    "normalized_text":"paranoia disappoint set everi time read tweet",
    "tokens":[
      "paranoia",
      "disappoint",
      "set",
      "everi",
      "time",
      "read",
      "tweet"
    ],
    "token_count":7,
    "processed_text":"paranoia disappoint set everi time read tweet"
  },
  {
    "label":4,
    "text":"ate weight homemad browni batter tonight sold one textbook today",
    "cleaned_text":"ate weight homemad browni batter tonight sold one textbook today",
    "normalized_text":"ate weight homemad browni batter tonight sold one textbook today",
    "tokens":[
      "ate",
      "weight",
      "homemad",
      "browni",
      "batter",
      "tonight",
      "sold",
      "one",
      "textbook",
      "today"
    ],
    "token_count":10,
    "processed_text":"ate weight homemad browni batter tonight sold one textbook today"
  },
  {
    "label":0,
    "text":"ahhhgh still tri finish project wish could swallow swollen gland set",
    "cleaned_text":"ahhhgh still tri finish project wish could swallow swollen gland set",
    "normalized_text":"ahhhgh still tri finish project wish could swallow swollen gland set",
    "tokens":[
      "ahhhgh",
      "still",
      "tri",
      "finish",
      "project",
      "wish",
      "swallow",
      "swollen",
      "gland",
      "set"
    ],
    "token_count":10,
    "processed_text":"ahhhgh still tri finish project wish swallow swollen gland set"
  },
  {
    "label":4,
    "text":"good morn everyon im breakfast right yaay mc muffin without ham",
    "cleaned_text":"good morn everyon im breakfast right yaay mc muffin without ham",
    "normalized_text":"good morn everyon im breakfast right yaay mc muffin without ham",
    "tokens":[
      "good",
      "morn",
      "everyon",
      "im",
      "breakfast",
      "right",
      "yaay",
      "mc",
      "muffin",
      "without",
      "ham"
    ],
    "token_count":11,
    "processed_text":"good morn everyon im breakfast right yaay mc muffin without ham"
  },
  {
    "label":4,
    "text":"neighbor deliv greet messag receiv great hear degre separ",
    "cleaned_text":"neighbor deliv greet messag receiv great hear degre separ",
    "normalized_text":"neighbor deliv greet messag receiv great hear degre separ",
    "tokens":[
      "neighbor",
      "deliv",
      "greet",
      "messag",
      "receiv",
      "great",
      "hear",
      "degr",
      "separ"
    ],
    "token_count":9,
    "processed_text":"neighbor deliv greet messag receiv great hear degr separ"
  },
  {
    "label":0,
    "text":"super tiredand forgot ipod home",
    "cleaned_text":"super tiredand forgot ipod home",
    "normalized_text":"super tiredand forgot ipod home",
    "tokens":[
      "super",
      "tiredand",
      "forgot",
      "ipod",
      "home"
    ],
    "token_count":5,
    "processed_text":"super tiredand forgot ipod home"
  },
  {
    "label":4,
    "text":"never turn hot fudg sunda even heistheebest",
    "cleaned_text":"never turn hot fudg sunda even heistheebest",
    "normalized_text":"never turn hot fudg sunda even heistheebest",
    "tokens":[
      "never",
      "turn",
      "hot",
      "fudg",
      "sunda",
      "even",
      "heistheebest"
    ],
    "token_count":7,
    "processed_text":"never turn hot fudg sunda even heistheebest"
  },
  {
    "label":4,
    "text":"how pet",
    "cleaned_text":"how pet",
    "normalized_text":"how pet",
    "tokens":[
      "pet"
    ],
    "token_count":1,
    "processed_text":"pet"
  },
  {
    "label":0,
    "text":"math revis good",
    "cleaned_text":"math revis good",
    "normalized_text":"math revis good",
    "tokens":[
      "math",
      "revi",
      "good"
    ],
    "token_count":3,
    "processed_text":"math revi good"
  },
  {
    "label":4,
    "text":"twilight crew look amaz mtv award proud lt",
    "cleaned_text":"twilight crew look amaz mtv award proud lt",
    "normalized_text":"twilight crew look amaz mtv award proud lt",
    "tokens":[
      "twilight",
      "crew",
      "look",
      "amaz",
      "mtv",
      "award",
      "proud",
      "lt"
    ],
    "token_count":8,
    "processed_text":"twilight crew look amaz mtv award proud lt"
  },
  {
    "label":4,
    "text":"ta muchli thing pass wish painkil would actual work give knit frog",
    "cleaned_text":"ta muchli thing pass wish painkil would actual work give knit frog",
    "normalized_text":"ta muchli thing pass wish painkil would actual work give knit frog",
    "tokens":[
      "ta",
      "muchli",
      "thing",
      "pass",
      "wish",
      "painkil",
      "actual",
      "work",
      "give",
      "knit",
      "frog"
    ],
    "token_count":11,
    "processed_text":"ta muchli thing pass wish painkil actual work give knit frog"
  },
  {
    "label":0,
    "text":"weeeee work next thursday well unless count thing like wash clean walk dogsdamn",
    "cleaned_text":"weeeee work next thursday well unless count thing like wash clean walk dogsdamn",
    "normalized_text":"weeeee work next thursday well unless count thing like wash clean walk dogsdamn",
    "tokens":[
      "weeeee",
      "work",
      "next",
      "thursday",
      "well",
      "unless",
      "count",
      "thing",
      "like",
      "wash",
      "clean",
      "walk",
      "dogsdamn"
    ],
    "token_count":13,
    "processed_text":"weeeee work next thursday well unless count thing like wash clean walk dogsdamn"
  },
  {
    "label":4,
    "text":"aw thank rose shape beauti like mute shade im go look shop heart coupl",
    "cleaned_text":"aw thank rose shape beauti like mute shade im go look shop heart coupl",
    "normalized_text":"aw thank rose shape beauti like mute shade im go look shop heart coupl",
    "tokens":[
      "aw",
      "thank",
      "rose",
      "shape",
      "beauti",
      "like",
      "mute",
      "shade",
      "im",
      "go",
      "look",
      "shop",
      "heart",
      "coupl"
    ],
    "token_count":14,
    "processed_text":"aw thank rose shape beauti like mute shade im go look shop heart coupl"
  },
  {
    "label":4,
    "text":"id agre statement regard lenka defend fiest fellow calgarian",
    "cleaned_text":"id agre statement regard lenka defend fiest fellow calgarian",
    "normalized_text":"id agre statement regard lenka defend fiest fellow calgarian",
    "tokens":[
      "id",
      "agr",
      "statement",
      "regard",
      "lenka",
      "defend",
      "fiest",
      "fellow",
      "calgarian"
    ],
    "token_count":9,
    "processed_text":"id agr statement regard lenka defend fiest fellow calgarian"
  },
  {
    "label":0,
    "text":"cousin tata",
    "cleaned_text":"cousin tata",
    "normalized_text":"cousin tata",
    "tokens":[
      "cousin",
      "tata"
    ],
    "token_count":2,
    "processed_text":"cousin tata"
  },
  {
    "label":0,
    "text":"miss forgot last meet day",
    "cleaned_text":"miss forgot last meet day",
    "normalized_text":"miss forgot last meet day",
    "tokens":[
      "miss",
      "forgot",
      "last",
      "meet",
      "day"
    ],
    "token_count":5,
    "processed_text":"miss forgot last meet day"
  },
  {
    "label":4,
    "text":"im sure wash machin portug",
    "cleaned_text":"im sure wash machin portug",
    "normalized_text":"im sure wash machin portug",
    "tokens":[
      "im",
      "sure",
      "wash",
      "machin",
      "portug"
    ],
    "token_count":5,
    "processed_text":"im sure wash machin portug"
  },
  {
    "label":4,
    "text":"musicmonday air jordin spark",
    "cleaned_text":"musicmonday air jordin spark",
    "normalized_text":"musicmonday air jordin spark",
    "tokens":[
      "musicmonday",
      "air",
      "jordin",
      "spark"
    ],
    "token_count":4,
    "processed_text":"musicmonday air jordin spark"
  },
  {
    "label":0,
    "text":"finnish wash night court dui still winless damn time sleep tomorrow morn",
    "cleaned_text":"finnish wash night court dui still winless damn time sleep tomorrow morn",
    "normalized_text":"finnish wash night court dui still winless damn time sleep tomorrow morn",
    "tokens":[
      "finnish",
      "wash",
      "night",
      "court",
      "dui",
      "still",
      "winless",
      "damn",
      "time",
      "sleep",
      "tomorrow",
      "morn"
    ],
    "token_count":12,
    "processed_text":"finnish wash night court dui still winless damn time sleep tomorrow morn"
  },
  {
    "label":0,
    "text":"garden work agre websit",
    "cleaned_text":"garden work agre websit",
    "normalized_text":"garden work agre websit",
    "tokens":[
      "garden",
      "work",
      "agr",
      "websit"
    ],
    "token_count":4,
    "processed_text":"garden work agr websit"
  },
  {
    "label":0,
    "text":"cant find eminem cd bum cuz realli need blast right",
    "cleaned_text":"cant find eminem cd bum cuz realli need blast right",
    "normalized_text":"cant find eminem cd bum cuz realli need blast right",
    "tokens":[
      "cant",
      "find",
      "eminem",
      "cd",
      "bum",
      "cuz",
      "realli",
      "need",
      "blast",
      "right"
    ],
    "token_count":10,
    "processed_text":"cant find eminem cd bum cuz realli need blast right"
  },
  {
    "label":4,
    "text":"good morn god bless unto",
    "cleaned_text":"good morn god bless unto",
    "normalized_text":"good morn god bless unto",
    "tokens":[
      "good",
      "morn",
      "god",
      "bless",
      "unto"
    ],
    "token_count":5,
    "processed_text":"good morn god bless unto"
  },
  {
    "label":4,
    "text":"lolz thought couldnt rember awwww goood tho",
    "cleaned_text":"lolz thought couldnt rember awwww goood tho",
    "normalized_text":"lolz thought couldnt rember awwww goood tho",
    "tokens":[
      "lolz",
      "thought",
      "couldnt",
      "rember",
      "awwww",
      "goood",
      "tho"
    ],
    "token_count":7,
    "processed_text":"lolz thought couldnt rember awwww goood tho"
  },
  {
    "label":4,
    "text":"cant wait",
    "cleaned_text":"cant wait",
    "normalized_text":"cant wait",
    "tokens":[
      "cant",
      "wait"
    ],
    "token_count":2,
    "processed_text":"cant wait"
  },
  {
    "label":4,
    "text":"morn afternoon even might meet eve",
    "cleaned_text":"morn afternoon even might meet eve",
    "normalized_text":"morn afternoon even might meet eve",
    "tokens":[
      "morn",
      "afternoon",
      "even",
      "meet",
      "eve"
    ],
    "token_count":5,
    "processed_text":"morn afternoon even meet eve"
  },
  {
    "label":0,
    "text":"tri add say block updat",
    "cleaned_text":"tri add say block updat",
    "normalized_text":"tri add say block updat",
    "tokens":[
      "tri",
      "add",
      "say",
      "block",
      "updat"
    ],
    "token_count":5,
    "processed_text":"tri add say block updat"
  },
  {
    "label":0,
    "text":"dont even get right nowso confus sad",
    "cleaned_text":"dont even get right nowso confus sad",
    "normalized_text":"dont even get right nowso confus sad",
    "tokens":[
      "dont",
      "even",
      "get",
      "right",
      "nowso",
      "confu",
      "sad"
    ],
    "token_count":7,
    "processed_text":"dont even get right nowso confu sad"
  },
  {
    "label":0,
    "text":"awww could drunk twitter play fuck marri kill lol",
    "cleaned_text":"awww could drunk twitter play fuck marri kill lol",
    "normalized_text":"awww could drunk twitter play fuck marri kill lol",
    "tokens":[
      "awww",
      "drunk",
      "twitter",
      "play",
      "fuck",
      "marri",
      "kill",
      "lol"
    ],
    "token_count":8,
    "processed_text":"awww drunk twitter play fuck marri kill lol"
  },
  {
    "label":0,
    "text":"omg woke min late imposs find ticket seat open date realli go sale",
    "cleaned_text":"omg woke min late imposs find ticket seat open date realli go sale",
    "normalized_text":"omg woke min late imposs find ticket seat open date realli go sale",
    "tokens":[
      "omg",
      "woke",
      "min",
      "late",
      "imposs",
      "find",
      "ticket",
      "seat",
      "open",
      "date",
      "realli",
      "go",
      "sale"
    ],
    "token_count":13,
    "processed_text":"omg woke min late imposs find ticket seat open date realli go sale"
  },
  {
    "label":4,
    "text":"went fb wasnt bother check notic",
    "cleaned_text":"went fb wasnt bother check notic",
    "normalized_text":"went fb wasnt bother check notic",
    "tokens":[
      "went",
      "fb",
      "wasnt",
      "bother",
      "check",
      "notic"
    ],
    "token_count":6,
    "processed_text":"went fb wasnt bother check notic"
  },
  {
    "label":0,
    "text":"cant think new combo",
    "cleaned_text":"cant think new combo",
    "normalized_text":"cant think new combo",
    "tokens":[
      "cant",
      "think",
      "new",
      "combo"
    ],
    "token_count":4,
    "processed_text":"cant think new combo"
  },
  {
    "label":4,
    "text":"drink arden apart",
    "cleaned_text":"drink arden apart",
    "normalized_text":"drink arden apart",
    "tokens":[
      "drink",
      "arden",
      "apart"
    ],
    "token_count":3,
    "processed_text":"drink arden apart"
  },
  {
    "label":4,
    "text":"look like everyth get better wher",
    "cleaned_text":"look like everyth get better wher",
    "normalized_text":"look like everyth get better wher",
    "tokens":[
      "look",
      "like",
      "everyth",
      "get",
      "better",
      "wher"
    ],
    "token_count":6,
    "processed_text":"look like everyth get better wher"
  },
  {
    "label":0,
    "text":"slam finger car dor fml ow",
    "cleaned_text":"slam finger car dor fml ow",
    "normalized_text":"slam finger car dor fml ow",
    "tokens":[
      "slam",
      "finger",
      "car",
      "dor",
      "fml",
      "ow"
    ],
    "token_count":6,
    "processed_text":"slam finger car dor fml ow"
  },
  {
    "label":4,
    "text":"ohh hey youv got new pic onlin cool like summertim",
    "cleaned_text":"ohh hey youv got new pic onlin cool like summertim",
    "normalized_text":"ohh hey youv got new pic onlin cool like summertim",
    "tokens":[
      "ohh",
      "hey",
      "youv",
      "got",
      "new",
      "pic",
      "onlin",
      "cool",
      "like",
      "summertim"
    ],
    "token_count":10,
    "processed_text":"ohh hey youv got new pic onlin cool like summertim"
  },
  {
    "label":4,
    "text":"awwww that great u love",
    "cleaned_text":"awwww that great u love",
    "normalized_text":"awwww that great u love",
    "tokens":[
      "awwww",
      "great",
      "love"
    ],
    "token_count":3,
    "processed_text":"awwww great love"
  },
  {
    "label":0,
    "text":"last day high school everrrrr",
    "cleaned_text":"last day high school everrrrr",
    "normalized_text":"last day high school everrrrr",
    "tokens":[
      "last",
      "day",
      "high",
      "school",
      "everrrrr"
    ],
    "token_count":5,
    "processed_text":"last day high school everrrrr"
  },
  {
    "label":0,
    "text":"josh made feel like shit",
    "cleaned_text":"josh made feel like shit",
    "normalized_text":"josh made feel like shit",
    "tokens":[
      "josh",
      "made",
      "feel",
      "like",
      "shit"
    ],
    "token_count":5,
    "processed_text":"josh made feel like shit"
  },
  {
    "label":4,
    "text":"bad homi ima find lol",
    "cleaned_text":"bad homi ima find lol",
    "normalized_text":"bad homi ima find lol",
    "tokens":[
      "bad",
      "homi",
      "ima",
      "find",
      "lol"
    ],
    "token_count":5,
    "processed_text":"bad homi ima find lol"
  },
  {
    "label":0,
    "text":"cough hurt bad today",
    "cleaned_text":"cough hurt bad today",
    "normalized_text":"cough hurt bad today",
    "tokens":[
      "cough",
      "hurt",
      "bad",
      "today"
    ],
    "token_count":4,
    "processed_text":"cough hurt bad today"
  },
  {
    "label":4,
    "text":"amaz hug world come soon godspe",
    "cleaned_text":"amaz hug world come soon godspe",
    "normalized_text":"amaz hug world come soon godspe",
    "tokens":[
      "amaz",
      "hug",
      "world",
      "come",
      "soon",
      "godsp"
    ],
    "token_count":6,
    "processed_text":"amaz hug world come soon godsp"
  },
  {
    "label":0,
    "text":"thank suggest didnt work sadli think temporari twitterwid issu",
    "cleaned_text":"thank suggest didnt work sadli think temporari twitterwid issu",
    "normalized_text":"thank suggest didnt work sadli think temporari twitterwid issu",
    "tokens":[
      "thank",
      "suggest",
      "didnt",
      "work",
      "sadli",
      "think",
      "temporari",
      "twitterwid",
      "issu"
    ],
    "token_count":9,
    "processed_text":"thank suggest didnt work sadli think temporari twitterwid issu"
  },
  {
    "label":4,
    "text":"todaywa good day",
    "cleaned_text":"todaywa good day",
    "normalized_text":"todaywa good day",
    "tokens":[
      "todaywa",
      "good",
      "day"
    ],
    "token_count":3,
    "processed_text":"todaywa good day"
  },
  {
    "label":4,
    "text":"luxuri read fashion magazin leisur sunday afternoon",
    "cleaned_text":"luxuri read fashion magazin leisur sunday afternoon",
    "normalized_text":"luxuri read fashion magazin leisur sunday afternoon",
    "tokens":[
      "luxuri",
      "read",
      "fashion",
      "magazin",
      "leisur",
      "sunday",
      "afternoon"
    ],
    "token_count":7,
    "processed_text":"luxuri read fashion magazin leisur sunday afternoon"
  },
  {
    "label":0,
    "text":"yeah mum wont explain lge nea cnsbe sgot p dw aq ng sgot e ngeexplain ln nmn aq",
    "cleaned_text":"yeah mum wont explain lge nea cnsbe sgot p dw aq ng sgot e ngeexplain ln nmn aq",
    "normalized_text":"yeah mum wont explain lge nea cnsbe sgot p dw aq ng sgot e ngeexplain ln nmn aq",
    "tokens":[
      "yeah",
      "mum",
      "wont",
      "explain",
      "lge",
      "nea",
      "cnsbe",
      "sgot",
      "dw",
      "aq",
      "ng",
      "sgot",
      "ngeexplain",
      "ln",
      "nmn",
      "aq"
    ],
    "token_count":16,
    "processed_text":"yeah mum wont explain lge nea cnsbe sgot dw aq ng sgot ngeexplain ln nmn aq"
  },
  {
    "label":4,
    "text":"call cthulu",
    "cleaned_text":"call cthulu",
    "normalized_text":"call cthulu",
    "tokens":[
      "call",
      "cthulu"
    ],
    "token_count":2,
    "processed_text":"call cthulu"
  },
  {
    "label":4,
    "text":"call passion cook here free ebook bunch cool recip",
    "cleaned_text":"call passion cook here free ebook bunch cool recip",
    "normalized_text":"call passion cook here free ebook bunch cool recip",
    "tokens":[
      "call",
      "passion",
      "cook",
      "free",
      "ebook",
      "bunch",
      "cool",
      "recip"
    ],
    "token_count":8,
    "processed_text":"call passion cook free ebook bunch cool recip"
  },
  {
    "label":4,
    "text":"good morn im work shop today awhil main street buda texa come see buy antiqu furnitur",
    "cleaned_text":"good morn im work shop today awhil main street buda texa come see buy antiqu furnitur",
    "normalized_text":"good morn im work shop today awhil main street buda texa come see buy antiqu furnitur",
    "tokens":[
      "good",
      "morn",
      "im",
      "work",
      "shop",
      "today",
      "awhil",
      "main",
      "street",
      "buda",
      "texa",
      "come",
      "see",
      "buy",
      "antiqu",
      "furnitur"
    ],
    "token_count":16,
    "processed_text":"good morn im work shop today awhil main street buda texa come see buy antiqu furnitur"
  },
  {
    "label":4,
    "text":"well let know start rais flag march im sure peopl gather behind",
    "cleaned_text":"well let know start rais flag march im sure peopl gather behind",
    "normalized_text":"well let know start rais flag march im sure peopl gather behind",
    "tokens":[
      "well",
      "let",
      "know",
      "start",
      "rai",
      "flag",
      "march",
      "im",
      "sure",
      "peopl",
      "gather",
      "behind"
    ],
    "token_count":12,
    "processed_text":"well let know start rai flag march im sure peopl gather behind"
  },
  {
    "label":0,
    "text":"damn pinch punch st month didnt even realis",
    "cleaned_text":"damn pinch punch st month didnt even realis",
    "normalized_text":"damn pinch punch st month didnt even realis",
    "tokens":[
      "damn",
      "pinch",
      "punch",
      "st",
      "month",
      "didnt",
      "even",
      "reali"
    ],
    "token_count":8,
    "processed_text":"damn pinch punch st month didnt even reali"
  },
  {
    "label":4,
    "text":"final gettin epiphani album ye still like cd exit like momma use let buy new cd sam goodi",
    "cleaned_text":"final gettin epiphani album ye still like cd exit like momma use let buy new cd sam goodi",
    "normalized_text":"final gettin epiphani album ye still like cd exit like momma use let buy new cd sam goodi",
    "tokens":[
      "final",
      "gettin",
      "epiphani",
      "album",
      "ye",
      "still",
      "like",
      "cd",
      "exit",
      "like",
      "momma",
      "use",
      "let",
      "buy",
      "new",
      "cd",
      "sam",
      "goodi"
    ],
    "token_count":18,
    "processed_text":"final gettin epiphani album ye still like cd exit like momma use let buy new cd sam goodi"
  },
  {
    "label":4,
    "text":"your idol love brodie",
    "cleaned_text":"your idol love brodie",
    "normalized_text":"your idol love brodie",
    "tokens":[
      "idol",
      "love",
      "brodi"
    ],
    "token_count":3,
    "processed_text":"idol love brodi"
  },
  {
    "label":0,
    "text":"ate much late hour need get school work done whod thought would still say",
    "cleaned_text":"ate much late hour need get school work done whod thought would still say",
    "normalized_text":"ate much late hour need get school work done whod thought would still say",
    "tokens":[
      "ate",
      "much",
      "late",
      "hour",
      "need",
      "get",
      "school",
      "work",
      "done",
      "whod",
      "thought",
      "still",
      "say"
    ],
    "token_count":13,
    "processed_text":"ate much late hour need get school work done whod thought still say"
  },
  {
    "label":0,
    "text":"vatican processu contra templario say much trial templar",
    "cleaned_text":"vatican processu contra templario say much trial templar",
    "normalized_text":"vatican processu contra templario say much trial templar",
    "tokens":[
      "vatican",
      "processu",
      "contra",
      "templario",
      "say",
      "much",
      "trial",
      "templar"
    ],
    "token_count":8,
    "processed_text":"vatican processu contra templario say much trial templar"
  },
  {
    "label":4,
    "text":"reason homework nao",
    "cleaned_text":"reason homework nao",
    "normalized_text":"reason homework nao",
    "tokens":[
      "reason",
      "homework",
      "nao"
    ],
    "token_count":3,
    "processed_text":"reason homework nao"
  },
  {
    "label":0,
    "text":"drive sinc im stop train poo",
    "cleaned_text":"drive sinc im stop train poo",
    "normalized_text":"drive sinc im stop train poo",
    "tokens":[
      "drive",
      "sinc",
      "im",
      "stop",
      "train",
      "poo"
    ],
    "token_count":6,
    "processed_text":"drive sinc im stop train poo"
  },
  {
    "label":4,
    "text":"new blog entri quotmov zazzlequot welcom hood",
    "cleaned_text":"new blog entri quotmov zazzlequot welcom hood",
    "normalized_text":"new blog entri quotmov zazzlequot welcom hood",
    "tokens":[
      "new",
      "blog",
      "entri",
      "quotmov",
      "zazzlequot",
      "welcom",
      "hood"
    ],
    "token_count":7,
    "processed_text":"new blog entri quotmov zazzlequot welcom hood"
  },
  {
    "label":4,
    "text":"hahahaha fast time good movi",
    "cleaned_text":"hahahaha fast time good movi",
    "normalized_text":"hahahaha fast time good movi",
    "tokens":[
      "hahahaha",
      "fast",
      "time",
      "good",
      "movi"
    ],
    "token_count":5,
    "processed_text":"hahahaha fast time good movi"
  },
  {
    "label":0,
    "text":"oh feel angri think im piss offyep im piss think im piss close friend hmyeah oh",
    "cleaned_text":"oh feel angri think im piss offyep im piss think im piss close friend hmyeah oh",
    "normalized_text":"oh feel angri think im piss offyep im piss think im piss close friend hmyeah oh",
    "tokens":[
      "oh",
      "feel",
      "angri",
      "think",
      "im",
      "piss",
      "offyep",
      "im",
      "piss",
      "think",
      "im",
      "piss",
      "close",
      "friend",
      "hmyeah",
      "oh"
    ],
    "token_count":16,
    "processed_text":"oh feel angri think im piss offyep im piss think im piss close friend hmyeah oh"
  },
  {
    "label":4,
    "text":"bye glad your weve tweet hr gotta hw",
    "cleaned_text":"bye glad your weve tweet hr gotta hw",
    "normalized_text":"bye glad your weve tweet hr gotta hw",
    "tokens":[
      "bye",
      "glad",
      "weve",
      "tweet",
      "hr",
      "got",
      "ta",
      "hw"
    ],
    "token_count":8,
    "processed_text":"bye glad weve tweet hr got ta hw"
  },
  {
    "label":0,
    "text":"cant wait till get sleep till noon later havent slept long time miss tire suck",
    "cleaned_text":"cant wait till get sleep till noon later havent slept long time miss tire suck",
    "normalized_text":"cant wait till get sleep till noon later havent slept long time miss tire suck",
    "tokens":[
      "cant",
      "wait",
      "till",
      "get",
      "sleep",
      "till",
      "noon",
      "later",
      "havent",
      "slept",
      "long",
      "time",
      "miss",
      "tire",
      "suck"
    ],
    "token_count":15,
    "processed_text":"cant wait till get sleep till noon later havent slept long time miss tire suck"
  },
  {
    "label":4,
    "text":"aww cuti way back",
    "cleaned_text":"aww cuti way back",
    "normalized_text":"aww cuti way back",
    "tokens":[
      "aww",
      "cuti",
      "way",
      "back"
    ],
    "token_count":4,
    "processed_text":"aww cuti way back"
  },
  {
    "label":0,
    "text":"watch transform avec littl brother cheap date easi pleas didnt give crap megan fox though",
    "cleaned_text":"watch transform avec littl brother cheap date easi pleas didnt give crap megan fox though",
    "normalized_text":"watch transform avec littl brother cheap date easi pleas didnt give crap megan fox though",
    "tokens":[
      "watch",
      "transform",
      "avec",
      "littl",
      "brother",
      "cheap",
      "date",
      "easi",
      "plea",
      "didnt",
      "give",
      "crap",
      "megan",
      "fox",
      "though"
    ],
    "token_count":15,
    "processed_text":"watch transform avec littl brother cheap date easi plea didnt give crap megan fox though"
  },
  {
    "label":0,
    "text":"tri look guy end couldnt find",
    "cleaned_text":"tri look guy end couldnt find",
    "normalized_text":"tri look guy end couldnt find",
    "tokens":[
      "tri",
      "look",
      "guy",
      "end",
      "couldnt",
      "find"
    ],
    "token_count":6,
    "processed_text":"tri look guy end couldnt find"
  },
  {
    "label":0,
    "text":"wtf maci set firework hudson year rooftop bk view bc fing jersey get year",
    "cleaned_text":"wtf maci set firework hudson year rooftop bk view bc fing jersey get year",
    "normalized_text":"wtf maci set firework hudson year rooftop bk view bc fing jersey get year",
    "tokens":[
      "wtf",
      "maci",
      "set",
      "firework",
      "hudson",
      "year",
      "rooftop",
      "bk",
      "view",
      "bc",
      "fing",
      "jersey",
      "get",
      "year"
    ],
    "token_count":14,
    "processed_text":"wtf maci set firework hudson year rooftop bk view bc fing jersey get year"
  },
  {
    "label":0,
    "text":"time pass realli quickli dont get chanc realis mani peopl weve left behind",
    "cleaned_text":"time pass realli quickli dont get chanc realis mani peopl weve left behind",
    "normalized_text":"time pass realli quickli dont get chanc realis mani peopl weve left behind",
    "tokens":[
      "time",
      "pass",
      "realli",
      "quickli",
      "dont",
      "get",
      "chanc",
      "reali",
      "mani",
      "peopl",
      "weve",
      "left",
      "behind"
    ],
    "token_count":13,
    "processed_text":"time pass realli quickli dont get chanc reali mani peopl weve left behind"
  },
  {
    "label":0,
    "text":"haha still havent seen twilight cannot roleplay il watch later il join lmao privat twitter",
    "cleaned_text":"haha still havent seen twilight cannot roleplay il watch later il join lmao privat twitter",
    "normalized_text":"haha still havent seen twilight cannot roleplay il watch later il join lmao privat twitter",
    "tokens":[
      "haha",
      "still",
      "havent",
      "seen",
      "twilight",
      "roleplay",
      "il",
      "watch",
      "later",
      "il",
      "join",
      "lmao",
      "privat",
      "twitter"
    ],
    "token_count":14,
    "processed_text":"haha still havent seen twilight roleplay il watch later il join lmao privat twitter"
  },
  {
    "label":0,
    "text":"reason twitextiez dumb",
    "cleaned_text":"reason twitextiez dumb",
    "normalized_text":"reason twitextiez dumb",
    "tokens":[
      "reason",
      "twitextiez",
      "dumb"
    ],
    "token_count":3,
    "processed_text":"reason twitextiez dumb"
  },
  {
    "label":0,
    "text":"omg know ruin weekend sure want",
    "cleaned_text":"omg know ruin weekend sure want",
    "normalized_text":"omg know ruin weekend sure want",
    "tokens":[
      "omg",
      "know",
      "ruin",
      "weekend",
      "sure",
      "want"
    ],
    "token_count":6,
    "processed_text":"omg know ruin weekend sure want"
  },
  {
    "label":0,
    "text":"wanna go home play infam",
    "cleaned_text":"wanna go home play infam",
    "normalized_text":"wanna go home play infam",
    "tokens":[
      "wan",
      "na",
      "go",
      "home",
      "play",
      "infam"
    ],
    "token_count":6,
    "processed_text":"wan na go home play infam"
  },
  {
    "label":0,
    "text":"ethic test today didnt even sleep night cant stay awak",
    "cleaned_text":"ethic test today didnt even sleep night cant stay awak",
    "normalized_text":"ethic test today didnt even sleep night cant stay awak",
    "tokens":[
      "ethic",
      "test",
      "today",
      "didnt",
      "even",
      "sleep",
      "night",
      "cant",
      "stay",
      "awak"
    ],
    "token_count":10,
    "processed_text":"ethic test today didnt even sleep night cant stay awak"
  },
  {
    "label":4,
    "text":"play big surf island ive unlock trophi alreadi hah",
    "cleaned_text":"play big surf island ive unlock trophi alreadi hah",
    "normalized_text":"play big surf island ive unlock trophi alreadi hah",
    "tokens":[
      "play",
      "big",
      "surf",
      "island",
      "ive",
      "unlock",
      "trophi",
      "alreadi",
      "hah"
    ],
    "token_count":9,
    "processed_text":"play big surf island ive unlock trophi alreadi hah"
  },
  {
    "label":0,
    "text":"watch night museum tri spend much time possibl tomorrow",
    "cleaned_text":"watch night museum tri spend much time possibl tomorrow",
    "normalized_text":"watch night museum tri spend much time possibl tomorrow",
    "tokens":[
      "watch",
      "night",
      "museum",
      "tri",
      "spend",
      "much",
      "time",
      "possibl",
      "tomorrow"
    ],
    "token_count":9,
    "processed_text":"watch night museum tri spend much time possibl tomorrow"
  },
  {
    "label":4,
    "text":"hehe ben sunbath hour half alreadi get readi go park who come",
    "cleaned_text":"hehe ben sunbath hour half alreadi get readi go park who come",
    "normalized_text":"hehe ben sunbath hour half alreadi get readi go park who come",
    "tokens":[
      "hehe",
      "ben",
      "sunbath",
      "hour",
      "half",
      "alreadi",
      "get",
      "readi",
      "go",
      "park",
      "come"
    ],
    "token_count":11,
    "processed_text":"hehe ben sunbath hour half alreadi get readi go park come"
  },
  {
    "label":4,
    "text":"follow caus she loner follow dp realli",
    "cleaned_text":"follow caus she loner follow dp realli",
    "normalized_text":"follow caus she loner follow dp realli",
    "tokens":[
      "follow",
      "cau",
      "loner",
      "follow",
      "dp",
      "realli"
    ],
    "token_count":6,
    "processed_text":"follow cau loner follow dp realli"
  },
  {
    "label":0,
    "text":"hell yea fuck fake ass promot dont think make th",
    "cleaned_text":"hell yea fuck fake ass promot dont think make th",
    "normalized_text":"hell yea fuck fake ass promot dont think make th",
    "tokens":[
      "hell",
      "yea",
      "fuck",
      "fake",
      "ass",
      "promot",
      "dont",
      "think",
      "make",
      "th"
    ],
    "token_count":10,
    "processed_text":"hell yea fuck fake ass promot dont think make th"
  },
  {
    "label":0,
    "text":"how diabet test go glimps difficulti yesterday",
    "cleaned_text":"how diabet test go glimps difficulti yesterday",
    "normalized_text":"how diabet test go glimps difficulti yesterday",
    "tokens":[
      "diabet",
      "test",
      "go",
      "glimp",
      "difficulti",
      "yesterday"
    ],
    "token_count":6,
    "processed_text":"diabet test go glimp difficulti yesterday"
  },
  {
    "label":0,
    "text":"sunday father day gonna hard one first one without dad pass april still havent accept",
    "cleaned_text":"sunday father day gonna hard one first one without dad pass april still havent accept",
    "normalized_text":"sunday father day gonna hard one first one without dad pass april still havent accept",
    "tokens":[
      "sunday",
      "father",
      "day",
      "gon",
      "na",
      "hard",
      "one",
      "first",
      "one",
      "without",
      "dad",
      "pass",
      "april",
      "still",
      "havent",
      "accept"
    ],
    "token_count":16,
    "processed_text":"sunday father day gon na hard one first one without dad pass april still havent accept"
  },
  {
    "label":4,
    "text":"yay melbourn would nice holiday spot hard workinghard studi gal",
    "cleaned_text":"yay melbourn would nice holiday spot hard workinghard studi gal",
    "normalized_text":"yay melbourn would nice holiday spot hard workinghard studi gal",
    "tokens":[
      "yay",
      "melbourn",
      "nice",
      "holiday",
      "spot",
      "hard",
      "workinghard",
      "studi",
      "gal"
    ],
    "token_count":9,
    "processed_text":"yay melbourn nice holiday spot hard workinghard studi gal"
  },
  {
    "label":0,
    "text":"oh weather suck didnt want thought would high low instead cold windi amp rain",
    "cleaned_text":"oh weather suck didnt want thought would high low instead cold windi amp rain",
    "normalized_text":"oh weather suck didnt want thought would high low instead cold windi amp rain",
    "tokens":[
      "oh",
      "weather",
      "suck",
      "didnt",
      "want",
      "thought",
      "high",
      "low",
      "instead",
      "cold",
      "windi",
      "amp",
      "rain"
    ],
    "token_count":13,
    "processed_text":"oh weather suck didnt want thought high low instead cold windi amp rain"
  },
  {
    "label":0,
    "text":"oh wow forgot tonight last jay leno show",
    "cleaned_text":"oh wow forgot tonight last jay leno show",
    "normalized_text":"oh wow forgot tonight last jay leno show",
    "tokens":[
      "oh",
      "wow",
      "forgot",
      "tonight",
      "last",
      "jay",
      "leno",
      "show"
    ],
    "token_count":8,
    "processed_text":"oh wow forgot tonight last jay leno show"
  },
  {
    "label":0,
    "text":"counti allow pay ticket onlin conveni inconveni speed month ago though",
    "cleaned_text":"counti allow pay ticket onlin conveni inconveni speed month ago though",
    "normalized_text":"counti allow pay ticket onlin conveni inconveni speed month ago though",
    "tokens":[
      "counti",
      "allow",
      "pay",
      "ticket",
      "onlin",
      "conveni",
      "inconveni",
      "speed",
      "month",
      "ago",
      "though"
    ],
    "token_count":11,
    "processed_text":"counti allow pay ticket onlin conveni inconveni speed month ago though"
  },
  {
    "label":0,
    "text":"broke qbert record",
    "cleaned_text":"broke qbert record",
    "normalized_text":"broke qbert record",
    "tokens":[
      "broke",
      "qbert",
      "record"
    ],
    "token_count":3,
    "processed_text":"broke qbert record"
  },
  {
    "label":0,
    "text":"got hit wigger thug lesbian last night proceed yell quotwoo look butt jigglequot walk away insult",
    "cleaned_text":"got hit wigger thug lesbian last night proceed yell quotwoo look butt jigglequot walk away insult",
    "normalized_text":"got hit wigger thug lesbian last night proceed yell quotwoo look butt jigglequot walk away insult",
    "tokens":[
      "got",
      "hit",
      "wigger",
      "thug",
      "lesbian",
      "last",
      "night",
      "proceed",
      "yell",
      "quotwoo",
      "look",
      "butt",
      "jigglequot",
      "walk",
      "away",
      "insult"
    ],
    "token_count":16,
    "processed_text":"got hit wigger thug lesbian last night proceed yell quotwoo look butt jigglequot walk away insult"
  },
  {
    "label":4,
    "text":"cours crazi unregul heat winter ac summer gotta love hundredsyearold build",
    "cleaned_text":"cours crazi unregul heat winter ac summer gotta love hundredsyearold build",
    "normalized_text":"cours crazi unregul heat winter ac summer gotta love hundredsyearold build",
    "tokens":[
      "cour",
      "crazi",
      "unregul",
      "heat",
      "winter",
      "ac",
      "summer",
      "got",
      "ta",
      "love",
      "hundredsyearold",
      "build"
    ],
    "token_count":12,
    "processed_text":"cour crazi unregul heat winter ac summer got ta love hundredsyearold build"
  },
  {
    "label":0,
    "text":"ah shit singlecor init becom often window box cpu die",
    "cleaned_text":"ah shit singlecor init becom often window box cpu die",
    "normalized_text":"ah shit singlecor init becom often window box cpu die",
    "tokens":[
      "ah",
      "shit",
      "singlecor",
      "init",
      "becom",
      "often",
      "window",
      "box",
      "cpu",
      "die"
    ],
    "token_count":10,
    "processed_text":"ah shit singlecor init becom often window box cpu die"
  },
  {
    "label":0,
    "text":"sale ae depress around dark gloomi cloud cube",
    "cleaned_text":"sale ae depress around dark gloomi cloud cube",
    "normalized_text":"sale ae depress around dark gloomi cloud cube",
    "tokens":[
      "sale",
      "ae",
      "depress",
      "around",
      "dark",
      "gloomi",
      "cloud",
      "cube"
    ],
    "token_count":8,
    "processed_text":"sale ae depress around dark gloomi cloud cube"
  },
  {
    "label":4,
    "text":"woo home schooltweet later thoughjust go shower",
    "cleaned_text":"woo home schooltweet later thoughjust go shower",
    "normalized_text":"woo home schooltweet later thoughjust go shower",
    "tokens":[
      "woo",
      "home",
      "schooltweet",
      "later",
      "thoughjust",
      "go",
      "shower"
    ],
    "token_count":7,
    "processed_text":"woo home schooltweet later thoughjust go shower"
  },
  {
    "label":4,
    "text":"hey welcom twitter",
    "cleaned_text":"hey welcom twitter",
    "normalized_text":"hey welcom twitter",
    "tokens":[
      "hey",
      "welcom",
      "twitter"
    ],
    "token_count":3,
    "processed_text":"hey welcom twitter"
  },
  {
    "label":0,
    "text":"super bum fun littl right came tide came didnt bring board lesson learn",
    "cleaned_text":"super bum fun littl right came tide came didnt bring board lesson learn",
    "normalized_text":"super bum fun littl right came tide came didnt bring board lesson learn",
    "tokens":[
      "super",
      "bum",
      "fun",
      "littl",
      "right",
      "came",
      "tide",
      "came",
      "didnt",
      "bring",
      "board",
      "lesson",
      "learn"
    ],
    "token_count":13,
    "processed_text":"super bum fun littl right came tide came didnt bring board lesson learn"
  },
  {
    "label":0,
    "text":"ignor tweet saw ugghhh guy slimi sorri",
    "cleaned_text":"ignor tweet saw ugghhh guy slimi sorri",
    "normalized_text":"ignor tweet saw ugghhh guy slimi sorri",
    "tokens":[
      "ignor",
      "tweet",
      "saw",
      "ugghhh",
      "guy",
      "slimi",
      "sorri"
    ],
    "token_count":7,
    "processed_text":"ignor tweet saw ugghhh guy slimi sorri"
  },
  {
    "label":0,
    "text":"mission control inform incom weather front develop nowher",
    "cleaned_text":"mission control inform incom weather front develop nowher",
    "normalized_text":"mission control inform incom weather front develop nowher",
    "tokens":[
      "mission",
      "control",
      "inform",
      "incom",
      "weather",
      "front",
      "develop",
      "nowher"
    ],
    "token_count":8,
    "processed_text":"mission control inform incom weather front develop nowher"
  },
  {
    "label":4,
    "text":"coffe go school late",
    "cleaned_text":"coffe go school late",
    "normalized_text":"coffe go school late",
    "tokens":[
      "coff",
      "go",
      "school",
      "late"
    ],
    "token_count":4,
    "processed_text":"coff go school late"
  },
  {
    "label":0,
    "text":"tonsil strike unpleas feel voic gone still contagi medicin go home tea soon whiskey",
    "cleaned_text":"tonsil strike unpleas feel voic gone still contagi medicin go home tea soon whiskey",
    "normalized_text":"tonsil strike unpleas feel voic gone still contagi medicin go home tea soon whiskey",
    "tokens":[
      "tonsil",
      "strike",
      "unplea",
      "feel",
      "voic",
      "gone",
      "still",
      "contagi",
      "medicin",
      "go",
      "home",
      "tea",
      "soon",
      "whiskey"
    ],
    "token_count":14,
    "processed_text":"tonsil strike unplea feel voic gone still contagi medicin go home tea soon whiskey"
  },
  {
    "label":0,
    "text":"hate life moment",
    "cleaned_text":"hate life moment",
    "normalized_text":"hate life moment",
    "tokens":[
      "hate",
      "life",
      "moment"
    ],
    "token_count":3,
    "processed_text":"hate life moment"
  },
  {
    "label":4,
    "text":"glad brunch yesterday sunni beauti",
    "cleaned_text":"glad brunch yesterday sunni beauti",
    "normalized_text":"glad brunch yesterday sunni beauti",
    "tokens":[
      "glad",
      "brunch",
      "yesterday",
      "sunni",
      "beauti"
    ],
    "token_count":5,
    "processed_text":"glad brunch yesterday sunni beauti"
  },
  {
    "label":4,
    "text":"squish tightli understand need vent hun hope feel better",
    "cleaned_text":"squish tightli understand need vent hun hope feel better",
    "normalized_text":"squish tightli understand need vent hun hope feel better",
    "tokens":[
      "squish",
      "tightli",
      "understand",
      "need",
      "vent",
      "hun",
      "hope",
      "feel",
      "better"
    ],
    "token_count":9,
    "processed_text":"squish tightli understand need vent hun hope feel better"
  },
  {
    "label":4,
    "text":"woo hoo mile walk lordwil ill get powerwalk eveningrain rain stay away",
    "cleaned_text":"woo hoo mile walk lordwil ill get powerwalk eveningrain rain stay away",
    "normalized_text":"woo hoo mile walk lordwil ill get powerwalk eveningrain rain stay away",
    "tokens":[
      "woo",
      "hoo",
      "mile",
      "walk",
      "lordwil",
      "ill",
      "get",
      "powerwalk",
      "eveningrain",
      "rain",
      "stay",
      "away"
    ],
    "token_count":12,
    "processed_text":"woo hoo mile walk lordwil ill get powerwalk eveningrain rain stay away"
  },
  {
    "label":4,
    "text":"love inspir word wall",
    "cleaned_text":"love inspir word wall",
    "normalized_text":"love inspir word wall",
    "tokens":[
      "love",
      "inspir",
      "word",
      "wall"
    ],
    "token_count":4,
    "processed_text":"love inspir word wall"
  },
  {
    "label":4,
    "text":"hope enjoy hike ill dust boot get derbyshir peak uk",
    "cleaned_text":"hope enjoy hike ill dust boot get derbyshir peak uk",
    "normalized_text":"hope enjoy hike ill dust boot get derbyshir peak uk",
    "tokens":[
      "hope",
      "enjoy",
      "hike",
      "ill",
      "dust",
      "boot",
      "get",
      "derbyshir",
      "peak",
      "uk"
    ],
    "token_count":10,
    "processed_text":"hope enjoy hike ill dust boot get derbyshir peak uk"
  },
  {
    "label":4,
    "text":"nonono alway like",
    "cleaned_text":"nonono alway like",
    "normalized_text":"nonono alway like",
    "tokens":[
      "nonono",
      "alway",
      "like"
    ],
    "token_count":3,
    "processed_text":"nonono alway like"
  },
  {
    "label":0,
    "text":"tenni finish guess murray lost",
    "cleaned_text":"tenni finish guess murray lost",
    "normalized_text":"tenni finish guess murray lost",
    "tokens":[
      "tenni",
      "finish",
      "guess",
      "murray",
      "lost"
    ],
    "token_count":5,
    "processed_text":"tenni finish guess murray lost"
  },
  {
    "label":0,
    "text":"snack hous",
    "cleaned_text":"snack hous",
    "normalized_text":"snack hous",
    "tokens":[
      "snack",
      "hou"
    ],
    "token_count":2,
    "processed_text":"snack hou"
  },
  {
    "label":0,
    "text":"professor mail grade hous wont tell",
    "cleaned_text":"professor mail grade hous wont tell",
    "normalized_text":"professor mail grade hous wont tell",
    "tokens":[
      "professor",
      "mail",
      "grade",
      "hou",
      "wont",
      "tell"
    ],
    "token_count":6,
    "processed_text":"professor mail grade hou wont tell"
  },
  {
    "label":4,
    "text":"oh got new dvd today breakfast tiffani pretti woman like hot watch later",
    "cleaned_text":"oh got new dvd today breakfast tiffani pretti woman like hot watch later",
    "normalized_text":"oh got new dvd today breakfast tiffani pretti woman like hot watch later",
    "tokens":[
      "oh",
      "got",
      "new",
      "dvd",
      "today",
      "breakfast",
      "tiffani",
      "pretti",
      "woman",
      "like",
      "hot",
      "watch",
      "later"
    ],
    "token_count":13,
    "processed_text":"oh got new dvd today breakfast tiffani pretti woman like hot watch later"
  },
  {
    "label":4,
    "text":"lol love sound delici yet dirti",
    "cleaned_text":"lol love sound delici yet dirti",
    "normalized_text":"lol love sound delici yet dirti",
    "tokens":[
      "lol",
      "love",
      "sound",
      "delici",
      "yet",
      "dirti"
    ],
    "token_count":6,
    "processed_text":"lol love sound delici yet dirti"
  },
  {
    "label":4,
    "text":"fred icarli ohy",
    "cleaned_text":"fred icarli ohy",
    "normalized_text":"fred icarli ohy",
    "tokens":[
      "fred",
      "icarli",
      "ohi"
    ],
    "token_count":3,
    "processed_text":"fred icarli ohi"
  },
  {
    "label":4,
    "text":"im go watch termin salvat later movi beer friend",
    "cleaned_text":"im go watch termin salvat later movi beer friend",
    "normalized_text":"im go watch termin salvat later movi beer friend",
    "tokens":[
      "im",
      "go",
      "watch",
      "termin",
      "salvat",
      "later",
      "movi",
      "beer",
      "friend"
    ],
    "token_count":9,
    "processed_text":"im go watch termin salvat later movi beer friend"
  },
  {
    "label":0,
    "text":"im crave sort ice coffe chocol sort thing save money blow",
    "cleaned_text":"im crave sort ice coffe chocol sort thing save money blow",
    "normalized_text":"im crave sort ice coffe chocol sort thing save money blow",
    "tokens":[
      "im",
      "crave",
      "sort",
      "ice",
      "coff",
      "chocol",
      "sort",
      "thing",
      "save",
      "money",
      "blow"
    ],
    "token_count":11,
    "processed_text":"im crave sort ice coff chocol sort thing save money blow"
  },
  {
    "label":4,
    "text":"said girl cri eye song festiv",
    "cleaned_text":"said girl cri eye song festiv",
    "normalized_text":"said girl cri eye song festiv",
    "tokens":[
      "said",
      "girl",
      "cri",
      "eye",
      "song",
      "festiv"
    ],
    "token_count":6,
    "processed_text":"said girl cri eye song festiv"
  },
  {
    "label":4,
    "text":"heh great would like gb bump gb",
    "cleaned_text":"heh great would like gb bump gb",
    "normalized_text":"heh great would like gb bump gb",
    "tokens":[
      "heh",
      "great",
      "like",
      "gb",
      "bump",
      "gb"
    ],
    "token_count":6,
    "processed_text":"heh great like gb bump gb"
  },
  {
    "label":4,
    "text":"could spread word seven deadli sin arsen blog cheer",
    "cleaned_text":"could spread word seven deadli sin arsen blog cheer",
    "normalized_text":"could spread word seven deadli sin arsen blog cheer",
    "tokens":[
      "spread",
      "word",
      "seven",
      "deadli",
      "sin",
      "arsen",
      "blog",
      "cheer"
    ],
    "token_count":8,
    "processed_text":"spread word seven deadli sin arsen blog cheer"
  },
  {
    "label":4,
    "text":"school",
    "cleaned_text":"school",
    "normalized_text":"school",
    "tokens":[
      "school"
    ],
    "token_count":1,
    "processed_text":"school"
  },
  {
    "label":4,
    "text":"heheh went swim great",
    "cleaned_text":"heheh went swim great",
    "normalized_text":"heheh went swim great",
    "tokens":[
      "heheh",
      "went",
      "swim",
      "great"
    ],
    "token_count":4,
    "processed_text":"heheh went swim great"
  },
  {
    "label":4,
    "text":"wetdiffer pace wa im origin im use",
    "cleaned_text":"wetdiffer pace wa im origin im use",
    "normalized_text":"wetdiffer pace wa im origin im use",
    "tokens":[
      "wetdiff",
      "pace",
      "wa",
      "im",
      "origin",
      "im",
      "use"
    ],
    "token_count":7,
    "processed_text":"wetdiff pace wa im origin im use"
  },
  {
    "label":0,
    "text":"almost dm dm inbox autom dont check everyday anymor",
    "cleaned_text":"almost dm dm inbox autom dont check everyday anymor",
    "normalized_text":"almost dm dm inbox autom dont check everyday anymor",
    "tokens":[
      "almost",
      "dm",
      "dm",
      "inbox",
      "autom",
      "dont",
      "check",
      "everyday",
      "anymor"
    ],
    "token_count":9,
    "processed_text":"almost dm dm inbox autom dont check everyday anymor"
  },
  {
    "label":4,
    "text":"great guest asylum hope amaz time",
    "cleaned_text":"great guest asylum hope amaz time",
    "normalized_text":"great guest asylum hope amaz time",
    "tokens":[
      "great",
      "guest",
      "asylum",
      "hope",
      "amaz",
      "time"
    ],
    "token_count":6,
    "processed_text":"great guest asylum hope amaz time"
  },
  {
    "label":0,
    "text":"got homework done weekend suffer consequ monday",
    "cleaned_text":"got homework done weekend suffer consequ monday",
    "normalized_text":"got homework done weekend suffer consequ monday",
    "tokens":[
      "got",
      "homework",
      "done",
      "weekend",
      "suffer",
      "consequ",
      "monday"
    ],
    "token_count":7,
    "processed_text":"got homework done weekend suffer consequ monday"
  },
  {
    "label":4,
    "text":"cant wait see new hairstyl",
    "cleaned_text":"cant wait see new hairstyl",
    "normalized_text":"cant wait see new hairstyl",
    "tokens":[
      "cant",
      "wait",
      "see",
      "new",
      "hairstyl"
    ],
    "token_count":5,
    "processed_text":"cant wait see new hairstyl"
  },
  {
    "label":4,
    "text":"actual lot work content manag system like joomla etc could",
    "cleaned_text":"actual lot work content manag system like joomla etc could",
    "normalized_text":"actual lot work content manag system like joomla etc could",
    "tokens":[
      "actual",
      "lot",
      "work",
      "content",
      "manag",
      "system",
      "like",
      "joomla",
      "etc"
    ],
    "token_count":9,
    "processed_text":"actual lot work content manag system like joomla etc"
  },
  {
    "label":4,
    "text":"differ peopl vote home b cast multipl vote bgt former could work latter much",
    "cleaned_text":"differ peopl vote home b cast multipl vote bgt former could work latter much",
    "normalized_text":"differ peopl vote home b cast multipl vote bgt former could work latter much",
    "tokens":[
      "differ",
      "peopl",
      "vote",
      "home",
      "cast",
      "multipl",
      "vote",
      "bgt",
      "former",
      "work",
      "latter",
      "much"
    ],
    "token_count":12,
    "processed_text":"differ peopl vote home cast multipl vote bgt former work latter much"
  },
  {
    "label":0,
    "text":"hahahai love sooo funni got church learn live missionyea im behind",
    "cleaned_text":"hahahai love sooo funni got church learn live missionyea im behind",
    "normalized_text":"hahahai love sooo funni got church learn live missionyea im behind",
    "tokens":[
      "hahahai",
      "love",
      "sooo",
      "funni",
      "got",
      "church",
      "learn",
      "live",
      "missionyea",
      "im",
      "behind"
    ],
    "token_count":11,
    "processed_text":"hahahai love sooo funni got church learn live missionyea im behind"
  },
  {
    "label":0,
    "text":"ahah well atleast tri came heart realli meant guess didnt mean anyth u go bye",
    "cleaned_text":"ahah well atleast tri came heart realli meant guess didnt mean anyth u go bye",
    "normalized_text":"ahah well atleast tri came heart realli meant guess didnt mean anyth u go bye",
    "tokens":[
      "ahah",
      "well",
      "atleast",
      "tri",
      "came",
      "heart",
      "realli",
      "meant",
      "guess",
      "didnt",
      "mean",
      "anyth",
      "go",
      "bye"
    ],
    "token_count":14,
    "processed_text":"ahah well atleast tri came heart realli meant guess didnt mean anyth go bye"
  },
  {
    "label":4,
    "text":"live somervil near frankston holeholeholehol",
    "cleaned_text":"live somervil near frankston holeholeholehol",
    "normalized_text":"live somervil near frankston holeholeholehol",
    "tokens":[
      "live",
      "somervil",
      "near",
      "frankston",
      "holeholeholehol"
    ],
    "token_count":5,
    "processed_text":"live somervil near frankston holeholeholehol"
  },
  {
    "label":4,
    "text":"eat nasti ass cook pancak egg wife love",
    "cleaned_text":"eat nasti ass cook pancak egg wife love",
    "normalized_text":"eat nasti ass cook pancak egg wife love",
    "tokens":[
      "eat",
      "nasti",
      "ass",
      "cook",
      "pancak",
      "egg",
      "wife",
      "love"
    ],
    "token_count":8,
    "processed_text":"eat nasti ass cook pancak egg wife love"
  },
  {
    "label":0,
    "text":"miss u dont want u go",
    "cleaned_text":"miss u dont want u go",
    "normalized_text":"miss u dont want u go",
    "tokens":[
      "miss",
      "dont",
      "want",
      "go"
    ],
    "token_count":4,
    "processed_text":"miss dont want go"
  },
  {
    "label":0,
    "text":"hahaha hater go arent lot oceanup today miss",
    "cleaned_text":"hahaha hater go arent lot oceanup today miss",
    "normalized_text":"hahaha hater go arent lot oceanup today miss",
    "tokens":[
      "hahaha",
      "hater",
      "go",
      "arent",
      "lot",
      "oceanup",
      "today",
      "miss"
    ],
    "token_count":8,
    "processed_text":"hahaha hater go arent lot oceanup today miss"
  },
  {
    "label":0,
    "text":"head drive date back meim realli piss sad",
    "cleaned_text":"head drive date back meim realli piss sad",
    "normalized_text":"head drive date back meim realli piss sad",
    "tokens":[
      "head",
      "drive",
      "date",
      "back",
      "meim",
      "realli",
      "piss",
      "sad"
    ],
    "token_count":8,
    "processed_text":"head drive date back meim realli piss sad"
  },
  {
    "label":0,
    "text":"garag rate big drive much fun miss",
    "cleaned_text":"garag rate big drive much fun miss",
    "normalized_text":"garag rate big drive much fun miss",
    "tokens":[
      "garag",
      "rate",
      "big",
      "drive",
      "much",
      "fun",
      "miss"
    ],
    "token_count":7,
    "processed_text":"garag rate big drive much fun miss"
  },
  {
    "label":4,
    "text":"offici master account degre",
    "cleaned_text":"offici master account degre",
    "normalized_text":"offici master account degre",
    "tokens":[
      "offici",
      "master",
      "account",
      "degr"
    ],
    "token_count":4,
    "processed_text":"offici master account degr"
  },
  {
    "label":4,
    "text":"goin tha cuzzin house",
    "cleaned_text":"goin tha cuzzin house",
    "normalized_text":"goin tha cuzzin house",
    "tokens":[
      "goin",
      "tha",
      "cuzzin",
      "hous"
    ],
    "token_count":4,
    "processed_text":"goin tha cuzzin hous"
  },
  {
    "label":4,
    "text":"mean your lose weight bad thing",
    "cleaned_text":"mean your lose weight bad thing",
    "normalized_text":"mean your lose weight bad thing",
    "tokens":[
      "mean",
      "lose",
      "weight",
      "bad",
      "thing"
    ],
    "token_count":5,
    "processed_text":"mean lose weight bad thing"
  },
  {
    "label":0,
    "text":"warm summer day back yay endless insect splatter windscreen",
    "cleaned_text":"warm summer day back yay endless insect splatter windscreen",
    "normalized_text":"warm summer day back yay endless insect splatter windscreen",
    "tokens":[
      "warm",
      "summer",
      "day",
      "back",
      "yay",
      "endless",
      "insect",
      "splatter",
      "windscreen"
    ],
    "token_count":9,
    "processed_text":"warm summer day back yay endless insect splatter windscreen"
  },
  {
    "label":4,
    "text":"seen cutest asian babi market well want bring home lol nice see heap kid dog today",
    "cleaned_text":"seen cutest asian babi market well want bring home lol nice see heap kid dog today",
    "normalized_text":"seen cutest asian babi market well want bring home lol nice see heap kid dog today",
    "tokens":[
      "seen",
      "cutest",
      "asian",
      "babi",
      "market",
      "well",
      "want",
      "bring",
      "home",
      "lol",
      "nice",
      "see",
      "heap",
      "kid",
      "dog",
      "today"
    ],
    "token_count":16,
    "processed_text":"seen cutest asian babi market well want bring home lol nice see heap kid dog today"
  },
  {
    "label":0,
    "text":"feel like noth lay bed feel wont go away game tonight fml im go horribl",
    "cleaned_text":"feel like noth lay bed feel wont go away game tonight fml im go horribl",
    "normalized_text":"feel like noth lay bed feel wont go away game tonight fml im go horribl",
    "tokens":[
      "feel",
      "like",
      "noth",
      "lay",
      "bed",
      "feel",
      "wont",
      "go",
      "away",
      "game",
      "tonight",
      "fml",
      "im",
      "go",
      "horribl"
    ],
    "token_count":15,
    "processed_text":"feel like noth lay bed feel wont go away game tonight fml im go horribl"
  },
  {
    "label":4,
    "text":"great idea ad offlin usag cach",
    "cleaned_text":"great idea ad offlin usag cach",
    "normalized_text":"great idea ad offlin usag cach",
    "tokens":[
      "great",
      "idea",
      "ad",
      "offlin",
      "usag",
      "cach"
    ],
    "token_count":6,
    "processed_text":"great idea ad offlin usag cach"
  },
  {
    "label":0,
    "text":"need twug",
    "cleaned_text":"need twug",
    "normalized_text":"need twug",
    "tokens":[
      "need",
      "twug"
    ],
    "token_count":2,
    "processed_text":"need twug"
  },
  {
    "label":4,
    "text":"yeah made im exhaust though great see",
    "cleaned_text":"yeah made im exhaust though great see",
    "normalized_text":"yeah made im exhaust though great see",
    "tokens":[
      "yeah",
      "made",
      "im",
      "exhaust",
      "though",
      "great",
      "see"
    ],
    "token_count":7,
    "processed_text":"yeah made im exhaust though great see"
  },
  {
    "label":4,
    "text":"thank thank scoop",
    "cleaned_text":"thank thank scoop",
    "normalized_text":"thank thank scoop",
    "tokens":[
      "thank",
      "thank",
      "scoop"
    ],
    "token_count":3,
    "processed_text":"thank thank scoop"
  },
  {
    "label":0,
    "text":"dont car audio",
    "cleaned_text":"dont car audio",
    "normalized_text":"dont car audio",
    "tokens":[
      "dont",
      "car",
      "audio"
    ],
    "token_count":3,
    "processed_text":"dont car audio"
  },
  {
    "label":4,
    "text":"sun come thing r start look",
    "cleaned_text":"sun come thing r start look",
    "normalized_text":"sun come thing r start look",
    "tokens":[
      "sun",
      "come",
      "thing",
      "start",
      "look"
    ],
    "token_count":5,
    "processed_text":"sun come thing start look"
  },
  {
    "label":0,
    "text":"ah boo bet pump",
    "cleaned_text":"ah boo bet pump",
    "normalized_text":"ah boo bet pump",
    "tokens":[
      "ah",
      "boo",
      "bet",
      "pump"
    ],
    "token_count":4,
    "processed_text":"ah boo bet pump"
  },
  {
    "label":0,
    "text":"let",
    "cleaned_text":"let",
    "normalized_text":"let",
    "tokens":[
      "let"
    ],
    "token_count":1,
    "processed_text":"let"
  },
  {
    "label":0,
    "text":"woke dont rememb fall asleep guess sort faint due massiv pain worst week ever feel bless",
    "cleaned_text":"woke dont rememb fall asleep guess sort faint due massiv pain worst week ever feel bless",
    "normalized_text":"woke dont rememb fall asleep guess sort faint due massiv pain worst week ever feel bless",
    "tokens":[
      "woke",
      "dont",
      "rememb",
      "fall",
      "asleep",
      "guess",
      "sort",
      "faint",
      "due",
      "massiv",
      "pain",
      "worst",
      "week",
      "ever",
      "feel",
      "bless"
    ],
    "token_count":16,
    "processed_text":"woke dont rememb fall asleep guess sort faint due massiv pain worst week ever feel bless"
  },
  {
    "label":0,
    "text":"westfield last time long ive gotta work",
    "cleaned_text":"westfield last time long ive gotta work",
    "normalized_text":"westfield last time long ive gotta work",
    "tokens":[
      "westfield",
      "last",
      "time",
      "long",
      "ive",
      "got",
      "ta",
      "work"
    ],
    "token_count":8,
    "processed_text":"westfield last time long ive got ta work"
  },
  {
    "label":4,
    "text":"good look tonighti cant wait see new moon trailer good luckgood luckgood luckgood luckgood luckgood luck",
    "cleaned_text":"good look tonighti cant wait see new moon trailer good luckgood luckgood luckgood luckgood luckgood luck",
    "normalized_text":"good look tonighti cant wait see new moon trailer good luckgood luckgood luckgood luckgood luckgood luck",
    "tokens":[
      "good",
      "look",
      "tonighti",
      "cant",
      "wait",
      "see",
      "new",
      "moon",
      "trailer",
      "good",
      "luckgood",
      "luckgood",
      "luckgood",
      "luckgood",
      "luckgood",
      "luck"
    ],
    "token_count":16,
    "processed_text":"good look tonighti cant wait see new moon trailer good luckgood luckgood luckgood luckgood luckgood luck"
  },
  {
    "label":4,
    "text":"fwd skydiv yeeeeeeeeeehaaaaaaaaaaaaaaaa pleas tell",
    "cleaned_text":"fwd skydiv yeeeeeeeeeehaaaaaaaaaaaaaaaa pleas tell",
    "normalized_text":"fwd skydiv yeeeeeeeeeehaaaaaaaaaaaaaaaa pleas tell",
    "tokens":[
      "fwd",
      "skydiv",
      "plea",
      "tell"
    ],
    "token_count":4,
    "processed_text":"fwd skydiv plea tell"
  },
  {
    "label":4,
    "text":"cherri sorri youv weather get rest keep twitter good soul",
    "cleaned_text":"cherri sorri youv weather get rest keep twitter good soul",
    "normalized_text":"cherri sorri youv weather get rest keep twitter good soul",
    "tokens":[
      "cherri",
      "sorri",
      "youv",
      "weather",
      "get",
      "rest",
      "keep",
      "twitter",
      "good",
      "soul"
    ],
    "token_count":10,
    "processed_text":"cherri sorri youv weather get rest keep twitter good soul"
  },
  {
    "label":4,
    "text":"better still quotangl daemonsquot",
    "cleaned_text":"better still quotangl daemonsquot",
    "normalized_text":"better still quotangl daemonsquot",
    "tokens":[
      "better",
      "still",
      "quotangl",
      "daemonsquot"
    ],
    "token_count":4,
    "processed_text":"better still quotangl daemonsquot"
  },
  {
    "label":4,
    "text":"soooooo tire love day like",
    "cleaned_text":"soooooo tire love day like",
    "normalized_text":"soooooo tire love day like",
    "tokens":[
      "soooooo",
      "tire",
      "love",
      "day",
      "like"
    ],
    "token_count":5,
    "processed_text":"soooooo tire love day like"
  },
  {
    "label":0,
    "text":"mauric said flake he sad",
    "cleaned_text":"mauric said flake he sad",
    "normalized_text":"mauric said flake he sad",
    "tokens":[
      "mauric",
      "said",
      "flake",
      "sad"
    ],
    "token_count":4,
    "processed_text":"mauric said flake sad"
  },
  {
    "label":0,
    "text":"inhal cupcak bad lung get check",
    "cleaned_text":"inhal cupcak bad lung get check",
    "normalized_text":"inhal cupcak bad lung get check",
    "tokens":[
      "inhal",
      "cupcak",
      "bad",
      "lung",
      "get",
      "check"
    ],
    "token_count":6,
    "processed_text":"inhal cupcak bad lung get check"
  },
  {
    "label":0,
    "text":"go tell life june thaugust th school",
    "cleaned_text":"go tell life june thaugust th school",
    "normalized_text":"go tell life june thaugust th school",
    "tokens":[
      "go",
      "tell",
      "life",
      "june",
      "thaugust",
      "th",
      "school"
    ],
    "token_count":7,
    "processed_text":"go tell life june thaugust th school"
  },
  {
    "label":4,
    "text":"road mexico peopl doesnt realli like oh wait famili",
    "cleaned_text":"road mexico peopl doesnt realli like oh wait famili",
    "normalized_text":"road mexico peopl doesnt realli like oh wait famili",
    "tokens":[
      "road",
      "mexico",
      "peopl",
      "doesnt",
      "realli",
      "like",
      "oh",
      "wait",
      "famili"
    ],
    "token_count":9,
    "processed_text":"road mexico peopl doesnt realli like oh wait famili"
  },
  {
    "label":4,
    "text":"book first ever speak engag preach church sunday novemb give plenti time prepar",
    "cleaned_text":"book first ever speak engag preach church sunday novemb give plenti time prepar",
    "normalized_text":"book first ever speak engag preach church sunday novemb give plenti time prepar",
    "tokens":[
      "book",
      "first",
      "ever",
      "speak",
      "engag",
      "preach",
      "church",
      "sunday",
      "novemb",
      "give",
      "plenti",
      "time",
      "prepar"
    ],
    "token_count":13,
    "processed_text":"book first ever speak engag preach church sunday novemb give plenti time prepar"
  },
  {
    "label":0,
    "text":"maria lesli blah blah blah hurt ear mean",
    "cleaned_text":"maria lesli blah blah blah hurt ear mean",
    "normalized_text":"maria lesli blah blah blah hurt ear mean",
    "tokens":[
      "maria",
      "lesli",
      "blah",
      "blah",
      "blah",
      "hurt",
      "ear",
      "mean"
    ],
    "token_count":8,
    "processed_text":"maria lesli blah blah blah hurt ear mean"
  },
  {
    "label":0,
    "text":"bore home",
    "cleaned_text":"bore home",
    "normalized_text":"bore home",
    "tokens":[
      "bore",
      "home"
    ],
    "token_count":2,
    "processed_text":"bore home"
  },
  {
    "label":4,
    "text":"saw last night even better anticip way go pixar",
    "cleaned_text":"saw last night even better anticip way go pixar",
    "normalized_text":"saw last night even better anticip way go pixar",
    "tokens":[
      "saw",
      "last",
      "night",
      "even",
      "better",
      "anticip",
      "way",
      "go",
      "pixar"
    ],
    "token_count":9,
    "processed_text":"saw last night even better anticip way go pixar"
  },
  {
    "label":4,
    "text":"hey everyon repli column actual better reason follow someon ff",
    "cleaned_text":"hey everyon repli column actual better reason follow someon ff",
    "normalized_text":"hey everyon repli column actual better reason follow someon ff",
    "tokens":[
      "hey",
      "everyon",
      "repli",
      "column",
      "actual",
      "better",
      "reason",
      "follow",
      "someon",
      "ff"
    ],
    "token_count":10,
    "processed_text":"hey everyon repli column actual better reason follow someon ff"
  },
  {
    "label":4,
    "text":"sorri realli seem hardest word goodby drama ba hahaha naaah",
    "cleaned_text":"sorri realli seem hardest word goodby drama ba hahaha naaah",
    "normalized_text":"sorri realli seem hardest word goodby drama ba hahaha naaah",
    "tokens":[
      "sorri",
      "realli",
      "seem",
      "hardest",
      "word",
      "goodbi",
      "drama",
      "ba",
      "hahaha",
      "naaah"
    ],
    "token_count":10,
    "processed_text":"sorri realli seem hardest word goodbi drama ba hahaha naaah"
  },
  {
    "label":4,
    "text":"glad made pansi",
    "cleaned_text":"glad made pansi",
    "normalized_text":"glad made pansi",
    "tokens":[
      "glad",
      "made",
      "pansi"
    ],
    "token_count":3,
    "processed_text":"glad made pansi"
  },
  {
    "label":4,
    "text":"love go anywher everywher look like crap best friendddd radio blare",
    "cleaned_text":"love go anywher everywher look like crap best friendddd radio blare",
    "normalized_text":"love go anywher everywher look like crap best friendddd radio blare",
    "tokens":[
      "love",
      "go",
      "anywh",
      "everywh",
      "look",
      "like",
      "crap",
      "best",
      "friendddd",
      "radio",
      "blare"
    ],
    "token_count":11,
    "processed_text":"love go anywh everywh look like crap best friendddd radio blare"
  },
  {
    "label":4,
    "text":"well come back gotta look chill",
    "cleaned_text":"well come back gotta look chill",
    "normalized_text":"well come back gotta look chill",
    "tokens":[
      "well",
      "come",
      "back",
      "got",
      "ta",
      "look",
      "chill"
    ],
    "token_count":7,
    "processed_text":"well come back got ta look chill"
  },
  {
    "label":0,
    "text":"sunshin save bank new bank next publix lake ella got rob guess weekli bank robberi quota met",
    "cleaned_text":"sunshin save bank new bank next publix lake ella got rob guess weekli bank robberi quota met",
    "normalized_text":"sunshin save bank new bank next publix lake ella got rob guess weekli bank robberi quota met",
    "tokens":[
      "sunshin",
      "save",
      "bank",
      "new",
      "bank",
      "next",
      "publix",
      "lake",
      "ella",
      "got",
      "rob",
      "guess",
      "weekli",
      "bank",
      "robberi",
      "quota",
      "met"
    ],
    "token_count":17,
    "processed_text":"sunshin save bank new bank next publix lake ella got rob guess weekli bank robberi quota met"
  },
  {
    "label":4,
    "text":"aye far",
    "cleaned_text":"aye far",
    "normalized_text":"aye far",
    "tokens":[
      "aye",
      "far"
    ],
    "token_count":2,
    "processed_text":"aye far"
  },
  {
    "label":4,
    "text":"oliv juic",
    "cleaned_text":"oliv juic",
    "normalized_text":"oliv juic",
    "tokens":[
      "oliv",
      "juic"
    ],
    "token_count":2,
    "processed_text":"oliv juic"
  },
  {
    "label":0,
    "text":"feel sick one look",
    "cleaned_text":"feel sick one look",
    "normalized_text":"feel sick one look",
    "tokens":[
      "feel",
      "sick",
      "one",
      "look"
    ],
    "token_count":4,
    "processed_text":"feel sick one look"
  },
  {
    "label":4,
    "text":"also check placemat made week back realli help",
    "cleaned_text":"also check placemat made week back realli help",
    "normalized_text":"also check placemat made week back realli help",
    "tokens":[
      "also",
      "check",
      "placemat",
      "made",
      "week",
      "back",
      "realli",
      "help"
    ],
    "token_count":8,
    "processed_text":"also check placemat made week back realli help"
  },
  {
    "label":0,
    "text":"aw good look show tonight x",
    "cleaned_text":"aw good look show tonight x",
    "normalized_text":"aw good look show tonight x",
    "tokens":[
      "aw",
      "good",
      "look",
      "show",
      "tonight"
    ],
    "token_count":5,
    "processed_text":"aw good look show tonight"
  },
  {
    "label":0,
    "text":"rip leather jacket nice know",
    "cleaned_text":"rip leather jacket nice know",
    "normalized_text":"rip leather jacket nice know",
    "tokens":[
      "rip",
      "leather",
      "jacket",
      "nice",
      "know"
    ],
    "token_count":5,
    "processed_text":"rip leather jacket nice know"
  },
  {
    "label":0,
    "text":"mean realli miss hunni much right",
    "cleaned_text":"mean realli miss hunni much right",
    "normalized_text":"mean realli miss hunni much right",
    "tokens":[
      "mean",
      "realli",
      "miss",
      "hunni",
      "much",
      "right"
    ],
    "token_count":6,
    "processed_text":"mean realli miss hunni much right"
  },
  {
    "label":4,
    "text":"got back store got black purpl nail polish im go nail",
    "cleaned_text":"got back store got black purpl nail polish im go nail",
    "normalized_text":"got back store got black purpl nail polish im go nail",
    "tokens":[
      "got",
      "back",
      "store",
      "got",
      "black",
      "purpl",
      "nail",
      "polish",
      "im",
      "go",
      "nail"
    ],
    "token_count":11,
    "processed_text":"got back store got black purpl nail polish im go nail"
  },
  {
    "label":4,
    "text":"way beach ride convert ah life",
    "cleaned_text":"way beach ride convert ah life",
    "normalized_text":"way beach ride convert ah life",
    "tokens":[
      "way",
      "beach",
      "ride",
      "convert",
      "ah",
      "life"
    ],
    "token_count":6,
    "processed_text":"way beach ride convert ah life"
  },
  {
    "label":4,
    "text":"know right",
    "cleaned_text":"know right",
    "normalized_text":"know right",
    "tokens":[
      "know",
      "right"
    ],
    "token_count":2,
    "processed_text":"know right"
  },
  {
    "label":0,
    "text":"hate keyboard alway make mistak haha could talk yeah that betert write",
    "cleaned_text":"hate keyboard alway make mistak haha could talk yeah that betert write",
    "normalized_text":"hate keyboard alway make mistak haha could talk yeah that betert write",
    "tokens":[
      "hate",
      "keyboard",
      "alway",
      "make",
      "mistak",
      "haha",
      "talk",
      "yeah",
      "betert",
      "write"
    ],
    "token_count":10,
    "processed_text":"hate keyboard alway make mistak haha talk yeah betert write"
  },
  {
    "label":4,
    "text":"town woo",
    "cleaned_text":"town woo",
    "normalized_text":"town woo",
    "tokens":[
      "town",
      "woo"
    ],
    "token_count":2,
    "processed_text":"town woo"
  },
  {
    "label":4,
    "text":"get readi addi baptism",
    "cleaned_text":"get readi addi baptism",
    "normalized_text":"get readi addi baptism",
    "tokens":[
      "get",
      "readi",
      "addi",
      "baptism"
    ],
    "token_count":4,
    "processed_text":"get readi addi baptism"
  },
  {
    "label":0,
    "text":"hous powerless",
    "cleaned_text":"hous powerless",
    "normalized_text":"hous powerless",
    "tokens":[
      "hou",
      "powerless"
    ],
    "token_count":2,
    "processed_text":"hou powerless"
  },
  {
    "label":4,
    "text":"love girl rule",
    "cleaned_text":"love girl rule",
    "normalized_text":"love girl rule",
    "tokens":[
      "love",
      "girl",
      "rule"
    ],
    "token_count":3,
    "processed_text":"love girl rule"
  },
  {
    "label":0,
    "text":"realli need money dont much left",
    "cleaned_text":"realli need money dont much left",
    "normalized_text":"realli need money dont much left",
    "tokens":[
      "realli",
      "need",
      "money",
      "dont",
      "much",
      "left"
    ],
    "token_count":6,
    "processed_text":"realli need money dont much left"
  },
  {
    "label":4,
    "text":"got problem bitch aint one got girl problem feel bad ya son",
    "cleaned_text":"got problem bitch aint one got girl problem feel bad ya son",
    "normalized_text":"got problem bitch aint one got girl problem feel bad ya son",
    "tokens":[
      "got",
      "problem",
      "bitch",
      "aint",
      "one",
      "got",
      "girl",
      "problem",
      "feel",
      "bad",
      "ya",
      "son"
    ],
    "token_count":12,
    "processed_text":"got problem bitch aint one got girl problem feel bad ya son"
  },
  {
    "label":0,
    "text":"want danc muscl ach",
    "cleaned_text":"want danc muscl ach",
    "normalized_text":"want danc muscl ach",
    "tokens":[
      "want",
      "danc",
      "muscl",
      "ach"
    ],
    "token_count":4,
    "processed_text":"want danc muscl ach"
  },
  {
    "label":0,
    "text":"bore today answer question school",
    "cleaned_text":"bore today answer question school",
    "normalized_text":"bore today answer question school",
    "tokens":[
      "bore",
      "today",
      "answer",
      "question",
      "school"
    ],
    "token_count":5,
    "processed_text":"bore today answer question school"
  },
  {
    "label":4,
    "text":"dont blame might well whilst last",
    "cleaned_text":"dont blame might well whilst last",
    "normalized_text":"dont blame might well whilst last",
    "tokens":[
      "dont",
      "blame",
      "well",
      "whilst",
      "last"
    ],
    "token_count":5,
    "processed_text":"dont blame well whilst last"
  },
  {
    "label":0,
    "text":"cant find box tissu doctor laterfin",
    "cleaned_text":"cant find box tissu doctor laterfin",
    "normalized_text":"cant find box tissu doctor laterfin",
    "tokens":[
      "cant",
      "find",
      "box",
      "tissu",
      "doctor",
      "laterfin"
    ],
    "token_count":6,
    "processed_text":"cant find box tissu doctor laterfin"
  },
  {
    "label":0,
    "text":"realiz bass rental less week left",
    "cleaned_text":"realiz bass rental less week left",
    "normalized_text":"realiz bass rental less week left",
    "tokens":[
      "realiz",
      "bass",
      "rental",
      "less",
      "week",
      "left"
    ],
    "token_count":6,
    "processed_text":"realiz bass rental less week left"
  },
  {
    "label":0,
    "text":"miss blue bell dont michigan im pretti sure dont indiana either",
    "cleaned_text":"miss blue bell dont michigan im pretti sure dont indiana either",
    "normalized_text":"miss blue bell dont michigan im pretti sure dont indiana either",
    "tokens":[
      "miss",
      "blue",
      "bell",
      "dont",
      "michigan",
      "im",
      "pretti",
      "sure",
      "dont",
      "indiana",
      "either"
    ],
    "token_count":11,
    "processed_text":"miss blue bell dont michigan im pretti sure dont indiana either"
  },
  {
    "label":4,
    "text":"great afternoon later xoxo",
    "cleaned_text":"great afternoon later xoxo",
    "normalized_text":"great afternoon later xoxo",
    "tokens":[
      "great",
      "afternoon",
      "later",
      "xoxo"
    ],
    "token_count":4,
    "processed_text":"great afternoon later xoxo"
  },
  {
    "label":4,
    "text":"wait one week trip",
    "cleaned_text":"wait one week trip",
    "normalized_text":"wait one week trip",
    "tokens":[
      "wait",
      "one",
      "week",
      "trip"
    ],
    "token_count":4,
    "processed_text":"wait one week trip"
  },
  {
    "label":4,
    "text":"ok fruit problem inde best bryle",
    "cleaned_text":"ok fruit problem inde best bryle",
    "normalized_text":"ok fruit problem inde best bryle",
    "tokens":[
      "ok",
      "fruit",
      "problem",
      "ind",
      "best",
      "bryle"
    ],
    "token_count":6,
    "processed_text":"ok fruit problem ind best bryle"
  },
  {
    "label":0,
    "text":"im happi hear youll follow never receiv email twitter team conform howcom",
    "cleaned_text":"im happi hear youll follow never receiv email twitter team conform howcom",
    "normalized_text":"im happi hear youll follow never receiv email twitter team conform howcom",
    "tokens":[
      "im",
      "happi",
      "hear",
      "youll",
      "follow",
      "never",
      "receiv",
      "email",
      "twitter",
      "team",
      "conform",
      "howcom"
    ],
    "token_count":12,
    "processed_text":"im happi hear youll follow never receiv email twitter team conform howcom"
  },
  {
    "label":4,
    "text":"love hate dont care anymorethi life rule make itif hate fine love love",
    "cleaned_text":"love hate dont care anymorethi life rule make itif hate fine love love",
    "normalized_text":"love hate dont care anymorethi life rule make itif hate fine love love",
    "tokens":[
      "love",
      "hate",
      "dont",
      "care",
      "anymorethi",
      "life",
      "rule",
      "make",
      "itif",
      "hate",
      "fine",
      "love",
      "love"
    ],
    "token_count":13,
    "processed_text":"love hate dont care anymorethi life rule make itif hate fine love love"
  },
  {
    "label":0,
    "text":"go offic sad see sun almost gone",
    "cleaned_text":"go offic sad see sun almost gone",
    "normalized_text":"go offic sad see sun almost gone",
    "tokens":[
      "go",
      "offic",
      "sad",
      "see",
      "sun",
      "almost",
      "gone"
    ],
    "token_count":7,
    "processed_text":"go offic sad see sun almost gone"
  },
  {
    "label":0,
    "text":"diagnos insomnia suck",
    "cleaned_text":"diagnos insomnia suck",
    "normalized_text":"diagnos insomnia suck",
    "tokens":[
      "diagno",
      "insomnia",
      "suck"
    ],
    "token_count":3,
    "processed_text":"diagno insomnia suck"
  },
  {
    "label":4,
    "text":"ive met filipino artist anoop much much much bigger deal lol",
    "cleaned_text":"ive met filipino artist anoop much much much bigger deal lol",
    "normalized_text":"ive met filipino artist anoop much much much bigger deal lol",
    "tokens":[
      "ive",
      "met",
      "filipino",
      "artist",
      "anoop",
      "much",
      "much",
      "much",
      "bigger",
      "deal",
      "lol"
    ],
    "token_count":11,
    "processed_text":"ive met filipino artist anoop much much much bigger deal lol"
  },
  {
    "label":4,
    "text":"hope u good day",
    "cleaned_text":"hope u good day",
    "normalized_text":"hope u good day",
    "tokens":[
      "hope",
      "good",
      "day"
    ],
    "token_count":3,
    "processed_text":"hope good day"
  },
  {
    "label":4,
    "text":"go sleep",
    "cleaned_text":"go sleep",
    "normalized_text":"go sleep",
    "tokens":[
      "go",
      "sleep"
    ],
    "token_count":2,
    "processed_text":"go sleep"
  },
  {
    "label":0,
    "text":"take nap workno client till pm",
    "cleaned_text":"take nap workno client till pm",
    "normalized_text":"take nap workno client till pm",
    "tokens":[
      "take",
      "nap",
      "workno",
      "client",
      "till",
      "pm"
    ],
    "token_count":6,
    "processed_text":"take nap workno client till pm"
  },
  {
    "label":4,
    "text":"got phone one favorit ive miss ms barn",
    "cleaned_text":"got phone one favorit ive miss ms barn",
    "normalized_text":"got phone one favorit ive miss ms barn",
    "tokens":[
      "got",
      "phone",
      "one",
      "favorit",
      "ive",
      "miss",
      "ms",
      "barn"
    ],
    "token_count":8,
    "processed_text":"got phone one favorit ive miss ms barn"
  },
  {
    "label":4,
    "text":"girl that us real world call ballin budget mwah",
    "cleaned_text":"girl that us real world call ballin budget mwah",
    "normalized_text":"girl that us real world call ballin budget mwah",
    "tokens":[
      "girl",
      "us",
      "real",
      "world",
      "call",
      "ballin",
      "budget",
      "mwah"
    ],
    "token_count":8,
    "processed_text":"girl us real world call ballin budget mwah"
  },
  {
    "label":4,
    "text":"dunno leh appar tila tequila could",
    "cleaned_text":"dunno leh appar tila tequila could",
    "normalized_text":"dunno leh appar tila tequila could",
    "tokens":[
      "dunno",
      "leh",
      "appar",
      "tila",
      "tequila"
    ],
    "token_count":5,
    "processed_text":"dunno leh appar tila tequila"
  },
  {
    "label":0,
    "text":"mani rooster still chook go v differ team could good thingbut ill miss fitzyampsoliola",
    "cleaned_text":"mani rooster still chook go v differ team could good thingbut ill miss fitzyampsoliola",
    "normalized_text":"mani rooster still chook go v differ team could good thingbut ill miss fitzyampsoliola",
    "tokens":[
      "mani",
      "rooster",
      "still",
      "chook",
      "go",
      "differ",
      "team",
      "good",
      "thingbut",
      "ill",
      "miss",
      "fitzyampsoliola"
    ],
    "token_count":12,
    "processed_text":"mani rooster still chook go differ team good thingbut ill miss fitzyampsoliola"
  },
  {
    "label":4,
    "text":"scream dang gone church hand itch beat littl buttlol give glori today",
    "cleaned_text":"scream dang gone church hand itch beat littl buttlol give glori today",
    "normalized_text":"scream dang gone church hand itch beat littl buttlol give glori today",
    "tokens":[
      "scream",
      "dang",
      "gone",
      "church",
      "hand",
      "itch",
      "beat",
      "littl",
      "buttlol",
      "give",
      "glori",
      "today"
    ],
    "token_count":12,
    "processed_text":"scream dang gone church hand itch beat littl buttlol give glori today"
  },
  {
    "label":0,
    "text":"overslept work suppos two hour ago",
    "cleaned_text":"overslept work suppos two hour ago",
    "normalized_text":"overslept work suppos two hour ago",
    "tokens":[
      "overslept",
      "work",
      "suppo",
      "two",
      "hour",
      "ago"
    ],
    "token_count":6,
    "processed_text":"overslept work suppo two hour ago"
  },
  {
    "label":4,
    "text":"morn ya",
    "cleaned_text":"morn ya",
    "normalized_text":"morn ya",
    "tokens":[
      "morn",
      "ya"
    ],
    "token_count":2,
    "processed_text":"morn ya"
  },
  {
    "label":4,
    "text":"im graduat church",
    "cleaned_text":"im graduat church",
    "normalized_text":"im graduat church",
    "tokens":[
      "im",
      "graduat",
      "church"
    ],
    "token_count":3,
    "processed_text":"im graduat church"
  },
  {
    "label":0,
    "text":"love miss daddi",
    "cleaned_text":"love miss daddi",
    "normalized_text":"love miss daddi",
    "tokens":[
      "love",
      "miss",
      "daddi"
    ],
    "token_count":3,
    "processed_text":"love miss daddi"
  },
  {
    "label":0,
    "text":"ah wknd lame plu contact friend mine week yeah stuck hous parent n",
    "cleaned_text":"ah wknd lame plu contact friend mine week yeah stuck hous parent n",
    "normalized_text":"ah wknd lame plu contact friend mine week yeah stuck hous parent n",
    "tokens":[
      "ah",
      "wknd",
      "lame",
      "plu",
      "contact",
      "friend",
      "mine",
      "week",
      "yeah",
      "stuck",
      "hou",
      "parent"
    ],
    "token_count":12,
    "processed_text":"ah wknd lame plu contact friend mine week yeah stuck hou parent"
  },
  {
    "label":4,
    "text":"like prefer firefox browser extens power",
    "cleaned_text":"like prefer firefox browser extens power",
    "normalized_text":"like prefer firefox browser extens power",
    "tokens":[
      "like",
      "prefer",
      "firefox",
      "browser",
      "exten",
      "power"
    ],
    "token_count":6,
    "processed_text":"like prefer firefox browser exten power"
  },
  {
    "label":4,
    "text":"mega prayer night start expect big thing",
    "cleaned_text":"mega prayer night start expect big thing",
    "normalized_text":"mega prayer night start expect big thing",
    "tokens":[
      "mega",
      "prayer",
      "night",
      "start",
      "expect",
      "big",
      "thing"
    ],
    "token_count":7,
    "processed_text":"mega prayer night start expect big thing"
  },
  {
    "label":0,
    "text":"flip lifestyl around goodby sleep hello work",
    "cleaned_text":"flip lifestyl around goodby sleep hello work",
    "normalized_text":"flip lifestyl around goodby sleep hello work",
    "tokens":[
      "flip",
      "lifestyl",
      "around",
      "goodbi",
      "sleep",
      "hello",
      "work"
    ],
    "token_count":7,
    "processed_text":"flip lifestyl around goodbi sleep hello work"
  },
  {
    "label":0,
    "text":"go tri sleep think mild case pigbirdcow flu",
    "cleaned_text":"go tri sleep think mild case pigbirdcow flu",
    "normalized_text":"go tri sleep think mild case pigbirdcow flu",
    "tokens":[
      "go",
      "tri",
      "sleep",
      "think",
      "mild",
      "case",
      "pigbirdcow",
      "flu"
    ],
    "token_count":8,
    "processed_text":"go tri sleep think mild case pigbirdcow flu"
  },
  {
    "label":0,
    "text":"leav dad hous go mum hous cant realli bother go back caus got nice homework",
    "cleaned_text":"leav dad hous go mum hous cant realli bother go back caus got nice homework",
    "normalized_text":"leav dad hous go mum hous cant realli bother go back caus got nice homework",
    "tokens":[
      "leav",
      "dad",
      "hou",
      "go",
      "mum",
      "hou",
      "cant",
      "realli",
      "bother",
      "go",
      "back",
      "cau",
      "got",
      "nice",
      "homework"
    ],
    "token_count":15,
    "processed_text":"leav dad hou go mum hou cant realli bother go back cau got nice homework"
  },
  {
    "label":4,
    "text":"omg soooo funni cnt stop laugh",
    "cleaned_text":"omg soooo funni cnt stop laugh",
    "normalized_text":"omg soooo funni cnt stop laugh",
    "tokens":[
      "omg",
      "soooo",
      "funni",
      "cnt",
      "stop",
      "laugh"
    ],
    "token_count":6,
    "processed_text":"omg soooo funni cnt stop laugh"
  },
  {
    "label":4,
    "text":"tri use twitter",
    "cleaned_text":"tri use twitter",
    "normalized_text":"tri use twitter",
    "tokens":[
      "tri",
      "use",
      "twitter"
    ],
    "token_count":3,
    "processed_text":"tri use twitter"
  },
  {
    "label":0,
    "text":"nicebut rather eat chines anyway im full haha chesterday dravensday tuesday",
    "cleaned_text":"nicebut rather eat chines anyway im full haha chesterday dravensday tuesday",
    "normalized_text":"nicebut rather eat chines anyway im full haha chesterday dravensday tuesday",
    "tokens":[
      "nicebut",
      "rather",
      "eat",
      "chine",
      "anyway",
      "im",
      "full",
      "haha",
      "chesterday",
      "dravensday",
      "tuesday"
    ],
    "token_count":11,
    "processed_text":"nicebut rather eat chine anyway im full haha chesterday dravensday tuesday"
  },
  {
    "label":0,
    "text":"check realiz dont get mani repli ask question",
    "cleaned_text":"check realiz dont get mani repli ask question",
    "normalized_text":"check realiz dont get mani repli ask question",
    "tokens":[
      "check",
      "realiz",
      "dont",
      "get",
      "mani",
      "repli",
      "ask",
      "question"
    ],
    "token_count":8,
    "processed_text":"check realiz dont get mani repli ask question"
  },
  {
    "label":0,
    "text":"noooo",
    "cleaned_text":"noooo",
    "normalized_text":"noooo",
    "tokens":[
      "noooo"
    ],
    "token_count":1,
    "processed_text":"noooo"
  },
  {
    "label":0,
    "text":"happi birthday leia sorri cant better get ass back",
    "cleaned_text":"happi birthday leia sorri cant better get ass back",
    "normalized_text":"happi birthday leia sorri cant better get ass back",
    "tokens":[
      "happi",
      "birthday",
      "leia",
      "sorri",
      "cant",
      "better",
      "get",
      "ass",
      "back"
    ],
    "token_count":9,
    "processed_text":"happi birthday leia sorri cant better get ass back"
  },
  {
    "label":4,
    "text":"ah andi think would quotshrulkquot",
    "cleaned_text":"ah andi think would quotshrulkquot",
    "normalized_text":"ah andi think would quotshrulkquot",
    "tokens":[
      "ah",
      "andi",
      "think",
      "quotshrulkquot"
    ],
    "token_count":4,
    "processed_text":"ah andi think quotshrulkquot"
  },
  {
    "label":4,
    "text":"sound like your realli enjoy coachella vega readi parti return",
    "cleaned_text":"sound like your realli enjoy coachella vega readi parti return",
    "normalized_text":"sound like your realli enjoy coachella vega readi parti return",
    "tokens":[
      "sound",
      "like",
      "realli",
      "enjoy",
      "coachella",
      "vega",
      "readi",
      "parti",
      "return"
    ],
    "token_count":9,
    "processed_text":"sound like realli enjoy coachella vega readi parti return"
  },
  {
    "label":0,
    "text":"bought copi sun feel dirti",
    "cleaned_text":"bought copi sun feel dirti",
    "normalized_text":"bought copi sun feel dirti",
    "tokens":[
      "bought",
      "copi",
      "sun",
      "feel",
      "dirti"
    ],
    "token_count":5,
    "processed_text":"bought copi sun feel dirti"
  },
  {
    "label":0,
    "text":"get well miss alreadi manila",
    "cleaned_text":"get well miss alreadi manila",
    "normalized_text":"get well miss alreadi manila",
    "tokens":[
      "get",
      "well",
      "miss",
      "alreadi",
      "manila"
    ],
    "token_count":5,
    "processed_text":"get well miss alreadi manila"
  },
  {
    "label":0,
    "text":"much hw begin",
    "cleaned_text":"much hw begin",
    "normalized_text":"much hw begin",
    "tokens":[
      "much",
      "hw",
      "begin"
    ],
    "token_count":3,
    "processed_text":"much hw begin"
  },
  {
    "label":0,
    "text":"awe hope get might sell mine prob wont happen whatev haha",
    "cleaned_text":"awe hope get might sell mine prob wont happen whatev haha",
    "normalized_text":"awe hope get might sell mine prob wont happen whatev haha",
    "tokens":[
      "awe",
      "hope",
      "get",
      "sell",
      "mine",
      "prob",
      "wont",
      "happen",
      "whatev",
      "haha"
    ],
    "token_count":10,
    "processed_text":"awe hope get sell mine prob wont happen whatev haha"
  },
  {
    "label":0,
    "text":"go hikari san hous yay get hang luca taiko junna oh wea si",
    "cleaned_text":"go hikari san hous yay get hang luca taiko junna oh wea si",
    "normalized_text":"go hikari san hous yay get hang luca taiko junna oh wea si",
    "tokens":[
      "go",
      "hikari",
      "san",
      "hou",
      "yay",
      "get",
      "hang",
      "luca",
      "taiko",
      "junna",
      "oh",
      "wea",
      "si"
    ],
    "token_count":13,
    "processed_text":"go hikari san hou yay get hang luca taiko junna oh wea si"
  },
  {
    "label":0,
    "text":"style warrior come th ye uk behind mac releas",
    "cleaned_text":"style warrior come th ye uk behind mac releas",
    "normalized_text":"style warrior come th ye uk behind mac releas",
    "tokens":[
      "style",
      "warrior",
      "come",
      "th",
      "ye",
      "uk",
      "behind",
      "mac",
      "relea"
    ],
    "token_count":9,
    "processed_text":"style warrior come th ye uk behind mac relea"
  },
  {
    "label":0,
    "text":"someth afooti feel depth soul go awaychang come must blood shed",
    "cleaned_text":"someth afooti feel depth soul go awaychang come must blood shed",
    "normalized_text":"someth afooti feel depth soul go awaychang come must blood shed",
    "tokens":[
      "someth",
      "afooti",
      "feel",
      "depth",
      "soul",
      "go",
      "awaychang",
      "come",
      "blood",
      "shed"
    ],
    "token_count":10,
    "processed_text":"someth afooti feel depth soul go awaychang come blood shed"
  },
  {
    "label":0,
    "text":"unfollow bunch peopl includ oh well",
    "cleaned_text":"unfollow bunch peopl includ oh well",
    "normalized_text":"unfollow bunch peopl includ oh well",
    "tokens":[
      "unfollow",
      "bunch",
      "peopl",
      "includ",
      "oh",
      "well"
    ],
    "token_count":6,
    "processed_text":"unfollow bunch peopl includ oh well"
  },
  {
    "label":0,
    "text":"wish boy werent dumb",
    "cleaned_text":"wish boy werent dumb",
    "normalized_text":"wish boy werent dumb",
    "tokens":[
      "wish",
      "boy",
      "werent",
      "dumb"
    ],
    "token_count":4,
    "processed_text":"wish boy werent dumb"
  },
  {
    "label":0,
    "text":"talk like im obes ew ugh",
    "cleaned_text":"talk like im obes ew ugh",
    "normalized_text":"talk like im obes ew ugh",
    "tokens":[
      "talk",
      "like",
      "im",
      "obe",
      "ew",
      "ugh"
    ],
    "token_count":6,
    "processed_text":"talk like im obe ew ugh"
  },
  {
    "label":4,
    "text":"yea nice",
    "cleaned_text":"yea nice",
    "normalized_text":"yea nice",
    "tokens":[
      "yea",
      "nice"
    ],
    "token_count":2,
    "processed_text":"yea nice"
  },
  {
    "label":4,
    "text":"bing follow open pictur pc first within editor resiz save upload twitter",
    "cleaned_text":"bing follow open pictur pc first within editor resiz save upload twitter",
    "normalized_text":"bing follow open pictur pc first within editor resiz save upload twitter",
    "tokens":[
      "bing",
      "follow",
      "open",
      "pictur",
      "pc",
      "first",
      "within",
      "editor",
      "resiz",
      "save",
      "upload",
      "twitter"
    ],
    "token_count":12,
    "processed_text":"bing follow open pictur pc first within editor resiz save upload twitter"
  },
  {
    "label":0,
    "text":"watch grandma boy amp check emailsgonna start write letter fianc miss much",
    "cleaned_text":"watch grandma boy amp check emailsgonna start write letter fianc miss much",
    "normalized_text":"watch grandma boy amp check emailsgonna start write letter fianc miss much",
    "tokens":[
      "watch",
      "grandma",
      "boy",
      "amp",
      "check",
      "emailsgonna",
      "start",
      "write",
      "letter",
      "fianc",
      "miss",
      "much"
    ],
    "token_count":12,
    "processed_text":"watch grandma boy amp check emailsgonna start write letter fianc miss much"
  },
  {
    "label":4,
    "text":"snow sent arctic cant see",
    "cleaned_text":"snow sent arctic cant see",
    "normalized_text":"snow sent arctic cant see",
    "tokens":[
      "snow",
      "sent",
      "arctic",
      "cant",
      "see"
    ],
    "token_count":5,
    "processed_text":"snow sent arctic cant see"
  },
  {
    "label":0,
    "text":"dont doubt soo what",
    "cleaned_text":"dont doubt soo what",
    "normalized_text":"dont doubt soo what",
    "tokens":[
      "dont",
      "doubt",
      "soo"
    ],
    "token_count":3,
    "processed_text":"dont doubt soo"
  },
  {
    "label":0,
    "text":"rocket fail",
    "cleaned_text":"rocket fail",
    "normalized_text":"rocket fail",
    "tokens":[
      "rocket",
      "fail"
    ],
    "token_count":2,
    "processed_text":"rocket fail"
  }
]