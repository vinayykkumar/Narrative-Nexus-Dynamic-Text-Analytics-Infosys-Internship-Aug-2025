{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07b30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a41b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Nagababu/Downloads/spamdata.csv.csv\", encoding=\"latin-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5899d8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                               text  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...   \n",
      "1      ham                      Ok lar... Joking wif u oni...   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3      ham  U dun say so early hor... U c already then say...   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "...    ...                                                ...   \n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
      "5568   ham              Will Ì_ b going to esplanade fr home?   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...   \n",
      "5571   ham                         Rofl. Its true to its name   \n",
      "\n",
      "                                             clean_text  \\\n",
      "0     go until jurong point crazy available only in ...   \n",
      "1                               ok lar joking wif u oni   \n",
      "2     free entry in a wkly comp to win fa cup final ...   \n",
      "3           u dun say so early hor u c already then say   \n",
      "4     nah i dont think he goes to usf he lives aroun...   \n",
      "...                                                 ...   \n",
      "5567  this is the nd time we have tried contact u u ...   \n",
      "5568               will ì_ b going to esplanade fr home   \n",
      "5569  pity was in mood for that soany other suggestions   \n",
      "5570  the guy did some bitching but i acted like id ...   \n",
      "5571                          rofl its true to its name   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [go, jurong, point, crazy, available, bugis, n...   \n",
      "1                        [ok, lar, joking, wif, u, oni]   \n",
      "2     [free, entry, wkly, comp, win, fa, cup, final,...   \n",
      "3         [u, dun, say, early, hor, u, c, already, say]   \n",
      "4     [nah, dont, think, goes, usf, lives, around, t...   \n",
      "...                                                 ...   \n",
      "5567  [nd, time, tried, contact, u, u, å, pound, pri...   \n",
      "5568                [ì_, b, going, esplanade, fr, home]   \n",
      "5569                   [pity, mood, soany, suggestions]   \n",
      "5570  [guy, bitching, acted, like, id, interested, b...   \n",
      "5571                                 [rofl, true, name]   \n",
      "\n",
      "                                                stemmed  \\\n",
      "0     [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
      "1                          [ok, lar, joke, wif, u, oni]   \n",
      "2     [free, entri, wkli, comp, win, fa, cup, final,...   \n",
      "3         [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
      "4     [nah, dont, think, goe, usf, live, around, tho...   \n",
      "...                                                 ...   \n",
      "5567  [nd, time, tri, contact, u, u, å, pound, prize...   \n",
      "5568                    [ì_, b, go, esplanad, fr, home]   \n",
      "5569                       [piti, mood, soani, suggest]   \n",
      "5570  [guy, bitch, act, like, id, interest, buy, som...   \n",
      "5571                                 [rofl, true, name]   \n",
      "\n",
      "                                             lemmatized  \\\n",
      "0     [go, jurong, point, crazy, available, bugis, n...   \n",
      "1                        [ok, lar, joking, wif, u, oni]   \n",
      "2     [free, entry, wkly, comp, win, fa, cup, final,...   \n",
      "3         [u, dun, say, early, hor, u, c, already, say]   \n",
      "4     [nah, dont, think, go, usf, life, around, though]   \n",
      "...                                                 ...   \n",
      "5567  [nd, time, tried, contact, u, u, å, pound, pri...   \n",
      "5568                [ì_, b, going, esplanade, fr, home]   \n",
      "5569                    [pity, mood, soany, suggestion]   \n",
      "5570  [guy, bitching, acted, like, id, interested, b...   \n",
      "5571                                 [rofl, true, name]   \n",
      "\n",
      "                                             final_text  \\\n",
      "0     go jurong point crazy available bugis n great ...   \n",
      "1                               ok lar joking wif u oni   \n",
      "2     free entry wkly comp win fa cup final tkts st ...   \n",
      "3                   u dun say early hor u c already say   \n",
      "4              nah dont think go usf life around though   \n",
      "...                                                 ...   \n",
      "5567  nd time tried contact u u å pound prize claim ...   \n",
      "5568                       ì_ b going esplanade fr home   \n",
      "5569                         pity mood soany suggestion   \n",
      "5570  guy bitching acted like id interested buying s...   \n",
      "5571                                     rofl true name   \n",
      "\n",
      "                                              stopwords  \n",
      "0     [go, jurong, point, crazy, available, bugis, n...  \n",
      "1                        [ok, lar, joking, wif, u, oni]  \n",
      "2     [free, entry, wkly, comp, win, fa, cup, final,...  \n",
      "3         [u, dun, say, early, hor, u, c, already, say]  \n",
      "4     [nah, dont, think, goes, usf, lives, around, t...  \n",
      "...                                                 ...  \n",
      "5567  [nd, time, tried, contact, u, u, å, pound, pri...  \n",
      "5568                [ì_, b, going, esplanade, fr, home]  \n",
      "5569                   [pity, mood, soany, suggestions]  \n",
      "5570  [guy, bitching, acted, like, id, interested, b...  \n",
      "5571                                 [rofl, true, name]  \n",
      "\n",
      "[5572 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Keep only useful columns\n",
    "# Renaming columns\n",
    "df = df.rename(columns={'v1':'label', 'v2':'text'})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b82ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                               text  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...   \n",
      "1      ham                      Ok lar... Joking wif u oni...   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3      ham  U dun say so early hor... U c already then say...   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "...    ...                                                ...   \n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
      "5568   ham              Will Ì_ b going to esplanade fr home?   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...   \n",
      "5571   ham                         Rofl. Its true to its name   \n",
      "\n",
      "                                             clean_text  \\\n",
      "0     go until jurong point crazy available only in ...   \n",
      "1                               ok lar joking wif u oni   \n",
      "2     free entry in a wkly comp to win fa cup final ...   \n",
      "3           u dun say so early hor u c already then say   \n",
      "4     nah i dont think he goes to usf he lives aroun...   \n",
      "...                                                 ...   \n",
      "5567  this is the nd time we have tried contact u u ...   \n",
      "5568               will ì_ b going to esplanade fr home   \n",
      "5569  pity was in mood for that soany other suggestions   \n",
      "5570  the guy did some bitching but i acted like id ...   \n",
      "5571                          rofl its true to its name   \n",
      "\n",
      "                                                 tokens  \\\n",
      "0     [go, jurong, point, crazy, available, bugis, n...   \n",
      "1                        [ok, lar, joking, wif, u, oni]   \n",
      "2     [free, entry, wkly, comp, win, fa, cup, final,...   \n",
      "3         [u, dun, say, early, hor, u, c, already, say]   \n",
      "4     [nah, dont, think, goes, usf, lives, around, t...   \n",
      "...                                                 ...   \n",
      "5567  [nd, time, tried, contact, u, u, å, pound, pri...   \n",
      "5568                [ì_, b, going, esplanade, fr, home]   \n",
      "5569                   [pity, mood, soany, suggestions]   \n",
      "5570  [guy, bitching, acted, like, id, interested, b...   \n",
      "5571                                 [rofl, true, name]   \n",
      "\n",
      "                                                stemmed  \\\n",
      "0     [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
      "1                          [ok, lar, joke, wif, u, oni]   \n",
      "2     [free, entri, wkli, comp, win, fa, cup, final,...   \n",
      "3         [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
      "4     [nah, dont, think, goe, usf, live, around, tho...   \n",
      "...                                                 ...   \n",
      "5567  [nd, time, tri, contact, u, u, å, pound, prize...   \n",
      "5568                    [ì_, b, go, esplanad, fr, home]   \n",
      "5569                       [piti, mood, soani, suggest]   \n",
      "5570  [guy, bitch, act, like, id, interest, buy, som...   \n",
      "5571                                 [rofl, true, name]   \n",
      "\n",
      "                                             lemmatized  \\\n",
      "0     [go, jurong, point, crazy, available, bugis, n...   \n",
      "1                        [ok, lar, joking, wif, u, oni]   \n",
      "2     [free, entry, wkly, comp, win, fa, cup, final,...   \n",
      "3         [u, dun, say, early, hor, u, c, already, say]   \n",
      "4     [nah, dont, think, go, usf, life, around, though]   \n",
      "...                                                 ...   \n",
      "5567  [nd, time, tried, contact, u, u, å, pound, pri...   \n",
      "5568                [ì_, b, going, esplanade, fr, home]   \n",
      "5569                    [pity, mood, soany, suggestion]   \n",
      "5570  [guy, bitching, acted, like, id, interested, b...   \n",
      "5571                                 [rofl, true, name]   \n",
      "\n",
      "                                             final_text  \\\n",
      "0     go jurong point crazy available bugis n great ...   \n",
      "1                               ok lar joking wif u oni   \n",
      "2     free entry wkly comp win fa cup final tkts st ...   \n",
      "3                   u dun say early hor u c already say   \n",
      "4              nah dont think go usf life around though   \n",
      "...                                                 ...   \n",
      "5567  nd time tried contact u u å pound prize claim ...   \n",
      "5568                       ì_ b going esplanade fr home   \n",
      "5569                         pity mood soany suggestion   \n",
      "5570  guy bitching acted like id interested buying s...   \n",
      "5571                                     rofl true name   \n",
      "\n",
      "                                              stopwords  \n",
      "0     [go, jurong, point, crazy, available, bugis, n...  \n",
      "1                        [ok, lar, joking, wif, u, oni]  \n",
      "2     [free, entry, wkly, comp, win, fa, cup, final,...  \n",
      "3         [u, dun, say, early, hor, u, c, already, say]  \n",
      "4     [nah, dont, think, goes, usf, lives, around, t...  \n",
      "...                                                 ...  \n",
      "5567  [nd, time, tried, contact, u, u, å, pound, pri...  \n",
      "5568                [ì_, b, going, esplanade, fr, home]  \n",
      "5569                   [pity, mood, soany, suggestions]  \n",
      "5570  [guy, bitching, acted, like, id, interested, b...  \n",
      "5571                                 [rofl, true, name]  \n",
      "\n",
      "[5572 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2. Handle Missing Values\n",
    "# -------------------------------\n",
    "df.dropna(inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "587e277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go until jurong point crazy available only in ...\n",
      "1                                 ok lar joking wif u oni\n",
      "2       free entry in a wkly comp to win fa cup final ...\n",
      "3             u dun say so early hor u c already then say\n",
      "4       nah i dont think he goes to usf he lives aroun...\n",
      "                              ...                        \n",
      "5567    this is the nd time we have tried contact u u ...\n",
      "5568                 will ì_ b going to esplanade fr home\n",
      "5569    pity was in mood for that soany other suggestions\n",
      "5570    the guy did some bitching but i acted like id ...\n",
      "5571                            rofl its true to its name\n",
      "Name: clean_text, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3. Text Cleaning Function\n",
    "# -------------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercasing\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "print(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed51788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [go, until, jurong, point, crazy, available, o...\n",
      "1                          [ok, lar, joking, wif, u, oni]\n",
      "2       [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
      "3       [u, dun, say, so, early, hor, u, c, already, t...\n",
      "4       [nah, i, dont, think, he, goes, to, usf, he, l...\n",
      "                              ...                        \n",
      "5567    [this, is, the, nd, time, we, have, tried, con...\n",
      "5568        [will, ì_, b, going, to, esplanade, fr, home]\n",
      "5569    [pity, was, in, mood, for, that, soany, other,...\n",
      "5570    [the, guy, did, some, bitching, but, i, acted,...\n",
      "5571                     [rofl, its, true, to, its, name]\n",
      "Name: tokens, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 4. Tokenization\n",
    "# -------------------------------\n",
    "df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ce1294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df():\n",
    "\n",
    "# 4. Tokenization\n",
    "     df['tokens'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "# 5. Initialize Stemmer\n",
    "     stemmer = PorterStemmer()\n",
    "\n",
    "# 6. Apply Stemming\n",
    "     df['stemmed'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# 7. Print result\n",
    "     print(df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1eab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Go, until, jurong, point, ,, crazy, .., Avail...\n",
      "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
      "2       [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3       [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
      "4       [Nah, I, do, n't, think, he, go, to, usf, ,, h...\n",
      "                              ...                        \n",
      "5567    [This, is, the, 2nd, time, we, have, tried, 2,...\n",
      "5568     [Will, Ì_, b, going, to, esplanade, fr, home, ?]\n",
      "5569    [Pity, ,, *, wa, in, mood, for, that, ., So, ....\n",
      "5570    [The, guy, did, some, bitching, but, I, acted,...\n",
      "5571                   [Rofl, ., Its, true, to, it, name]\n",
      "Name: lemmatized, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 7. Lemmatization\n",
    "# -------------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(df['lemmatized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fad5e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Go until jurong point , crazy .. Available onl...\n",
      "1                         Ok lar ... Joking wif u oni ...\n",
      "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3       U dun say so early hor ... U c already then sa...\n",
      "4       Nah I do n't think he go to usf , he life arou...\n",
      "                              ...                        \n",
      "5567    This is the 2nd time we have tried 2 contact u...\n",
      "5568               Will Ì_ b going to esplanade fr home ?\n",
      "5569    Pity , * wa in mood for that . So ... any othe...\n",
      "5570    The guy did some bitching but I acted like i '...\n",
      "5571                           Rofl . Its true to it name\n",
      "Name: final_text, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 8. Join Tokens Back to Text\n",
    "# -------------------------------\n",
    "df['final_text'] = df['lemmatized'].apply(lambda x: ' '.join(x))\n",
    "print(df['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef681a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 9. Bag of Words & TF-IDF\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Bag of Words\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cv = \u001b[43mCountVectorizer\u001b[49m()\n\u001b[32m      6\u001b[39m X_bow = cv.fit_transform(df[\u001b[33m'\u001b[39m\u001b[33mfinal_text\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# TF-IDF\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 9. Bag of Words & TF-IDF\n",
    "# -------------------------------\n",
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_bow = cv.fit_transform(df['final_text'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['final_text'])\n",
    "\n",
    "# -------------------------------\n",
    "# Final Output\n",
    "# -------------------------------\n",
    "print(\"Shape of Bag of Words Matrix:\", X_bow.shape)\n",
    "print(\"Shape of TF-IDF Matrix:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c5c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
      "\n",
      "                                          lemmatized  \n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...  \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]  \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...  \n",
      "4  [Nah, I, do, n't, think, he, go, to, usf, ,, h...  \n",
      "Example tokenized message: ['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization on 'message' column\n",
    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Show first 5 rows\n",
    "print(df.head())\n",
    "\n",
    "# Example tokenized message\n",
    "print(\"Example tokenized message:\", df['tokens'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58054a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nagababu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nagababu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nagababu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                    processed_tokens  \n",
      "0  [go, jurong, point, crazi, avail, bugi, n, gre...  \n",
      "1                       [ok, lar, joke, wif, u, oni]  \n",
      "2  [free, entri, wkli, comp, win, fa, cup, final,...  \n",
      "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
      "4  [nah, dont, think, goe, usf, live, around, tho...  \n",
      "Example processed tokens: ['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required resources (only first time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"spamdata.csv.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Keep only the important columns (v1 = label, v2 = message)\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']  # rename columns\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove unwanted characters (URLs, numbers, punctuation, special chars)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # URLs\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)  # HTML tags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # keep only alphabets\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_tokens'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Show results\n",
    "print(df.head())\n",
    "\n",
    "# Example: first processed message\n",
    "print(\"Example processed tokens:\", df['processed_tokens'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3b1b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words shape: (5572, 6963)\n",
      "TF-IDF shape: (5572, 6963)\n",
      "Sample BoW features: ['aa' 'aah' 'aaniy' 'aaooooright' 'aathilov' 'aathiwher' 'ab' 'abbey'\n",
      " 'abdomen' 'abeg' 'abel' 'aberdeen' 'abi' 'abil' 'abiola' 'abj' 'abl'\n",
      " 'abnorm' 'abouta' 'abroad']\n",
      "Sample TF-IDF features: ['aa' 'aah' 'aaniy' 'aaooooright' 'aathilov' 'aathiwher' 'ab' 'abbey'\n",
      " 'abdomen' 'abeg' 'abel' 'aberdeen' 'abi' 'abil' 'abiola' 'abj' 'abl'\n",
      " 'abnorm' 'abouta' 'abroad']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove unwanted characters (URLs, HTML tags, numbers, punctuation)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # URLs\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)  # HTML tags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # keep only alphabets\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "    \n",
    "    return \" \".join(lemmatized)  # return string (needed for vectorizers)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['clean_text'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# -----------------------------\n",
    "# Bag of Words (Count Vectorizer)\n",
    "# -----------------------------\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_bow = count_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# -----------------------------\n",
    "# TF-IDF Vectorization\n",
    "# -----------------------------\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Show shapes of matrices\n",
    "print(\"Bag of Words shape:\", X_bow.shape)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n",
    "# Example features\n",
    "print(\"Sample BoW features:\", count_vectorizer.get_feature_names_out()[:20])\n",
    "print(\"Sample TF-IDF features:\", tfidf_vectorizer.get_feature_names_out()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f98b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                    processed_tokens  \\\n",
      "0  [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
      "1                       [ok, lar, joke, wif, u, oni]   \n",
      "2  [free, entri, wkli, comp, win, fa, cup, final,...   \n",
      "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
      "4  [nah, dont, think, goe, usf, live, around, tho...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...  \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]  \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...  \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...  \n",
      "Example tokenized message: ['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization on 'message' column\n",
    "df['tokens'] = df['message'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Show first 5 rows\n",
    "print(df.head())\n",
    "\n",
    "# Example tokenized message\n",
    "print(\"Example tokenized message:\", df['tokens'][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
